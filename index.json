[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R","Machine Learning"],"content":" Preamble: This document focuses on the analysis of the diamonds data frame.\nDescriotion of data frame diamonds can be found at https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html\n Research questions: am i getting a fair deal when I purchase a diamond?\n The goal is to build a predictive model for diamonds, that is going to help figure out whether a given diamond is a good deal or a rip-off!\n   Structure of analysis: I will use Linear Regression to predict the diamond price using other varaibles in the diamonds dataframe.\n Get to know the Data str(diamonds) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels \u0026quot;Fair\u0026quot;\u0026lt;\u0026quot;Good\u0026quot;\u0026lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels \u0026quot;D\u0026quot;\u0026lt;\u0026quot;E\u0026quot;\u0026lt;\u0026quot;F\u0026quot;\u0026lt;\u0026quot;G\u0026quot;\u0026lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels \u0026quot;I1\u0026quot;\u0026lt;\u0026quot;SI2\u0026quot;\u0026lt;\u0026quot;SI1\u0026quot;\u0026lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... summary(diamonds) ## carat cut color clarity ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 ## Max. :5.0100 I: 5422 VVS1 : 3655 ## J: 2808 (Other): 2531 ## depth table price x ## Min. :43.00 Min. :43.00 Min. : 326 Min. : 0.000 ## 1st Qu.:61.00 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 ## Median :61.80 Median :57.00 Median : 2401 Median : 5.700 ## Mean :61.75 Mean :57.46 Mean : 3933 Mean : 5.731 ## 3rd Qu.:62.50 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 ## Max. :79.00 Max. :95.00 Max. :18823 Max. :10.740 ## ## y z ## Min. : 0.000 Min. : 0.000 ## 1st Qu.: 4.720 1st Qu.: 2.910 ## Median : 5.710 Median : 3.530 ## Mean : 5.735 Mean : 3.539 ## 3rd Qu.: 6.540 3rd Qu.: 4.040 ## Max. :58.900 Max. :31.800 ##   Scatterplot We’ll start by examining two variables in the set. A scatterplot is a powerful tool to help you understand the relationship between two continuous variables.\nWe can quickly see if the relationship is linear or not. In this case, we can use a variety of diamond characteristics to help us figure out whether the price advertised for any given diamond is reasonable or a rip-off.\nConsider the price of a diamond and it’s carat weight.\n## create a scatterplot of price and carat ggplot(diamonds, aes(carat, price)) + geom_point(fill = I(\u0026quot;#F79420\u0026quot;), color = I(\u0026quot;black\u0026quot;), shape = 23) + xlim(0, quantile(diamonds$carat,0.99)) + ylim(0, quantile(diamonds$price,0.99)) + ggtitle(\u0026#39;Price vs. Carat\u0026#39;) + theme_classic() ## Warning: Removed 926 rows containing missing values (geom_point). Observations:\n The larger the diamond is (or the more carats it has), the more expensive the diamond is (price), which is probably what we would have expected.  ## create a scatterplot of price and carat with linear trend ggplot(diamonds, aes(carat, price)) + geom_point(fill = I(\u0026quot;#F79420\u0026quot;), color = I(\u0026quot;black\u0026quot;), shape = 23) + stat_smooth(method = \u0026quot;lm\u0026quot;) + scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99)) ) + scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99)) ) + ggtitle(\u0026quot;Price vs. Carat\u0026quot;) + theme_classic() ## Warning: Removed 926 rows containing non-finite values (stat_smooth). ## Warning: Removed 926 rows containing missing values (geom_point). ## Warning: Removed 4 rows containing missing values (geom_smooth). Observations:\n The linear trend line doesn’t go through the center of the data at some key places. It should curve in certain parts of the graph, i.e slope up more towards the end. If we tried to use this for predictions, we might be off some key places inside and outside of the existing data that we have displayed.  ## sample 10,000 diamonds from the set to get a snapshop of the large dataframe set.seed(20022012) diamond_samp \u0026lt;- diamonds[sample(1:length(diamonds$price), 10000), ] ggpairs(diamond_samp, lower = list(continuous = wrap(\u0026quot;points\u0026quot;, shape = I(\u0026#39;.\u0026#39;))), upper = list(combo = wrap(\u0026quot;box\u0026quot;, outlier.shape = I(\u0026#39;.\u0026#39;)))) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Observations:\n Price is almost linearly correlated with carat: These are the critical factors driving price. Price appears related to cut/color/clarity but, is not very clear from this plot. Price appears not to be directly related to depth and table.  ## create hist of price and price(log10) plot1 \u0026lt;- ggplot(diamonds, aes(price)) + geom_histogram(color = \u0026#39;blue\u0026#39;, fill = \u0026#39;blue\u0026#39;, binwidth = 200) + scale_x_continuous(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) + ggtitle(\u0026#39;Price\u0026#39;) + theme_classic() plot2 \u0026lt;- ggplot(diamonds, aes(price)) + geom_histogram(color = \u0026#39;red\u0026#39;, fill = \u0026#39;red\u0026#39;, binwidth = 0.01) + scale_x_log10(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) + ggtitle(\u0026#39;Price(log10)\u0026#39;) + theme_classic() grid.arrange(plot1, plot2, ncol = 2) ## Warning: Removed 1 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_bar). Observations:\n Price histogram is skewed to the right, while the log10(price) tends to be a bell curve distributed. Also, the two peaks in the log10(price) plot coincides with the 1st and 3rd quantile of price.  ## create scatterplot of price and price(log10) p1 \u0026lt;- ggplot(diamonds, aes(carat, price, color=clarity)) + geom_point() + ggtitle(\u0026quot;Price by Carat\u0026quot;) + theme_classic() p2 \u0026lt;- ggplot(diamonds, aes(carat, price, color=clarity)) + geom_point() + scale_y_continuous(trans = log10_trans()) + ggtitle(\u0026quot;Price(log10) by Carat\u0026quot;) + theme_classic() grid.arrange(p1, p2, ncol=1) Observations:\n On the log scale, the prices look less dispersed at the high end of carat size and price, however, we can do better. Let’s try using the cube root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. Remember, volume is on a cubic scale!  ### create a new function to transform the carat variable cuberoot_trans = function() trans_new(\u0026#39;cuberoot\u0026#39;, transform = function(x) x^(1/3), inverse = function(x) x^3) ### use the cuberoot_trans function ggplot(diamonds, aes(carat, price, color=clarity)) + geom_point(alpha = 1/2, size = 1, position = \u0026quot;jitter\u0026quot;) + scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3), breaks = c(0.2, 0.5, 1, 2, 3)) + scale_y_continuous(trans = log10_trans(), limits = c(350, 15000), breaks = c(350, 1000, 5000, 10000, 15000)) + ggtitle(\u0026#39;Price(log10) by Cube-Root of Carat\u0026#39;) + theme_classic() ## Warning: Removed 1691 rows containing missing values (geom_point). Observations:\n The price(log10) is almost linear with cuberoot of carat. We can now move ahead and see how to model our data using just a linear model.  Price vs. Carat and Clarity ## to work around overplotting, the alpha, size, and and jitter options are used in our plot ggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha = 1/2, size = 1, position = \u0026#39;jitter\u0026#39;, aes(color=clarity)) + scale_color_brewer(type = \u0026#39;div\u0026#39;, guide = guide_legend(title = \u0026#39;Clarity\u0026#39;, reverse = T, override.aes = list(alpha = 1, size = 2))) + scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3), breaks = c(0.2, 0.5, 1, 2, 3)) + scale_y_continuous(trans = log10_trans(), limits = c(350, 15000), breaks = c(350, 1000, 5000, 10000, 15000)) + ggtitle(\u0026#39;Price(log10) by Cube-Root of Carat and Clarity\u0026#39;) + theme_classic() ## Warning: Removed 1693 rows containing missing values (geom_point). Observations:\n Clarity factors into the price of a diamond. Hence, a better clarity results in a higher price than lower end clarity.   Price vs. Carat and Cut ## to work around overplotting, the alpha, size, and and jitter options are used in our plot ggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha = 1/2, size = 1, position = \u0026#39;jitter\u0026#39;, aes(color=cut)) + scale_color_brewer(type = \u0026#39;div\u0026#39;, guide = guide_legend(title = \u0026#39;Cut\u0026#39;, reverse = T, override.aes = list(alpha = 1, size = 2))) + scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3), breaks = c(0.2, 0.5, 1, 2, 3)) + scale_y_continuous(trans = log10_trans(), limits = c(350, 15000), breaks = c(350, 1000, 5000, 10000, 15000)) + ggtitle(\u0026#39;Price(log10) by Cube-Root of Carat and Cut\u0026#39;) + theme_classic() ## Warning: Removed 1696 rows containing missing values (geom_point). Observations:\n Whilst cut does not show as obvious pattern as clarity, it’s still clear that with the same carat the diamonds with the best cut are priced higher. Hence, I think cut should be also included in the price prediction algorithm. Note, clarity explains a lot of the variance found in price!   Price vs. Carat and Color ## to work around overplotting, the alpha, size, and and jitter options are used in our plot ggplot(diamonds, aes(x = carat, y = price)) + geom_point(alpha = 1/2, size = 1, position = \u0026#39;jitter\u0026#39;, aes(color=color)) + scale_color_brewer(type = \u0026#39;div\u0026#39;, guide = guide_legend(title = \u0026#39;Color\u0026#39;, reverse = F, override.aes = list(alpha = 1, size = 2))) + scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3), breaks = c(0.2, 0.5, 1, 2, 3)) + scale_y_continuous(trans = log10_trans(), limits = c(350, 15000), breaks = c(350, 1000, 5000, 10000, 15000)) + ggtitle(\u0026#39;Price(log10) by Cube-Root of Carat and Color\u0026#39;) + theme_classic() ## Warning: Removed 1688 rows containing missing values (geom_point). ggplot(diamonds) + geom_bar(mapping = aes(clarity, fill=cut), position = \u0026quot;fill\u0026quot; ) + scale_fill_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;orange\u0026quot;, \u0026quot;darkgreen\u0026quot;, \u0026quot;dodgerblue\u0026quot;, \u0026quot;purple4\u0026quot;)) + labs(title = \u0026quot;Clearer diamonds tend to be of higher quality cut\u0026quot;, subtitle = \u0026quot;The majority of IF diamonds are an \\\u0026quot;Ideal\\\u0026quot; cut\u0026quot;) + ylab(\u0026quot;proportion\u0026quot;) + theme_classic() Observations:\n This looks similar with previous clarity plot. Color should be also considered as an factor for price.   Build the Linear Model m1 \u0026lt;- lm(I(log10(price)) ~ I(carat^(1/3)), diamonds) m2 \u0026lt;- update(m1,~ . +carat) m3 \u0026lt;- update(m2,~ . +cut) m4 \u0026lt;- update(m3,~ . +color) m5 \u0026lt;- update(m4,~ . +clarity) mtable(m1, m2, m3, m4, m5, sdigits = 4) ## ## Calls: ## m1: lm(formula = I(log10(price)) ~ I(carat^(1/3)), data = diamonds) ## m2: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat, data = diamonds) ## m3: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut, ## data = diamonds) ## m4: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + ## color, data = diamonds) ## m5: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + ## color + clarity, data = diamonds) ## ## ============================================================================================== ## m1 m2 m3 m4 m5 ## ---------------------------------------------------------------------------------------------- ## (Intercept) 1.225*** 0.451*** 0.380*** 0.405*** 0.180*** ## (0.003) (0.008) (0.008) (0.007) (0.004) ## I(carat^(1/3)) 2.414*** 3.721*** 3.780*** 3.665*** 3.971*** ## (0.003) (0.014) (0.013) (0.012) (0.007) ## carat -0.494*** -0.505*** -0.431*** -0.474*** ## (0.005) (0.005) (0.004) (0.003) ## cut: .L 0.097*** 0.097*** 0.052*** ## (0.002) (0.002) (0.001) ## cut: .Q -0.027*** -0.027*** -0.013*** ## (0.002) (0.001) (0.001) ## cut: .C 0.022*** 0.022*** 0.006*** ## (0.001) (0.001) (0.001) ## cut: ^4 0.008*** 0.008*** -0.001 ## (0.001) (0.001) (0.001) ## color: .L -0.162*** -0.191*** ## (0.001) (0.001) ## color: .Q -0.056*** -0.040*** ## (0.001) (0.001) ## color: .C 0.001 -0.006*** ## (0.001) (0.001) ## color: ^4 0.012*** 0.005*** ## (0.001) (0.001) ## color: ^5 -0.007*** -0.001* ## (0.001) (0.001) ## color: ^6 -0.010*** 0.001 ## (0.001) (0.001) ## clarity: .L 0.394*** ## (0.001) ## clarity: .Q -0.104*** ## (0.001) ## clarity: .C 0.057*** ## (0.001) ## clarity: ^4 -0.027*** ## (0.001) ## clarity: ^5 0.011*** ## (0.001) ## clarity: ^6 -0.001 ## (0.001) ## clarity: ^7 0.014*** ## (0.001) ## ---------------------------------------------------------------------------------------------- ## R-squared 0.9236 0.9349 0.9391 0.9514 0.9839 ## adj. R-squared 0.9236 0.9349 0.9391 0.9514 0.9839 ## sigma 0.1218 0.1124 0.1087 0.0972 0.0559 ## F 652012.0628 387489.3661 138654.5235 87959.4667 173791.0840 ## p 0.0000 0.0000 0.0000 0.0000 0.0000 ## Log-likelihood 37025.2108 41356.3916 43150.2943 49222.9505 79078.9821 ## Deviance 800.2475 681.5220 637.6655 509.1030 168.2821 ## AIC -74044.4217 -82704.7832 -86284.5886 -98417.9011 -158115.9642 ## BIC -74017.7348 -82669.2007 -86213.4236 -98293.3623 -157929.1560 ## N 53940 53940 53940 53940 53940 ## ============================================================================================== Observations:\n We get some very nice R square values. We are accounting for almost all of the variance in price using carat, cut, color and clarity. If we want to know whether the price of a diamond is reasonable, we could use this model.  thisDiamond \u0026lt;- data.frame(carat = 1, cut = \u0026#39;Very Good\u0026#39;, color = \u0026#39;G\u0026#39;, clarity = \u0026#39;VS2\u0026#39;) modelEstimate \u0026lt;- predict(m5, newdata = thisDiamond, interval = \u0026quot;prediction\u0026quot;, level = .95) 10^modelEstimate ## fit lwr upr ## 1 5232.111 4065.993 6732.668 exp(modelEstimate) ## fit lwr upr ## 1 41.20984 36.93526 45.97911   ","date":1548374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548374400,"objectID":"b506b2b4f73ef7d85ae50f2c5063071a","permalink":"/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/","publishdate":"2019-01-25T00:00:00Z","relpermalink":"/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/","section":"project","summary":"Preamble: This document focuses on the analysis of the diamonds data frame.\nDescriotion of data frame diamonds can be found at https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html\n Research questions: am i getting a fair deal when I purchase a diamond?\n The goal is to build a predictive model for diamonds, that is going to help figure out whether a given diamond is a good deal or a rip-off!\n   Structure of analysis: I will use Linear Regression to predict the diamond price using other varaibles in the diamonds dataframe.","tags":["regression"],"title":"Predict the diamond price based on the 4 c's","type":"project"},{"authors":null,"categories":["R"],"content":" Introduction The data hosted at [data.world] and contains information about Sales in the US.\n Get Data Data Set 1: Sales from the Retail Trade and Food Services Report from the US Census. This dataset only covers Department Stores, though the report covers a wide range of retail types. [1992-2016]\nData Set 2 US Retail Sales by Store Type with Growth Rate [2009-2014]\n#1992-2016 #https://data.world/retail/department-store-sales GET(\u0026quot;https://query.data.world/s/gdk7iwtlisq6vkktmybqqr7hjjty5s\u0026quot;, write_disk(tf \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;))) ## Response [https://download.data.world/file_download/retail/department-store-sales/retail-trade-report-department-stores.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNDIzLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI0YWU5NmEzZjc4Y2EyOGE5MWM1ZDZlMTgxYzg5YjI0NjIzZDY0ZThlIn0.jBFlyy1aloE-EpeYyosD3iRaDPoY75DBqdh7suLoZxKcrsG8N5GtOiFb6sNMjTqqclsHX7P8RUw7T5sAArPbcw] ## Date: 2019-01-26 21:57 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 62.5 kB ## \u0026lt;ON DISK\u0026gt; /tmp/RtmpbDizpx/file425bddb3fff.xls df1 \u0026lt;- read_excel(tf) #2009-2014 # https://data.world/garyhoov/retail-sales-growth GET(\u0026quot;https://query.data.world/s/py7kinxvyuxjpzwdjs2ti4wdmui6bi\u0026quot;, write_disk(tf \u0026lt;- tempfile(fileext = \u0026quot;.xls\u0026quot;))) ## Response [https://download.data.world/file_download/garyhoov/retail-sales-growth/US%20Retail%20Sales%20by%20Store%20Type%202009-2014.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNTAwLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI5OWRhMDIwMzRlY2Q1YmZmZTRmODFjYzJlMTg4ZmUxOGQyZmEyNDdlIn0.NLTr571lKSZMKhmvIFFQGuoVeFVFr9DrQ7nxBO3LOcLTJUrKivBxWpUrcJcY8dxnkL4FlGba3wsL65c3wLzzxA] ## Date: 2019-01-26 21:57 ## Status: 200 ## Content-Type: application/vnd.ms-excel ## Size: 169 kB ## \u0026lt;ON DISK\u0026gt; /tmp/RtmpbDizpx/file425b7afcc0a6.xls df2 \u0026lt;- read_excel(tf) ## New names: ## * `` -\u0026gt; `..2` ## * `` -\u0026gt; `..3` ## * `` -\u0026gt; `..4` ## * `` -\u0026gt; `..5` ## * `` -\u0026gt; `..6` ## * … and 24 more ## the the first row and make that the column names of the data frame colnames(df2) \u0026lt;- df2[1,]  Save Raw Data ## use saveRDS() to save each object as a .rds file saveRDS(df1, file = \u0026#39;df_department.rds\u0026#39;) saveRDS(df2, file = \u0026#39;df_retail.rds\u0026#39;)  Wrangle Data ## work with df2 df_retail \u0026lt;- df2 %\u0026gt;% ## remove the r from the column names of df2 magrittr::set_colnames(gsub(\u0026quot;r\u0026quot;,\u0026quot;\u0026quot;,df2[1,])) %\u0026gt;% ## add a new column called \u0026quot;business\u0026quot; mutate(business = gsub(\u0026quot;[…]|[.]\u0026quot;,\u0026quot;\u0026quot;,`Kind of business`)) %\u0026gt;% ## filter to include Retail sales or Department stores sales filter(grepl(\u0026#39;Retail sales, total |Department stores\u0026#39;, business)) %\u0026gt;% ## only look at columns with year information in them select(.,c(matches(\u0026#39;19|20\u0026#39;),business)) %\u0026gt;% ## take year column and collapse them into a single column gather(., \u0026quot;year\u0026quot;, \u0026quot;n\u0026quot;, 1:(ncol(.)-1)) %\u0026gt;% ## make sure the count column `n` is numeric mutate(n=as.numeric(n)) %\u0026gt;% ## filter to only include the businesses we\u0026#39;re interested in filter(business == \u0026quot;Retail sales, total \u0026quot;| business==\u0026quot;Department stores \u0026quot;) ## work with df1 df_department \u0026lt;- df1 %\u0026gt;% ## split Period column into one column called \u0026quot;month\u0026quot; and one called \u0026quot;year\u0026quot; separate(Period, into = c(\u0026#39;month\u0026#39;, \u0026#39;year\u0026#39;), extra = \u0026#39;drop\u0026#39;, remove = FALSE) %\u0026gt;% ## add a column `value` which contains the ## information from the `Value (in millions)` mutate(value = `Value (in millions)`) %\u0026gt;% ## group the data frame by the `year` column group_by(year) %\u0026gt;% ## Summarize the data by creating a new column ## call this column `n` ## have it contain the sum of the `value` column summarize(n = sum(value)) %\u0026gt;% ### create a new column called `business` ## set the value of this column to be \u0026quot;department stores\u0026quot; ## for the entire data set mutate(business = \u0026#39;department stores\u0026#39;) %\u0026gt;% ## reorder column names to be : business, year, n select(business, year, n)  Merging Data ## Now, combine the two data frames df_total \u0026lt;- left_join(df_retail, df_department, by = c(\u0026#39;business\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;n\u0026#39;))  Plotting Data ## Plot Retail Sales data ggplot(df_retail, aes(x=year,y=n,colour=business)) + geom_point()  ## Plot Department Sales data ggplot(df_department, aes(x=year,y=n)) + geom_point()  ## Plot Combined Data ggplot(df_total, aes(x=year,y=as.numeric(n), colour=business)) + geom_point()  ","date":1547424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547424000,"objectID":"873eb27b26cd501e1ea00cb54f045220","permalink":"/project/data_tidying/data-tidying-project/","publishdate":"2019-01-14T00:00:00Z","relpermalink":"/project/data_tidying/data-tidying-project/","section":"project","summary":"Data tidying","tags":[],"title":"Data Tidying Project","type":"project"},{"authors":null,"categories":["Python","Machine Learning"],"content":" Intro \u0026amp; Background Linear regression being one of the most basic and popular algorithms in machine learning, so when aspiring data scientist starts off in this field, linear regression is inevitably the first algorithm they come across.\n Analysis Approach You can click on the link below to see the working code in Python and reproduce it to test the strength of your own password.\nhttps://github.com/2series/100_Days_of_ML_Code/blob/master/Linear_Regression_from_Scratch.ipynb\n ","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"cd456bcd8546834f11d4a6704dc6788a","permalink":"/project/machine_learning_using_lr/machine-learning-using-linear-regression-to-predict-house-prices/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/project/machine_learning_using_lr/machine-learning-using-linear-regression-to-predict-house-prices/","section":"project","summary":"A simple linear regression model to predict the price of real estate","tags":["Python","regression"],"title":"Machine Learning using linear regression to predict house prices","type":"project"},{"authors":null,"categories":["Python","Blockchain"],"content":" Intro \u0026amp; Background Blockchain is a data structure that was first introduced by Satoshi Nakamoto in the Bitcoin protocol white paper a decade ago. Bitcoin’s blockchain stores transaction data, but we can store any type of data in a blockchain.\nEthereum, for example, enables users to store code snippets called ‘smart contracts’ in their blockchain. In this project, I build a simple blockchain in Python that uses proof-of-work consensus, just like the Bitcoin protocol does. There’s a lot of misunderstanding around what the blockchain is and what it can do, so I hope this project demonstrates how simple it really is. Enjoy!\n Analysis Approach You can click on the link below to see the working code in Python.\nhttps://github.com/2series/100_Days_of_ML_Code/blob/master/Simple_Blockchain.ipynb\n ","date":1546732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546732800,"objectID":"89b378d60f65639aad953c96a5774dc1","permalink":"/project/simple_blockchain-project/build-a-simple-blockchain/","publishdate":"2019-01-06T00:00:00Z","relpermalink":"/project/simple_blockchain-project/build-a-simple-blockchain/","section":"project","summary":"Build a simple blockchain in under 5 minutes","tags":["Python"],"title":"Build A Simple Blockchain","type":"project"},{"authors":null,"categories":["Python"],"content":" Intro \u0026amp; Background A function written that uses regular expressions to make sure the password string it is passed is strong. A strong password is defined as one that is at least eight characters long, contains both uppercase and lowercase characters, and has at least one digit. You may need to test the string against multiple regex patterns to validate its strength.\n Analysis Approach You can click on the link below to see the working code in Python and reproduce it to test the strength of your own password.\nhttps://github.com/2series/100_Days_of_ML_Code/blob/master/Password_Detection_Strength.ipynb\n ","date":1546732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546732800,"objectID":"3677a323d997ff147051605b7dc0348c","permalink":"/project/password_detection_strength/password-detection-strength/","publishdate":"2019-01-06T00:00:00Z","relpermalink":"/project/password_detection_strength/password-detection-strength/","section":"project","summary":"Test the strength of your password!","tags":["Python"],"title":"Password Detection Strength","type":"project"},{"authors":null,"categories":["R"],"content":" Preamble: This document focuses on the analysis of the mtcars dataframe.\nDescription of dataframe mtcars can be found at https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n Research questions: is a vehicle with auto or manual transmission better in terms of miles p/gallons(mpg)?\n quantify the (mpg) difference between auto \u0026amp; manual transmission.\n   Structure of analysis: I will asssess both queries from different perspectives employing a set of methodologies that can be broadly grouped as follows:\n Univariate Analysis on target varibale (mpg). Bivariate Analysis on target varibale \u0026amp; relevant covariates. Multivariate Analysis by estimating a set of regresssion models for the conditional mean of mpg. For model selection, I compare the best fit and forward stepwise selection process.   Univariate Analysis Analysing the target variable alone by splitting the observations into two groups, i.e. vehicles with auto or manual transmission. I shall deploy 3 analysis:\n Compute sample means by group ie auto VS manual. Validate if the difference of the group means are statistically significant by computing a 95% confidence interval for means’ difference. Verify the robustness of this result by executing a permutation test with Monte Carlo trials that shuffle the allocation group \u0026gt; mpg.  Get to know the data str(mtcars) ## \u0026#39;data.frame\u0026#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... We notice that the set is relatively small! We’ll look at the desriptive statistics for each field - (min, 1st Q, Median, Mean, 3rd Q, max)\nsummary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000  Sample means by group #### generate subset: automatic and manual cars #### cars_auto = subset(mtcars, am == 0) cars_manu = subset(mtcars, am == 1) # dimensions dim(mtcars) ## [1] 32 11 dim(cars_auto); dim(cars_manu) ## [1] 19 11 ## [1] 13 11 # sample means mpg by group mean(cars_auto$mpg); mean(cars_manu$mpg) ## [1] 17.14737 ## [1] 24.39231 sd(cars_auto$mpg); sd(cars_manu$mpg) ## [1] 3.833966 ## [1] 6.166504 # % increase in mpg based on the sample mean (mean(cars_manu$mpg) - mean(cars_auto$mpg))/mean(cars_auto$mpg) ## [1] 0.4225103   Including plots To get a feel for the distribution of some of the data to be analyzed, we plot some histograms, the first against mpg - auto transmission, the second against mpg - manual transission:\nboxplot(mpg ~ am, data = mtcars, col=rgb(0.3,0.2,0.5,0.6), ylab = \u0026quot;mpg\u0026quot;, xlab = \u0026quot;am\u0026quot;) Conclusions:\n mpg empirical mean of vehicles with manual transmission is greater than cars with auto transmission, however this also has a higher variance.  95% confidence interval for the difference of the group means The analysis on sample means concludes that sample mean of mpg for vehicles with manual trasmission is greater than automatic:\nNow I test if this difference (i.e. in the sample means) is statistically significant (from zero).\nI execute a t.test for unpaired samples: I assume inequality in variances for the two groups for the computation of the pooled variance.\n#### 95% confidence interval for mean difference #### # Question: is the sample mean difference significant? t.test(cars_manu$mpg, cars_auto$mpg, paired = F, var.equal = F) ## ## Welch Two Sample t-test ## ## data: cars_manu$mpg and cars_auto$mpg ## t = 3.7671, df = 18.332, p-value = 0.001374 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 3.209684 11.280194 ## sample estimates: ## mean of x mean of y ## 24.39231 17.14737 Conclusions:\n 95% interval does not contain 0 sample mean difference is significant at 95% (p-value 0.1%)   Permutation test on groups association I test the robustness of results obtained in the previous step.\nI execute a permutation test by shuffling the allocation mean \u0026gt; groups with 100,000 trials of Montecarlo simulation.\n#### Permutation test #### # what if I shuffle the am groups and calculate the mean? # get target variable and group vectors y = mtcars$mpg group = mtcars$am y; group ## [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 ## [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 ## [29] 15.8 19.7 15.0 21.4 ## [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 # baseline group means and difference baselineMeans = tapply(mtcars$mpg, mtcars$am, mean) baselineMeansDiff = baselineMeans[2] - baselineMeans[1] tStat = function(w, g) mean(w[g == 1]) - mean(w[g == 0]) observedDiff = tStat(y, group) # check if function works - should be 0: baselineMeansDiff - observedDiff ## 1 ## 0 # execute shuffle: permutations = sapply(1:100000, function(i) tStat(y, sample(group)))  Plot the analysis: # shuffle experiment results plots: par(mfrow = c(2, 1), mar = c(4, 4, 2, 2)) hist(permutations, main = \u0026quot;Distribution of shuffled group mean differences\u0026quot;) # distribution of difference of averages of permuted groups plot(permutations, type = \u0026quot;b\u0026quot;, main = \u0026quot;Shuffled group mean trials\u0026quot;, xlab = \u0026quot;trial\u0026quot;, ylab = \u0026quot;shuffled group mean differences\u0026quot;, ylim = c(-14, 14)) abline(h = observedDiff, col = \u0026quot;red\u0026quot;, lwd = 3) # there is not even 1 case where by chance I get a difference greater than the observed! mean(permutations \u0026gt; observedDiff) ## [1] 0.00015 Conclusions:\n out of 100,000 trails only 0.002% has breached the observed value for the diffs in the group empirical means. concluding that empirical means diffs of groups is robust with regards to random reshuffling and is not likely to be generated by pure chance. is this correct?   Bivariate Analysis Analyse the behaviour of target variable (mpg) conditional upon a set of explanatory variables.\n#### generate subset: automatic and manual cars #### cars_auto = subset(mtcars, am == 0) cars_manu = subset(mtcars, am == 1) #### Visual inspection of all covariates #### pairs(mtcars) #### 4 bivariate analysis: hp / wt / drat / disp #### par(mfrow = c(2, 2), mar = c(2, 3, 2, 3)) # plot1 with(mtcars, plot(hp, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs hp - by transmission type\u0026quot;)) # no data with(cars_auto, points(hp, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(hp, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend model1_auto = lm(mpg ~ hp, data = cars_auto) model1_manu = lm(mpg ~ hp, data = cars_manu) abline(model1_auto, col = \u0026quot;red\u0026quot;, lwd = 2) abline(model1_manu, col = \u0026quot;blue\u0026quot;, lwd = 2) abline(v = 175, lty = 2) # plot2 with(mtcars, plot(wt, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs weight - by transmission type\u0026quot;)) # no data with(cars_auto, points(wt, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(wt, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend abline(v = 3.2, lty = 2) # plot 3 with(mtcars, plot(drat, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs drat - by transmission type\u0026quot;)) # no data with(cars_auto, points(drat, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(drat, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend model2_auto = lm(mpg ~ drat, data = cars_auto) model2_manu = lm(mpg ~ drat, data = cars_manu) abline(model2_auto, col = \u0026quot;red\u0026quot;, lwd = 2) abline(model2_manu, col = \u0026quot;blue\u0026quot;, lwd = 2) abline(v = 175, lty = 2) # plot 4 with(mtcars, plot(disp, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs disp - by transmission type\u0026quot;)) # no data with(cars_auto, points(disp, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(disp, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend labels = with(mtcars, paste(as.character(disp), as.character(mpg), sep = \u0026quot;,\u0026quot;)) # generate point labels with(mtcars, text(disp, mpg, labels = labels, cex = 0.7, pos = 2)) abline(v = 167.6, lty = 2) Conclusions:\n mpg vs hp: linear negative relation: as horse power of the engine (hp) increases, the mileage (mpg) reduces. Vehicles with manual transmission seems however to be more efficient: the group restricted regression (blue) has a higher intercept. It has to be highlighted however, that the parameters of blue regression might be influenced by two extreme values with high hp - the regression should be re-estimated by removing the two datapoints. mpg vs weight: negative relation, the functional form might be non-linear (hyperbolic ?), as weight of the vehicle increases, the mileage decreases. The weight variable seems to provide perfect separation between manual and auto transmission vehilces, i.e. all vehicles that are heavier than 3.2 ton (circa) are auto and vice-versa. mpg vs drat: the functional form is not clear: it appears also to be an increase in the variance as the rear axel ratio (drat) increases. To verify this a regression model using all observations has to be estimated and analyse the residuals for verifying if the model is heteroskedastic. mpg vs disp: seems to have a negative (hyperbolic ?) relation: as the displacement (disp) of the engine increases, the mileage decreases. Also, in this case it seems that disp accounts for perfect separation in the transmission type: almost all vehilces with disp \u0026gt; 180 are auto.   Multivariate analysis Run a set of regression models for estimating the impact of some predictions on mpg.\nFor model selection, I employ the following techniques:\n Manual selection of regressors: I hand pick regressors for: Best fit procedure Forward stepwise procedure   Manual selection Analysis of covariance matrix:\n### analyse covariance matrix for regressor selection: z \u0026lt;- cor(mtcars) require(lattice) ## Loading required package: lattice levelplot(z) A model with only transmission:\n# only am data = mtcars data$am = as.factor(data$am) model2 = lm(mpg ~ am, data = data) # get results summary(model2) ## ## Call: ## lm(formula = mpg ~ am, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3923 -3.0923 -0.2974 3.2439 9.5077 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 17.147 1.125 15.247 1.13e-15 *** ## am1 7.245 1.764 4.106 0.000285 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 4.902 on 30 degrees of freedom ## Multiple R-squared: 0.3598, Adjusted R-squared: 0.3385 ## F-statistic: 16.86 on 1 and 30 DF, p-value: 0.000285 Observations:\n the intercept is 17.15: exactly the same mean of mpg for vehicles with auto transmission. the coefficient of am is 7.24: exactly the difference of mpg means for vehicles with manual / auto transmission. the sum of intercept and am coefficient gives the mpg unconditional mean for vehicles with manual transmission.   Best Fit Procedure Run the best fit procedure for identifying the optimal number of regressors that minimises the cp, which is (…)\n#### model selection using leaps #### data = mtcars data$log_mpg = log(data$mpg) # add log of y #### method 1. best fit #### regfit.full = regsubsets(log_mpg ~. , data = data, nvmax = 10) reg.summary = summary(regfit.full) reg.summary ## Subset selection object ## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10) ## 11 Variables (and intercept) ## Forced in Forced out ## mpg FALSE FALSE ## cyl FALSE FALSE ## disp FALSE FALSE ## hp FALSE FALSE ## drat FALSE FALSE ## wt FALSE FALSE ## qsec FALSE FALSE ## vs FALSE FALSE ## am FALSE FALSE ## gear FALSE FALSE ## carb FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: exhaustive ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot;  Plot the analysis # how I selected the optimal number of variables? plot(reg.summary$cp, xlab = \u0026quot;Number of variables\u0026quot;, ylab = \u0026quot;cp\u0026quot;, type = \u0026quot;b\u0026quot;)  Forward Stepwise Procedure regfit.fwd = regsubsets(log_mpg ~ ., data = data, nvmax = 10, method = \u0026quot;forward\u0026quot;) summary(regfit.fwd) ## Subset selection object ## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10, method = \u0026quot;forward\u0026quot;) ## 11 Variables (and intercept) ## Forced in Forced out ## mpg FALSE FALSE ## cyl FALSE FALSE ## disp FALSE FALSE ## hp FALSE FALSE ## drat FALSE FALSE ## wt FALSE FALSE ## qsec FALSE FALSE ## vs FALSE FALSE ## am FALSE FALSE ## gear FALSE FALSE ## carb FALSE FALSE ## 1 subsets of each size up to 10 ## Selection Algorithm: forward ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 2 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 3 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; ## 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; ## 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; ## 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot;  Plot the analysis plot(regfit.fwd, scale = \u0026quot;Cp\u0026quot;) Appendix\nA model including all regressors.\n#### lm with all variables / no split #### # prepare data data = mtcars data$am = as.factor(data$am) model1 = lm(mpg ~ ., data = data) # get results summary(model1) ## ## Call: ## lm(formula = mpg ~ ., data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs 0.31776 2.10451 0.151 0.8814 ## am1 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## carb -0.19942 0.82875 -0.241 0.8122 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07  Plot the analysis # plot residual analysis par(mfrow = c(2, 2)) plot(model1) # plot hist par(mfrow = c(1, 1)) hist(model1$residuals) # normality test on residuals shapiro.test(model1$residuals) ## ## Shapiro-Wilk normality test ## ## data: model1$residuals ## W = 0.95694, p-value = 0.2261   ","date":1546567994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546567994,"objectID":"e7334ef87192c7900880836c0b9223cc","permalink":"/post/mtcars/2019-01-03-r-rmarkdown/","publishdate":"2019-01-03T21:13:14-05:00","relpermalink":"/post/mtcars/2019-01-03-r-rmarkdown/","section":"post","summary":"MauWhich is fuel efficient?","tags":["plot","regression"],"title":"mtcars Data Analysis","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]