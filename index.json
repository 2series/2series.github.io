[{"authors":["admin"],"categories":null,"content":" Core Competencies Analytical Thinker Focused on gaining interpretable insights from data. Experienced in working with large datasets and using advanced data analysis to answer complex questions with accuracy, applying sophisticated tools and techniques. Proficient in mastering new knowledge and techniques quickly. Able to understand and articulate what questions can and can‚Äôt be answered given certain data.\nModel Building Experienced in using statistical techniques and modeling with real-world, messy data. Able to apply sophisticated mathematics to understanding data. Proficient in the modern machine learning toolkit, including supervised and unsupervised learning techniques, and practically how to build predictive models.\nCommunications Articulate, insightful, and able to communicate technical procedures and results to expert and non-expert collaborators. Capable of turning a complex analysis into a compelling story so that decision-makers can move forward with an appropriate strategy. Flexible, creative, resourceful, and effective in both independent and collaborative environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Core Competencies Analytical Thinker Focused on gaining interpretable insights from data. Experienced in working with large datasets and using advanced data analysis to answer complex questions with accuracy, applying sophisticated tools and techniques. Proficient in mastering new knowledge and techniques quickly. Able to understand and articulate what questions can and can‚Äôt be answered given certain data.\nModel Building Experienced in using statistical techniques and modeling with real-world, messy data. Able to apply sophisticated mathematics to understanding data.","tags":null,"title":"Rihad Variawa","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" The analysis consists of two parts:\nA simulation exercise. Basic inferential data analysis.  Analysis 1. A simulation exercise. In this analysis I‚Äôll investigate the exponential distribution in R and compare it with the Central Limit Theorem. The exponential distribution can be simulated in R with rexp(n, lambda) where lambda is the rate parameter. The mean of exponential distribution is 1/lambda and the standard deviation is also 1/lambda. Set lambda = 0.2 for all of the simulations. I‚Äôll investigate the distribution of averages of 40 exponentials. Note that you will need to do a thousand simulations.\nIllustrate via simulation and associated explanatory text the properties of the distribution of the mean of 40 exponentials. You should\nShow the sample mean and compare it to the theoretical mean of the distribution. Show how variable the sample is (via variance) and compare it to the theoretical variance of the distribution. Show that the distribution is approximately normal.    Task # set seed for reproducability set.seed(31) # set lambda to 0.2 lambda \u0026lt;- 0.2 # 40 samples n \u0026lt;- 40 # 1000 simulations simulations \u0026lt;- 1000 # simulate simulated_exponentials \u0026lt;- replicate(simulations, rexp(n, lambda)) # calculate mean of exponentials means_exponentials \u0026lt;- apply(simulated_exponentials, 2, mean)  Question 1 Show where the distribution is centered at and compare it to the theoretical center of the distribution.\nanalytical_mean \u0026lt;- mean(means_exponentials) analytical_mean [1] 4.993867 # analytical mean theory_mean \u0026lt;- 1/lambda theory_mean [1] 5 # visualization hist(means_exponentials, xlab = \u0026quot;mean\u0026quot;, main = \u0026quot;Exponential Function Simulations\u0026quot;) abline(v = analytical_mean, col = \u0026quot;red\u0026quot;) abline(v = theory_mean, col = \u0026quot;orange\u0026quot;) The analytics mean is 4.993867 the theoretical mean 5. The center of distribution of averages of 40 exponentials is very close to the theoretical center of the distribution.\n Question 2 Show how variable it is and compare it to the theoretical variance of the distribution..\n# standard deviation of distribution standard_deviation_dist \u0026lt;- sd(means_exponentials) standard_deviation_dist [1] 0.7931608 # standard deviation from analytical expression standard_deviation_theory \u0026lt;- (1/lambda)/sqrt(n) standard_deviation_theory [1] 0.7905694 # variance of distribution variance_dist \u0026lt;- standard_deviation_dist^2 variance_dist [1] 0.6291041 # variance from analytical expression variance_theory \u0026lt;- ((1/lambda)*(1/sqrt(n)))^2 variance_theory [1] 0.625 Standard Deviation of the distribution is 0.7931608 with the theoretical SD calculated as 0.7905694. The Theoretical variance is calculated as ((1 / ??) * (1/???n))2 = 0.625. The actual variance of the distribution is 0.6291041\n Question 3 Show that the distribution is approximately normal.\nxfit \u0026lt;- seq(min(means_exponentials), max(means_exponentials), length=100) yfit \u0026lt;- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(n))) hist(means_exponentials,breaks=n,prob=T,col=\u0026quot;orange\u0026quot;,xlab = \u0026quot;means\u0026quot;,main=\u0026quot;Density of means\u0026quot;,ylab=\u0026quot;density\u0026quot;) lines(xfit, yfit, pch=22, col=\u0026quot;black\u0026quot;, lty=5) # compare the distribution of averages of 40 exponentials to a normal distribution qqnorm(means_exponentials) qqline(means_exponentials, col = 2) Due to Due to the central limit theorem (CLT), the distribution of averages of 40 exponentials is very close to a normal distribution.\n2. Basic inferential data analysis. Now in the second portion of this analysis, we‚Äôre going to analyze the ToothGrowth data in the R datasets package.\n Load the ToothGrowth data and perform some basic exploratory data analyses Provide a basic summary of the data. Use confidence intervals and/or hypothesis tests to compare tooth growth by supp and dose. (Only use the techniques from class, even if there‚Äôs other approaches worth considering) State your conclusions and the assumptions needed for your conclusions.    Load the ToothGrowth data and perform some basic exploratory data analyses # load the data ToothGrowth data(ToothGrowth) # preview the structure of the data str(ToothGrowth) \u0026#39;data.frame\u0026#39;: 60 obs. of 3 variables: $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... $ supp: Factor w/ 2 levels \u0026quot;OJ\u0026quot;,\u0026quot;VC\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ... $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... # preview first 5 rows of the data head(ToothGrowth, 5)  len supp dose 1 4.2 VC 0.5 2 11.5 VC 0.5 3 7.3 VC 0.5 4 5.8 VC 0.5 5 6.4 VC 0.5  Provide a basic summary of the data. # data summary summary(ToothGrowth)  len supp dose Min. : 4.20 OJ:30 Min. :0.500 1st Qu.:13.07 VC:30 1st Qu.:0.500 Median :19.25 Median :1.000 Mean :18.81 Mean :1.167 3rd Qu.:25.27 3rd Qu.:2.000 Max. :33.90 Max. :2.000  # compare means of the different delivery methods tapply(ToothGrowth$len,ToothGrowth$supp, mean)  OJ VC 20.66333 16.96333  # plot data graphically ggplot(ToothGrowth, aes(factor(dose), len, fill = factor(dose))) + geom_boxplot() + # facet_grid(.~supp)+ facet_grid(.~supp, labeller = as_labeller( c(\u0026quot;OJ\u0026quot; = \u0026quot;Orange juice\u0026quot;, \u0026quot;VC\u0026quot; = \u0026quot;Ascorbic Acid\u0026quot;))) + labs(title = \u0026quot;Tooth growth of 60 guinea pigs by dosage and\\nby delivery method of vitamin C\u0026quot;, x = \u0026quot;Dose in milligrams/day\u0026quot;, y = \u0026quot;Tooth Lengh\u0026quot;) + scale_fill_discrete(name = \u0026quot;Dosage of\\nvitamin C\\nin mg/day\u0026quot;) + theme_classic()  Use confidence intervals and/or hypothesis tests to compare tooth growth by supp and dose. # comparison by delivery method for the same dosage t05 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 0.5) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 0.5) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) t1 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 1) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 1) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) t2 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 2) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 2) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) # summary of the conducted t.tests, which compare the delivery methods by dosage, # take p-values and CI summaryBYsupp \u0026lt;- data.frame( \u0026quot;p-value\u0026quot; = c(t05$p.value, t1$p.value, t2$p.value), \u0026quot;Conf.Low\u0026quot; = c(t05$conf.int[1],t1$conf.int[1], t2$conf.int[1]), \u0026quot;Conf.High\u0026quot; = c(t05$conf.int[2],t1$conf.int[2], t2$conf.int[2]), row.names = c(\u0026quot;Dosage .05\u0026quot;,\u0026quot;Dosage 1\u0026quot;,\u0026quot;Dosage 2\u0026quot;)) # show data table summaryBYsupp  p.value Conf.Low Conf.High Dosage .05 0.006358607 1.719057 8.780943 Dosage 1 0.001038376 2.802148 9.057852 Dosage 2 0.963851589 -3.798070 3.638070  Conclusion With 95% confidence we reject the null hypothesis, stating that there is no difference in the tooth growth by the delivery method for .5 and 1 milligrams/day. We observe p-values less than the treshold of .05 and the confidence levels don‚Äôt include 0. So, for dosage of .5 milligrams/day and 1 milligrams/day does matter the delivery method. With 95% confidence we fail to reject the null hypothesis, stating that there is no difference in the tooth growth by the delivery method for 2 milligrams/day. We observe p-values more than the treshold of .05 and the confidence levels include 0. So, for dosage of 2 milligrams/day the delivery method doesn‚Äôt matter.\n ","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"8256f7fe219884432862c8d02845ba97","permalink":"/project/exponential_distribution/stats/","publishdate":"2019-04-30T00:00:00Z","relpermalink":"/project/exponential_distribution/stats/","section":"project","summary":"Central limit theorem","tags":["R","Data Analytics"],"title":"Investigating The Exponential Distribution \u0026 Compare It With The Central Limit Theorem","type":"project"},{"authors":null,"categories":null,"content":" Introduction It is now possible to collect a large amount of data about personal movement using activity monitoring devices such as a Fitbit, Nike Fuelband, or Jawbone Up. These type of devices are part of the ‚Äúquantified self‚Äù movement ‚Äì a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. But these data remain under-utilized both because the raw data are hard to obtain and there is a lack of statistical methods and software for processing and interpreting the data.\nThis analysis makes use of data from a personal activity monitoring device. This device collects data at 5 minute intervals through-out the day. The data consists of two months of data from an anonymous individual collected during the months of October and November, 2012 and include the number of steps taken in 5 minute intervals each day.\nThe data for this analysis can be downloaded from the course web site:\n Dataset: Activity monitoring data  The variables included in this dataset are:\n steps: Number of steps taking in a 5-minute interval (missing values are coded as ùôΩùô∞) date: The date on which the measurement was taken in YYYY-MM-DD format interval: Identifier for the 5-minute interval in which measurement was taken  The dataset is stored in a comma-separated-value (CSV) file and there are a total of 17,568 observations in this dataset.\n Loading and preprocessing the data Unzip data to obtain a csv file.\nfileUrl \u0026lt;- \u0026quot;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip\u0026quot; download.file(fileUrl, destfile = paste0(getwd(), \u0026#39;/repdata%2Fdata%2Factivity.zip\u0026#39;), method = \u0026quot;curl\u0026quot;) unzip(\u0026quot;repdata%2Fdata%2Factivity.zip\u0026quot;,exdir = \u0026quot;data\u0026quot;)  Reading csv Data into Data.Table. activityDT \u0026lt;- data.table::fread(input = \u0026quot;data/activity.csv\u0026quot;)  What is mean total number of steps taken per day? Calculate the total number of steps taken per day.  Total_Steps \u0026lt;- activityDT[, c(lapply(.SD, sum, na.rm = FALSE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(date)] head(Total_Steps, 10)  date steps 1: 2012-10-01 NA 2: 2012-10-02 126 3: 2012-10-03 11352 4: 2012-10-04 12116 5: 2012-10-05 13294 6: 2012-10-06 15420 7: 2012-10-07 11015 8: 2012-10-08 NA 9: 2012-10-09 12811 10: 2012-10-10 9900 Make a histogram of the total number of steps taken each day.  ggplot(Total_Steps, aes(x = steps)) + geom_histogram(fill = \u0026quot;blue\u0026quot;, binwidth = 1000) + labs(title = \u0026quot;Daily Steps\u0026quot;, x = \u0026quot;Steps\u0026quot;, y = \u0026quot;Frequency\u0026quot;) + theme_classic() Warning: Removed 8 rows containing non-finite values (stat_bin). Calculate and report the mean and median of the total number of steps taken per day.  Total_Steps[, .(Mean_Steps = mean(steps, na.rm = TRUE), Median_Steps = median(steps, na.rm = TRUE))]  Mean_Steps Median_Steps 1: 10766.19 10765  What is the average daily activity pattern? Make a time series plot (i.e.¬†ùöùùö¢ùöôùöé = ‚Äúùöï‚Äù) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis).  IntervalDT \u0026lt;- activityDT[, c(lapply(.SD, mean, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(interval)] ggplot(IntervalDT, aes(x = interval , y = steps)) + geom_line(color=\u0026quot;blue\u0026quot;, size=1) + labs(title = \u0026quot;Avg. Daily Steps\u0026quot;, x = \u0026quot;Interval\u0026quot;, y = \u0026quot;Avg. Steps per day\u0026quot;) + theme_classic() Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?  IntervalDT[steps == max(steps), .(max_interval = interval)]  max_interval 1: 835  Imputing missing values Calculate and report the total number of missing values in the dataset (i.e.¬†the total number of rows with ùôΩùô∞s)  activityDT[is.na(steps), .N ] [1] 2304 # alternative solution nrow(activityDT[is.na(steps),]) [1] 2304 Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc.  Filling in missing values with median of dataset.\nactivityDT[is.na(steps), \u0026quot;steps\u0026quot;] \u0026lt;- activityDT[, c(lapply(.SD, median, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;)] Create a new dataset that is equal to the original dataset, but with the missing data filled in.  data.table::fwrite(x = activityDT, file = \u0026quot;data/tidyData.csv\u0026quot;, quote = FALSE) Make a histogram of the total number of steps taken each day and calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?  # total number of steps taken per day Total_Steps \u0026lt;- activityDT[, c(lapply(.SD, sum)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(date)] # mean and median total number of steps taken per day Total_Steps[, .(Mean_Steps = mean(steps), Median_Steps = median(steps))]  Mean_Steps Median_Steps 1: 9354.23 10395 ggplot(Total_Steps, aes(x = steps)) + geom_histogram(fill = \u0026quot;blue\u0026quot;, binwidth = 1000) + labs(title = \u0026quot;Daily Steps\u0026quot;, x = \u0026quot;Steps\u0026quot;, y = \u0026quot;Frequency\u0026quot;) + theme_classic() Type of Estimate | Mean_Steps | Median_Steps\nFirst Part (with na) | 10765 | 10765 Second Part (fillin in na with median) | 9354.23 | 10395\n Are there differences in activity patterns between weekdays and weekends? Create a new factor variable in the dataset with two levels ‚Äì ‚Äúweekday‚Äù and ‚Äúweekend‚Äù indicating whether a given date is a weekday or weekend day.  Just recreating activityDT from scratch then making the new factor variable. (No need to, just want to be clear on what the entire process is.)\nactivityDT \u0026lt;- data.table::fread(input = \u0026quot;data/activity.csv\u0026quot;) activityDT[, date := as.POSIXct(date, format = \u0026quot;%Y-%m-%d\u0026quot;)] activityDT[, `Day of Week`:= weekdays(x = date)] activityDT[grepl(pattern = \u0026quot;Monday|Tuesday|Wednesday|Thursday|Friday\u0026quot;, x = `Day of Week`), \u0026quot;weekday or weekend\u0026quot;] \u0026lt;- \u0026quot;weekday\u0026quot; activityDT[grepl(pattern = \u0026quot;Saturday|Sunday\u0026quot;, x = `Day of Week`), \u0026quot;weekday or weekend\u0026quot;] \u0026lt;- \u0026quot;weekend\u0026quot; activityDT[, `weekday or weekend` := as.factor(`weekday or weekend`)] head(activityDT, 10)  steps date interval Day of Week weekday or weekend 1: NA 2012-10-01 0 Monday weekday 2: NA 2012-10-01 5 Monday weekday 3: NA 2012-10-01 10 Monday weekday 4: NA 2012-10-01 15 Monday weekday 5: NA 2012-10-01 20 Monday weekday 6: NA 2012-10-01 25 Monday weekday 7: NA 2012-10-01 30 Monday weekday 8: NA 2012-10-01 35 Monday weekday 9: NA 2012-10-01 40 Monday weekday 10: NA 2012-10-01 45 Monday weekday Make a panel plot containing a time series plot (i.e.¬†ùöùùö¢ùöôùöé = ‚Äúùöï‚Äù) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday, days or weekend days (y-axis).  activityDT[is.na(steps), \u0026quot;steps\u0026quot;] \u0026lt;- activityDT[, c(lapply(.SD, median, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;)] IntervalDT \u0026lt;- activityDT[, c(lapply(.SD, mean, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(interval, `weekday or weekend`)] ggplot(IntervalDT , aes(x = interval , y = steps, color=`weekday or weekend`)) + geom_line() + labs(title = \u0026quot;Avg. Daily Steps by Weektype\u0026quot;, x = \u0026quot;Interval\u0026quot;, y = \u0026quot;No. of Steps\u0026quot;) + facet_wrap(~`weekday or weekend` , ncol = 1, nrow=2) + theme_classic()  ","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"198ed71bfc4b71a3fddc384c9dbe710f","permalink":"/project/personal_movement/pmd/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/project/personal_movement/pmd/","section":"project","summary":"An analysis of personal activity monitoring device","tags":["R","Data Analytics"],"title":"Personal Movement Monitoring Devices","type":"project"},{"authors":null,"categories":null,"content":" The federal funds rate is the key interest rate that the U.S. Federal Reserve uses to influence economic growth. The Federal Open Market Committee meets regularly to decide whether to increase, decrease, or maintain the target interest rate. Their choice has important ramifications that cascade through the economy, so the announcement of the interest rates is eagerly awaited each month.\nIn this analysis, I‚Äôll use analytics to try to predict when the Fed will raise interest rates. I‚Äôll look at monthly economic and political data dating back to the mid-1960‚Äôs. In this analysis, the dependent variable will be the binary outcome variable RaisedFedFunds, which takes value 1 if the federal funds rate was increased that month and 0 if it was lowered or stayed the same. For each month, the file federalFundsRate.csv.xz contains the following independent variables:\n Date: The date the change was announced. Chairman: The name of the Federal Reserve Chairman at the time the change was announced. PreviousRate: The federal funds rate in the prior month. Streak: The current streak of raising or not raising the rate, e.g. +8 indicates the rate has been increased 8 months in a row, whereas -3 indicates the rate has been lowered or stayed the same for 3 months in a row. GDP: The U.S. Gross Domestic Product, in Billions of Chained 2009 US Dollars. Unemployment: The unemployment rate in the U.S. CPI: The Consumer Price Index, an indicator of inflation, in the U.S. HomeownershipRate: The rate of homeownership in the U.S. DebtAsPctGDP: The U.S. national debt as a percentage of GDP DemocraticPres: Whether the sitting U.S. President is a Democrat (DemocraticPres=1) or a Republican (DemocraticPres=0) MonthsUntilElection: The number of remaining months until the next U.S. presidential election.  Problem 1 - Loading the Data Use the read.csv function to load the contents of federalFundsRate.csv.xz file into a dataframe called fedFunds, using stringsAsFactors=FALSE.\nWhat proportion of months did the Fed raise the interest rate?\nfedFunds \u0026lt;- read.csv(\u0026quot;federalFundsRate.csv.xz\u0026quot;) str(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : int 1 1 1 1 1 1 1 1 1 1 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : int 1 1 1 1 1 1 1 0 1 1 ... summary(fedFunds)  Date Chairman PreviousRate 1966-02-01: 1 Greenspan, Alan :221 Min. : 0.070 1966-03-01: 1 Bernanke, Ben : 96 1st Qu.: 3.290 1966-04-01: 1 Burns, Arthur : 96 Median : 5.390 1966-05-01: 1 Volcker, Paul : 96 Mean : 5.651 1966-06-01: 1 Martin, William M.: 48 3rd Qu.: 7.880 1966-07-01: 1 Miller, G. William: 17 Max. :19.100 (Other) :579 (Other) : 11 Streak GDP Unemployment CPI Min. :-16.000 Min. : 4202 Min. : 3.400 Min. : 31.88 1st Qu.: -2.000 1st Qu.: 6039 1st Qu.: 5.000 1st Qu.: 63.40 Median : 1.000 Median : 8907 Median : 5.900 Median :129.10 Mean : 1.094 Mean : 9450 Mean : 6.181 Mean :127.71 3rd Qu.: 3.000 3rd Qu.:12956 3rd Qu.: 7.300 3rd Qu.:180.00 Max. : 27.000 Max. :16206 Max. :10.800 Max. :237.63 HomeownershipRate DebtAsPctGDP DemocraticPres MonthsUntilElection Min. :63.20 Min. : 30.60 Min. :0.0000 Min. : 0.00 1st Qu.:64.20 1st Qu.: 62.35 1st Qu.:0.0000 1st Qu.:12.00 Median :64.80 Median : 6039.16 Median :0.0000 Median :24.00 Mean :65.41 Mean : 6317.32 Mean :0.4256 Mean :23.58 3rd Qu.:66.50 3rd Qu.:10529.38 3rd Qu.:1.0000 3rd Qu.:35.00 Max. :69.20 Max. :16205.59 Max. :1.0000 Max. :47.00 RaisedFedFunds Min. :0.0000 1st Qu.:0.0000 Median :1.0000 Mean :0.5026 3rd Qu.:1.0000 Max. :1.0000  table(fedFunds$RaisedFedFunds)  0 1 291 294  294 / (291 + 294) [1] 0.5025641  Problem 2 - The Longest-Serving Fed Chair Which Fed Reserve Chair has presided over the most interest rate decisions?\ntable(fedFunds$Chairman)  Bernanke, Ben Burns, Arthur Greenspan, Alan 96 96 221 Martin, William M. Miller, G. William N/A 48 17 2 Volcker, Paul Yellen, Janet 96 9  Greenspan, Alan   Problem 3 - Converting Variables to Factors Convert the following variables to factors using the as.factor function:\n Chairman DemocraticPres RaisedFedFunds  Which of the following methods requires the dependent variables be stored as a factor variable when training a model for classification?\nstr(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : int 1 1 1 1 1 1 1 1 1 1 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : int 1 1 1 1 1 1 1 0 1 1 ... fedFunds$Chairman \u0026lt;- as.factor(fedFunds$Chairman) fedFunds$DemocraticPres \u0026lt;- as.factor(fedFunds$DemocraticPres) fedFunds$RaisedFedFunds \u0026lt;- as.factor(fedFunds$RaisedFedFunds) str(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 1 2 2 ... Random forest (randomForest)   Problem 4 - Splitting the dataframe into a Training \u0026amp; Testing Set Obtain a random training/testing set split with:\nset.seed(201) library(caTools) spl \u0026lt;- sample.split(fedFunds$RaisedFedFunds, 0.7) Split months into a training dataframe called ‚Äútraining‚Äù using the observations for which spl is TRUE and a testing dataframe called ‚Äútesting‚Äù using the observations for which spl is FALSE.\ntraining \u0026lt;- subset(fedFunds, spl == TRUE) testing \u0026lt;- subset(fedFunds, spl == FALSE) Why do we use the sample.split() function to split into a training and testing set? #### It balances the dependent variable between the training and testing sets\n Problem 5 - Training a Logistic Regression Model Train a logistic regression model using independent variables ‚ÄúPreviousRate‚Äù, ‚ÄúStreak‚Äù, ‚ÄúUnemployment‚Äù, ‚ÄúHomeownershipRate‚Äù, ‚ÄúDemocraticPres‚Äù, and ‚ÄúMonthsUntilElection‚Äù, using the training set to obtain the model.\nLogIntRate \u0026lt;- glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, family = binomial) Which of the following characteristics is the most statistically significant associated with an increased chance of the fed funds rate being raised?\nsummary(LogIntRate)  Call: glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, family = binomial, data = training) Deviance Residuals: Min 1Q Median 3Q Max -2.8177 -1.0121 0.2301 1.0491 2.5297 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.121012 5.155774 1.769 0.0769 . PreviousRate -0.003427 0.032350 -0.106 0.9156 Streak 0.157658 0.025147 6.270 3.62e-10 *** Unemployment -0.047449 0.065438 -0.725 0.4684 HomeownershipRate -0.136451 0.076872 -1.775 0.0759 . DemocraticPres1 0.347829 0.233200 1.492 0.1358 MonthsUntilElection -0.006931 0.007678 -0.903 0.3666 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 568.37 on 409 degrees of freedom Residual deviance: 492.69 on 403 degrees of freedom AIC: 506.69 Number of Fisher Scoring iterations: 4 A longer consecutive STREAK of months in which the fed funds rate was raised   Problem 6 - Predicting Using a Logistic Regression Model Imagine you are an analyst at a bank and your manager has asked you to predict whether the fed funds rate will be raised next month.\nYou know that the rate has been lowered for 3 straight months (Streak = -3) and that the previous month‚Äôs rate was 1.7%.\nThe unemployment rate is 5.1% and the homeownership rate is 65.3%.\nThe current U.S. president is a Republican and the next election will be held in 18 months. According to the logistic regression model you built in Problem 5.\nWhat is the predicted probability that the interest rate will be raised?\n9.121012 + PreviousRate(-0.003427) + Streak0.157658 + Unemployment(-0.047449) + HomeownershipRate(-0.136451) + DemocraticPres10.347829 + MonthsUntilElection(-0.006931) 9.121012 + 1.7*(-0.003427) - 3*0.157658 + 5.1*(-0.047449) + 65.3*(-0.136451) + 0*0.347829 + 18*(-0.006931) [1] -0.6347861  -0.6347861 ==\u0026gt; Need to plug it into the logistic response function problem6 \u0026lt;- training[1, ] problem6$PreviousRate \u0026lt;- 1.7 problem6$Streak \u0026lt;- -3 problem6$Unemployment \u0026lt;- 5.1 problem6$HomeownershipRate \u0026lt;- 65.3 problem6$DemocraticPres \u0026lt;- as.factor(0) problem6$MonthsUntilElection \u0026lt;- 18 problem6  Date Chairman PreviousRate Streak GDP Unemployment 1 1966-02-01 Martin, William M. 1.7 -3 4201.891 5.1 CPI HomeownershipRate DebtAsPctGDP DemocraticPres MonthsUntilElection 1 31.88 65.3 40.26076 0 18 RaisedFedFunds 1 1 str(problem6) \u0026#39;data.frame\u0026#39;: 1 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 $ PreviousRate : num 1.7 $ Streak : num -3 $ GDP : num 4202 $ Unemployment : num 5.1 $ CPI : num 31.9 $ HomeownershipRate : num 65.3 $ DebtAsPctGDP : num 40.3 $ DemocraticPres : Factor w/ 1 level \u0026quot;0\u0026quot;: 1 $ MonthsUntilElection: num 18 $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 problem6PredProb \u0026lt;- predict(LogIntRate, newdata = problem6, type = \u0026quot;response\u0026quot;) problem6PredProb  1 0.3464297    Problem 7 - Interpreting Model Coefficients What is the meaning of the coefficient labeled ‚ÄúDemocraticPres1‚Äù in the logistic regression summary output?\nsummary(LogIntRate)  Call: glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, family = binomial, data = training) Deviance Residuals: Min 1Q Median 3Q Max -2.8177 -1.0121 0.2301 1.0491 2.5297 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.121012 5.155774 1.769 0.0769 . PreviousRate -0.003427 0.032350 -0.106 0.9156 Streak 0.157658 0.025147 6.270 3.62e-10 *** Unemployment -0.047449 0.065438 -0.725 0.4684 HomeownershipRate -0.136451 0.076872 -1.775 0.0759 . DemocraticPres1 0.347829 0.233200 1.492 0.1358 MonthsUntilElection -0.006931 0.007678 -0.903 0.3666 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 568.37 on 409 degrees of freedom Residual deviance: 492.69 on 403 degrees of freedom AIC: 506.69 Number of Fisher Scoring iterations: 4 When the president is Democratic, the odds of the fed funds rate increasing are 41.6% higher than in an otherise identical month (i.e.¬†identical among the variables in the model). EXPLANATION: The coefficients of the model are the log odds associated with that variable; so we see that the odds of being sold are exp(0.347829)=1.41599 those of an otherwise identical month. This means the month is predicted to have 41.6% higher odds of being sold.\n  Problem 8 - Obtaining Test Set Predictions Using our logistic regression model, obtain predictions on the test-set. Then, using a probability threshold of 0.5, create a confusion matrix for the test-set.\nOn how many test-set observations does our logistic regression model make a different prediction than the prediction the naive baseline model would make?\n(Remember that the naive baseline model we use always predicts the most frequent outcome in the training set for all observations in the test-set.)\nstr(testing) \u0026#39;data.frame\u0026#39;: 175 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 14 15 16 18 19 31 32 37 38 39 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 5 4.53 4.05 3.98 3.79 6.02 6.03 6.3 6.61 6.79 ... $ Streak : int 1 -1 -2 1 -1 -2 1 2 3 4 ... $ GDP : num 4325 4325 4329 4329 4366 ... $ Unemployment : num 3.8 3.8 3.8 3.9 3.8 3.7 3.5 3.4 3.4 3.4 ... $ CPI : num 33 33 33.1 33.3 33.4 34.9 35 35.7 35.8 36.1 ... $ HomeownershipRate : num 63.3 63.3 63.9 63.9 63.8 64.1 64.1 64.1 64.1 64.1 ... $ DebtAsPctGDP : num 4324.9 4324.9 37.9 4328.7 38.8 ... $ DemocraticPres : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 1 1 1 ... $ MonthsUntilElection: int 20 19 18 16 15 3 2 45 44 43 ... $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 1 1 1 1 2 2 1 2 2 2 ... PredProb \u0026lt;- predict(LogIntRate, newdata = testing, type = \u0026quot;response\u0026quot;) table(testing$RaisedFedFunds, PredProb \u0026gt;= 0.5)  FALSE TRUE 0 60 27 1 31 57 table(training$RaisedFedFunds)  0 1 204 206  91 (60 + 31 were predicted less than 0.5)   Problem 9 - Computing Test-Set AUC What is the test-set AUC of the logistic regression model?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess PredTestLogROCR \u0026lt;- prediction(PredProb, testing$RaisedFedFunds) performance(PredTestLogROCR, \u0026quot;auc\u0026quot;)@y.values [[1]] [1] 0.704023  Problem 10 - Interpreting AUC What is the meaning of the AUC? #### The proportion of the time the model can differentiate between a randomly selected month during which the fed funds were raised and a randomly selected month during which the federal funds were not raised.\n Problem 11 - ROC Curves Which logistic regression threshold is associated with the upper-right corner of the ROC plot (true positive rate 1 and false positive rate 1)? #### 0\nEXPLANATION A model with threshold 0 predicts 1 for all observations, yielding a 100% true positive rate and a 100% false positive rate.\n  Problem 12 - ROC Curves Plot the colorized ROC curve for the logistic regression model‚Äôs performance on the test-set. At roughly which logistic regression cut-off does the model achieve a true positive rate of 85% and a false positive rate of 60%?\nROCRperf \u0026lt;- performance(PredTestLogROCR, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;) plot(ROCRperf, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7)) 0.37   Problem 13 - Cross-Validation to Select Parameters Which of the following best describes how 10-fold cross-validation works when selecting between 2 different parameter values? #### 20 models are trained on subsets of the training set and evaluated on a portion of the training set\n Problem 14 - Cross-Validation for a CART Model Set the random seed to 201 (even though you have already done so earlier in the problem).\nThen use the caret package and the train function to perform 10-fold cv with the training data set to select the best cp value for a CART model that predicts the dependent variable ‚ÄúRaisedFedFunds‚Äù using the independent variables ‚ÄúPreviousRate,‚Äù ‚ÄúStreak,‚Äù ‚ÄúUnemployment,‚Äù ‚ÄúHomeownershipRate,‚Äù ‚ÄúDemocraticPres,‚Äù and ‚ÄúMonthsUntilElection.‚Äù Select the cp value from a grid consisting of the 50 values 0.001, 0.002, ‚Ä¶, 0.05.\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 library(e1071) set.seed(201) # define cross-validation experiment numFolds \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;, number = 10) cpGrid \u0026lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001))  Define cv experiment\nnumFolds \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;, number = 10) cpGrid \u0026lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001))  Perform the cv\ntrainCV \u0026lt;- train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = \u0026quot;rpart\u0026quot;, trControl = numFolds, tuneGrid = cpGrid) trainCV CART 410 samples 6 predictor 2 classes: \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 369, 368, 368, 369, 370, 370, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.001 0.6248461 0.2498288 0.002 0.6366928 0.2737781 0.003 0.6465099 0.2940348 0.004 0.6465099 0.2940348 0.005 0.6465099 0.2940348 0.006 0.6513298 0.3037278 0.007 0.6513298 0.3037278 0.008 0.6488298 0.2987278 0.009 0.6462108 0.2934897 0.010 0.6437718 0.2887290 0.011 0.6532956 0.3075752 0.012 0.6532956 0.3075752 0.013 0.6532956 0.3075752 0.014 0.6386527 0.2782894 0.015 0.6386527 0.2782894 0.016 0.6386527 0.2782894 0.017 0.6287718 0.2585275 0.018 0.6287718 0.2585275 0.019 0.6287718 0.2585275 0.020 0.6385918 0.2780499 0.021 0.6385918 0.2780499 0.022 0.6385918 0.2784431 0.023 0.6385918 0.2784431 0.024 0.6432956 0.2882859 0.025 0.6432956 0.2882859 0.026 0.6605575 0.3228097 0.027 0.6605575 0.3228097 0.028 0.6680575 0.3378097 0.029 0.6680575 0.3378097 0.030 0.6680575 0.3381290 0.031 0.6680575 0.3381290 0.032 0.6680575 0.3381290 0.033 0.6680575 0.3381290 0.034 0.6680575 0.3381290 0.035 0.6680575 0.3381290 0.036 0.6729355 0.3474661 0.037 0.6729355 0.3474661 0.038 0.6729355 0.3474661 0.039 0.6729355 0.3474661 0.040 0.6729355 0.3474661 0.041 0.6729355 0.3474661 0.042 0.6729355 0.3474661 0.043 0.6729355 0.3474661 0.044 0.6729355 0.3474661 0.045 0.6729355 0.3474661 0.046 0.6729355 0.3474661 0.047 0.6729355 0.3474661 0.048 0.6729355 0.3474661 0.049 0.6729355 0.3474661 0.050 0.6729355 0.3474661 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.05. What cp value maximizes the cv accuracy? #### 0.016\n Problem 15 - Train CART Model Create and plot the CART model trained with the parameter identified in Problem 14, again predicting the dependent variable using ‚ÄúPreviousRate‚Äù, ‚ÄúStreak‚Äù, ‚ÄúUnemployment‚Äù, ‚ÄúHomeownershipRate‚Äù, ‚ÄúDemocraticPres‚Äù, and ‚ÄúMonthsUntilElection‚Äù.\nWhat variable is used as the first (upper-most) split in the tree?\nlibrary(rpart) library(rpart.plot) TreeIntRate \u0026lt;- trainCV$finalModel prp(TreeIntRate) TreeIntRate n= 410 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 410 204 1 (0.4975610 0.5024390) 2) Streak\u0026lt; 2.5 300 115 0 (0.6166667 0.3833333) * 3) Streak\u0026gt;=2.5 110 19 1 (0.1727273 0.8272727) * TreeIntRate2 \u0026lt;- rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = \u0026quot;class\u0026quot;, cp = 0.016) prp(TreeIntRate2) Streak   Problem 16 - Predicting Using a CART Model If you were to use the CART model you created in Problem 15 to explore the question asked of the analyst in Problem 6, what would you predict for next month?\nRemember: The rate has been lowered for 3 straight months (Streak = -3). The previous month‚Äôs rate was 1.7%. The unemployment rate is 5.1%. The homeownership rate is 65.3%. The current U.S. president is a Republican and the next election will be held in 18 months. #### The Fed will not raise the federal funds rate. The Fed will not raise the fed funds rate.\n Problem 17 - Test-Set Accuracy for CART Model Using the CART model you created in Problem 15, obtain predictions on the test-set (using the parameter type=‚Äúclass‚Äù with the predict function).\nThen, create a confusion matrix for the test-set.\nPredClassTree \u0026lt;- predict(TreeIntRate2, newdata = testing, type = \u0026quot;class\u0026quot;) What is the accuracy of your CART model?\ntable(PredClassTree, testing$RaisedFedFunds)  PredClassTree 0 1 0 64 40 1 23 48 (64 + 48) / nrow(testing) [1] 0.64  ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"33bd0eebb3741850783dce947bc6ba4a","permalink":"/project/interest_forecast/interest_forecast/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/interest_forecast/interest_forecast/","section":"project","summary":"Predict when the Fed will raise interest rates","tags":["R","Data Analytics","Machine Learning"],"title":"Forecasting U.S. Federal Reserve Interest Rate Hikes","type":"project"},{"authors":null,"categories":null,"content":" The use of coal in the United States peaked in 2005, and since then has decreased by 25%, being replaced by renewable energy sources and more efficient use (Lovins, 2014). As the US pursues a portfolio of more diverse, sustainable and secure energy sources, there are many questions to consider.\nWhat are effective factors in incentivizing states to adopt more environmentally friendly energy generation methods? How do these factors vary by state? How can we direct resources to different places in the country and ensure that they effectively drive renewable energy sources adoption?  To derive insights and explore these questions, I‚Äôll take a combination of generation, usage, and greenhouse emission data by state and combine it with macro-economic and political information.\nFor this analysis, I‚Äôve gathered data from various sources to include the following information for each state within the U.S. for the years spanning year 2000 to year 2013. The aggregated dataset energy.csv.xz results in a total of 27 variables and 699 observations. Each observation contains one record per state per year. Here‚Äôs a detailed description of the variables:\n GenTotal: Annual generation of energy using all types of energy sources (coal, nuclear, hydroelectric, solar, etc.) normalized by the state population at a given year. GenTotalRenewable: Annual generation of energy using all renewable energy sources normalized by the state population at a given year. GenHydro, GenSolar: Annual generation of energy using each type of energy source as a percent of the total energy generation. GenTotalRenewableBinary, GenSolarBinary: 1 if generation from solar or other renewable energy sources increased between a year n and a year n+1. 0 if it did not increase. AllSourcesCO2, AllSourcesSO2 and AllSourcesNOx: Annual emissions per state in metric tons, normalized by the respective state population at a given year and caused by all energy generation sources. EPriceResidential, EPriceCommercial, EPriceIndustrial, EPriceTransportation, EPriceTotal: Average electricity price per state, per sector (residential, industrial, commercial, etc.) ESalesResidential, ESalesCommercial, ESalesIndustrial, ESalesTransportation, ESalesTotal: Annual normalized sales of electricity per state, per sector. CumlRegulatory, CumlFinancial: Number of energy-related financial incentives and regulations created by a state per year. Demographic data such as annual wages per capita and presidential results (0 if a state voted republican, 1 is democrat).  Problem 1 - Total Renewable Energy Generation Load energy.csv into a data frame called energy.\nenergy \u0026lt;- read.csv(\u0026quot;energy.csv.xz\u0026quot;) str(energy) \u0026#39;data.frame\u0026#39;: 699 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ YEAR : int 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 ... $ GenTotal : num 9.81 10.64 10.54 9.79 9.9 ... $ GenHydro : num 0.163 0.2 0.213 0.25 0.23 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.163 0.2 0.214 0.25 0.231 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 1 0 0 0 1 0 1 1 ... $ AllSourcesCO2 : num 7.26 7.31 6.94 6.15 7.25 ... $ AllSourcesSO2 : num 0.0224 0.01193 0.01148 0.00675 0.00659 ... $ AllSourcesNOx : num 0.0289 0.0277 0.0294 0.0242 0.0377 ... $ EPriceResidential : num 11.4 12.1 12.1 12 12.4 ... $ EPriceCommercial : num 9.77 10.29 10.13 10.49 10.99 ... $ EPriceIndustrial : num 7.56 7.61 7.65 7.86 8.33 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.1 10.5 10.5 10.5 11 ... $ EsalesResidential : num 0.349 0.347 0.354 0.357 0.356 ... $ EsalesCommercial : num 0.421 0.42 0.41 0.444 0.449 ... $ EsalesIndustrial : num 0.195 0.198 0.199 0.198 0.195 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0343 0.0356 0.0379 0 0 ... $ EsalesTotal : num 8.46 8.61 8.51 8.59 8.78 ... $ CumlFinancial : int 1 1 1 1 1 2 7 7 10 10 ... $ CumlRegulatory : int 1 1 1 1 1 1 2 2 2 3 ... $ Total.salary : num 17.6 18.7 19.6 20.4 21.4 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... summary(energy)  STATE YEAR GenTotal GenHydro AK : 14 Min. :2000 Min. : 4.591 Min. :0.00000 AL : 14 1st Qu.:2003 1st Qu.:10.037 1st Qu.:0.00888 AR : 14 Median :2006 Median :14.647 Median :0.02291 AZ : 14 Mean :2006 Mean :16.956 Mean :0.09853 CA : 14 3rd Qu.:2010 3rd Qu.:18.018 3rd Qu.:0.07041 CO : 14 Max. :2013 Max. :92.097 Max. :0.92076 (Other):615 GenSolar GenTotalRenewable GenSolarBinary Min. :-1.300e-08 Min. :0.00000 Min. :0.0000 1st Qu.: 0.000e+00 1st Qu.:0.02027 1st Qu.:0.0000 Median : 0.000e+00 Median :0.04475 Median :0.0000 Mean : 3.344e-04 Mean :0.12245 Mean :0.2318 3rd Qu.: 0.000e+00 3rd Qu.:0.10771 3rd Qu.:0.0000 Max. : 2.045e-02 Max. :0.92076 Max. :1.0000 GenTotalRenewableBinary AllSourcesCO2 AllSourcesSO2 Min. :0.0000 Min. : 0.01059 Min. :0.00003 1st Qu.:0.0000 1st Qu.: 4.84645 1st Qu.:0.00925 Median :1.0000 Median : 8.34005 Median :0.02375 Mean :0.5923 Mean :11.61430 Mean :0.03687 3rd Qu.:1.0000 3rd Qu.:12.99889 3rd Qu.:0.04197 Max. :1.0000 Max. :93.96429 Max. :0.34568 NA\u0026#39;s :50 NA\u0026#39;s :50 AllSourcesNOx EPriceResidential EPriceCommercial EPriceIndustrial Min. :0.00067 Min. : 5.130 Min. : 4.240 Min. : 3.010 1st Qu.:0.00604 1st Qu.: 7.965 1st Qu.: 6.760 1st Qu.: 4.695 Median :0.01428 Median : 9.490 Median : 8.100 Median : 5.820 Mean :0.02058 Mean :10.424 Mean : 8.969 Mean : 6.660 3rd Qu.:0.02288 3rd Qu.:11.825 3rd Qu.:10.065 3rd Qu.: 7.500 Max. :0.35610 Max. :37.340 Max. :34.880 Max. :30.820 NA\u0026#39;s :50 EPriceTransportation EPriceTotal EsalesResidential EsalesCommercial Min. : 0.000 Min. : 4.170 Min. :0.1594 Min. :0.1690 1st Qu.: 0.000 1st Qu.: 6.515 1st Qu.:0.3297 1st Qu.:0.2816 Median : 1.615 Median : 7.860 Median :0.3563 Median :0.3385 Mean : 4.131 Mean : 8.809 Mean :0.3595 Mean :0.3411 3rd Qu.: 7.940 3rd Qu.:10.095 3rd Qu.:0.3926 3rd Qu.:0.3877 Max. :15.440 Max. :34.040 Max. :0.5287 Max. :0.5381 NA\u0026#39;s :1 EsalesIndustrial EsalesTransportation EsalesOther EsalesTotal Min. :0.06372 Min. :0.0000000 Min. :0.0000 Min. : 6.745 1st Qu.:0.21331 1st Qu.:0.0000000 1st Qu.:0.0000 1st Qu.:10.336 Median :0.29978 Median :0.0000000 Median :0.0092 Median :13.153 Mean :0.29232 Mean :0.0010349 Mean :0.0170 Mean :13.145 3rd Qu.:0.35791 3rd Qu.:0.0002786 3rd Qu.:0.0216 3rd Qu.:15.353 Max. :0.59561 Max. :0.0229279 Max. :0.1074 Max. :31.336 NA\u0026#39;s :449 CumlFinancial CumlRegulatory Total.salary presidential.results Min. : 0.00 Min. : 0.000 Min. :10.65 Min. :0.0000 1st Qu.: 2.00 1st Qu.: 3.000 1st Qu.:16.84 1st Qu.:0.0000 Median : 8.00 Median : 6.000 Median :19.51 Median :0.0000 Mean : 17.58 Mean : 6.838 Mean :20.01 Mean :0.4578 3rd Qu.: 23.50 3rd Qu.:10.000 3rd Qu.:22.83 3rd Qu.:1.0000 Max. :154.00 Max. :42.000 Max. :33.85 Max. :1.0000 Import Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.3433 3rd Qu.:1.0000 Max. :1.0000  Renewable energy sources are considered to include geothermal, hydroelectric, biomass, solar and wind.\nWhich state in the US seems to have the highest total generation of energy from renewable sources (using the variable GenTotalRenewable)?\nenergy[which.max(energy$GenTotalRenewable), ]  STATE YEAR GenTotal GenHydro GenSolar GenTotalRenewable 169 ID 2000 9.165016 0.9207631 0 0.9207631 GenSolarBinary GenTotalRenewableBinary AllSourcesCO2 AllSourcesSO2 169 0 0 0.6368657 0.004590802 AllSourcesNOx EPriceResidential EPriceCommercial EPriceIndustrial 169 0.002714775 5.39 4.24 3.11 EPriceTransportation EPriceTotal EsalesResidential EsalesCommercial 169 0 4.17 0.3068433 0.3095307 EsalesIndustrial EsalesTransportation EsalesOther EsalesTotal 169 0.368208 0 0.01541804 17.57071 CumlFinancial CumlRegulatory Total.salary presidential.results Import 169 2 2 12.81243 0 1 Idaho (ID) Which year did the above state produce the highest energy generation from renewable resources? #### 2000\n  Problem 2 - Relationship Between Politics and Greenhouse Emissions What is the average CO2 emissions from all sources of energy for: Note Using na.rm = TRUE in our calculations!\n states during years in which they voted republican?  mean(subset(energy, presidential.results == 0)$AllSourcesCO2, na.rm = TRUE) [1] 16.44296  states during years in which they voted democrat?  mean(subset(energy, presidential.results == 1)$AllSourcesCO2, na.rm = TRUE) [1] 5.783781 States that voted democrat have on average higher NOx emissions than states that voted republican across all years.\nIs this statement true or false?\nmean(subset(energy, presidential.results == 0)$AllSourcesNOx, na.rm = TRUE) [1] 0.02985461 mean(subset(energy, presidential.results == 1)$AllSourcesNOx, na.rm = TRUE) [1] 0.009377028 False   Problem 3 - Relationship Between Greenhouse Emissions and Energy Sales What is the correlation between overall CO2 emissions and energy sales made to industrial facilities?\nNote that the variables AllSourcesCO2 and EsalesIndustrial contain NAs. Using the parameter: use=‚Äúcomplete‚Äù to handle NAs in this question!\ncor(energy$AllSourcesCO2, energy$EsalesIndustrial, use = \u0026quot;complete\u0026quot;) [1] 0.5385867 Choose the correct answers from the following statements:\ncor(energy$AllSourcesSO2, energy$EsalesIndustrial, use = \u0026quot;complete\u0026quot;) [1] 0.4812317 cor(energy$AllSourcesNOx, energy$EsalesResidential, use = \u0026quot;complete\u0026quot;) [1] -0.5038829 cor(energy$AllSourcesCO2, energy$EsalesCommercial, use = \u0026quot;complete\u0026quot;) [1] -0.373383 Overall SO2 emissions are likely higher with increased industrial energy sales   Problem 4 - Boxplot of Energy Prices per State Creating a boxplot of the total energy price (EPriceTotal) by State across the data, and a table summarizing the mean of EPriceTotal by State.\npriceBoxplot \u0026lt;- ggplot(energy, aes(STATE, EPriceTotal)) priceBoxplot + geom_boxplot() What observations do we make? #### - The boxplot shows a clear outlier, the state of Hawaii, with much higher energy price compared to the rest of the U.S. #### - When looking at the average energy prices, there seems to be three price tiers ($5-$9, $10-$14, and $20+)\nWhich state has the lowest average energy price of all? Using a table to explore this question.\nsum(is.na(energy$EPriceTotal)) [1] 0 mean(energy$EPriceTotal) [1] 8.809213 avgPriceByState \u0026lt;- aggregate(EPriceTotal ~ STATE, energy, mean) avgPriceByState[which.min(avgPriceByState$EPriceTotal), ]  STATE EPriceTotal 50 WY 5.51 Wyoming (WY) Is this state associated with the highest mean total energy generation (GenTotal)?\navgGenByState \u0026lt;- aggregate(GenTotal ~ STATE, energy, mean) avgGenByState  STATE GenTotal 1 AK 9.810218 2 AL 30.582000 3 AR 19.090133 4 AZ 17.211604 5 CA 5.576285 6 CO 10.367634 7 CT 9.358409 8 DE 8.256761 9 FL 12.027089 10 GA 13.969187 11 HI 8.623563 12 IA 16.270149 13 ID 8.358471 14 IL 15.193858 15 IN 19.664150 16 KS 16.732968 17 KY 22.399999 18 LA 21.384516 19 MA 6.434295 20 MD 8.352530 21 ME 13.148792 22 MI 11.201083 23 MN 10.199283 24 MO 15.062760 25 MS 16.377409 26 MT 28.942351 27 NC 13.915444 28 ND 49.909462 29 NE 18.470352 30 NH 15.671056 31 NJ 7.108700 32 NM 18.130563 33 NV 14.311625 34 NY 7.197383 35 OH 12.664461 36 OK 18.787146 37 OR 14.537263 38 PA 17.326080 39 RI 6.488628 40 SC 22.476570 41 SD 10.719863 42 TN 14.751132 43 TX 17.020082 44 UT 15.587630 45 VA 9.720367 46 VT 10.177005 47 WA 16.426394 48 WI 11.079302 49 WV 47.417141 50 WY 88.393083 avgGenByState[which.max(avgGenByState$GenTotal), ]  STATE GenTotal 50 WY 88.39308  True   Problem 5 - Prediction Model for Solar Generation We are interested in predicting whether states are going to increase their solar energy generation over the next year.\nLet‚Äôs subset our dataset into a training and a testing set by using the following code:\nset.seed(144) spl \u0026lt;- sample(1 : nrow(energy), size = 0.7 * nrow(energy)) train \u0026lt;- energy[spl, ] test \u0026lt;- energy[-spl, ] Let‚Äôs create a logistic regression model ‚Äúmod‚Äù using the train set to predict the binary variable GenSolarBinary.\nTo do so, consider the following as potential predictive variables: GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary, Import.\nmod \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train, family = binomial) Which variable is most predictive in the model?\nsummary(mod)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -4.0165 -0.4374 -0.2398 -0.0719 2.6342 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -6.345e+00 9.148e-01 -6.935 4.05e-12 *** GenHydro -3.652e+00 1.114e+00 -3.279 0.00104 ** GenSolar 1.076e+03 3.322e+02 3.238 0.00121 ** CumlFinancial 2.814e-02 9.563e-03 2.943 0.00325 ** CumlRegulatory 1.996e-01 5.045e-02 3.955 7.65e-05 *** Total.salary 1.393e-01 4.465e-02 3.119 0.00181 ** Import -3.142e-01 3.447e-01 -0.912 0.36190 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 528.69 on 488 degrees of freedom Residual deviance: 282.82 on 482 degrees of freedom AIC: 296.82 Number of Fisher Scoring iterations: 7 CumlRegulatory   Problem 6 - Performance on the Test Set Computing the predictions on the test-set. Using a threshold of 0.5, what is the accuracy of our model on the test-set?\npredTest \u0026lt;- predict(mod, newdata = test, type = \u0026quot;response\u0026quot;) table(test$GenSolarBinary, predTest \u0026gt; 0.5)  FALSE TRUE 0 154 7 1 31 18 (154 + 18) / nrow(test) [1] 0.8190476 Cool!\nWhat is the accuracy for states voting republican?\ntestWithPred \u0026lt;- test testWithPred$predTest \u0026lt;- predTest str(testWithPred) \u0026#39;data.frame\u0026#39;: 210 obs. of 28 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... $ predTest : num 0.01417 0.0541 0.07459 0.15572 0.00879 ... testWithPredRep \u0026lt;- subset(testWithPred, presidential.results == 0) table(testWithPredRep$GenSolarBinary, testWithPredRep$predTest \u0026gt; 0.5)  FALSE TRUE 0 90 0 1 18 2 (90 + 2) / nrow(testWithPredRep) [1] 0.8363636 What is the accuracy for states voting democrat?\ntestWithPredDem \u0026lt;- subset(testWithPred, presidential.results == 1) table(testWithPredDem$GenSolarBinary, testWithPredDem$predTest \u0026gt; 0.5)  FALSE TRUE 0 64 7 1 13 16 (64 + 16) / nrow(testWithPredDem) [1] 0.8  Problem 7 - Clustering of the Observations We can perhaps improve our accuracy if we implement a cluster-the-predict approach. I‚Äôm interested in clustering the observations based on information about the regulatory and financial incentives, the elections outcome and the population wealth in each state across the years, in addition to whether the state was an energy importer or not.\nLet‚Äôs create a train.limited and test.limited datasets, where we only keep the variables CumlRegulatory, CumlFinancial, presidential.results, Total.salary, and Import.\nstr(train) \u0026#39;data.frame\u0026#39;: 489 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 3 36 37 25 16 32 2 37 4 41 ... $ YEAR : int 2006 2003 2002 2012 2010 2009 2011 2000 2011 2007 ... $ GenTotal : num 18.5 17.3 13.4 18.3 16.9 ... $ GenHydro : num 0.029722 0.029664 0.73065 0 0.000276 ... $ GenSolar : num 0 0 0 0 0 ... $ GenTotalRenewable : num 0.030363 0.030562 0.739667 0.000301 0.07246 ... $ GenSolarBinary : int 0 0 0 0 0 1 0 0 1 0 ... $ GenTotalRenewableBinary: int 1 1 0 0 1 1 0 0 0 0 ... $ AllSourcesCO2 : num 10.28 14.09 2.05 8.13 12.78 ... $ AllSourcesSO2 : num 0.0291 0.0316 0.004 0.0144 0.0144 ... $ AllSourcesNOx : num 0.01358 0.02472 0.00324 0.00786 0.01617 ... $ EPriceResidential : num 8.85 7.47 7.12 10.26 10.03 ... $ EPriceCommercial : num 6.96 6.38 6.59 9.33 8.25 ... $ EPriceIndustrial : num 5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 6.55 5.09 ... $ EPriceTransportation : num 0 0 6.5 0 0 0 0 6.68 0 0 ... $ EPriceTotal : num 6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 9.71 6.89 ... $ EsalesResidential : num 0.366 0.4 0.388 0.372 0.355 ... $ EsalesCommercial : num 0.248 0.336 0.329 0.281 0.382 ... $ EsalesIndustrial : num 0.386 0.264 0.272 0.347 0.263 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num NA 0 0.0111 NA NA ... $ EsalesTotal : num 16.6 14.4 12.9 16.2 14.2 ... $ CumlFinancial : int 4 1 12 19 3 23 15 5 42 5 ... $ CumlRegulatory : int 4 1 9 1 6 8 2 5 21 1 ... $ Total.salary : num 15.4 13.4 16.3 15.3 20.8 ... $ presidential.results : int 0 0 1 0 0 1 0 1 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 1 ... train.limited \u0026lt;- subset(train, select = CumlFinancial:Import) str(test) \u0026#39;data.frame\u0026#39;: 210 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... test.limited \u0026lt;- subset(test, select = CumlFinancial:Import) Using the ‚ÄúpreProcess‚Äù function on the train.limited set, I can compute the train.norm and test.norm.\npreprocTrain \u0026lt;- preProcess(train.limited) train.norm \u0026lt;- predict(preprocTrain, train.limited) preprocTest \u0026lt;- preProcess(test.limited) test.norm \u0026lt;- predict(preprocTest, test.limited) Why didn‚Äôt I include the dependent variable GenSolarBinary in this clustering phase? #### - Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the cluster-then-predict methodology\nLet‚Äôs use kmeans clustering for this problem with a seed of 144, k=2 and keep the maximum number of iterations at 1,000.\nset.seed(144) k \u0026lt;- 2 trainKMC \u0026lt;- kmeans(train.norm, centers = k, iter.max = 1000) set.seed(144) testKMC \u0026lt;- kmeans(test.norm, centers = k, iter.max = 1000) Using the flexclust package, identifying the clusters and call train1 the subset of train corresponding to the first cluster, and train2 the subset of train corresponding to the second cluster.\ntrainKMC.kcca \u0026lt;- as.kcca(trainKMC, train.norm) # clusters(trainKMC.kcca) trainClusters \u0026lt;- predict(trainKMC.kcca) table(trainClusters) trainClusters 1 2 308 181  train1 \u0026lt;- subset(train, trainClusters == 1) train2 \u0026lt;- subset(train, trainClusters == 2) str(train1) \u0026#39;data.frame\u0026#39;: 308 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 3 36 37 25 16 32 2 37 41 28 ... $ YEAR : int 2006 2003 2002 2012 2010 2009 2011 2000 2007 2009 ... $ GenTotal : num 18.5 17.3 13.4 18.3 16.9 ... $ GenHydro : num 0.029722 0.029664 0.73065 0 0.000276 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.030363 0.030562 0.739667 0.000301 0.07246 ... $ GenSolarBinary : int 0 0 0 0 0 1 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 0 0 1 1 0 0 0 1 ... $ AllSourcesCO2 : num 10.28 14.09 2.05 8.13 12.78 ... $ AllSourcesSO2 : num 0.0291 0.0316 0.004 0.0144 0.0144 ... $ AllSourcesNOx : num 0.01358 0.02472 0.00324 0.00786 0.01617 ... $ EPriceResidential : num 8.85 7.47 7.12 10.26 10.03 ... $ EPriceCommercial : num 6.96 6.38 6.59 9.33 8.25 ... $ EPriceIndustrial : num 5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 5.09 5.25 ... $ EPriceTransportation : num 0 0 6.5 0 0 0 0 6.68 0 0 ... $ EPriceTotal : num 6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 6.89 6.63 ... $ EsalesResidential : num 0.366 0.4 0.388 0.372 0.355 ... $ EsalesCommercial : num 0.248 0.336 0.329 0.281 0.382 ... $ EsalesIndustrial : num 0.386 0.264 0.272 0.347 0.263 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num NA 0 0.0111 NA NA ... $ EsalesTotal : num 16.6 14.4 12.9 16.2 14.2 ... $ CumlFinancial : int 4 1 12 19 3 23 15 5 5 8 ... $ CumlRegulatory : int 4 1 9 1 6 8 2 5 1 4 ... $ Total.salary : num 15.4 13.4 16.3 15.3 20.8 ... $ presidential.results : int 0 0 1 0 0 1 0 1 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 1 0 ... str(train2) \u0026#39;data.frame\u0026#39;: 181 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 4 9 8 19 14 8 30 10 48 22 ... $ YEAR : int 2011 2011 2002 2007 2006 2006 2010 2011 2004 2011 ... $ GenTotal : num 16.7 11.61 7.46 7.24 15.13 ... $ GenHydro : num 0.08485 0.00082 0 0.01694 0.0009 ... $ GenSolar : num 0.000771 0.000567 0 0 0 ... $ GenTotalRenewable : num 0.08841 0.01191 0 0.04075 0.00531 ... $ GenSolarBinary : int 1 1 0 1 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 0 0 0 1 1 1 1 0 0 1 ... $ AllSourcesCO2 : num 8.27 5.99 7.54 3.96 7.95 ... $ AllSourcesSO2 : num 0.00459 0.00592 0.03869 0.00787 0.0243 ... $ AllSourcesNOx : num 0.00815 0.00434 0.01426 0.00307 0.00962 ... $ EPriceResidential : num 11.08 11.51 8.7 16.23 8.42 ... $ EPriceCommercial : num 9.5 9.85 7.15 15.2 7.95 ... $ EPriceIndustrial : num 6.55 8.55 4.85 13.03 4.69 ... $ EPriceTransportation : num 0 8.81 0 9.24 5.59 0 0 7.94 0 8.53 ... $ EPriceTotal : num 9.71 10.61 6.91 15.16 7.07 ... $ EsalesResidential : num 0.441 0.517 0.335 0.352 0.326 ... $ EsalesCommercial : num 0.394 0.408 0.315 0.475 0.355 ... $ EsalesIndustrial : num 0.165 0.075 0.345 0.165 0.315 ... $ EsalesTransportation : num 0 0.00038 0 0.00705 0.00364 ... $ EsalesOther : num NA NA 0.00498 NA NA ... $ EsalesTotal : num 11.58 11.78 14.94 8.79 11.2 ... $ CumlFinancial : int 42 59 2 33 6 2 32 39 3 37 ... $ CumlRegulatory : int 21 14 4 12 6 7 9 7 7 12 ... $ Total.salary : num 22 20.7 21.3 28.9 22.7 ... $ presidential.results : int 0 1 1 1 1 1 1 0 1 1 ... $ Import : int 0 1 1 1 0 1 0 1 1 0 ... testKMC.kcca \u0026lt;- as.kcca(testKMC, test.norm) # clusters(testKMC.kcca) testClusters \u0026lt;- predict(testKMC.kcca) table(testClusters) testClusters 1 2 130 80  test1 \u0026lt;- subset(test, testClusters == 1) test2 \u0026lt;- subset(test, testClusters == 2) str(test1) \u0026#39;data.frame\u0026#39;: 130 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... str(test2) \u0026#39;data.frame\u0026#39;: 80 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 4 5 5 6 6 6 7 7 7 7 ... $ YEAR : int 2009 2004 2010 2008 2010 2013 2001 2003 2005 2007 ... $ GenTotal : num 17 5.48 5.48 10.84 9.95 ... $ GenHydro : num 0.0574 0.1753 0.1638 0.0382 0.0311 ... $ GenSolar : num 0.000126 0.002931 0.003769 0.000343 0.000838 ... $ GenTotalRenewable : num 0.058 0.2787 0.2711 0.0996 0.1012 ... $ GenSolarBinary : int 1 0 1 1 1 1 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 1 1 1 1 0 0 1 1 ... $ AllSourcesCO2 : num 8.12 1.69 1.49 8.45 7.95 ... $ AllSourcesSO2 : num 4.99e-03 6.28e-04 6.77e-05 1.11e-02 8.81e-03 ... $ AllSourcesNOx : num 0.00935 0.00276 0.00214 0.01272 0.01081 ... $ EPriceResidential : num 10.7 12.2 14.8 10.1 11 ... $ EPriceCommercial : num 9.35 11.64 13.09 8.57 9.13 ... $ EPriceIndustrial : num 6.65 9.27 9.8 6.65 6.9 ... $ EPriceTransportation : num 0 6.42 8.27 8.32 9.34 ... $ EPriceTotal : num 9.56 11.35 13.01 8.59 9.15 ... $ EsalesResidential : num 0.447 0.331 0.338 0.34 0.342 ... $ EsalesCommercial : num 0.4 0.472 0.469 0.394 0.37 ... $ EsalesIndustrial : num 0.153 0.194 0.191 0.265 0.287 ... $ EsalesTransportation : num 0 0.003572 0.003177 0.000932 0.000876 ... $ EsalesOther : num NA 0 NA NA NA ... $ EsalesTotal : num 11.15 7.09 6.94 10.58 10.39 ... $ CumlFinancial : int 33 17 138 30 58 70 3 3 4 10 ... $ CumlRegulatory : int 17 20 28 18 19 20 5 5 7 12 ... $ Total.salary : num 21.2 20.8 24 27 26.3 ... $ presidential.results : int 0 1 1 1 1 1 1 1 1 1 ... $ Import : int 0 1 1 0 1 1 1 1 0 1 ... Select the correct statement(s) below:\ntable(train1$presidential.results)  0 1 248 60  table(train2$presidential.results)  0 1 21 160  mean(train1$CumlFinancial) [1] 7.766234 mean(train2$CumlFinancial) [1] 34.87845 mean(train1$CumlRegulatory) [1] 3.983766 mean(train2$CumlRegulatory) [1] 12.28729 mean(train1$AllSourcesCO2, na.rm = TRUE) [1] 15.09418 mean(train2$AllSourcesCO2, na.rm = TRUE) [1] 5.841604 mean(train1$AllSourcesSO2, na.rm = TRUE) [1] 0.04923232 mean(train2$AllSourcesSO2, na.rm = TRUE) [1] 0.01533239 mean(train1$AllSourcesNOx, na.rm = TRUE) [1] 0.0282891 mean(train2$AllSourcesNOx, na.rm = TRUE) [1] 0.007871944 - On average, train1 contains more republican states than train2  - On average, train1 contains states that have recorded more CO2, SO2 and NOx emissions than train2   Problem 8 - Creating the Model on the First Cluster Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod1 using a logistic regression on train1.\nmod1 \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train1, family = binomial) What variable is most predictive?\nsummary(mod1)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train1) Deviance Residuals: Min 1Q Median 3Q Max -1.8654 -0.2715 -0.1692 -0.1194 2.8802 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -7.574e+00 1.945e+00 -3.893 9.89e-05 *** GenHydro -1.201e+00 2.082e+00 -0.577 0.56398 GenSolar 2.324e+04 1.104e+04 2.106 0.03524 * CumlFinancial 6.836e-02 2.159e-02 3.166 0.00154 ** CumlRegulatory 1.589e-01 1.145e-01 1.388 0.16521 Total.salary 1.549e-01 9.190e-02 1.686 0.09188 . Import -3.323e-01 9.628e-01 -0.345 0.72995 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 131.538 on 307 degrees of freedom Residual deviance: 90.376 on 301 degrees of freedom AIC: 104.38 Number of Fisher Scoring iterations: 7 CumlFinancial   Problem 9 - Evaluating the Model Obtained Using the First Cluster What is the accuracy on test1, the subset of test corresponding to the first cluster?\npredTest1 \u0026lt;- predict(mod1, newdata = test1, type = \u0026quot;response\u0026quot;) table(test1$GenSolarBinary, predTest1 \u0026gt; 0.5)  FALSE TRUE 0 114 1 1 11 4 (114 + 4) / nrow(test1) [1] 0.9076923 Wow!\nI‚Äôd like to know if mod1 gives us an edge over mod on the dataset test1.\nUsing mod, predicting GenSolarBinary for the observation in test1 and report the accuracy below:\npredTest1WithMod \u0026lt;- predict(mod, newdata = test1, type = \u0026quot;response\u0026quot;) table(test1$GenSolarBinary, predTest1WithMod \u0026gt; 0.5)  FALSE 0 115 1 15 115 / nrow(test1) [1] 0.8846154  Problem 10 - Creating the Model on the Second Cluster Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod2 using a logistic regression on train2.\nmod2 \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train2, family = binomial) Select the correct statement(s) below?\nsummary(mod2)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train2) Deviance Residuals: Min 1Q Median 3Q Max -3.6321 -0.7964 0.0083 0.8536 1.7143 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -4.14627 1.43969 -2.880 0.00398 ** GenHydro -3.88698 1.23196 -3.155 0.00160 ** GenSolar 951.59397 322.86360 2.947 0.00321 ** CumlFinancial 0.02077 0.01008 2.060 0.03939 * CumlRegulatory 0.17315 0.06541 2.647 0.00812 ** Total.salary 0.08472 0.05576 1.519 0.12869 Import -0.72189 0.40427 -1.786 0.07416 . --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 250.25 on 180 degrees of freedom Residual deviance: 173.65 on 174 degrees of freedom AIC: 187.65 Number of Fisher Scoring iterations: 7 summary(mod1)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train1) Deviance Residuals: Min 1Q Median 3Q Max -1.8654 -0.2715 -0.1692 -0.1194 2.8802 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -7.574e+00 1.945e+00 -3.893 9.89e-05 *** GenHydro -1.201e+00 2.082e+00 -0.577 0.56398 GenSolar 2.324e+04 1.104e+04 2.106 0.03524 * CumlFinancial 6.836e-02 2.159e-02 3.166 0.00154 ** CumlRegulatory 1.589e-01 1.145e-01 1.388 0.16521 Total.salary 1.549e-01 9.190e-02 1.686 0.09188 . Import -3.323e-01 9.628e-01 -0.345 0.72995 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 131.538 on 307 degrees of freedom Residual deviance: 90.376 on 301 degrees of freedom AIC: 104.38 Number of Fisher Scoring iterations: 7 - Unlike mod1, the number of regulatory policies is more predictive than the number of financial incentives in mod2   Problem 11 - Evaluating the Model Obtained Using the Second Cluster Using the threshold of 0.5, what is the accuracy on test2, the subset of test corresponding to the second cluster?\npredTest2 \u0026lt;- predict(mod2, newdata = test2, type = \u0026quot;response\u0026quot;) table(test2$GenSolarBinary, predTest2 \u0026gt; 0.5)  FALSE TRUE 0 40 6 1 14 20 (40 + 20) / nrow(test2) [1] 0.75 We would like to know if mod2 gives us an edge over mod on the dataset test2.\nUsing mod, predicting GenSolarBinary for the observation in test2 and report the accuracy below:\npredTest2WithMod \u0026lt;- predict(mod, newdata = test2, type = \u0026quot;response\u0026quot;) table(test2$GenSolarBinary, predTest2WithMod \u0026gt; 0.5)  FALSE TRUE 0 39 7 1 16 18 (39 + 18) / nrow(test2) [1] 0.7125  Problem 12 - Evaluating the Performance of the Cluster-the-Predict Algorithm To compute the overall test-set accuracy of the cluster-the-predict approach, I can combine all the test-set predictions into a single vector ‚ÄúAllPredictions‚Äù and all the true outcomes into a single vector ‚ÄúAllOutcomes‚Äù.\nAllPredictions \u0026lt;- c(predTest1, predTest2) AllOutcomes \u0026lt;- c(test1$GenSolarBinary, test2$GenSolarBinary) What is the overall accuracy on the test-set, using the cluster-then-predict approach, again using a threshold of 0.5?\ntable(AllOutcomes, AllPredictions \u0026gt; 0.5)  AllOutcomes FALSE TRUE 0 154 7 1 25 24 length(AllOutcomes) [1] 210 length(AllPredictions) [1] 210 (154 + 24) / length(AllOutcomes) [1] 0.847619  ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"60264bdb1439568f19dea01c11783261","permalink":"/project/energy_patterns/energy_patterns/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/energy_patterns/energy_patterns/","section":"project","summary":"Exploring renewable energies","tags":["R","Data Analytics","Machine Learning"],"title":"Patterns In Renewable Energy Generation","type":"project"},{"authors":null,"categories":null,"content":" Clustering can be used for market segmentation, the idea of dividing airline passengers into small, more similar groups, and then designing a marketing strategy specifically for each group. In this analysis, I‚Äôll see how this idea can be applied to retail consumer data.\nI‚Äôll use the dataset Households.csv.xz, which contains data collected over two years for a group of 2,500 households. Each row (observation) in our dataset represents a unique household. The dataset contains the following variables:\n NumVisits = the number of times the household visited the retailer AvgProdCount = the average number of products purchased per transaction AvgDiscount = the average discount per transaction from coupon usage (in %) - NOTE: Do not divide this value by 100! AvgSalesValue = the average sales value per transaction MorningPct = the percentage of visits in the morning (8am - 1:59pm) AfternoonPct = the percentage of visits in the afternoon (2pm - 7:59pm)  Note that some visits can occur outside of morning and afternoon hours. That is, visits from 8pm - 7:59am are possible.\nThis dataset was derived from source files provided by dunnhumby, a customer science company based in the UK.\nProblem 1 - Reading in the data Read the dataset Households.csv.xz into R.\nHouseHolds \u0026lt;- read.csv(\u0026quot;Households.csv.xz\u0026quot;) str(HouseHolds) \u0026#39;data.frame\u0026#39;: 2500 obs. of 6 variables: $ NumVisits : int 86 45 47 30 40 250 59 113 20 9 ... $ AvgProdCount : num 20.08 15.87 19.62 10.03 5.55 ... $ AvgDiscount : num 8.11 7.44 14.37 3.85 2.96 ... $ AvgSalesValue: num 50.4 43.4 56.5 40 19.5 ... $ MorningPct : num 46.51 8.89 14.89 13.33 2.5 ... $ AfternoonPct : num 51.2 60 76.6 56.7 67.5 ... summary(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. : 1.0 Min. : 1.186 Min. : 0.089 Min. : 2.388 1st Qu.: 39.0 1st Qu.: 6.123 1st Qu.: 3.006 1st Qu.: 18.329 Median : 79.0 Median : 8.979 Median : 4.865 Median : 27.417 Mean : 110.6 Mean :10.291 Mean : 5.713 Mean : 31.621 3rd Qu.: 142.2 3rd Qu.:13.116 3rd Qu.: 7.327 3rd Qu.: 40.546 Max. :1300.0 Max. :56.600 Max. :47.176 Max. :165.829 MorningPct AfternoonPct Min. : 0.00 Min. : 0.00 1st Qu.: 16.67 1st Qu.: 42.20 Median : 26.09 Median : 52.00 Mean : 28.73 Mean : 51.45 3rd Qu.: 37.17 3rd Qu.: 61.29 Max. :100.00 Max. :100.00  head(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct AfternoonPct 1 86 20.08140 8.105116 50.35070 46.511628 51.16279 2 45 15.86667 7.444222 43.42978 8.888889 60.00000 3 47 19.61702 14.365106 56.45128 14.893617 76.59574 4 30 10.03333 3.855000 40.00367 13.333333 56.66667 5 40 5.55000 2.958250 19.47650 2.500000 67.50000 6 250 7.16400 3.313360 23.98464 25.600000 61.20000 How many households have logged transactions at the retailer only in the morning?\nnrow(subset(HouseHolds, MorningPct == 100)) [1] 4 How many households have logged transactions at the retailer only in the afternoon?\nnrow(subset(HouseHolds, AfternoonPct == 100)) [1] 13  Problem 2 - Descriptive statistics Of the households that spend more than $150 per transaction on average, what is the minimum average discount per transaction?\nmin(subset(HouseHolds, AvgSalesValue \u0026gt; 150)$AvgDiscount) [1] 15.64607 Of the households who have an average discount per transaction greater than 25%, what is the minimum average sales value per transaction?\nmin(subset(HouseHolds, AvgDiscount \u0026gt; 25)$AvgSalesValue) [1] 50.1175 What proportion of households visited the retailer at least 300 times?\nnrow(subset(HouseHolds, NumVisits \u0026gt;= 300)) / nrow(HouseHolds) [1] 0.0592  Problem 3 - Importance of Normalizing When clustering data, its often important to normalize the variables so that they are all on the same scale.\nIf you clustered this dataset without normalizing, which variable would you expect to dominate in the distance calculations?\nsummary(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. : 1.0 Min. : 1.186 Min. : 0.089 Min. : 2.388 1st Qu.: 39.0 1st Qu.: 6.123 1st Qu.: 3.006 1st Qu.: 18.329 Median : 79.0 Median : 8.979 Median : 4.865 Median : 27.417 Mean : 110.6 Mean :10.291 Mean : 5.713 Mean : 31.621 3rd Qu.: 142.2 3rd Qu.:13.116 3rd Qu.: 7.327 3rd Qu.: 40.546 Max. :1300.0 Max. :56.600 Max. :47.176 Max. :165.829 MorningPct AfternoonPct Min. : 0.00 Min. : 0.00 1st Qu.: 16.67 1st Qu.: 42.20 Median : 26.09 Median : 52.00 Mean : 28.73 Mean : 51.45 3rd Qu.: 37.17 3rd Qu.: 61.29 Max. :100.00 Max. :100.00  NumVisits   Problem 4 - Normalizing the Data Normalize all of the variables in the dataset (Note that these codes assume that our dataset is called ‚ÄúHouseholds‚Äù, and create the normalized dataset ‚ÄúHouseholdsNorm‚Äù. You can change the names to anything you want by editing the codes.)\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 preproc \u0026lt;- preProcess(HouseHolds) HouseHoldsNorm \u0026lt;- predict(preproc, HouseHolds) (Remember that for each variable, the normalization process subtracts the mean and divides by the standard deviation. In our normalized dataset, all of the variables should have mean 0 and standard deviation 1.\nsummary(HouseHoldsNorm)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. :-0.9475 Min. :-1.5239 Min. :-1.4010 Min. :-1.5342 1st Qu.:-0.6190 1st Qu.:-0.6976 1st Qu.:-0.6743 1st Qu.:-0.6976 Median :-0.2731 Median :-0.2197 Median :-0.2112 Median :-0.2206 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 3rd Qu.: 0.2737 3rd Qu.: 0.4728 3rd Qu.: 0.4021 3rd Qu.: 0.4684 Max. :10.2828 Max. : 7.7500 Max. :10.3293 Max. : 7.0432 MorningPct AfternoonPct Min. :-1.6779 Min. :-3.22843 1st Qu.:-0.7047 1st Qu.:-0.58006 Median :-0.1546 Median : 0.03458 Mean : 0.0000 Mean : 0.00000 3rd Qu.: 0.4926 3rd Qu.: 0.61755 Max. : 4.1617 Max. : 3.04658  What is the maximum value of NumVisits in the normalized dataset? #### 10.2828\nWhat is the minimum value of AfternoonPct in the normalized dataset? #### -3.22843\nCreate a dendrogram of our data:\nset.seed(200) distances \u0026lt;- dist(HouseHoldsNorm, method = \u0026quot;euclidean\u0026quot;) ClusterShoppers \u0026lt;- hclust(distances, method = \u0026quot;ward.D\u0026quot;) plot(ClusterShoppers, labels = FALSE)  Problem 5 - Interpreting the Dendrogram Based on the dendrogram, how many clusters do you think would be appropriate for this problem? #### 2, 3, 5\n Problem 6 - K-means Clustering Run the k-means clustering algorithm on our normalized dataset, selecting 10 clusters. Right before using the kmeans function, Remember ‚Äúset.seed(200)‚Äù.\nset.seed(200) k \u0026lt;- 10 KMC \u0026lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000) str(KMC) List of 9 $ cluster : int [1:2500] 7 3 1 3 5 6 1 1 3 8 ... $ centers : num [1:10, 1:6] -0.248 -0.483 -0.234 -0.18 -0.246 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:10] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:6] \u0026quot;NumVisits\u0026quot; \u0026quot;AvgProdCount\u0026quot; \u0026quot;AvgDiscount\u0026quot; \u0026quot;AvgSalesValue\u0026quot; ... $ totss : num 14994 $ withinss : num [1:10] 628 449 700 282 580 ... $ tot.withinss: num 4828 $ betweenss : num 10166 $ size : int [1:10] 246 51 490 118 504 226 141 284 52 388 $ iter : int 5 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; kmeansGroups \u0026lt;- KMC$cluster table(kmeansGroups) kmeansGroups 1 2 3 4 5 6 7 8 9 10 246 51 490 118 504 226 141 284 52 388  How many observations are in the smallest cluster?\nmin(table(kmeansGroups)) [1] 51 51 How many observations are in the largest cluster?\nmax(table(kmeansGroups)) [1] 504  504   Problem 7 - Understanding the Clusters Now, use the cluster assignments from k-means clustering together with the cluster centroids to explore the next few questions.\nkmeans1 \u0026lt;- subset(HouseHolds, kmeansGroups == 1) kmeans2 \u0026lt;- subset(HouseHolds, kmeansGroups == 2) kmeans3 \u0026lt;- subset(HouseHolds, kmeansGroups == 3) kmeans4 \u0026lt;- subset(HouseHolds, kmeansGroups == 4) kmeans5 \u0026lt;- subset(HouseHolds, kmeansGroups == 5) kmeans6 \u0026lt;- subset(HouseHolds, kmeansGroups == 6) kmeans7 \u0026lt;- subset(HouseHolds, kmeansGroups == 7) kmeans8 \u0026lt;- subset(HouseHolds, kmeansGroups == 8) kmeans9 \u0026lt;- subset(HouseHolds, kmeansGroups == 9) kmeans10 \u0026lt;- subset(HouseHolds, kmeansGroups == 10) colMeans(kmeans1)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 81.89431 19.11594 10.92924 59.49868 22.76746 AfternoonPct 61.93939  colMeans(kmeans2)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 54.70588 32.62351 19.65784 99.73684 32.15593 AfternoonPct 49.41508  colMeans(kmeans3)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 83.508163 12.081068 6.881078 37.391552 25.609449 AfternoonPct 51.185788  colMeans(kmeans4)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 89.788136 7.053082 3.877403 21.175564 71.391580 AfternoonPct 22.584436  colMeans(kmeans5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 82.18254 5.89608 2.90764 17.51880 19.36659 AfternoonPct 55.03936  colMeans(kmeans6)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 281.796460 8.117065 4.297144 25.446228 29.851517 AfternoonPct 51.583481  colMeans(kmeans7)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 99.886525 15.469831 9.343551 50.447122 53.462000 AfternoonPct 35.836861  colMeans(kmeans8)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 80.288732 9.992614 5.288399 29.327693 13.350751 AfternoonPct 74.066827  colMeans(kmeans9)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 626.903846 5.203533 2.632325 16.278150 24.641085 AfternoonPct 48.731981  colMeans(kmeans10)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 70.729381 6.479314 3.527893 19.688575 37.377204 AfternoonPct 38.916046  Which cluster best fits the description ‚Äúmorning shoppers stopping in to make a quick purchase‚Äù? #### Cluster 4\n Problem 8 - Understanding the Clusters Which cluster best fits the description ‚Äúshoppers with high average product count and high average value per visit‚Äù? #### Cluster 2\n Problem 9 - Understanding the Clusters Which cluster best fits the description ‚Äúfrequent shoppers with low value per visit‚Äù? #### Cluster 9\n Problem 10 - Random Behavior If we ran hierarchical clustering a second time without making any additional calls to set.seed, we would expect: #### Identical results to the first hierarchical clustering\nIf we ran k-means clustering a second time without making any additional calls to set.seed, we would expect: #### Different results from the first k-means clustering\nIf we ran k-means clustering a second time, again running the code set.seed(200) right before doing the clustering, we would expect: #### Identical results to the first k-means clustering\nIf we ran k-means clustering a second time, running the code set.seed(100) right before doing the clustering, we would expect: #### Different results from the first k-means clustering\n Problem 11 - The Number of Clusters Suppose the marketing department at the retail store decided that the 10 clusters were too specific, and they wanted more general clusters to describe the consumer base.\nWould they want to increase or decrease the number of clusters? #### Decrease the number of clusters\n Problem 12 - Increasing the Number of Clusters Run the k-means clustering algorithm again, this time selecting 5 clusters. Right before the ‚Äúkmeans‚Äù function, set the random seed to 5000.\nset.seed(5000) k \u0026lt;- 5 KMC.5 \u0026lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000) str(KMC.5) List of 9 $ cluster : int [1:2500] 5 5 1 5 2 2 5 5 5 2 ... $ centers : num [1:5, 1:6] -0.398 -0.193 -0.169 2.695 -0.176 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:5] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:6] \u0026quot;NumVisits\u0026quot; \u0026quot;AvgProdCount\u0026quot; \u0026quot;AvgDiscount\u0026quot; \u0026quot;AvgSalesValue\u0026quot; ... $ totss : num 14994 $ withinss : num [1:5] 1264 1838 1336 754 1604 $ tot.withinss: num 6795 $ betweenss : num 8199 $ size : int [1:5] 182 994 428 172 724 $ iter : int 7 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; kmeansGroups.5 \u0026lt;- KMC.5$cluster table(kmeansGroups.5) kmeansGroups.5 1 2 3 4 5 182 994 428 172 724  How many observations are in the smallest cluster?\nmin(table(kmeansGroups.5)) [1] 172 172 How many observations are in the largest cluster?\nmax(table(kmeansGroups.5)) [1] 994  994   Problem 13 - Describing the Clusters Use the cluster assignments from k-means clustering with 5 clusters, which cluster best fits the description ‚Äúfrequent shoppers with low value per visit‚Äù?\nkmeans1.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 1) kmeans2.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 2) kmeans3.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 3) kmeans4.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 4) kmeans5.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 5) colMeans(kmeans1.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 64.60989 24.47666 14.69849 76.09221 31.49236 AfternoonPct 53.37519  colMeans(kmeans2.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 88.274648 6.573304 3.355126 19.679329 21.516913 AfternoonPct 53.900113  colMeans(kmeans3.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 91.095794 8.541218 4.910884 26.956499 54.594773 AfternoonPct 32.958057  colMeans(kmeans4.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 422.273256 7.074807 3.710112 22.317813 27.386172 AfternoonPct 51.043708  colMeans(kmeans5.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 90.276243 13.628410 7.640804 41.803608 22.980428 AfternoonPct 58.626883  Cluster 4   Problem 14 - Understanding Centroids Why do we typically use cluster centroids to describe the clusters? #### The cluster centroid captures the average behavior in the cluster, and can be used to summarize the general pattern in the cluster.\n Problem 15 - Using a Visualization Which of the following visualizations could be used to observe the distribution of NumVisits, broken down by cluster? #### - A box plot of the variable NumVisits, subdivided by cluster #### - ggplot with NumVisits on the x-axis and the cluster number on the y-axis, plotting with geom_point()\n ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"4f02b10dc027954958aa0c2b14e68c0f","permalink":"/project/retail_consumers/retail_consumers/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/retail_consumers/retail_consumers/","section":"project","summary":"Understanding consumer behaviour","tags":["R","Data Analytics","Machine Learning"],"title":"Understanding Retail Consumers Behaviour","type":"project"},{"authors":null,"categories":null,"content":" The cliche goes that the world is an increasingly interconnected place, and the connections between different entities are often best represented with a graph. Graphs are comprised of vertices (also often called ‚Äúnodes‚Äù) and edges connecting those nodes. In this analysis, I‚Äôll explore how to visualize networks using the igraph package in R.\nI‚Äôll visualize social networking data using anonymized data from Facebook; this data was originally curated in a recent paper about computing social circles in social networks. In our visualizations, the vertices in our network will represent Facebook users and the edges will represent these users being Facebook friends with each other.\nThe first file I‚Äôll use, edges.csv, contains variables V1 and V2, which label the endpoints of edges in our network. Each row represents a pair of users in our graph who are Facebook friends. For a pair of friends A and B, edges.csv will only contain a single row ‚Äì the smaller identifier will be listed first in this row. From this row, I‚Äôll know that A is friends with B and B is friends with A.\nThe second file, users.csv, contains information about the Facebook users, who are the vertices in our network. This file contains the following variables:\n id: A unique identifier for this user; this is the value that appears in the rows of edges.csv gender: An identifier for the gender of a user taking the values A and B. Because the data is anonymized, we don‚Äôt know which value refers to males and which value refers to females. school: An identifier for the school the user attended taking the values A and AB (users with AB attended school A as well as another school B). Because the data is anonymized, we don‚Äôt know the schools represented by A and B. locale: An identifier for the locale of the user taking the values A and B. Because the data is anonymized, we don‚Äôt know which value refers to what locale.  Problem 1.1 - Summarizing the Data Load the data from edges.csv into a dataframe called edges, and load the data from users.csv into a dataframe called users.\nedges \u0026lt;- read.csv(\u0026quot;edges.csv\u0026quot;) users \u0026lt;- read.csv(\u0026quot;users.csv\u0026quot;) How many Facebook users are there in our dataset?\nstr(users) \u0026#39;data.frame\u0026#39;: 59 obs. of 4 variables: $ id : int 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 ... $ gender: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 2 3 3 3 3 3 2 3 3 2 ... $ school: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;AB\u0026quot;: 2 1 1 1 1 2 1 1 2 1 ... $ locale: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 3 3 3 3 3 3 2 3 3 2 ... In our dataset, what is the average number of friends per user? Hint: this question is tricky, and it might help to start by thinking about a small example with two users who are friends.\nhead(edges)  V1 V2 1 4019 4026 2 4023 4031 3 4023 4030 4 4027 4032 5 3988 4021 6 3982 3986 edges[1,] # users 4019 and 4026 are friends  V1 V2 1 4019 4026 str(subset(edges, V1 == 4019)) # user 4019 has 2 connections as V1 \u0026#39;data.frame\u0026#39;: 2 obs. of 2 variables: $ V1: int 4019 4019 $ V2: int 4026 4030 str(subset(edges, V2 == 4019)) # user 4019 has 5 connections as V2 \u0026#39;data.frame\u0026#39;: 5 obs. of 2 variables: $ V1: int 3997 3994 3998 4009 3981 $ V2: int 4019 4019 4019 4019 4019 str(subset(edges, V1 == 4026)) # user 4026 has 1 connections as V1 \u0026#39;data.frame\u0026#39;: 1 obs. of 2 variables: $ V1: int 4026 $ V2: int 4030 str(subset(edges, V2 == 4026)) # user 4026 has 7 connections as V2 \u0026#39;data.frame\u0026#39;: 7 obs. of 2 variables: $ V1: int 4019 4000 3995 4017 3986 3982 4021 $ V2: int 4026 4026 4026 4026 4026 4026 4026 edges2 \u0026lt;- edges edges2$PK \u0026lt;- row.names(edges2) edges2  V1 V2 PK 1 4019 4026 1 2 4023 4031 2 3 4023 4030 3 4 4027 4032 4 5 3988 4021 5 6 3982 3986 6 7 3994 3998 7 8 3998 3999 8 9 3993 3995 9 10 3982 4021 10 11 3982 4037 11 12 3997 4019 12 13 3994 4019 13 14 3992 4017 14 15 3981 3998 15 16 3997 4018 16 17 4009 4030 17 18 3994 4018 18 19 3995 4000 19 20 4000 4026 20 21 4027 4038 21 22 4031 4038 22 23 4000 4021 23 24 3986 4030 24 25 3985 4014 25 26 3994 4030 26 27 3998 4021 27 28 3994 4009 28 29 3982 4023 29 30 3998 4019 30 31 4020 4031 31 32 4009 4023 32 33 3994 3997 33 34 3981 4023 34 35 3997 4030 35 36 3997 4021 36 37 4023 4034 37 38 3993 4004 38 39 3994 3996 39 40 4000 4030 40 41 3998 4014 41 42 4004 4013 42 43 4016 4025 43 44 3990 4016 44 45 3999 4005 45 46 4004 4023 46 47 4002 4020 47 48 3998 4018 48 49 3985 3995 49 50 3989 3991 50 51 4000 4017 51 52 4003 4009 52 53 3982 4030 53 54 3982 3994 54 55 3998 4005 55 56 3995 4014 56 57 4021 4030 57 58 594 4011 58 59 3993 4030 59 60 4020 4030 60 61 3989 4038 61 62 3989 4011 62 63 4009 4019 63 64 4004 4020 64 65 3995 4026 65 66 4017 4026 66 67 3989 4013 67 68 4020 4037 68 69 3998 4002 69 70 3995 4023 70 71 3983 4017 71 72 3999 4036 72 73 3982 3997 73 74 3990 4007 74 75 3985 3988 75 76 4018 4030 76 77 4026 4030 77 78 3997 4023 78 79 3996 4028 79 80 3982 3988 80 81 3988 4030 81 82 4013 4023 82 83 4014 4021 83 84 4014 4037 84 85 3986 4021 85 86 4017 4021 86 87 3982 4009 87 88 3998 4023 88 89 3998 4009 89 90 594 3989 90 91 3992 4000 91 92 4011 4031 92 93 4019 4030 93 94 4020 4038 94 95 3997 3998 95 96 4023 4038 96 97 4004 4031 97 98 4027 4031 98 99 4014 4038 99 100 3986 4000 100 101 3982 4003 101 102 3986 4033 102 103 3981 3994 103 104 4004 4038 104 105 3985 3993 105 106 4000 4033 106 107 4013 4038 107 108 4018 4023 108 109 4003 4030 109 110 3990 4025 110 111 3986 4026 111 112 3996 4002 112 113 4001 4029 113 114 4014 4030 114 115 4020 4027 115 116 3982 3998 116 117 3988 3993 117 118 4002 4031 118 119 3988 3995 119 120 3986 4014 120 121 4003 4023 121 122 3981 4019 122 123 3997 4009 123 124 4014 4023 124 125 4004 4030 125 126 4006 4027 126 127 594 4031 127 128 4007 4025 128 129 3981 4018 129 130 3981 3997 130 131 3982 4026 131 132 4014 4017 132 133 3991 4031 133 134 3987 4012 134 135 4007 4016 135 136 3995 4004 136 137 4017 4030 137 138 4002 4023 138 139 3994 4023 139 140 3982 4014 140 141 3981 4009 141 142 4021 4026 142 143 4013 4031 143 144 3986 4017 144 145 4002 4027 145 146 3985 4004 146  Problem 1.2 - Summarizing the Data Out of all the students who listed a school, what was the most common locale?\nsummary(users)  id gender school locale Min. : 594 : 2 :40 : 3 1st Qu.:3994 A:15 A :17 A: 6 Median :4009 B:42 AB: 2 B:50 Mean :3952 3rd Qu.:4024 Max. :4038  table(users$school, users$locale)  A B 3 6 31 A 0 0 17 AB 0 0 2 Locale B   Problem 1.3 - Summarizing the Data Is it possible that either school A or B is an all-girls or all-boys school?\ntable(users$gender, users$school)  A AB 1 1 0 A 11 3 1 B 28 13 1 No   Problem 2.1 - Creating a Network We can create a new graph object using the graph.data.frame() function. Based on ?graph.data.frame, using the following code we will create a graph g describing our social network, with the attributes of each user correctly loaded.\n?graph.data.frame g \u0026lt;- graph.data.frame(edges, FALSE, users) g IGRAPH 097ec57 UN-- 59 146 -- + attr: name (v/c), gender (v/c), school (v/c), locale (v/c) + edges from 097ec57 (vertex names): [1] 4019--4026 4023--4031 4023--4030 4027--4032 3988--4021 3982--3986 [7] 3994--3998 3998--3999 3993--3995 3982--4021 3982--4037 3997--4019 [13] 3994--4019 3992--4017 3981--3998 3997--4018 4009--4030 3994--4018 [19] 3995--4000 4000--4026 4027--4038 4031--4038 4000--4021 3986--4030 [25] 3985--4014 3994--4030 3998--4021 3994--4009 3982--4023 3998--4019 [31] 4020--4031 4009--4023 3994--3997 3981--4023 3997--4030 3997--4021 [37] 4023--4034 3993--4004 3994--3996 4000--4030 3998--4014 4004--4013 [43] 4016--4025 3990--4016 3999--4005 4004--4023 4002--4020 3998--4018 + ... omitted several edges Note: A directed graph is one where the edges only go one way ‚Äì they point from one vertex to another. The other option is an undirected graph, which means that the relations between the vertices are symmetric.\nNow, we want to plot our graph. By default, the vertices are large and have text labels of a user‚Äôs identifier, this would clutter the output.\nWe will plot with no text labels and smaller vertices:\nplot(g, vertex.size=5, vertex.label=NA) In this graph, there are a number of groups of nodes where all the nodes in each group are connected but, the groups are disjoint from one another, forming ‚Äúislands‚Äù in the graph. Such groups are called ‚Äúconnected components,‚Äù or ‚Äúcomponents‚Äù for short.\nHow many connected components with at least 2 nodes are there in the graph? #### 4\nHow many users are there with no friends in the network? #### 7\n Problem 2.3 - Creating a Network In our graph, the ‚Äúdegree‚Äù of a node is its number of friends. We have already seen that some nodes in our graph have degree 0 (these are the nodes with no friends), while others have much higher degree. We can use degree(g) to compute the degree of all the nodes in our graph g.\ndegree(g) 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 7 13 1 0 5 8 1 6 5 3 2 2 5 10 8 594 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 3 3 10 13 3 8 1 6 4 9 2 1 3 0 9 4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023 4024 0 3 1 5 11 0 3 8 6 7 7 10 0 17 0 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 3 8 6 1 1 18 10 1 2 1 0 1 3 8  How many users are friends with 10 or more other Facebook users in this network?\nsum(degree(g) \u0026gt;= 10) [1] 9  Problem 2.4 - Creating a Network In a network, it‚Äôs often visually useful to draw attention to ‚Äúimportant‚Äù nodes in the network. While this might mean different things in different contexts, in a social network we might consider a user with a large number of friends to be an important user. From the previous problem, we know this is the same as saying that nodes with a high degree are important users.\nTo visually draw attention to these nodes, we will change the size of the vertices so the vertices with high degrees are larger. To do this, we will change the ‚Äúsize‚Äù attribute of the vertices of our graph to be an increasing function of their degrees:\nV(g)$size \u0026lt;- degree(g)/2+2 Now, that we have specified the vertex size of each vertex, we will no longer use the vertex.size parameter when we plot our graph:\nplot(g, vertex.label=NA) What is the largest size we assigned to any node in our graph?\nmax(V(g)$size) [1] 11 What is the smallest size we assigned to any node in our graph?\nmin(V(g)$size) [1] 2  Problem 3.1 - Coloring Vertices Thus far, we have changed the ‚Äúsize‚Äù attributes of our vertices. However, we can also change the colors of vertices to capture additional information about the Facebook users we are depicting.\nWhen changing the size of nodes, we first obtained the vertices of our graph with V(g) and then accessed the the size attribute with V(g)\\(size. To change the color, we will update the attribute V(g)\\)color.\nTo color the vertices based on the gender of the user, we will need access to that variable. When we created our graph g, we provided it with the dataframe users, which had variables gender, school, and locale. These are now stored as attributes V(g)\\(gender, V(g)\\)school, and V(g)$locale.\nWe can update the colors by setting the color to black for all vertices, than setting it to red for the vertices with gender A and setting it to gray for the vertices with gender B:\nV(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$gender == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$gender == \u0026quot;B\u0026quot;] = \u0026quot;gray\u0026quot; Ploting the resulting graph.\nWhat is the gender of the users with the highest degree in the graph?\nplot(g, vertex.label=NA) Gender B\n Problem 3.2 - Coloring Vertices Now, color the vertices based on the school that each user in our network attended.\ntable(V(g)$school)  A AB 40 17 2  V(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$school == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$school == \u0026quot;AB\u0026quot;] = \u0026quot;gray\u0026quot; plot(g, vertex.label=NA) Are the two users who attended both schools A and B Facebook friends with each other? #### Yes\nWhat best describes the users with highest degree? #### Some, but not all, of the high-degree users attended school A\n Problem 3.3 - Coloring Vertices Now, color the vertices based on the locale of the user.\ntable(V(g)$locale)  A B 3 6 50  V(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$locale == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$locale == \u0026quot;B\u0026quot;] = \u0026quot;gray\u0026quot; plot(g, vertex.label=NA) The large connected component is most associated with which locale? #### Locale B\nThe 4-user connected component is most associated with which locale? #### Locale A\n Problem 4 - Other Plotting Options The help page is a helpful tool when making visualizations. The following questions with the help of ?igraph.plotting and experimentation in our R console.\n?igraph.plotting Which igraph plotting function would enable us to plot our graph in 3-D?\n?rglplot rglplot\nWhat parameter to the plot() function would we use to change the edge width when plotting g?\n?plot.igraph edge.width\n ","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"53743b1e3bee51610022fc5a424544d2","permalink":"/project/visualizing_network/visualizing_network/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/project/visualizing_network/visualizing_network/","section":"project","summary":"Visualize social networking data from Facebook","tags":["R","Data Analytics","Machine Learning"],"title":"Network Data Visualizing","type":"project"},{"authors":null,"categories":null,"content":" Prevously, we used logistic regression on polling data in order to construct US presidential election predictions. We separated our data into a training set, containing data from 2004 and 2008 polls, and a test-set, containing the data from 2012 polls. We then proceeded to develop a logistic regression model to forecast the 2012 US presidential election.\nIn this problem, we‚Äôll revisit our logistic regression model, and learn how to plot the output on a map of the United States. Unlike what we did prevously, this time we‚Äôll be plotting predictions rather than data!\nFirst, load the ggplot2, maps, and ggmap packages using the library function. All three packages should be installed on your computer from lecture, but if not, you may need to install them too using the install.packages function.\nThen, load the US map and save it to the variable statesMap:\nstatesMap = map_data(‚Äústate‚Äù)\nThe maps package contains other built-in maps, including a US county map, a world map, and maps for France and Italy.\n# Load states map statesMap = map_data(\u0026quot;state\u0026quot;) Problem 1.1 - Drawing a Map of the US If you look at the structure of the statesMap dataframe using the str function, you should see that there are 6 variables. One of the variables, group, defines the different shapes or polygons on the map. Sometimes a state may have multiple groups, for example, if it includes islands.\nHow many different groups are there?\nstr(statesMap) \u0026#39;data.frame\u0026#39;: 15537 obs. of 6 variables: $ long : num -87.5 -87.5 -87.5 -87.5 -87.6 ... $ lat : num 30.4 30.4 30.4 30.3 30.3 ... $ group : num 1 1 1 1 1 1 1 1 1 1 ... $ order : int 1 2 3 4 5 6 7 8 9 10 ... $ region : chr \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; ... $ subregion: chr NA NA NA NA ... length(table(statesMap$group)) [1] 63 The variable ‚Äúorder‚Äù defines the order to connect the points within each group, and the variable ‚Äúregion‚Äù gives the name of the state.\n Problem 1.2 - Drawing a Map of the US You can draw a map of the US by typing the following code:\nggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = \u0026quot;white\u0026quot;, color = \u0026quot;black\u0026quot;) We specified two colors in geom_polygon ‚Äì fill and color. Which one defined the color of the outline of the states? #### color\n Problem 2.1 - Coloring the States by Predictions Now, let‚Äôs color the map of the US according to our 2012 US presidential election predictions from the Unit 3 Recitation. We‚Äôll rebuild the model here, using the dataset PollingImputed.csv. Be sure to use this file so that you don‚Äôt have to redo the imputation to fill in the missing values, like we did in the Unit 3 Recitation.\nLoad the data using the read.csv function, and call it ‚Äúpolling‚Äù. Then split the data using the subset function into a training set called ‚ÄúTrain‚Äù that has observations from 2004 and 2008, and a testing set called ‚ÄúTest‚Äù that has observations from 2012.\npolling \u0026lt;- read.csv(\u0026quot;PollingImputed.csv\u0026quot;) str(polling) \u0026#39;data.frame\u0026#39;: 145 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 1 1 2 2 3 3 3 4 4 4 ... $ Year : int 2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ... $ Rasmussen : int 11 21 19 16 5 5 8 7 10 13 ... $ SurveyUSA : int 18 25 21 18 15 3 5 5 7 21 ... $ DiffCount : int 5 5 1 6 8 9 4 8 5 2 ... $ PropR : num 1 1 1 1 1 ... $ Republican: int 1 1 1 1 1 1 1 1 1 1 ... table(polling$Year)  2004 2008 2012 50 50 45  Train \u0026lt;- subset(polling, Year \u0026lt;= 2008) Test \u0026lt;- subset(polling, Year \u0026gt; 2008) str(Train) \u0026#39;data.frame\u0026#39;: 100 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 1 1 2 2 3 3 4 4 5 5 ... $ Year : int 2004 2008 2004 2008 2004 2008 2004 2008 2004 2008 ... $ Rasmussen : int 11 21 19 16 5 5 7 10 -11 -27 ... $ SurveyUSA : int 18 25 21 18 15 3 5 7 -11 -24 ... $ DiffCount : int 5 5 1 6 8 9 8 5 -8 -5 ... $ PropR : num 1 1 1 1 1 1 1 1 0 0 ... $ Republican: int 1 1 1 1 1 1 1 1 0 0 ... str(Test) \u0026#39;data.frame\u0026#39;: 45 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 3 4 5 6 7 9 10 11 12 13 ... $ Year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... $ Rasmussen : int 8 13 -12 3 -7 2 5 -22 31 -22 ... $ SurveyUSA : int 5 21 -14 -2 -13 0 8 -24 24 -16 ... $ DiffCount : int 4 2 -6 -5 -8 6 4 -2 1 -5 ... $ PropR : num 0.833 1 0 0.308 0 ... $ Republican: int 1 1 0 0 0 0 1 0 1 0 ... Note that we only have 45 states in our testing set, since we are missing observations for Alaska, Delaware, Alabama, Wyoming, and Vermont, so these states will not appear colored in our map.\nThen, create a logistic regression model and make predictions on the test-set using the following code:\nmod2 \u0026lt;- glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = \u0026quot;binomial\u0026quot;) TestPrediction \u0026lt;- predict(mod2, newdata = Test, type = \u0026quot;response\u0026quot;) TestPrediction gives the predicted probabilities for each state, but let‚Äôs also create a vector of Republican/Democrat predictions by using the following code:\nTestPredictionBinary \u0026lt;- as.numeric(TestPrediction \u0026gt; 0.5) Now, put the predictions and state labels in a data.frame so that we can use ggplot:\npredictionDataFrame \u0026lt;- data.frame(TestPrediction, TestPredictionBinary, Test$State) To make sure everything went smoothly, answer the following.\nFor how many states is our binary prediction 1 (for 2012), corresponding to Republican?\nstr(TestPredictionBinary)  num [1:45] 1 1 0 0 0 1 1 0 1 0 ... table(TestPredictionBinary) TestPredictionBinary 0 1 23 22  22\nWhat is the average predicted probability of our model (on the Test set, for 2012)?\nmean(TestPrediction) [1] 0.4852626  Problem 2.2 - Coloring the States by Predictions Now, we need to merge ‚ÄúpredictionDataFrame‚Äù with the map data ‚ÄústatesMap‚Äù. Before doing so, we need to convert the Test.State variable to lowercase, so that it matches the region variable in statesMap.\npredictionDataFrame$region \u0026lt;- tolower(predictionDataFrame$Test.State) # Now, merge the two data frames using the following command: predictionMap \u0026lt;- merge(statesMap, predictionDataFrame, by = \u0026quot;region\u0026quot;) # Lastly, we need to make sure the observations are in order so that the map is # drawn properly, by typing the following: predictionMap \u0026lt;- predictionMap[order(predictionMap$order),] How many observations are there in predictionMap?\nstr(predictionMap) \u0026#39;data.frame\u0026#39;: 15034 obs. of 9 variables: $ region : chr \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; ... $ long : num -115 -115 -115 -115 -115 ... $ lat : num 35 35.1 35.1 35.2 35.2 ... $ group : num 2 2 2 2 2 2 2 2 2 2 ... $ order : int 204 205 206 207 208 209 210 211 212 213 ... $ subregion : chr NA NA NA NA ... $ TestPrediction : num 0.974 0.974 0.974 0.974 0.974 ... $ TestPredictionBinary: num 1 1 1 1 1 1 1 1 1 1 ... $ Test.State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... How many observations are there in statesMap?\nstr(statesMap) \u0026#39;data.frame\u0026#39;: 15537 obs. of 6 variables: $ long : num -87.5 -87.5 -87.5 -87.5 -87.6 ... $ lat : num 30.4 30.4 30.4 30.3 30.3 ... $ group : num 1 1 1 1 1 1 1 1 1 1 ... $ order : int 1 2 3 4 5 6 7 8 9 10 ... $ region : chr \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; ... $ subregion: chr NA NA NA NA ...  Problem 2.3 - Coloring the States by Predictions When we merged the data in the previous problem, it caused the number of observations to change. Why? Check out the help page for merge by typing ?merge to help us answer this question. #### Because we only make predictions for 45 states, we no longer have observations for some of the states. These observations were removed in the merging process.\n Problem 2.4 - Coloring the States by Predictions Now we are ready to color the US map with our predictions! You can color the states according to our binary predictions by typing the following code:\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = \u0026quot;black\u0026quot;) The states appear light blue and dark blue in this map. Which color represents a Republican prediction? #### Light blue\n Problem 2.5 - Coloring the States by Predictions We see that the legend displays a blue gradient for outcomes between 0 and 1. However, when plotting the binary predictions there are only two possible outcomes: 0 or 1.\nLet‚Äôs replot the map with discrete outcomes. We can also change the color scheme to blue and red, to match the blue color associated with the Democratic Party in the US and the red color associated with the Republican Party in the US.\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) Alternatively, we could plot the probabilities instead of the binary predictions. Change the plot code above to instead color the states by the variable TestPrediction. You should see a gradient of colors ranging from red to blue.\nDo the colors of the states in the map for TestPrediction look different from the colors of the states in the map with TestPredictionBinary? Why or why not?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) round(TestPrediction, 2)  7 10 13 16 19 24 27 30 33 36 39 42 45 48 51 0.97 1.00 0.00 0.01 0.00 0.96 0.99 0.00 1.00 0.00 1.00 0.06 0.95 0.99 1.00 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 0.00 0.00 0.00 0.00 0.00 0.93 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.95 99 102 105 108 111 114 117 120 123 126 129 134 137 140 143 1.00 0.00 1.00 0.00 0.00 0.00 1.00 0.99 1.00 1.00 1.00 0.02 0.00 1.00 0.00  The two maps look very similar. This is because most of our predicted probabilities are close to 0 or close to 1. NOTE: If you have a hard time seeing the red/blue gradient, feel free to change the color scheme, by changing the arguments low = ‚Äúblue‚Äù and high = ‚Äúred‚Äù to colors of your choice (to see all of the color options in R, type colors() in your R console). You can even change it to a gray scale, by changing the low and high colors to ‚Äúgray‚Äù and ‚Äúblack‚Äù.\n  Problem 3.1 - Understanding the Predictions In the 2012 election, the state of Florida ended up being a very close race. It was ultimately won by the Democratic party. Did we predict this state correctly or incorrectly? To see the names and locations of the different states, take a look at the World Atlas map here. #### We incorrectly predicted this state by predicting that it would be won by the Republican party.\n Problem 3.2 - Understanding the Predictions What was our predicted probability for the state of Florida?\npredictionDataFrame[predictionDataFrame$region == \u0026quot;florida\u0026quot;, ]  TestPrediction TestPredictionBinary Test.State region 24 0.9640395 1 Florida florida What does this imply? #### Our prediction model did not do a very good job of correctly predicting the state of Florida, and we were very confident in our incorrect prediction.\n PROBLEM 4 - PARAMETER SETTINGS In this part, we‚Äôll explore what the different parameter settings of geom_polygon do. Throughout the problem, use the help page for geom_polygon, which can be accessed by ?geom_polygon. To see more information about a certain parameter, just type a question mark and then the parameter name to get the help page for that parameter. Experiment with different parameter settings to try and replicate the plots!\nWe‚Äôll be asking questions about the following three plots:\n Problem 4.1 - Parameter Settings Plots (1) and (2) were created by setting different parameters of geom_polygon to the value 3.\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) ?geom_polygon What is the name of the parameter we set to have value 3 to create plot (1)?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, linetype = 3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) linetype\nWhat is the name of the parameter we set to have value 3 to create plot (2)?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, size = 3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) size\n Problem 4.2 - Parameter Settings Plot (3) was created by changing the value of a different geom_polygon parameter to have value 0.3. Which parameter did we use?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, alpha = 0.3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) alpha\n ","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"b734c087b7c1ea845e4e1ed28554edef","permalink":"/project/election_forecasting/election_forecasting/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/project/election_forecasting/election_forecasting/","section":"project","summary":"How to plot the output on a map of the United States","tags":["R","Data Analytics","Machine Learning"],"title":"Revisiting Election Forecasting","type":"project"},{"authors":null,"categories":null,"content":" Market segmentation is a strategy that divides a broad target market of customers into smaller, more similar groups, and then designs a marketing strategy specifically for each group. Clustering is a common technique for market segmentation since it automatically finds similar groups given a dataset.\nIn this analysis, I‚Äôll see how clustering can be used to find similar groups of customers who belong to an airline‚Äôs frequent flyer program. The airline is trying to learn more about its customers so that it can target different customer segments with different types of mileage offers.\nThe file AirlinesCluster.csv contains information on 3,999 members of the frequent flyer program. This data comes from the textbook Data Mining for Business Intelligence, by Galit Shmueli, Nitin R. Patel, and Peter C. Bruce. For more information, see the website for the book.\nThere are seven different variables in the dataset, described below:\n Balance = number of miles eligible for award travel QualMiles = number of miles qualifying for TopFlight status BonusMiles = number of miles earned from non-flight bonus transactions in the past 12 months BonusTrans = number of non-flight bonus transactions in the past 12 months FlightMiles = number of flight miles in the past 12 months FlightTrans = number of flight transactions in the past 12 months DaysSinceEnroll = number of days since enrolled in the frequent flyer program  Problem 1.1 - Normalizing the Data Read the dataset AirlinesCluster.csv into R and call it ‚Äúairlines‚Äù.\nairlines \u0026lt;- read.csv(\u0026quot;AirlinesCluster.csv\u0026quot;) Looking at the summary of airlines, which TWO variables have (on average) the smallest values?\nsummary(airlines)  Balance QualMiles BonusMiles BonusTrans Min. : 0 Min. : 0.0 Min. : 0 Min. : 0.0 1st Qu.: 18528 1st Qu.: 0.0 1st Qu.: 1250 1st Qu.: 3.0 Median : 43097 Median : 0.0 Median : 7171 Median :12.0 Mean : 73601 Mean : 144.1 Mean : 17145 Mean :11.6 3rd Qu.: 92404 3rd Qu.: 0.0 3rd Qu.: 23800 3rd Qu.:17.0 Max. :1704838 Max. :11148.0 Max. :263685 Max. :86.0 FlightMiles FlightTrans DaysSinceEnroll Min. : 0.0 Min. : 0.000 Min. : 2 1st Qu.: 0.0 1st Qu.: 0.000 1st Qu.:2330 Median : 0.0 Median : 0.000 Median :4096 Mean : 460.1 Mean : 1.374 Mean :4119 3rd Qu.: 311.0 3rd Qu.: 1.000 3rd Qu.:5790 Max. :30817.0 Max. :53.000 Max. :8296  BonusTrans and FlightTrans\n Problem 1.2 - Normalizing the Data In this problem, we will normalize our data before we run the clustering algorithms.\nWhy is it important to normalize the data before clustering? #### If we don‚Äôt normalize the data, the clustering will be dominated by the variables that are on a larger scale.\n Problem 1.3 - Normalizing the Data Let‚Äôs go ahead and normalize our data. You can normalize the variables in a dataframe by using the preProcess function in the ‚Äúcaret‚Äù package.\nNow, create a normalized dataframe called ‚ÄúairlinesNorm‚Äù by running the following code:\npreproc \u0026lt;- preProcess(airlines) airlinesNorm \u0026lt;- predict(preproc, airlines) The first code pre-processes the data, and the second code performs the normalization. If you look at the summary of airlinesNorm, you should see that all of the variables now have mean zero. You can also see that each of the variables has standard deviation 1 by using the sd() function.\nsummary(airlinesNorm)  Balance QualMiles BonusMiles BonusTrans Min. :-0.7303 Min. :-0.1863 Min. :-0.7099 Min. :-1.20805 1st Qu.:-0.5465 1st Qu.:-0.1863 1st Qu.:-0.6581 1st Qu.:-0.89568 Median :-0.3027 Median :-0.1863 Median :-0.4130 Median : 0.04145 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 3rd Qu.: 0.1866 3rd Qu.:-0.1863 3rd Qu.: 0.2756 3rd Qu.: 0.56208 Max. :16.1868 Max. :14.2231 Max. :10.2083 Max. : 7.74673 FlightMiles FlightTrans DaysSinceEnroll Min. :-0.3286 Min. :-0.36212 Min. :-1.99336 1st Qu.:-0.3286 1st Qu.:-0.36212 1st Qu.:-0.86607 Median :-0.3286 Median :-0.36212 Median :-0.01092 Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 3rd Qu.:-0.1065 3rd Qu.:-0.09849 3rd Qu.: 0.80960 Max. :21.6803 Max. :13.61035 Max. : 2.02284  sd(airlinesNorm$Balance) [1] 1 sd(airlinesNorm$FlightTrans) [1] 1 In the normalized data, which variable has the largest maximum value? #### FlightMiles\nIn the normalized data, which variable has the smallest minimum value? #### DaysSinceEnroll\n Problem 2.1 - Hierarchical Clustering Compute the distances between data points (using euclidean distance) and then run the Hierarchical clustering algorithm (using method=‚Äúward.D‚Äù) on the normalized data. It may take a few minutes for the code to finish since the dataset has a large number of observations for hierarchical clustering.\ndistAirlines \u0026lt;- dist(airlinesNorm, method = \u0026quot;euclidean\u0026quot;) hclustAirlines \u0026lt;- hclust(distAirlines, method = \u0026quot;ward.D\u0026quot;) Then, plot the dendrogram of the hierarchical clustering process. Suppose the airline is looking for somewhere between 2 and 10 clusters. According to the dendrogram, which of the following is NOT a good choice for the number of clusters?\nplot(hclustAirlines) 6\n Problem 2.2 - Hierarchical Clustering Suppose that after looking at the dendrogram and discussing with the marketing department, the airline decides to proceed with 5 clusters. Divide the data points into 5 clusters by using the cutree function.\nHow many data points are in Cluster 1?\nhclustGroups \u0026lt;- cutree(hclustAirlines, k = 5) table(hclustGroups) hclustGroups 1 2 3 4 5 776 519 494 868 1342  776\n Problem 2.3 - Hierarchical Clustering Now, use tapply to compare the average values in each of the variables for the 5 clusters (the centroids of the clusters). You may want to compute the average values of the unnormalized data so that it is easier to interpret. You can do this for the variable ‚ÄúBalance‚Äù.\nstr(airlines) \u0026#39;data.frame\u0026#39;: 3999 obs. of 7 variables: $ Balance : int 28143 19244 41354 14776 97752 16420 84914 20856 443003 104860 ... $ QualMiles : int 0 0 0 0 0 0 0 0 0 0 ... $ BonusMiles : int 174 215 4123 500 43300 0 27482 5250 1753 28426 ... $ BonusTrans : int 1 2 4 1 26 0 25 4 43 28 ... $ FlightMiles : int 0 0 0 0 2077 0 0 250 3850 1150 ... $ FlightTrans : int 0 0 0 0 4 0 0 1 12 3 ... $ DaysSinceEnroll: int 7000 6968 7034 6952 6935 6942 6994 6938 6948 6931 ... tapply(airlines$Balance, hclustGroups, mean)  1 2 3 4 5 57866.90 110669.27 198191.57 52335.91 36255.91  tapply(airlines$QualMiles, hclustGroups, mean)  1 2 3 4 5 0.6443299 1065.9826590 30.3461538 4.8479263 2.5111773  tapply(airlines$BonusMiles, hclustGroups, mean)  1 2 3 4 5 10360.124 22881.763 55795.860 20788.766 2264.788  tapply(airlines$BonusTrans, hclustGroups, mean)  1 2 3 4 5 10.823454 18.229287 19.663968 17.087558 2.973174  tapply(airlines$FlightMiles, hclustGroups, mean)  1 2 3 4 5 83.18428 2613.41811 327.67611 111.57373 119.32191  tapply(airlines$FlightTrans, hclustGroups, mean)  1 2 3 4 5 0.3028351 7.4026975 1.0688259 0.3444700 0.4388972  tapply(airlines$DaysSinceEnroll, hclustGroups, mean)  1 2 3 4 5 6235.365 4402.414 5615.709 2840.823 3060.081  Compared to the other clusters, Cluster 1 has the largest average values in which variables (if any)? #### DaysSinceEnroll\nHow would you describe the customers in Cluster 1? #### Infrequent but loyal customers.\n Problem 2.4 - Hierarchical Clustering Compared to the other clusters, Cluster 2 has the largest average values in which variables? #### QualMiles, FlightMiles, FlightTrans\nHow would you describe the customers in Cluster 2? #### Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.\n Problem 2.5 - Hierarchical Clustering Compared to the other clusters, Cluster 3 has the largest average values in which variables? #### Balance, BonusMiles, BonusTrans\nHow would you describe the customers in Cluster 3? #### Customers who have accumulated a large amount of miles, mostly through non-flight transactions.\n Problem 2.6 - Hierarchical Clustering Compared to the other clusters, Cluster 4 has the largest average values in which variables? #### None\nHow would you describe the customers in Cluster 4? #### Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.\n Problem 2.7 - Hierarchical Clustering Compared to the other clusters, Cluster 5 has the largest average values in which variables? #### None\nHow would you describe the customers in Cluster 5? #### Relatively new customers who don‚Äôt use the airline very often.\n Problem 3.1 - K-Means Clustering Now run the k-means clustering algorithm on the normalized data, again creating 5 clusters. Set the seed to 88 right before running the clustering algorithm, and set the argument iter.max to 1000.\nset.seed(88) kmeansAirlines \u0026lt;- kmeans(airlinesNorm, centers = 5, iter.max = 1000) str(kmeansAirlines) List of 9 $ cluster : int [1:3999] 4 4 4 4 1 4 3 4 2 3 ... $ centers : num [1:5, 1:7] 1.4444 1.0005 -0.0558 -0.1333 -0.4058 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:5] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:7] \u0026quot;Balance\u0026quot; \u0026quot;QualMiles\u0026quot; \u0026quot;BonusMiles\u0026quot; \u0026quot;BonusTrans\u0026quot; ... $ totss : num 27986 $ withinss : num [1:5] 4948 3624 2054 2040 2321 $ tot.withinss: num 14987 $ betweenss : num 12999 $ size : int [1:5] 408 141 993 1182 1275 $ iter : int 4 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; How many clusters have more than 1,000 observations?\ntable(kmeansAirlines$cluster)  1 2 3 4 5 408 141 993 1182 1275  2\n Problem 3.2 - K-Means Clustering Now, compare the cluster centroids to each other either by dividing the data points into groups and then using tapply, or by looking at the output of kmeansClust\\(centers, where \u0026quot;kmeansClust\u0026quot; is the name of the output of the kmeans function. (Note that the output of kmeansClust\\)centers will be for the normalized data. If you want to look at the average values for the unnormalized data, you need to use tapply like we did for hierarchical clustering.)\nkmeansGroups \u0026lt;- kmeansAirlines$cluster tapply(airlines$Balance, kmeansGroups, mean)  1 2 3 4 5 219161.40 174431.51 67977.44 60166.18 32706.67  tapply(airlines$QualMiles, kmeansGroups, mean)  1 2 3 4 5 539.57843 673.16312 34.99396 55.20812 126.46667  tapply(airlines$BonusMiles, kmeansGroups, mean)  1 2 3 4 5 62474.483 31985.085 24490.019 8709.712 3097.478  tapply(airlines$BonusTrans, kmeansGroups, mean)  1 2 3 4 5 21.524510 28.134752 18.429003 8.362098 4.284706  tapply(airlines$FlightMiles, kmeansGroups, mean)  1 2 3 4 5 623.8725 5859.2340 289.4713 203.2589 181.4698  tapply(airlines$FlightTrans, kmeansGroups, mean)  1 2 3 4 5 1.9215686 17.0000000 0.8851964 0.6294416 0.5403922  tapply(airlines$DaysSinceEnroll, kmeansGroups, mean)  1 2 3 4 5 5605.051 4684.901 3416.783 6109.540 2281.055  Do you expect Cluster 1 of the K-Means clustering output to necessarily be similar to Cluster 1 of the Hierarchical clustering output? #### No, because cluster ordering is not meaningful in either k-means clustering or hierarchical clustering.\n ","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"6fc6cd4924828f5b2c77b4d0866cb8fe","permalink":"/project/market_segmentation_for_airlines/market/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/project/market_segmentation_for_airlines/market/","section":"project","summary":"Finding similar groups of travellers who belong to an airline's frequent flyer program","tags":["R","Data Analytics","Machine Learning"],"title":"Market Segmentation For Airlines","type":"project"},{"authors":null,"categories":null,"content":" About cluster-then-predict, a methodology in which you first cluster observations and then build cluster-specific prediction models. In this problem, I‚Äôll use cluster-then-predict to predict future stock prices using historical stock data.\nWhen selecting which stocks to invest in, investors seek to obtain good future returns. In this analysis, I‚Äôll first use clustering to identify clusters of stocks that have similar returns over time. Then, use logistic regression to predict whether or not the stocks will have positive future returns.\nFor this problem, I‚Äôll use StocksCluster.csv, which contains monthly stock returns from the NASDAQ stock exchange. The NASDAQ is the second-largest stock exchange in the world, and it lists many tech companies. The stock price data used in this analysis was obtained from infochimps, a website providing access to many datasets.\nEach observation in the dataset is the monthly returns of a particular company in a particular year. The years included are 2000-2009. The companies are limited to tickers that were listed on the exchange for the entire period 2000-2009, and whose stock price never fell below $1. So, for example, one observation is for Yahoo in 2000, and another observation is for Yahoo in 2001. Our goal will be to predict whether or not the stock return in December will be positive, using the stock returns for the first 11 months of the year.\nThis dataset contains the following variables:\n ReturnJan = the return for the company‚Äôs stock during January (in the year of the observation). ReturnFeb = the return for the company‚Äôs stock during February (in the year of the observation). ReturnMar = the return for the company‚Äôs stock during March (in the year of the observation). ReturnApr = the return for the company‚Äôs stock during April (in the year of the observation). ReturnMay = the return for the company‚Äôs stock during May (in the year of the observation). ReturnJune = the return for the company‚Äôs stock during June (in the year of the observation). ReturnJuly = the return for the company‚Äôs stock during July (in the year of the observation). ReturnAug = the return for the company‚Äôs stock during August (in the year of the observation). ReturnSep = the return for the company‚Äôs stock during September (in the year of the observation). ReturnOct = the return for the company‚Äôs stock during October (in the year of the observation). ReturnNov = the return for the company‚Äôs stock during November (in the year of the observation). PositiveDec = whether or not the company‚Äôs stock had a positive return in December (in the year of the observation). This variable takes value 1 if the return was positive, and value 0 if the return was not positive.  For the first 11 variables, the value stored is a proportional change in stock value during that month. For instance, a value of 0.05 means the stock increased in value 5% during the month, while a value of -0.02 means the stock decreased in value 2% during the month.\nProblem 1.1 - Exploring the Dataset Load StocksCluster.csv into a dataframe called ‚Äústocks‚Äù.\nHow many observations are in the dataset?\nstocks \u0026lt;- read.csv(\u0026quot;StocksCluster.csv\u0026quot;) str(stocks) \u0026#39;data.frame\u0026#39;: 11580 obs. of 12 variables: $ ReturnJan : num 0.0807 -0.0107 0.0477 -0.074 -0.031 ... $ ReturnFeb : num 0.0663 0.1021 0.036 -0.0482 -0.2127 ... $ ReturnMar : num 0.0329 0.1455 0.0397 0.0182 0.0915 ... $ ReturnApr : num 0.1831 -0.0844 -0.1624 -0.0247 0.1893 ... $ ReturnMay : num 0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ... $ ReturnJune : num -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ... $ ReturnJuly : num -0.0205 -0.0253 -0.1354 -0.094 0.3553 ... $ ReturnAug : num 0.0247 0.2113 0.0334 0.0953 0.0568 ... $ ReturnSep : num -0.0204 -0.58 0 0.0567 0.0336 ... $ ReturnOct : num -0.1733 -0.2671 0.0917 -0.0963 0.0363 ... $ ReturnNov : num -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ... $ PositiveDec: int 0 0 0 1 1 1 1 0 0 0 ... 11580\n Problem 1.2 - Exploring the Dataset What proportion of the observations have positive returns in December?\ntable(stocks$PositiveDec)  0 1 5256 6324  6324 / (5256 + 6324) [1] 0.546114  Problem 1.3 - Exploring the Dataset What is the maximum correlation between any two return variables in the dataset? You should look at the pairwise correlations between ReturnJan, ReturnFeb, ReturnMar, ReturnApr, ReturnMay, ReturnJune, ReturnJuly, ReturnAug, ReturnSep, ReturnOct, and ReturnNov.\ncor(stocks[, 1:11])  ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay ReturnJan 1.00000000 0.06677458 -0.090496798 -0.037678006 -0.044411417 ReturnFeb 0.06677458 1.00000000 -0.155983263 -0.191351924 -0.095520920 ReturnMar -0.09049680 -0.15598326 1.000000000 0.009726288 -0.003892789 ReturnApr -0.03767801 -0.19135192 0.009726288 1.000000000 0.063822504 ReturnMay -0.04441142 -0.09552092 -0.003892789 0.063822504 1.000000000 ReturnJune 0.09223831 0.16999448 -0.085905486 -0.011027752 -0.021074539 ReturnJuly -0.08142976 -0.06177851 0.003374160 0.080631932 0.090850264 ReturnAug -0.02279202 0.13155979 -0.022005400 -0.051756051 -0.033125658 ReturnSep -0.02643715 0.04350177 0.076518327 -0.028920972 0.021962862 ReturnOct 0.14297723 -0.08732427 -0.011923758 0.048540025 0.017166728 ReturnNov 0.06763233 -0.15465828 0.037323535 0.031761837 0.048046590 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnJan 0.09223831 -0.0814297650 -0.0227920187 -0.0264371526 ReturnFeb 0.16999448 -0.0617785094 0.1315597863 0.0435017706 ReturnMar -0.08590549 0.0033741597 -0.0220053995 0.0765183267 ReturnApr -0.01102775 0.0806319317 -0.0517560510 -0.0289209718 ReturnMay -0.02107454 0.0908502642 -0.0331256580 0.0219628623 ReturnJune 1.00000000 -0.0291525996 0.0107105260 0.0447472692 ReturnJuly -0.02915260 1.0000000000 0.0007137558 0.0689478037 ReturnAug 0.01071053 0.0007137558 1.0000000000 0.0007407139 ReturnSep 0.04474727 0.0689478037 0.0007407139 1.0000000000 ReturnOct -0.02263599 -0.0547089088 -0.0755945614 -0.0580792362 ReturnNov -0.06527054 -0.0483738369 -0.1164890345 -0.0197197998 ReturnOct ReturnNov ReturnJan 0.14297723 0.06763233 ReturnFeb -0.08732427 -0.15465828 ReturnMar -0.01192376 0.03732353 ReturnApr 0.04854003 0.03176184 ReturnMay 0.01716673 0.04804659 ReturnJune -0.02263599 -0.06527054 ReturnJuly -0.05470891 -0.04837384 ReturnAug -0.07559456 -0.11648903 ReturnSep -0.05807924 -0.01971980 ReturnOct 1.00000000 0.19167279 ReturnNov 0.19167279 1.00000000 ReturnOct vs ReturnNov = 0.19167279\n Problem 1.4 - Exploring the Dataset Which month (from January through November) has the largest mean return across all observations in the dataset?\nstr(stocks) \u0026#39;data.frame\u0026#39;: 11580 obs. of 12 variables: $ ReturnJan : num 0.0807 -0.0107 0.0477 -0.074 -0.031 ... $ ReturnFeb : num 0.0663 0.1021 0.036 -0.0482 -0.2127 ... $ ReturnMar : num 0.0329 0.1455 0.0397 0.0182 0.0915 ... $ ReturnApr : num 0.1831 -0.0844 -0.1624 -0.0247 0.1893 ... $ ReturnMay : num 0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ... $ ReturnJune : num -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ... $ ReturnJuly : num -0.0205 -0.0253 -0.1354 -0.094 0.3553 ... $ ReturnAug : num 0.0247 0.2113 0.0334 0.0953 0.0568 ... $ ReturnSep : num -0.0204 -0.58 0 0.0567 0.0336 ... $ ReturnOct : num -0.1733 -0.2671 0.0917 -0.0963 0.0363 ... $ ReturnNov : num -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ... $ PositiveDec: int 0 0 0 1 1 1 1 0 0 0 ... colMeans(stocks[, 1:11])  ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.012631602 -0.007604784 0.019402336 0.026308147 0.024736591 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct 0.005937902 0.003050863 0.016198265 -0.014720768 0.005650844 ReturnNov 0.011387440  ReturnApr = 0.026308147\nWhich month (from January through November) has the smallest mean return across all observations in the dataset? #### ReturnSep = -0.014720768\n Problem 2.1 - Initial Logistic Regression Model Split the data into a training set and testing set, putting 70% of the data in the training set and 30% of the data in the testing set:\nlibrary(caTools) set.seed(144) spl \u0026lt;- sample.split(stocks$PositiveDec, SplitRatio = 0.7) stocksTrain \u0026lt;- subset(stocks, spl == TRUE) stocksTest \u0026lt;- subset(stocks, spl == FALSE) Then, use the stocksTrain dataframe to train a logistic regression model (name it StocksModel) to predict PositiveDec using all the other variables as independent variables.\nNot forgetting to add the argument family=binomial to our glm code.\nStocksModel \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain, family = binomial) What is the overall accuracy on the training set, using a threshold of 0.5?\nStocksModelTrainPred \u0026lt;- predict(StocksModel, type = \u0026quot;response\u0026quot;) head(StocksModelTrainPred)  1 2 4 6 7 8 0.6333193 0.3804326 0.5432996 0.6485711 0.5991750 0.4372892  table(stocksTrain$PositiveDec, StocksModelTrainPred \u0026gt;= 0.5)  FALSE TRUE 0 990 2689 1 787 3640 (990 + 3640) / nrow(stocksTrain) [1] 0.5711818  Problem 2.2 - Initial Logistic Regression Model Now obtain test-set predictions from StocksModel.\nWhat is the overall accuracy of the model on the test, again using a threshold of 0.5?\nStocksModelTestPred \u0026lt;- predict(StocksModel, newdata = stocksTest, type = \u0026quot;response\u0026quot;) head(StocksModelTestPred)  3 5 15 17 23 26 0.4506152 0.6470609 0.6089785 0.5708036 0.4758428 0.3631213  table(stocksTest$PositiveDec, StocksModelTestPred \u0026gt;= 0.5)  FALSE TRUE 0 417 1160 1 344 1553 (417 + 1553) / nrow(stocksTest) [1] 0.5670697  Problem 2.3 - Initial Logistic Regression Model What is the accuracy on the test-set of a baseline model that always predicts the most common outcome (PositiveDec = 1)?\ntable(stocksTest$PositiveDec)  0 1 1577 1897  1897 / (1577 + 1897) [1] 0.5460564  Problem 3.1 - Clustering Stocks Now, let‚Äôs cluster the stocks. The first step in this process is to remove the dependent variable.\nlimitedTrain \u0026lt;- stocksTrain limitedTrain$PositiveDec \u0026lt;- NULL limitedTest \u0026lt;- stocksTest limitedTest$PositiveDec \u0026lt;- NULL Why do we need to remove the dependent variable in the clustering phase of the cluster-then-predict methodology? #### Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the methodology\n Problem 3.2 - Clustering Stocks preProcess code from the caret package, which normalizes variables by subtracting by the mean and dividing by the standard deviation.\nIn cases where we have a training and testing set, we‚Äôll want to normalize by the mean and standard deviation of the variables in the training set. We can do this by passing just the training set to the preProcess function:\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 preproc \u0026lt;- preProcess(limitedTrain) normTrain \u0026lt;- predict(preproc, limitedTrain) normTest \u0026lt;- predict(preproc, limitedTest) What is the mean of the ReturnJan variable in normTrain?\nsummary(normTrain)  ReturnJan ReturnFeb ReturnMar Min. :-4.57682 Min. :-3.43004 Min. :-4.54609 1st Qu.:-0.48271 1st Qu.:-0.35589 1st Qu.:-0.40758 Median :-0.07055 Median :-0.01875 Median :-0.05778 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.35898 3rd Qu.: 0.25337 3rd Qu.: 0.36106 Max. :18.06234 Max. :34.92751 Max. :24.77296 ReturnApr ReturnMay ReturnJune Min. :-5.0227 Min. :-4.96759 Min. :-4.82957 1st Qu.:-0.4757 1st Qu.:-0.43045 1st Qu.:-0.45602 Median :-0.1104 Median :-0.06983 Median :-0.04354 Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.3400 3rd Qu.: 0.35906 3rd Qu.: 0.37273 Max. :14.6959 Max. :42.69158 Max. :10.84515 ReturnJuly ReturnAug ReturnSep Min. :-5.19139 Min. :-5.60378 Min. :-5.47078 1st Qu.:-0.51832 1st Qu.:-0.47163 1st Qu.:-0.39604 Median :-0.02372 Median :-0.07393 Median : 0.04767 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.47735 3rd Qu.: 0.39967 3rd Qu.: 0.42287 Max. :17.33975 Max. :27.14273 Max. :39.05435 ReturnOct ReturnNov Min. :-3.53719 Min. :-4.31684 1st Qu.:-0.42176 1st Qu.:-0.43564 Median :-0.01891 Median :-0.01878 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.37451 3rd Qu.: 0.42560 Max. :31.25996 Max. :17.18255  What is the mean of the ReturnJan variable in normTest?\nsummary(normTest)  ReturnJan ReturnFeb ReturnMar Min. :-3.743836 Min. :-3.251044 Min. :-4.07731 1st Qu.:-0.485690 1st Qu.:-0.348951 1st Qu.:-0.40662 Median :-0.066856 Median :-0.006860 Median :-0.05674 Mean :-0.000419 Mean :-0.003862 Mean : 0.00583 3rd Qu.: 0.357729 3rd Qu.: 0.264647 3rd Qu.: 0.35653 Max. : 8.412973 Max. : 9.552365 Max. : 9.00982 ReturnApr ReturnMay ReturnJune Min. :-4.47865 Min. :-5.84445 Min. :-4.73628 1st Qu.:-0.51121 1st Qu.:-0.43819 1st Qu.:-0.44968 Median :-0.11414 Median :-0.05346 Median :-0.02678 Mean :-0.03638 Mean : 0.02651 Mean : 0.04315 3rd Qu.: 0.32742 3rd Qu.: 0.42290 3rd Qu.: 0.43010 Max. : 6.84589 Max. : 7.21362 Max. :29.00534 ReturnJuly ReturnAug ReturnSep Min. :-5.201454 Min. :-4.62097 Min. :-3.57222 1st Qu.:-0.512039 1st Qu.:-0.51546 1st Qu.:-0.38067 Median :-0.026576 Median :-0.10277 Median : 0.08215 Mean : 0.006016 Mean :-0.04973 Mean : 0.02939 3rd Qu.: 0.457193 3rd Qu.: 0.38781 3rd Qu.: 0.45847 Max. :12.790901 Max. : 6.66889 Max. : 7.09106 ReturnOct ReturnNov Min. :-3.807577 Min. :-4.881463 1st Qu.:-0.393856 1st Qu.:-0.396764 Median : 0.006783 Median :-0.002337 Mean : 0.029672 Mean : 0.017128 3rd Qu.: 0.419005 3rd Qu.: 0.424617 Max. : 7.428466 Max. :21.007786   Problem 3.3 - Clustering Stocks Why is the mean ReturnJan variable much closer to 0 in normTrain than in normTest? #### The distribution of the ReturnJan variable is different in the training and testing set\n Problem 3.4 - Clustering Stocks Set the random seed to 144 (it is important to do this again, even though we did it earlier). Run k-means clustering with 3 clusters on normTrain, storing the result in an object called km.\nset.seed(144) km \u0026lt;- kmeans(normTrain, centers = 3) Which cluster has the largest number of observations?\ntable(km$cluster)  1 2 3 3157 4696 253  Cluster 2\n Problem 3.5 - Clustering Stocks Recall from the recitation that we can use the flexclust package to obtain training set and testing set cluster assignments for our observations (note that the call to as.kcca may take a while to complete):\nlibrary(flexclust) Loading required package: grid Loading required package: modeltools Loading required package: stats4 km.kcca \u0026lt;- as.kcca(km, normTrain) clusterTrain = predict(km.kcca) clusterTest = predict(km.kcca, newdata=normTest) How many test-set observations were assigned to Cluster 2?\ntable(clusterTest) clusterTest 1 2 3 1298 2080 96  2080\n Problem 4.1 - Cluster-Specific Predictions Using the subset function, build dataframes stocksTrain1, stocksTrain2, and stocksTrain3, containing the elements in the stocksTrain dataframe assigned to clusters 1, 2, and 3, respectively (be careful to take subsets of stocksTrain, not of normTrain). Similarly build stocksTest1, stocksTest2, and stocksTest3 from the stocksTest dataframe.\nstocksTrain1 \u0026lt;- subset(stocksTrain, clusterTrain == 1) stocksTrain2 \u0026lt;- subset(stocksTrain, clusterTrain == 2) stocksTrain3 \u0026lt;- subset(stocksTrain, clusterTrain == 3) stocksTest1 \u0026lt;- subset(stocksTest, clusterTest == 1) stocksTest2 \u0026lt;- subset(stocksTest, clusterTest == 2) stocksTest3 \u0026lt;- subset(stocksTest, clusterTest == 3) Which training set dataframe has the highest average value of the dependent variable?\nmean(stocksTrain1$PositiveDec) [1] 0.6024707 mean(stocksTrain2$PositiveDec) [1] 0.5140545 mean(stocksTrain3$PositiveDec) [1] 0.4387352 stocksTrain1\n Problem 4.2 - Cluster-Specific Predictions Build logistic regression models StocksModel1, StocksModel2, and StocksModel3, which predict PositiveDec using all the other variables as independent variables. StocksModel1 should be trained on stocksTrain1, StocksModel2 should be trained on stocksTrain2, and StocksModel3 should be trained on stocksTrain3.\nStocksModel1 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain1, family = binomial) StocksModel2 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain2, family = binomial) StocksModel3 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain3, family = binomial) Which variables have a positive sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3 and a negative sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3? Select all that apply.\nStocksModel1$coefficients (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.17223985 0.02498357 -0.37207369 0.59554957 1.19047752 0.30420906 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov -0.01165375 0.19769226 0.51272941 0.58832685 -1.02253506 -0.74847186  StocksModel2$coefficients (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.1029318 0.8845148 0.3176221 -0.3797811 0.4929105 0.8965492 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov 1.5008787 0.7831487 -0.2448602 0.7368522 -0.2775631 -0.7874737  StocksModel3$coefficients  (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr -0.181895809 -0.009789345 -0.046883260 0.674179495 1.281466189 ReturnMay ReturnJune ReturnJuly ReturnAug ReturnSep 0.762511555 0.329433917 0.774164370 0.982605385 0.363806823 ReturnOct ReturnNov 0.782242086 -0.873752144  rbind(StocksModel1$coefficients \u0026gt; 0, StocksModel2$coefficients \u0026gt; 0, StocksModel3$coefficients \u0026gt; 0)  (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay [1,] TRUE TRUE FALSE TRUE TRUE TRUE [2,] TRUE TRUE TRUE FALSE TRUE TRUE [3,] FALSE FALSE FALSE TRUE TRUE TRUE ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov [1,] FALSE TRUE TRUE TRUE FALSE FALSE [2,] TRUE TRUE FALSE TRUE FALSE FALSE [3,] TRUE TRUE TRUE TRUE TRUE FALSE ReturnJan, ReturnFeb, ReturnMar, ReturnJune, ReturnAug, ReturnOct\n Problem 4.3 - Cluster-Specific Predictions Using StocksModel1, make test-set predictions called PredictTest1 on the dataframe stocksTest1. Using StocksModel2, make test-set predictions called PredictTest2 on the dataframe stocksTest2. Using StocksModel3, make test-set predictions called PredictTest3 on the dataframe stocksTest3.\nPredictTest1 \u0026lt;- predict(StocksModel1, newdata = stocksTest1, type = \u0026quot;response\u0026quot;) PredictTest2 \u0026lt;- predict(StocksModel2, newdata = stocksTest2, type = \u0026quot;response\u0026quot;) PredictTest3 \u0026lt;- predict(StocksModel3, newdata = stocksTest3, type = \u0026quot;response\u0026quot;) What is the overall accuracy of StocksModel1 on the test-set stocksTest1, using a threshold of 0.5?\ntable(stocksTest1$PositiveDec, PredictTest1 \u0026gt;= 0.5)  FALSE TRUE 0 30 471 1 23 774 (30 + 774) / nrow(stocksTest1) [1] 0.6194145 What is the overall accuracy of StocksModel2 on the test-set stocksTest2, using a threshold of 0.5?\ntable(stocksTest2$PositiveDec, PredictTest2 \u0026gt;= 0.5)  FALSE TRUE 0 388 626 1 309 757 (388 + 757) / nrow(stocksTest2) [1] 0.5504808 What is the overall accuracy of StocksModel3 on the test-set stocksTest3, using a threshold of 0.5?\ntable(stocksTest3$PositiveDec, PredictTest3 \u0026gt;= 0.5)  FALSE TRUE 0 49 13 1 21 13 (49 + 13) / nrow(stocksTest3) [1] 0.6458333  Problem 4.4 - Cluster-Specific Predictions To compute the overall test-set accuracy of the cluster-then-predict approach, we can combine all the test-set predictions into a single vector and all the true outcomes into a single vector:\nAllPredictions \u0026lt;- c(PredictTest1, PredictTest2, PredictTest3) AllOutcomes \u0026lt;- c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec) What is the overall test-set accuracy of the cluster-then-predict approach, again using a threshold of 0.5?\ntable(AllOutcomes, AllPredictions \u0026gt;= 0.5)  AllOutcomes FALSE TRUE 0 467 1110 1 353 1544 length(AllOutcomes) [1] 3474 (467 + 1544) / length(AllOutcomes) [1] 0.5788716  Conclusion We see a modest improvement over the original logistic regression model. Since predicting stock returns is a notoriously hard problem, this is a good increase in accuracy. By investing in stocks for which we are more confident that they will have positive returns (by selecting the ones with higher predicted probabilities), this cluster-then-predict model can give us an edge over the original logistic regression model.\n ","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"5a590c082ca3588c01aea5bfd1ad85e0","permalink":"/project/stock_returns/stock/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/project/stock_returns/stock/","section":"project","summary":"Selecting which stocks to invest-in?","tags":["R","Data Analytics","Machine Learning"],"title":"Stock Returns Prediction","type":"project"},{"authors":null,"categories":null,"content":" The medical literature is enormous! Pubmed, a database of medical publications maintained by the U.S. National Library of Medicine, has indexed over 23 million medical publications. Further, the rate of medical publication has increased over time, and now there are nearly 1 million new publications in the field each year, or more than one per minute.\nThe large size and fast-changing nature of the medical literature has increased the need for reviews, which search databases like Pubmed for papers on a particular topic and then report results from the papers found. While such reviews are often performed manually, with multiple people reviewing each search result, this is tedious and time consuming. In this analysis, I‚Äôll see how text analytics can be used to automate the process of information retrieval.\nThe dataset consists of the titles (variable title) and abstracts (variable abstract) of papers retrieved in a Pubmed search. Each search result is labeled with whether the paper is a clinical trial testing a drug therapy for cancer (variable trial). These labels were obtained by two people reviewing each search result and accessing the actual paper if necessary, as part of a literature review of clinical trials testing drug therapies for advanced and metastatic breast cancer.\nLoading the packages  Problem 1.1 - Loading the Data Load clinical_trial.csv into a dataframe called trials (remembering to add the argument stringsAsFactors=FALSE when working with text analytics, so that the text is read in properply), and investigate the dataframe.\ntrials \u0026lt;- read.csv(\u0026quot;clinical_trial.csv\u0026quot;, stringsAsFactors = FALSE) str(trials) \u0026#39;data.frame\u0026#39;: 1860 obs. of 3 variables: $ title : chr \u0026quot;Treatment of Hodgkin\u0026#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).\u0026quot; \u0026quot;Cell mediated immune status in malignancy--pretherapy and post-therapy assessment.\u0026quot; \u0026quot;Neoadjuvant vinorelbine-capecitabine versus docetaxel-doxorubicin-cyclophosphamide in early nonresponsive breas\u0026quot;| __truncated__ \u0026quot;Randomized phase 3 trial of fluorouracil, epirubicin, and cyclophosphamide alone or followed by Paclitaxel for \u0026quot;| __truncated__ ... $ abstract: chr \u0026quot;\u0026quot; \u0026quot;Twenty-eight cases of malignancies of different kinds were studied to assess T-cell activity and population bef\u0026quot;| __truncated__ \u0026quot;BACKGROUND: Among breast cancer patients, nonresponse to initial neoadjuvant chemotherapy is associated with un\u0026quot;| __truncated__ \u0026quot;BACKGROUND: Taxanes are among the most active drugs for the treatment of metastatic breast cancer, and, as a co\u0026quot;| __truncated__ ... $ trial : int 1 0 1 1 1 0 1 0 0 0 ... summary(trials)  title abstract trial Length:1860 Length:1860 Min. :0.0000 Class :character Class :character 1st Qu.:0.0000 Mode :character Mode :character Median :0.0000 Mean :0.4392 3rd Qu.:1.0000 Max. :1.0000  IMPORTANT NOTE: Should you get an error like ‚Äúinvalid multibyte string‚Äù when performing certain parts of this analysis, use the argument fileEncoding=‚Äúlatin1‚Äù when reading in the file with read.csv. This should cause those errors to go away. We can use R‚Äôs string functions to learn more about the titles and abstracts of the located papers. The nchar() function counts the number of characters in a piece of text.\nUsing the nchar() function on the variables in the dataframe. How many characters are there in the longest abstract? (Longest here is defined as the abstract with the largest number of characters.)\nmax(nchar(trials$abstract)) [1] 3708 which.max(nchar(trials$abstract)) [1] 664 trials[664, ]  title 664 Five versus more than five years of tamoxifen therapy for breast cancer patients with negative lymph nodes and estrogen receptor-positive tumors. abstract 664 BACKGROUND: In 1982, the National Surgical Adjuvant Breast and Bowel Project initiated a randomized, double-blinded, placebo-controlled trial (B-14) to determine the effectiveness of adjuvant tamoxifen therapy in patients with primary operable breast cancer who had estrogen receptor-positive tumors and no axillary lymph node involvement. The findings indicated that tamoxifen therapy provided substantial benefit to patients with early stage disease. However, questions arose about how long the observed benefit would persist, about the duration of therapy necessary to maintain maximum benefit, and about the nature and severity of adverse effects from prolonged treatment.PURPOSE: We evaluated the outcome of patients in the B-14 trial through 10 years of follow-up. In addition, the effects of 5 years versus more than 5 years of tamoxifen therapy were compared.METHODS: In the trial, patients were initially assigned to receive either tamoxifen at 20 mg/day (n = 1404) or placebo (n = 1414). Tamoxifen-treated patients who remained disease free after 5 years of therapy were then reassigned to receive either another 5 years of tamoxifen (n = 322) or 5 years of placebo (n = 321). After the study began, another group of patients who met the same protocol eligibility requirements as the randomly assigned patients were registered to receive tamoxifen (n = 1211). Registered patients who were disease free after 5 years of treatment were also randomly assigned to another 5 years of tamoxifen (n = 261) or to 5 years of placebo (n = 249). To compare 5 years with more than 5 years of tamoxifen therapy, data relating to all patients reassigned to an additional 5 years of the drug were combined. Patients who were not reassigned to either tamoxifen or placebo continued to be followed in the study. Survival, disease-free survival, and distant disease-free survival (relating to failure at distant sites) were estimated by use of the Kaplan-Meier method; differences between the treatment groups were assessed by use of the logrank test. The relative risks of failure (with 95% confidence intervals [CIs]) were determined by use of the Cox proportional hazards model. Reported P values are two-sided.RESULTS: Through 10 years of follow-up, a significant advantage in disease-free survival (69% versus 57%, P \u0026lt; .0001; relative risk = 0.66; 95% CI = 0.58-0.74), distant disease-free survival (76% versus 67%, P \u0026lt; .0001; relative risk = 0.70; 95% CI = 0.61-0.81), and survival (80% versus 76%, P = .02; relative risk = 0.84; 95% CI = 0.71-0.99) was found for patients in the group first assigned to receive tamoxifen. The survival benefit extended to those 49 years of age or younger and to those 50 years of age or older. Tamoxifen therapy was associated with a 37% reduction in the incidence of contralateral (opposite) breast cancer (P = .007). Through 4 years after the reassignment of tamoxifen-treated patients to either continued-therapy or placebo groups, advantages in disease-free survival (92% versus 86%, P = .003) and distant disease-free survival (96% versus 90%, P = .01) were found for those who discontinued tamoxifen treatment. Survival was 96% for those who discontinued tamoxifen compared with 94% for those who continued tamoxifen treatment (P = .08). A higher incidence of thromboembolic events was seen in tamoxifen-treated patients (through 5 years, 1.7% versus 0.4%). Except for endometrial cancer, the incidence of second cancers was not increased with tamoxifen therapy.CONCLUSIONS AND IMPLICATIONS: The benefit from 5 years of tamoxifen therapy persists through 10 years of follow-up. No additional advantage is obtained from continuing tamoxifen therapy for more than 5 years. trial 664 1 nchar(trials[664, ]$abstract) [1] 3708 3708\n Problem 1.2 - Loading the Data How many search results provided no abstract? (HINT: A search result provided no abstract if the number of characters in the abstract field is zero.)\nsum(nchar(trials$abstract) == 0) [1] 112  Problem 1.3 - Loading the Data Find the observation with the minimum number of characters in the title (the variable ‚Äútitle‚Äù) out of all of the observations in this dataset. What is the text of the title of this article?\nInclude capitalization and punctuation in our response, but don‚Äôt include the quotes.\nmin(nchar(trials$title)) [1] 28 which.min(nchar(trials$title)) [1] 1258 trials[1258,]  title 1258 A decade of letrozole: FACE. abstract 1258 Third-generation nonsteroidal aromatase inhibitors (AIs), letrozole and anastrozole, are superior to tamoxifen as initial therapy for early breast cancer but have not been directly compared in a head-to-head adjuvant trial. Cumulative evidence suggests that AIs are not equivalent in terms of potency of estrogen suppression and that there may be differences in clinical efficacy. Thus, with no data from head-to-head comparisons of the AIs as adjuvant therapy yet available, the question of whether there are efficacy differences between the AIs remains. To help answer this question, the Femara versus Anastrozole Clinical Evaluation (FACE) is a phase IIIb open-label, randomized, multicenter trial designed to test whether letrozole or anastrozole has superior efficacy as adjuvant treatment of postmenopausal women with hormone receptor (HR)- and lymph node-positive breast cancer. Eligible patients (target accrual, N=4,000) are randomized to receive either letrozole 2.5 mg or anastrozole 1 mg daily for up to 5 years. The primary objective is to compare disease-free survival at 5 years. Secondary end points include safety, overall survival, time to distant metastases, and time to contralateral breast cancer. The FACE trial will determine whether or not letrozole offers a greater clinical benefit to postmenopausal women with HR+ early breast cancer at increased risk of early recurrence compared with anastrozole. trial 1258 0 trials[1258,]$title [1] \u0026quot;A decade of letrozole: FACE.\u0026quot;  Problem 2.1 - Preparing the Corpus Because we have both title and abstract information for trials, we need to build two corpera instead of one. Naming them corpusTitle and corpusAbstract.\nThe code performs the following tasks (you might need to load the ‚Äútm‚Äù package first if it isn‚Äôt already loaded). Making sure to perform them in this order.\n# 1) Convert the title variable to corpusTitle and the abstract variable to corpusAbstract corpusTitle \u0026lt;- Corpus(VectorSource(trials$title)) corpusTitle[[1]]$content [1] \u0026quot;Treatment of Hodgkin\u0026#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).\u0026quot; corpusAbstract \u0026lt;- Corpus(VectorSource(trials$abstract)) corpusAbstract[[1]]$content [1] \u0026quot;\u0026quot; # 2) Convert corpusTitle and corpusAbstract to lowercase corpusTitle = tm_map(corpusTitle, content_transformer(tolower)) Warning in tm_map.SimpleCorpus(corpusTitle, content_transformer(tolower)): transformation drops documents corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower)) Warning in tm_map.SimpleCorpus(corpusAbstract, content_transformer(tolower)): transformation drops documents #corpusTitle = tm_map(corpusTitle, PlainTextDocument) #corpusAbstract = tm_map(corpusAbstract, PlainTextDocument) # 3) Remove the punctuation in corpusTitle and corpusAbstract corpusTitle = tm_map(corpusTitle, removePunctuation) Warning in tm_map.SimpleCorpus(corpusTitle, removePunctuation): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediated immune status in malignancypretherapy and posttherapy assessment\u0026quot; corpusAbstract = tm_map(corpusAbstract, removePunctuation) Warning in tm_map.SimpleCorpus(corpusAbstract, removePunctuation): transformation drops documents # 4) Remove the English language stop words from corpusTitle and corpusAbstract corpusTitle \u0026lt;- tm_map(corpusTitle, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusTitle, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediated immune status malignancypretherapy posttherapy assessment\u0026quot; corpusAbstract \u0026lt;- tm_map(corpusAbstract, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusAbstract, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusAbstract[[2]]$content [1] \u0026quot;twentyeight cases malignancies different kinds studied assess tcell activity population institution therapy fifteen cases diagnosed nonmetastasising squamous cell carcinoma larynx pharynx laryngopharynx hypopharynx tonsils seven cases nonmetastasising infiltrating duct carcinoma breast 6 cases nonhodgkins lymphoma nhl observed 3 15 cases 20 squamous cell carcinoma cases mantoux test mt negative tcell population less 40 2 7 cases 286 infiltrating duct carcinoma breast mt negative tcell population less 40 3 6 cases 50 nhl mt negative tcell population less 40 normal controls consisting apparently normal healthy adults tcell population 40 mt positive patients showed negative skin test tcell population less 40 subjected assessment tcell population activity appropriate therapy clinical cure disease observed 2 3 cases 6666 squamous cell carcinomas 2 2 cases 100 adenocarcinomas one 3 cases 3333 nhl showed positive conversion tcell population 40\u0026quot; # 5) Stem the words in corpusTitle and corpusAbstract (each stemming might take a few minutes) corpusTitle = tm_map(corpusTitle, stemDocument) Warning in tm_map.SimpleCorpus(corpusTitle, stemDocument): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediat immun status malignancypretherapi posttherapi assess\u0026quot; corpusAbstract = tm_map(corpusAbstract, stemDocument) Warning in tm_map.SimpleCorpus(corpusAbstract, stemDocument): transformation drops documents corpusAbstract[[2]]$content [1] \u0026quot;twentyeight case malign differ kind studi assess tcell activ popul institut therapi fifteen case diagnos nonmetastasis squamous cell carcinoma larynx pharynx laryngopharynx hypopharynx tonsil seven case nonmetastasis infiltr duct carcinoma breast 6 case nonhodgkin lymphoma nhl observ 3 15 case 20 squamous cell carcinoma case mantoux test mt negat tcell popul less 40 2 7 case 286 infiltr duct carcinoma breast mt negat tcell popul less 40 3 6 case 50 nhl mt negat tcell popul less 40 normal control consist appar normal healthi adult tcell popul 40 mt posit patient show negat skin test tcell popul less 40 subject assess tcell popul activ appropri therapi clinic cure diseas observ 2 3 case 6666 squamous cell carcinoma 2 2 case 100 adenocarcinoma one 3 case 3333 nhl show posit convers tcell popul 40\u0026quot; # 6) Build a document term matrix called dtmTitle from corpusTitle and dtmAbstract from corpusAbstract dtmTitle = DocumentTermMatrix(corpusTitle) dtmTitle \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 2836)\u0026gt;\u0026gt; Non-/sparse entries: 23416/5251544 Sparsity : 100% Maximal term length: 49 Weighting : term frequency (tf) dtmAbstract = DocumentTermMatrix(corpusAbstract) dtmAbstract \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 12451)\u0026gt;\u0026gt; Non-/sparse entries: 153290/23005570 Sparsity : 99% Maximal term length: 67 Weighting : term frequency (tf) # 7) Limit dtmTitle and dtmAbstract to terms with sparseness of at most 95% (aka terms that appear in at least 5% of documents) dtmTitle \u0026lt;- removeSparseTerms(dtmTitle, 0.95) dtmTitle \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 31)\u0026gt;\u0026gt; Non-/sparse entries: 10683/46977 Sparsity : 81% Maximal term length: 15 Weighting : term frequency (tf) dtmAbstract \u0026lt;- removeSparseTerms(dtmAbstract, 0.95) dtmAbstract \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 335)\u0026gt;\u0026gt; Non-/sparse entries: 91969/531131 Sparsity : 85% Maximal term length: 15 Weighting : term frequency (tf) # 8) Convert dtmTitle and dtmAbstract to data frames (keep the names dtmTitle and dtmAbstract) dtmTitle \u0026lt;- as.data.frame(as.matrix(dtmTitle)) dtmAbstract \u0026lt;- as.data.frame(as.matrix(dtmAbstract)) When removing stop words, use tm_map(corpusTitle, removeWords, sw) and tm_map(corpusAbstract, removeWords, sw) instead of tm_map(corpusTitle, removeWords, stopwords(‚Äúenglish‚Äù)) and tm_map(corpusAbstract, removeWords, stopwords(‚Äúenglish‚Äù)).\nlength(stopwords(\u0026quot;english\u0026quot;)) [1] 174 How many terms remain in dtmTitle after removing sparse terms (aka how many columns does it have)?\nstr(dtmTitle) \u0026#39;data.frame\u0026#39;: 1860 obs. of 31 variables: $ cancer : num 1 0 1 1 1 1 0 1 1 2 ... $ treatment : num 1 0 0 0 1 0 0 0 0 1 ... $ breast : num 0 0 1 1 1 1 0 1 1 1 ... $ earli : num 0 0 1 1 0 0 0 1 0 0 ... $ iii : num 0 0 1 0 0 0 0 0 0 1 ... $ phase : num 0 0 1 1 0 0 0 0 0 1 ... $ random : num 0 0 1 1 1 0 0 0 0 1 ... $ trial : num 0 0 1 1 1 0 0 1 1 1 ... $ versus : num 0 0 1 0 0 0 0 1 0 0 ... $ cyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ chemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ combin : num 0 0 0 0 1 0 1 0 0 0 ... $ effect : num 0 0 0 0 1 0 0 1 0 1 ... $ metastat : num 0 0 0 0 1 0 0 0 0 0 ... $ patient : num 0 0 0 0 1 0 1 0 1 1 ... $ respons : num 0 0 0 0 0 1 0 0 0 0 ... $ advanc : num 0 0 0 0 0 0 1 0 0 0 ... $ postmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ randomis : num 0 0 0 0 0 0 0 1 1 0 ... $ studi : num 0 0 0 0 0 0 0 1 0 0 ... $ tamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ women : num 0 0 0 0 0 0 0 1 0 0 ... $ adjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ group : num 0 0 0 0 0 0 0 0 1 1 ... $ therapi : num 0 0 0 0 0 0 0 0 0 0 ... $ compar : num 0 0 0 0 0 0 0 0 0 0 ... $ doxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ docetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ result : num 0 0 0 0 0 0 0 0 0 0 ... $ plus : num 0 0 0 0 0 0 0 0 0 0 ... $ clinic : num 0 0 0 0 0 0 0 0 0 0 ... 31 How many terms remain in dtmAbstract?\nstr(dtmAbstract) \u0026#39;data.frame\u0026#39;: 1860 obs. of 335 variables: $ 100 : num 0 1 0 0 0 0 0 0 0 0 ... $ activ : num 0 2 0 1 0 0 1 0 0 0 ... $ assess : num 0 2 1 2 0 1 0 0 0 3 ... $ breast : num 0 2 3 3 3 4 2 2 2 3 ... $ carcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ case : num 0 11 0 0 1 0 0 0 0 0 ... $ cell : num 0 3 0 0 0 1 0 0 0 0 ... $ clinic : num 0 1 0 1 0 0 0 0 0 0 ... $ consist : num 0 1 0 0 0 0 0 0 0 0 ... $ control : num 0 1 0 0 0 0 0 0 1 0 ... $ differ : num 0 1 2 1 3 0 0 1 0 1 ... $ diseas : num 0 1 0 1 3 0 0 1 0 0 ... $ less : num 0 4 1 0 0 0 0 0 0 6 ... $ negat : num 0 4 0 0 0 3 0 0 0 0 ... $ observ : num 0 2 1 0 1 0 0 0 0 0 ... $ one : num 0 1 0 0 0 0 0 0 0 0 ... $ patient : num 0 1 9 5 5 6 8 3 2 5 ... $ popul : num 0 8 0 0 0 0 0 0 0 0 ... $ posit : num 0 2 0 1 0 5 0 0 0 0 ... $ seven : num 0 1 0 0 0 0 0 0 0 0 ... $ show : num 0 2 0 0 1 0 1 0 0 3 ... $ studi : num 0 1 1 1 0 1 3 2 0 1 ... $ test : num 0 2 1 1 0 0 0 0 0 0 ... $ therapi : num 0 2 0 1 0 0 0 0 0 0 ... $ 500 : num 0 0 1 0 2 0 0 0 0 0 ... $ addit : num 0 0 2 0 0 0 0 0 0 0 ... $ among : num 0 0 2 4 0 0 0 0 1 0 ... $ arm : num 0 0 7 4 2 0 0 1 0 1 ... $ assign : num 0 0 2 1 0 0 0 1 1 1 ... $ associ : num 0 0 1 3 0 2 0 1 2 0 ... $ background : num 0 0 1 1 1 0 0 1 1 0 ... $ better : num 0 0 1 0 1 1 0 0 0 0 ... $ cancer : num 0 0 2 3 3 3 2 2 3 0 ... $ chang : num 0 0 1 0 0 0 0 0 0 0 ... $ chemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ compar : num 0 0 1 2 0 0 0 1 1 1 ... $ complet : num 0 0 3 0 1 1 1 2 0 0 ... $ conclus : num 0 0 1 0 1 0 0 0 0 0 ... $ confid : num 0 0 1 1 0 0 0 0 0 0 ... $ continu : num 0 0 2 0 1 0 0 2 0 0 ... $ cycl : num 0 0 6 0 2 0 1 0 0 0 ... $ cyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ day : num 0 0 1 0 0 0 3 0 0 0 ... $ decreas : num 0 0 1 0 0 0 0 0 1 0 ... $ defin : num 0 0 2 0 0 0 0 0 0 0 ... $ demonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ docetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ doxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ effect : num 0 0 2 0 0 1 0 2 0 1 ... $ efficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ enrol : num 0 0 1 0 0 0 1 0 0 1 ... $ four : num 0 0 4 0 0 0 0 0 0 0 ... $ hematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ initi : num 0 0 3 0 0 0 0 1 0 0 ... $ interv : num 0 0 1 1 0 0 0 0 0 0 ... $ least : num 0 0 2 0 0 0 0 0 0 0 ... $ lymph : num 0 0 1 4 0 3 0 0 1 0 ... $ method : num 0 0 1 0 1 0 0 1 1 0 ... $ mgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ neoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ node : num 0 0 1 3 0 0 0 0 1 0 ... $ number : num 0 0 1 1 0 2 1 0 0 0 ... $ outcom : num 0 0 2 0 0 0 0 0 0 2 ... $ patholog : num 0 0 3 0 0 1 0 0 0 0 ... $ per : num 0 0 1 0 0 0 0 0 0 0 ... $ previous : num 0 0 1 0 1 0 2 0 0 0 ... $ random : num 0 0 2 1 1 0 0 1 1 2 ... $ rate : num 0 0 1 1 2 0 1 0 1 0 ... $ receiv : num 0 0 2 0 1 1 3 0 0 2 ... $ reduct : num 0 0 1 2 0 0 0 1 0 0 ... $ regimen : num 0 0 2 0 0 0 1 0 0 0 ... $ respond : num 0 0 2 0 0 0 0 0 0 0 ... $ respons : num 0 0 7 0 4 2 2 0 0 0 ... $ result : num 0 0 1 0 1 0 1 0 0 0 ... $ similar : num 0 0 2 0 0 0 0 0 1 0 ... $ size : num 0 0 1 3 0 1 0 0 0 0 ... $ statist : num 0 0 1 3 0 0 0 0 0 0 ... $ surgeri : num 0 0 1 1 0 0 0 0 0 0 ... $ toler : num 0 0 1 0 0 0 1 0 0 0 ... $ toxic : num 0 0 2 0 1 0 1 0 0 5 ... $ treatment : num 0 0 3 6 14 0 0 4 1 1 ... $ tumor : num 0 0 2 4 0 0 2 0 0 0 ... $ two : num 0 0 3 0 1 0 0 1 0 0 ... $ 001 : num 0 0 0 1 0 0 0 1 0 0 ... $ adjuv : num 0 0 0 2 0 1 0 2 4 0 ... $ age : num 0 0 0 1 0 0 0 0 0 0 ... $ also : num 0 0 0 1 0 0 0 0 2 0 ... $ analysi : num 0 0 0 2 0 0 0 1 0 0 ... $ analyz : num 0 0 0 1 0 0 0 0 0 0 ... $ axillari : num 0 0 0 1 0 0 0 0 1 0 ... $ death : num 0 0 0 2 0 0 0 0 1 0 ... $ dfs : num 0 0 0 3 0 0 0 0 0 0 ... $ diseasefre : num 0 0 0 1 0 2 0 0 3 0 ... $ drug : num 0 0 0 1 0 0 0 0 0 0 ... $ elig : num 0 0 0 1 0 0 0 0 0 0 ... $ endpoint : num 0 0 0 2 0 0 0 0 1 3 ... $ epirubicin : num 0 0 0 1 1 0 0 0 0 0 ... $ estim : num 0 0 0 1 1 0 0 0 0 0 ... $ fluorouracil : num 0 0 0 1 1 0 0 0 0 0 ... [list output truncated]  335   Problem 2.2 - Preparing the Corpus What is the most likely reason why dtmAbstract has so many more terms than dtmTitle? #### Abstracts tend to have many more words than titles\n Problem 2.3 - Preparing the Corpus What is the most frequent word stem across all the abstracts? Hint: you can use colSums() to compute the frequency of a word across all the abstracts.\n?colSums colSums(dtmAbstract)  100 activ assess breast 225 509 668 3859 carcinoma case cell clinic 251 233 359 944 consist control differ diseas 200 621 1176 950 less negat observ one 351 258 700 570 patient popul posit seven 8381 162 511 108 show studi test therapi 516 1965 282 1564 500 addit among arm 169 420 365 1038 assign associ background better 435 604 397 186 cancer chang chemotherapi compar 3726 431 2344 1359 complet conclus confid continu 628 842 241 281 cycl cyclophosphamid day decreas 962 632 1245 350 defin demonstr docetaxel doxorubicin 123 251 514 486 effect efficaci enrol four 1340 591 221 369 hematolog initi interv least 117 275 349 177 lymph method mgm2 neoadjuv 249 892 1093 293 node number outcom patholog 477 296 335 254 per previous random rate 218 355 1520 1253 receiv reduct regimen respond 1908 301 807 200 respons result similar size 2051 1485 438 177 statist surgeri toler toxic 384 407 373 1065 treatment tumor two 001 2894 1122 889 162 adjuv age also analysi 1162 429 364 587 analyz axillari death dfs 124 292 215 310 diseasefre drug elig endpoint 364 332 196 213 epirubicin estim fluorouracil follow 339 139 215 675 found hazard her2 hormon 238 301 314 428 includ involv marker metastat 529 180 189 755 model multivari nodeposit oper 180 154 199 193 overal paclitaxel predict primari 962 397 369 718 prognost proport ratio receptor 242 125 344 573 reduc relaps respect risk 400 254 758 635 sampl secondari signific status 172 158 2043 538 surviv type valu week 1927 126 256 1074 women year agent benefit 1484 1335 240 551 combin detect determin durat 926 148 352 344 either evalu everi evid 532 926 487 150 firstlin howev life may 182 339 178 413 measur object partial perform 411 400 295 342 plus potenti progress prolong 622 156 622 125 qualiti score singl stabl 189 254 149 154 subgroup term time total 192 153 881 397 use vomit whether 0001 1053 174 235 249 5fluorouracil aim correl express 208 185 203 356 factor followup grade group 552 494 580 2668 high independ larg level 378 149 108 743 longer low median month 193 196 1180 1575 premenopaus progesteron randomis remain 303 114 264 158 trend tumour wherea achiev 115 320 173 245 administ advanc daili dose 322 556 412 1123 indic infus intraven neutropenia 269 237 192 234 phase prior support treat 481 305 183 893 trial aromatas caus due 1417 171 115 154 earli end estrogen increas 325 221 421 729 inhibitor lower particip point 182 236 144 202 postmenopaus receptorposit tamoxifen versus 590 152 1632 570 within alon can distant 172 472 191 149 find first higher iii 177 421 415 266 improv limit mastectomi metastas 562 127 165 352 occur postop radiotherapi recurr 312 177 244 465 site stage system advers 183 286 193 256 baselin common doubleblind event 340 191 149 409 greater mean placebo purpos 279 310 475 434 obtain oral present relat 147 422 218 351 suggest bone administr cours 274 514 218 283 given safeti schedul seen 374 265 215 199 standard without although new 305 306 191 171 accord anthracyclin base eight 182 207 124 124 histolog investig local set 127 295 300 191 analys sever three import 177 288 564 138 shown 005 avail data 117 124 108 405 incid period profil report 300 170 158 357 endocrin hundr multicent design 266 195 126 219 growth human pretreat well 208 144 175 328 examin six appear identifi 190 261 164 148 nausea provid regress five 239 155 120 173 conduct prospect develop sequenti 177 239 259 168 serum comparison frequent cmf 315 116 153 586 consid rang select possibl 131 248 124 130 inform major need function 124 122 128 188 failur confirm requir experienc 262 178 168 167 characterist methotrex progressionfre general 119 265 158 126 prevent andor mbc main 143 128 276 131 side superior start tissu 168 161 131 197 second regard enter 138 105 117  max(colSums(dtmAbstract)) [1] 8381 which.max(colSums(dtmAbstract)) patient 17  dtmAbstract$patient  [1] 0 1 9 5 5 6 8 3 2 5 2 4 2 1 1 3 0 8 6 0 3 7 7 [24] 4 2 5 2 5 0 0 3 2 3 5 12 3 5 4 7 2 0 1 2 3 6 5 [47] 1 4 7 6 2 5 6 5 5 6 9 5 1 5 10 6 3 3 6 1 4 4 0 [70] 6 9 5 9 1 11 5 0 3 5 6 3 8 8 2 6 9 3 7 4 1 6 13 [93] 3 12 0 5 5 3 0 1 8 16 5 13 0 7 2 7 0 7 3 2 5 1 6 [116] 13 4 5 4 4 4 6 5 6 5 3 1 3 3 0 5 1 3 9 8 2 3 7 [139] 5 3 4 4 11 2 9 1 1 6 8 1 2 1 1 7 0 2 4 6 16 0 3 [162] 0 3 8 3 5 2 11 5 0 0 2 8 8 1 4 7 11 6 5 6 1 7 2 [185] 14 13 4 23 7 3 6 3 3 2 3 4 11 2 4 12 3 6 4 3 6 9 4 [208] 8 2 4 2 6 1 7 6 5 5 14 5 13 0 11 1 7 6 3 5 4 3 7 [231] 0 0 7 2 4 5 7 0 4 4 16 0 8 3 0 3 7 6 6 2 2 1 5 [254] 4 7 8 1 1 5 4 2 3 16 2 1 5 7 3 4 0 6 3 2 1 5 6 [277] 6 2 9 6 2 6 5 7 9 10 9 5 2 4 5 0 0 3 3 6 4 6 3 [300] 9 6 3 3 8 5 0 7 9 4 2 7 3 5 7 4 0 2 6 8 1 4 5 [323] 2 1 2 2 4 6 5 13 2 0 10 4 4 4 9 7 2 5 3 4 4 4 4 [346] 3 9 2 5 0 9 0 4 5 2 7 5 1 4 5 5 5 4 2 0 2 9 9 [369] 1 0 6 3 7 8 0 2 6 5 2 1 2 1 8 3 6 6 7 17 3 7 6 [392] 3 8 9 1 8 5 10 0 7 4 6 8 3 8 0 9 8 3 1 0 0 7 4 [415] 8 5 4 4 5 5 4 4 0 3 3 5 1 9 3 2 3 3 8 4 9 7 0 [438] 5 1 0 10 7 6 5 7 4 5 2 13 1 6 7 3 4 5 7 0 5 4 4 [461] 2 6 4 3 2 2 5 5 5 0 1 0 8 2 0 2 0 2 8 11 0 2 3 [484] 5 7 6 4 3 6 4 7 10 12 7 11 7 2 8 6 2 4 4 5 5 3 8 [507] 3 0 0 5 4 5 1 4 0 0 6 3 3 3 6 2 3 2 3 4 11 3 5 [530] 7 2 6 6 2 4 6 1 13 5 3 7 4 3 3 4 3 8 3 5 1 5 3 [553] 0 11 5 7 2 1 0 6 4 5 5 4 3 8 4 4 1 4 2 7 9 14 10 [576] 0 6 2 1 8 0 2 5 2 2 1 5 0 10 6 5 0 1 1 4 4 4 0 [599] 0 4 8 3 3 8 7 5 7 1 2 3 3 9 5 11 8 8 6 4 3 2 0 [622] 2 6 9 1 0 12 3 2 12 9 6 3 6 0 4 5 4 1 13 3 0 2 5 [645] 3 1 8 5 9 5 5 9 5 5 7 6 5 6 6 3 0 6 7 13 7 9 5 [668] 6 7 5 2 6 1 3 0 3 4 3 3 0 10 0 7 0 6 3 9 4 7 12 [691] 0 0 7 9 0 7 6 6 3 2 5 2 0 5 5 7 4 5 5 3 6 2 7 [714] 1 2 1 6 4 11 9 2 0 8 10 2 7 2 6 1 8 0 0 4 3 9 5 [737] 2 3 1 7 4 0 0 0 6 3 13 3 5 5 0 4 2 3 6 4 5 0 1 [760] 4 9 12 4 0 10 5 9 4 3 10 6 2 3 5 3 9 3 6 1 5 7 0 [783] 4 5 3 4 1 0 1 3 10 17 0 7 5 2 0 9 0 1 5 8 0 6 9 [806] 2 0 9 10 0 3 3 4 6 5 5 1 1 12 10 4 0 5 4 6 3 7 2 [829] 1 0 0 4 0 10 5 8 9 8 1 3 2 6 1 12 2 0 5 2 9 3 4 [852] 7 5 3 10 6 6 9 0 4 7 4 1 6 0 6 5 8 3 3 0 4 9 8 [875] 2 8 3 11 7 1 5 4 7 6 8 3 1 5 6 6 0 5 20 5 4 13 2 [898] 1 0 2 1 3 4 10 4 5 5 1 9 0 1 0 0 11 3 12 4 6 7 4 [921] 0 3 3 5 6 11 0 1 4 10 2 5 8 1 0 7 6 2 5 4 0 2 1 [944] 6 5 10 3 0 0 3 12 3 4 10 0 5 3 3 9 0 1 3 17 1 8 14 [967] 6 3 3 2 8 0 6 8 1 0 11 10 10 18 0 7 9 2 5 1 7 5 5 [990] 5 6 4 3 7 8 2 10 5 0 10 5 5 0 5 3 1 4 4 3 2 6 1 [1013] 2 6 3 6 14 2 0 6 5 3 4 9 2 5 8 4 5 3 4 7 5 6 2 [1036] 3 4 7 8 11 8 6 9 4 7 5 14 7 2 12 0 5 3 6 1 4 0 1 [1059] 6 5 4 3 0 2 2 1 6 5 2 1 1 9 6 11 4 1 5 5 0 5 10 [1082] 0 5 2 2 4 5 7 1 4 5 2 10 0 0 5 0 2 3 3 6 2 8 3 [1105] 3 3 1 5 6 0 0 2 4 5 2 8 10 7 9 5 10 4 7 10 6 10 6 [1128] 3 0 0 7 6 8 4 4 4 0 8 7 7 1 6 6 10 8 4 4 5 11 7 [1151] 5 7 4 6 4 4 7 3 12 12 0 3 0 4 2 7 4 4 6 0 6 6 1 [1174] 9 5 6 11 6 15 3 2 3 6 4 5 4 1 3 0 3 0 2 6 0 1 7 [1197] 2 8 0 0 0 4 11 9 2 10 3 6 5 2 1 11 0 6 1 5 4 3 2 [1220] 2 6 7 1 11 6 10 5 4 7 10 4 0 4 4 0 2 8 7 5 4 6 4 [1243] 0 1 7 6 5 0 12 7 7 7 5 7 0 11 5 1 8 11 3 1 8 4 3 [1266] 3 7 2 8 2 4 0 7 5 0 1 0 9 7 1 7 4 7 8 3 1 6 3 [1289] 2 4 3 2 3 9 6 2 3 6 5 11 5 7 8 0 6 4 5 4 1 1 3 [1312] 5 2 0 6 3 3 0 6 6 0 7 11 0 3 3 7 5 2 0 3 5 2 0 [1335] 4 4 3 6 4 3 4 5 0 7 1 6 6 6 5 5 13 15 2 7 12 3 2 [1358] 6 5 5 5 9 7 0 5 7 4 8 2 0 2 2 7 8 2 4 0 7 9 4 [1381] 4 6 2 8 4 0 1 10 4 4 5 3 4 6 4 8 2 6 12 5 5 5 4 [1404] 5 11 4 4 7 5 6 7 2 3 6 3 3 2 5 2 4 1 2 2 7 7 4 [1427] 13 4 12 4 7 7 4 2 0 7 6 7 6 7 4 5 5 8 5 2 5 1 3 [1450] 0 10 2 0 5 3 3 11 1 0 3 1 5 0 2 0 3 2 0 6 0 3 2 [1473] 4 11 0 0 1 4 1 3 2 1 5 0 2 3 2 3 10 3 4 2 0 0 9 [1496] 4 8 4 0 2 5 5 0 0 2 0 8 3 0 9 4 3 2 2 4 1 0 6 [1519] 2 2 2 0 3 0 0 6 0 0 4 2 4 12 2 8 6 2 1 0 6 3 4 [1542] 5 0 6 1 1 2 5 1 3 4 11 9 0 6 2 7 5 3 7 5 0 2 7 [1565] 10 2 3 3 5 3 4 2 3 4 3 1 6 2 1 3 1 3 2 3 5 4 3 [1588] 0 1 0 5 1 3 9 1 6 4 1 1 7 2 0 1 7 3 0 5 2 5 2 [1611] 2 2 0 6 0 5 1 4 0 0 1 7 1 4 6 2 0 1 8 2 3 5 0 [1634] 7 7 7 4 7 7 3 5 6 1 6 4 16 5 8 7 3 1 7 3 10 4 7 [1657] 10 4 9 1 10 4 6 9 0 4 5 2 9 5 1 8 4 12 8 6 3 6 10 [1680] 2 3 1 1 13 4 2 5 7 10 7 10 5 4 3 8 4 3 6 5 2 9 6 [1703] 7 4 4 7 3 6 9 5 3 4 4 6 8 4 6 1 8 7 10 6 7 7 1 [1726] 9 6 15 10 10 9 8 6 5 8 5 6 7 4 8 9 8 8 9 10 6 5 6 [1749] 6 9 11 4 4 11 4 5 6 7 4 6 3 7 4 7 9 8 4 5 10 6 2 [1772] 0 8 5 5 6 3 7 4 0 2 4 1 0 3 6 10 3 14 3 10 0 5 0 [1795] 8 0 5 0 9 0 3 0 0 8 3 5 5 2 0 0 4 16 2 7 3 0 0 [1818] 0 7 6 0 2 5 5 0 4 5 10 9 3 5 9 4 1 4 4 2 0 3 2 [1841] 4 4 5 0 9 8 4 0 0 0 8 2 4 4 0 0 0 0 0 0  Problem 3.1 - Building a model We want to combine dtmTitle and dtmAbstract into a single dataframe to make predictions. However, some of the variables in these dataframes have the same names. To fix this issue, run the following code:\ncolnames(dtmTitle) \u0026lt;- paste0(\u0026quot;T\u0026quot;, colnames(dtmTitle)) str(dtmTitle) \u0026#39;data.frame\u0026#39;: 1860 obs. of 31 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... colnames(dtmAbstract) \u0026lt;- paste0(\u0026quot;A\u0026quot;, colnames(dtmAbstract)) str(dtmAbstract) \u0026#39;data.frame\u0026#39;: 1860 obs. of 335 variables: $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... $ Areceiv : num 0 0 2 0 1 1 3 0 0 2 ... $ Areduct : num 0 0 1 2 0 0 0 1 0 0 ... $ Aregimen : num 0 0 2 0 0 0 1 0 0 0 ... $ Arespond : num 0 0 2 0 0 0 0 0 0 0 ... $ Arespons : num 0 0 7 0 4 2 2 0 0 0 ... $ Aresult : num 0 0 1 0 1 0 1 0 0 0 ... $ Asimilar : num 0 0 2 0 0 0 0 0 1 0 ... $ Asize : num 0 0 1 3 0 1 0 0 0 0 ... $ Astatist : num 0 0 1 3 0 0 0 0 0 0 ... $ Asurgeri : num 0 0 1 1 0 0 0 0 0 0 ... $ Atoler : num 0 0 1 0 0 0 1 0 0 0 ... $ Atoxic : num 0 0 2 0 1 0 1 0 0 5 ... $ Atreatment : num 0 0 3 6 14 0 0 4 1 1 ... $ Atumor : num 0 0 2 4 0 0 2 0 0 0 ... $ Atwo : num 0 0 3 0 1 0 0 1 0 0 ... $ A001 : num 0 0 0 1 0 0 0 1 0 0 ... $ Aadjuv : num 0 0 0 2 0 1 0 2 4 0 ... $ Aage : num 0 0 0 1 0 0 0 0 0 0 ... $ Aalso : num 0 0 0 1 0 0 0 0 2 0 ... $ Aanalysi : num 0 0 0 2 0 0 0 1 0 0 ... $ Aanalyz : num 0 0 0 1 0 0 0 0 0 0 ... $ Aaxillari : num 0 0 0 1 0 0 0 0 1 0 ... $ Adeath : num 0 0 0 2 0 0 0 0 1 0 ... $ Adfs : num 0 0 0 3 0 0 0 0 0 0 ... $ Adiseasefre : num 0 0 0 1 0 2 0 0 3 0 ... $ Adrug : num 0 0 0 1 0 0 0 0 0 0 ... $ Aelig : num 0 0 0 1 0 0 0 0 0 0 ... $ Aendpoint : num 0 0 0 2 0 0 0 0 1 3 ... $ Aepirubicin : num 0 0 0 1 1 0 0 0 0 0 ... $ Aestim : num 0 0 0 1 1 0 0 0 0 0 ... $ Afluorouracil : num 0 0 0 1 1 0 0 0 0 0 ... [list output truncated] What was the effect of these functions? #### Adding the letter T in front of all the title variable names and adding the letter A in front of all the abstract variable names.\n Problem 3.2 - Building a Model Using cbind(), combine dtmTitle and dtmAbstract into a single dataframe called dtm:\ndtm \u0026lt;- cbind(dtmTitle, dtmAbstract) str(dtm) \u0026#39;data.frame\u0026#39;: 1860 obs. of 366 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... [list output truncated] Now, add the dependent variable ‚Äútrial‚Äù to dtm, copying it from the original dataframe called trials.\nHow many columns are in this combined dataframe?\ndtm$trial \u0026lt;- trials$trial str(dtm) \u0026#39;data.frame\u0026#39;: 1860 obs. of 367 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... [list output truncated] 367   Problem 3.3 - Building a Model Now that we have prepared our dataframe, it‚Äôs time to split it into a training and testing set and to build regression models. Set the random seed to 144 and use the sample.split function from the caTools package to split dtm into dataframes named ‚Äútrain‚Äù and ‚Äútest‚Äù, putting 70% of the data in the training set.\nset.seed(144) trialSplit \u0026lt;- sample.split(dtm$trial, 0.7) train \u0026lt;- subset(dtm, trialSplit == TRUE) test \u0026lt;- subset(dtm, trialSplit == FALSE) What is the accuracy of the baseline model on the training set? (Remember that the baseline model predicts the most frequent outcome in the training set for all observations.)\ntable(train$trial)  0 1 730 572  730 / (730 + 572) [1] 0.5606759  Problem 3.4 - Building a Model Build a CART model called trialCART, using all the independent variables in the training set to train the model, and then plot the CART model. Just use the default parameters to build the model (don‚Äôt add a minbucket or cp value). Remember to add the method=‚Äúclass‚Äù argument, since this is a classification problem.\ntrialCART \u0026lt;- rpart(trial ~ ., data = train, method = \u0026quot;class\u0026quot;) What is the name of the first variable the model split on?\nprp(trialCART) Tphase   Problem 3.5 - Building a Model Obtain the training set predictions for the model (do not yet predict on the test-set). Extract the predicted probability of a result being a trial (recall that this involves not setting a type argument, and keeping only the second column of the predict output).\nWhat is the maximum predicted probability for any result?\npredTrain \u0026lt;- predict(trialCART) predTrain[1:10,]  0 1 1 0.8636364 0.13636364 2 0.8636364 0.13636364 3 0.1281139 0.87188612 5 0.2176871 0.78231293 6 0.9454545 0.05454545 7 0.2176871 0.78231293 10 0.1281139 0.87188612 12 0.7125000 0.28750000 13 0.1281139 0.87188612 14 0.7125000 0.28750000 predTrainProb \u0026lt;- predTrain[, 2] max(predTrainProb) [1] 0.8718861 summary(predTrainProb)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.05455 0.13636 0.28750 0.43932 0.78231 0.87189  Problem 3.6 - Building a Model Without running the analysis, how do you expect the maximum predicted probability to differ in the testing set? #### The maximum predicted probability will likely be exactly the same in the testing set. Because the CART tree assigns the same predicted probability to each leaf node and there are a small number of leaf nodes compared to data points, we expect exactly the same maximum predicted probability.\n  Problem 3.7 - Building a Model For these questions, use a threshold probability of 0.5 to predict that an observation is a clinical trial.\ntable(train$trial, predTrainProb \u0026gt;= 0.5)  FALSE TRUE 0 631 99 1 131 441 What is the training set accuracy of the CART model?\n(631 + 441) / nrow(train) [1] 0.8233487 What is the training set sensitivity of the CART model?\n441 / (441 + 131) [1] 0.770979 What is the training set specificity of the CART model?\n631 / (631 + 99) [1] 0.8643836  Problem 4.1 - Evaluating the model on the testing set Evaluate the CART model on the testing set using the predict function and creating a vector of predicted probabilities predTest.\npred \u0026lt;- predict(trialCART, newdata = test) pred[1:10,]  0 1 4 0.1281139 0.8718861 8 0.8636364 0.1363636 9 0.7125000 0.2875000 11 0.7125000 0.2875000 19 0.7125000 0.2875000 31 0.8636364 0.1363636 40 0.8636364 0.1363636 42 0.8636364 0.1363636 43 0.8636364 0.1363636 48 0.7125000 0.2875000 predTest \u0026lt;- pred[, 2] What is the testing set accuracy, assuming a probability threshold of 0.5 for predicting that a result is a clinical trial?\ntable(test$trial, predTest \u0026gt;= 0.5)  FALSE TRUE 0 261 52 1 83 162 (261 + 162) / nrow(test) [1] 0.7580645  Problem 4.2 - Evaluating the Model on the Testing Set Using the ROCR package, what is the testing set AUC of the prediction model?\npredROCR \u0026lt;- prediction(predTest, test$trial) performance(predROCR, \u0026quot;auc\u0026quot;)@y.values [[1]] [1] 0.8371063  Problem 5.1 - Decision-Maker Tradeoffs What is the cost associated with the model in Step 1 making a false negative prediction? #### A paper that should have been included in Set A will be missed, affecting the quality of the results of Step 3.\n Problem 5.2 - Decision-Maker Tradeoffs What is the cost associated with the model in Step 1 making a false positive prediction? #### A paper will be mistakenly added to Set A, yielding additional work in Step 2 of the process but not affecting the quality of the results of Step 3.\n Problem 5.3 - Decision-Maker Tradeoffs Given the costs associated with false positives and false negatives, which of the following is most accurate? #### A false negative is more costly than a false positive; the decision maker should use a probability threshold less than 0.5 for the machine learning model.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"538e0fdefd2e3a8dcae36f46dbac43e1","permalink":"/project/medicine/medicine/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/medicine/medicine/","section":"project","summary":"Automate the process of information retrieval","tags":["R","Data Analytics","Machine Learning"],"title":"Automating Reviews In Medicine","type":"project"},{"authors":null,"categories":null,"content":" The United States government periodically collects demographic information by conducting a census.\nIn this analysis, I am are going to use census information about individuals to predict how much a person earns ‚Äì in particular, whether the person earns more than $50,000 per year. This data comes from the UCI Machine Learning Repository.\nThe file census.csv contains 1994 census data for 31,978 individuals in the U.S. The dataset includes the following 13 variables:\n age = the age of the individual in years workclass = the classification of the individual‚Äôs working status (does the person work for the federal government, work for the local government, work without pay, and so on) education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on) maritalstatus = the marital status of the individual occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on) relationship = relationship of individual to his/her household race = the individual‚Äôs race sex = the individual‚Äôs sex capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price) capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price) hoursperweek = the number of hours the individual works per week nativecountry = the native country of the individual over50k = whether or not the individual earned more than $50,000 in 1994  Problem 1.1 - A Logistic Regression Model Let‚Äôs begin by building a logistic regression model to predict whether an individual‚Äôs earnings are above $50,000 (the variable ‚Äúover50k‚Äù) using all of the other variables as independent variables.\nFirst, read the dataset census.csv into R.\ncensus \u0026lt;- read.csv(\u0026quot;census.csv\u0026quot;) str(census) \u0026#39;data.frame\u0026#39;: 31978 obs. of 13 variables: $ age : int 39 50 38 53 28 37 49 52 31 42 ... $ workclass : Factor w/ 9 levels \u0026quot; ?\u0026quot;,\u0026quot; Federal-gov\u0026quot;,..: 8 7 5 5 5 5 5 7 5 5 ... $ education : Factor w/ 16 levels \u0026quot; 10th\u0026quot;,\u0026quot; 11th\u0026quot;,..: 10 10 12 2 10 13 7 12 13 10 ... $ maritalstatus: Factor w/ 7 levels \u0026quot; Divorced\u0026quot;,\u0026quot; Married-AF-spouse\u0026quot;,..: 5 3 1 3 3 3 4 3 5 3 ... $ occupation : Factor w/ 15 levels \u0026quot; ?\u0026quot;,\u0026quot; Adm-clerical\u0026quot;,..: 2 5 7 7 11 5 9 5 11 5 ... $ relationship : Factor w/ 6 levels \u0026quot; Husband\u0026quot;,\u0026quot; Not-in-family\u0026quot;,..: 2 1 2 1 6 6 2 1 2 1 ... $ race : Factor w/ 5 levels \u0026quot; Amer-Indian-Eskimo\u0026quot;,..: 5 5 5 3 3 5 3 5 5 5 ... $ sex : Factor w/ 2 levels \u0026quot; Female\u0026quot;,\u0026quot; Male\u0026quot;: 2 2 2 2 1 1 1 2 1 2 ... $ capitalgain : int 2174 0 0 0 0 0 0 0 14084 5178 ... $ capitalloss : int 0 0 0 0 0 0 0 0 0 0 ... $ hoursperweek : int 40 13 40 40 40 40 16 45 50 40 ... $ nativecountry: Factor w/ 41 levels \u0026quot; Cambodia\u0026quot;,\u0026quot; Canada\u0026quot;,..: 39 39 39 39 5 39 23 39 39 39 ... $ over50k : Factor w/ 2 levels \u0026quot; \u0026lt;=50K\u0026quot;,\u0026quot; \u0026gt;50K\u0026quot;: 1 1 1 1 1 1 1 2 2 2 ... summary(census)  age workclass education Min. :17.00 Private :22286 HS-grad :10368 1st Qu.:28.00 Self-emp-not-inc: 2499 Some-college: 7187 Median :37.00 Local-gov : 2067 Bachelors : 5210 Mean :38.58 ? : 1809 Masters : 1674 3rd Qu.:48.00 State-gov : 1279 Assoc-voc : 1366 Max. :90.00 Self-emp-inc : 1074 11th : 1167 (Other) : 964 (Other) : 5006 maritalstatus occupation Divorced : 4394 Prof-specialty :4038 Married-AF-spouse : 23 Craft-repair :4030 Married-civ-spouse :14692 Exec-managerial:3992 Married-spouse-absent: 397 Adm-clerical :3721 Never-married :10488 Sales :3584 Separated : 1005 Other-service :3212 Widowed : 979 (Other) :9401 relationship race sex Husband :12947 Amer-Indian-Eskimo: 311 Female:10608 Not-in-family : 8156 Asian-Pac-Islander: 956 Male :21370 Other-relative: 952 Black : 3028 Own-child : 5005 Other : 253 Unmarried : 3384 White :27430 Wife : 1534 capitalgain capitalloss hoursperweek nativecountry Min. : 0 Min. : 0.00 Min. : 1.00 United-States:29170 1st Qu.: 0 1st Qu.: 0.00 1st Qu.:40.00 Mexico : 643 Median : 0 Median : 0.00 Median :40.00 Philippines : 198 Mean : 1064 Mean : 86.74 Mean :40.42 Germany : 137 3rd Qu.: 0 3rd Qu.: 0.00 3rd Qu.:45.00 Canada : 121 Max. :99999 Max. :4356.00 Max. :99.00 Puerto-Rico : 114 (Other) : 1595 over50k \u0026lt;=50K:24283 \u0026gt;50K : 7695  Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60% of the observations, while the testing set contains 40% of the observations.\nlibrary(caTools) set.seed(2000) censusSplit = sample.split(census$over50k, SplitRatio = 0.6) censusTrain = subset(census, censusSplit == TRUE) censusTest = subset(census, censusSplit == FALSE) nrow(censusTrain) [1] 19187 nrow(censusTest) [1] 12791 Next, build a logistic regression model to predict the dependent variable ‚Äúover50k‚Äù, using all of the other variables in the dataset as independent variables. Use the training set to build the model.\nWhich variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.)\nCensusLog \u0026lt;- glm(over50k ~ ., data = censusTrain, family = binomial) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(CensusLog)  Call: glm(formula = over50k ~ ., family = binomial, data = censusTrain) Deviance Residuals: Min 1Q Median 3Q Max -5.1065 -0.5037 -0.1804 -0.0008 3.3383 Coefficients: (1 not defined because of singularities) Estimate Std. Error z value (Intercept) -8.658e+00 1.379e+00 -6.279 age 2.548e-02 2.139e-03 11.916 workclass Federal-gov 1.105e+00 2.014e-01 5.489 workclass Local-gov 3.675e-01 1.821e-01 2.018 workclass Never-worked -1.283e+01 8.453e+02 -0.015 workclass Private 6.012e-01 1.626e-01 3.698 workclass Self-emp-inc 7.575e-01 1.950e-01 3.884 workclass Self-emp-not-inc 1.855e-01 1.774e-01 1.046 workclass State-gov 4.012e-01 1.961e-01 2.046 workclass Without-pay -1.395e+01 6.597e+02 -0.021 education 11th 2.225e-01 2.867e-01 0.776 education 12th 6.380e-01 3.597e-01 1.774 education 1st-4th -7.075e-01 7.760e-01 -0.912 education 5th-6th -3.170e-01 4.880e-01 -0.650 education 7th-8th -3.498e-01 3.126e-01 -1.119 education 9th -1.258e-01 3.539e-01 -0.355 education Assoc-acdm 1.602e+00 2.427e-01 6.601 education Assoc-voc 1.541e+00 2.368e-01 6.506 education Bachelors 2.177e+00 2.218e-01 9.817 education Doctorate 2.761e+00 2.893e-01 9.544 education HS-grad 1.006e+00 2.169e-01 4.638 education Masters 2.421e+00 2.353e-01 10.289 education Preschool -2.237e+01 6.864e+02 -0.033 education Prof-school 2.938e+00 2.753e-01 10.672 education Some-college 1.365e+00 2.195e-01 6.219 maritalstatus Married-AF-spouse 2.540e+00 7.145e-01 3.555 maritalstatus Married-civ-spouse 2.458e+00 3.573e-01 6.880 maritalstatus Married-spouse-absent -9.486e-02 3.204e-01 -0.296 maritalstatus Never-married -4.515e-01 1.139e-01 -3.962 maritalstatus Separated 3.609e-02 1.984e-01 0.182 maritalstatus Widowed 1.858e-01 1.962e-01 0.947 occupation Adm-clerical 9.470e-02 1.288e-01 0.735 occupation Armed-Forces -1.008e+00 1.487e+00 -0.677 occupation Craft-repair 2.174e-01 1.109e-01 1.960 occupation Exec-managerial 9.400e-01 1.138e-01 8.257 occupation Farming-fishing -1.068e+00 1.908e-01 -5.599 occupation Handlers-cleaners -6.237e-01 1.946e-01 -3.204 occupation Machine-op-inspct -1.862e-01 1.376e-01 -1.353 occupation Other-service -8.183e-01 1.641e-01 -4.987 occupation Priv-house-serv -1.297e+01 2.267e+02 -0.057 occupation Prof-specialty 6.331e-01 1.222e-01 5.180 occupation Protective-serv 6.267e-01 1.710e-01 3.664 occupation Sales 3.276e-01 1.175e-01 2.789 occupation Tech-support 6.173e-01 1.533e-01 4.028 occupation Transport-moving NA NA NA relationship Not-in-family 7.881e-01 3.530e-01 2.233 relationship Other-relative -2.194e-01 3.137e-01 -0.699 relationship Own-child -7.489e-01 3.507e-01 -2.136 relationship Unmarried 7.041e-01 3.720e-01 1.893 relationship Wife 1.324e+00 1.331e-01 9.942 race Asian-Pac-Islander 4.830e-01 3.548e-01 1.361 race Black 3.644e-01 2.882e-01 1.265 race Other 2.204e-01 4.513e-01 0.488 race White 4.108e-01 2.737e-01 1.501 sex Male 7.729e-01 1.024e-01 7.545 capitalgain 3.280e-04 1.372e-05 23.904 capitalloss 6.445e-04 4.854e-05 13.277 hoursperweek 2.897e-02 2.101e-03 13.791 nativecountry Canada 2.593e-01 1.308e+00 0.198 nativecountry China -9.695e-01 1.327e+00 -0.730 nativecountry Columbia -1.954e+00 1.526e+00 -1.280 nativecountry Cuba 5.735e-02 1.323e+00 0.043 nativecountry Dominican-Republic -1.435e+01 3.092e+02 -0.046 nativecountry Ecuador -3.550e-02 1.477e+00 -0.024 nativecountry El-Salvador -6.095e-01 1.395e+00 -0.437 nativecountry England -6.707e-02 1.327e+00 -0.051 nativecountry France 5.301e-01 1.419e+00 0.374 nativecountry Germany 5.474e-02 1.306e+00 0.042 nativecountry Greece -2.646e+00 1.714e+00 -1.544 nativecountry Guatemala -1.293e+01 3.345e+02 -0.039 nativecountry Haiti -9.221e-01 1.615e+00 -0.571 nativecountry Holand-Netherlands -1.282e+01 2.400e+03 -0.005 nativecountry Honduras -9.584e-01 3.412e+00 -0.281 nativecountry Hong -2.362e-01 1.492e+00 -0.158 nativecountry Hungary 1.412e-01 1.555e+00 0.091 nativecountry India -8.218e-01 1.314e+00 -0.625 nativecountry Iran -3.299e-02 1.366e+00 -0.024 nativecountry Ireland 1.579e-01 1.473e+00 0.107 nativecountry Italy 6.100e-01 1.333e+00 0.458 nativecountry Jamaica -2.279e-01 1.387e+00 -0.164 nativecountry Japan 5.072e-01 1.375e+00 0.369 nativecountry Laos -6.831e-01 1.661e+00 -0.411 nativecountry Mexico -9.182e-01 1.303e+00 -0.705 nativecountry Nicaragua -1.987e-01 1.507e+00 -0.132 nativecountry Outlying-US(Guam-USVI-etc) -1.373e+01 8.502e+02 -0.016 nativecountry Peru -9.660e-01 1.678e+00 -0.576 nativecountry Philippines 4.393e-02 1.281e+00 0.034 nativecountry Poland 2.410e-01 1.383e+00 0.174 nativecountry Portugal 7.276e-01 1.477e+00 0.493 nativecountry Puerto-Rico -5.769e-01 1.357e+00 -0.425 nativecountry Scotland -1.188e+00 1.719e+00 -0.691 nativecountry South -8.183e-01 1.341e+00 -0.610 nativecountry Taiwan -2.590e-01 1.350e+00 -0.192 nativecountry Thailand -1.693e+00 1.737e+00 -0.975 nativecountry Trinadad\u0026amp;Tobago -1.346e+00 1.721e+00 -0.782 nativecountry United-States -8.594e-02 1.269e+00 -0.068 nativecountry Vietnam -1.008e+00 1.523e+00 -0.662 nativecountry Yugoslavia 1.402e+00 1.648e+00 0.851 Pr(\u0026gt;|z|) (Intercept) 3.41e-10 *** age \u0026lt; 2e-16 *** workclass Federal-gov 4.03e-08 *** workclass Local-gov 0.043641 * workclass Never-worked 0.987885 workclass Private 0.000218 *** workclass Self-emp-inc 0.000103 *** workclass Self-emp-not-inc 0.295646 workclass State-gov 0.040728 * workclass Without-pay 0.983134 education 11th 0.437738 education 12th 0.076064 . education 1st-4th 0.361897 education 5th-6th 0.516008 education 7th-8th 0.263152 education 9th 0.722228 education Assoc-acdm 4.10e-11 *** education Assoc-voc 7.74e-11 *** education Bachelors \u0026lt; 2e-16 *** education Doctorate \u0026lt; 2e-16 *** education HS-grad 3.52e-06 *** education Masters \u0026lt; 2e-16 *** education Preschool 0.973996 education Prof-school \u0026lt; 2e-16 *** education Some-college 5.00e-10 *** maritalstatus Married-AF-spouse 0.000378 *** maritalstatus Married-civ-spouse 6.00e-12 *** maritalstatus Married-spouse-absent 0.767155 maritalstatus Never-married 7.42e-05 *** maritalstatus Separated 0.855672 maritalstatus Widowed 0.343449 occupation Adm-clerical 0.462064 occupation Armed-Forces 0.498170 occupation Craft-repair 0.049972 * occupation Exec-managerial \u0026lt; 2e-16 *** occupation Farming-fishing 2.15e-08 *** occupation Handlers-cleaners 0.001353 ** occupation Machine-op-inspct 0.176061 occupation Other-service 6.14e-07 *** occupation Priv-house-serv 0.954385 occupation Prof-specialty 2.22e-07 *** occupation Protective-serv 0.000248 *** occupation Sales 0.005282 ** occupation Tech-support 5.63e-05 *** occupation Transport-moving NA relationship Not-in-family 0.025562 * relationship Other-relative 0.484263 relationship Own-child 0.032716 * relationship Unmarried 0.058392 . relationship Wife \u0026lt; 2e-16 *** race Asian-Pac-Islander 0.173504 race Black 0.206001 race Other 0.625263 race White 0.133356 sex Male 4.52e-14 *** capitalgain \u0026lt; 2e-16 *** capitalloss \u0026lt; 2e-16 *** hoursperweek \u0026lt; 2e-16 *** nativecountry Canada 0.842879 nativecountry China 0.465157 nativecountry Columbia 0.200470 nativecountry Cuba 0.965432 nativecountry Dominican-Republic 0.962972 nativecountry Ecuador 0.980829 nativecountry El-Salvador 0.662181 nativecountry England 0.959686 nativecountry France 0.708642 nativecountry Germany 0.966572 nativecountry Greece 0.122527 nativecountry Guatemala 0.969180 nativecountry Haiti 0.568105 nativecountry Holand-Netherlands 0.995736 nativecountry Honduras 0.778775 nativecountry Hong 0.874155 nativecountry Hungary 0.927653 nativecountry India 0.531661 nativecountry Iran 0.980736 nativecountry Ireland 0.914628 nativecountry Italy 0.647194 nativecountry Jamaica 0.869467 nativecountry Japan 0.712179 nativecountry Laos 0.680866 nativecountry Mexico 0.481103 nativecountry Nicaragua 0.895132 nativecountry Outlying-US(Guam-USVI-etc) 0.987115 nativecountry Peru 0.564797 nativecountry Philippines 0.972640 nativecountry Poland 0.861624 nativecountry Portugal 0.622327 nativecountry Puerto-Rico 0.670837 nativecountry Scotland 0.489616 nativecountry South 0.541809 nativecountry Taiwan 0.847878 nativecountry Thailand 0.329678 nativecountry Trinadad\u0026amp;Tobago 0.434105 nativecountry United-States 0.946020 nativecountry Vietnam 0.507799 nativecountry Yugoslavia 0.394874 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 21175 on 19186 degrees of freedom Residual deviance: 12104 on 19090 degrees of freedom AIC: 12298 Number of Fisher Scoring iterations: 15 age, workclass, education, maritalstatus, occupation, relationship, sex, capitalgain, capitalloss, housperweek   Problem 1.2 - A Logistic Regression Model What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)\npredictLog \u0026lt;- predict(CensusLog, newdata = censusTest, type = \u0026quot;response\u0026quot;) Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == : prediction from a rank-deficient fit may be misleading table(censusTest$over50k, predictLog \u0026gt; 0.5)  FALSE TRUE \u0026lt;=50K 9051 662 \u0026gt;50K 1190 1888 (9051 + 1888) / nrow(censusTest) [1] 0.8552107  Problem 1.3 - A Logistic Regression Model What is the baseline accuracy for the testing set? table(censusTest$over50k)\ntable(censusTest$over50k)  \u0026lt;=50K \u0026gt;50K 9713 3078  9713 / nrow(censusTest) [1] 0.7593621 Problem 1.4 - A Logistic Regression Model What is the area-under-the-curve (AUC) for this model on the test-set?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess ROCRpredLog = prediction(predictLog, censusTest$over50k) as.numeric(performance(ROCRpredLog, \u0026quot;auc\u0026quot;)@y.values) [1] 0.9061598   Problem 2.1 - A CART Model I have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.\nLet‚Äôs now build a classification tree to predict ‚Äúover50k‚Äù. Using the training set to build the model, and all of the other variables as independent variables.\nUsing the default parameters, so don‚Äôt set a value for minbucket or cp.¬†Remember to specify method=‚Äúclass‚Äù as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.\nHow many splits does the tree have in total?\nlibrary(rpart) library(rpart.plot) censusTree \u0026lt;- rpart(over50k ~ ., data = censusTrain, method=\u0026quot;class\u0026quot;) prp(censusTree) 4   Problem 2.2 - A CART Model Which variable does the tree split on at the first level (the very first split of the tree)? #### relationship\n Problem 2.3 - A CART Model Which variables does the tree split on at the second level (immediately after the first split of the tree)? #### capitalgain, education\n Problem 2.4 - A CART Model What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type=‚Äúclass‚Äù, or generate probabilities and use a threshold of 0.5 like in logistic regression.)\npredictTree \u0026lt;- as.vector(predict(censusTree, newdata = censusTest, type = \u0026quot;class\u0026quot;)) head(predictTree) [1] \u0026quot; \u0026gt;50K\u0026quot; \u0026quot; \u0026gt;50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026gt;50K\u0026quot;  table(censusTest$over50k, predictTree)  predictTree \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 9243 470 \u0026gt;50K 1482 1596 (9243 + 1596) / nrow(censusTest) [1] 0.8473927 This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.\n Problem 2.5 - A CART Model Let‚Äôs now consider the ROC curve and AUC for the CART model on the test-set. I will need to get predicted probabilities for the observations in the test-set to build the ROC curve and compute the AUC. Remember that you can do this by removing the type=‚Äúclass‚Äù argument when making predictions, and taking the second column of the resulting object.\npredictTreeProb \u0026lt;- predict(censusTree, newdata = censusTest) head(predictTreeProb)  \u0026lt;=50K \u0026gt;50K 2 0.2794982 0.72050176 5 0.2794982 0.72050176 7 0.9490143 0.05098572 8 0.6972807 0.30271934 11 0.6972807 0.30271934 12 0.2794982 0.72050176 head(predictTreeProb[, 2])  2 5 7 8 11 12 0.72050176 0.72050176 0.05098572 0.30271934 0.30271934 0.72050176  Plot the ROC curve for the CART model you have estimated. Observe that compared to the logistic regression ROC curve, the CART ROC curve is less smooth than the logistic regression ROC curve.\nWhich of the following explanations for this behavior is most correct? (HINT: Think about what the ROC curve is plotting and what changing the threshold does.)\nROCRpredTree = prediction(predictTreeProb[, 2], censusTest$over50k) ROCRperfTree = performance(ROCRpredTree, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;) plot(ROCRperfTree) The probabilities from the CART model take only a handful of values (five, one for each end bucket/leaf of the tree); the changes in the ROC curve correspond to setting the threshold to one of those values.   Problem 2.6 - A CART Model What is the AUC of the CART model on the test-set?\nas.numeric(performance(ROCRpredTree, \u0026quot;auc\u0026quot;)@y.values) [1] 0.8470256  Problem 3.1 - A Random Forest Model Before building a random forest model, I‚Äôll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression.\nFor this reason, before continuing I‚Äôll define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set.\nset.seed(1) trainSmall \u0026lt;- censusTrain[sample(nrow(censusTrain), 2000), ] Let‚Äôs now build a random forest model to predict ‚Äúover50k‚Äù, using the dataset ‚ÄútrainSmall‚Äù as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest ‚Äúcannot handle categorical predictors with more than 32 categories‚Äù, re-build the model without the nativecountry variable as one of the independent variables.)\nlibrary(randomForest) randomForest 4.6-14 Type rfNews() to see new features/changes/bug fixes. set.seed(1) CensusForest \u0026lt;- randomForest(over50k ~ ., data = trainSmall) Then, make predictions using this model on the entire test-set.\nWhat is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that you don‚Äôt need a ‚Äútype‚Äù argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that our accuracy might be different from the one reported here, since random forest models can still differ depending on our operating system, even when the random seed is set.)\npredictForest \u0026lt;- predict(CensusForest, newdata = censusTest) table(censusTest$over50k, predictForest)  predictForest \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 8843 870 \u0026gt;50K 1029 2049 (9586 + 1093) / nrow(censusTest) [1] 0.8348839  Problem 3.2 - A Random Forest Model As we discussed, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important.\nHowever, we can still compute metrics that give us insight into which variables are important. One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split.\nTo view this metric, run the following lines of code (replace ‚ÄúMODEL‚Äù with the name of your random forest model):\nvu \u0026lt;- varUsed(CensusForest, count=TRUE) vusorted \u0026lt;- sort(vu, decreasing = FALSE, index.return = TRUE) dotchart(vusorted$x, names(CensusForest$forest$xlevels[vusorted$ix])) This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis).\nWhich of the following variables is the most important in terms of the number of splits? #### age\n Problem 3.3 - A Random Forest Model A different metric we can look at is related to ‚Äúimpurity‚Äù, which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased.\nTherefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest.\nTo compute this metric, run the following code (replace ‚ÄúMODEL‚Äù with the name of your random forest model):\nvarImpPlot(CensusForest) Which one of the following variables is the most important in terms of mean reduction in impurity? #### occupation\n Problem 4.1 - Selecting cp by Cross-Validation We now conclude our analysis of this dataset by looking at how CART behaves with different choices of its parameters. Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments.\nAlso, remember we using the entire training set ‚Äútrain‚Äù when building this model. The train function might take some time to run.\nWhich value of cp does the train function recommend?\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2  Attaching package: \u0026#39;ggplot2\u0026#39; The following object is masked from \u0026#39;package:randomForest\u0026#39;: margin library(e1071) # number of folds tr.control = trainControl(method = \u0026quot;cv\u0026quot;, number = 10) # cp values cartGrid \u0026lt;- expand.grid( .cp = seq(0.002,0.1,0.002)) cartGrid  .cp 1 0.002 2 0.004 3 0.006 4 0.008 5 0.010 6 0.012 7 0.014 8 0.016 9 0.018 10 0.020 11 0.022 12 0.024 13 0.026 14 0.028 15 0.030 16 0.032 17 0.034 18 0.036 19 0.038 20 0.040 21 0.042 22 0.044 23 0.046 24 0.048 25 0.050 26 0.052 27 0.054 28 0.056 29 0.058 30 0.060 31 0.062 32 0.064 33 0.066 34 0.068 35 0.070 36 0.072 37 0.074 38 0.076 39 0.078 40 0.080 41 0.082 42 0.084 43 0.086 44 0.088 45 0.090 46 0.092 47 0.094 48 0.096 49 0.098 50 0.100 # Cross-validation set.seed(2) tr = train(over50k ~ ., data = censusTrain, method = \u0026quot;rpart\u0026quot;, trControl = tr.control, tuneGrid = cartGrid) tr CART 19187 samples 12 predictor 2 classes: \u0026#39; \u0026lt;=50K\u0026#39;, \u0026#39; \u0026gt;50K\u0026#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 17268, 17268, 17269, 17269, 17269, 17268, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.002 0.8510972 0.55404931 0.004 0.8482829 0.55537475 0.006 0.8452078 0.53914084 0.008 0.8442176 0.53817486 0.010 0.8433317 0.53305978 0.012 0.8433317 0.53305978 0.014 0.8433317 0.53305978 0.016 0.8413510 0.52349296 0.018 0.8400480 0.51528594 0.020 0.8381193 0.50351272 0.022 0.8381193 0.50351272 0.024 0.8381193 0.50351272 0.026 0.8381193 0.50351272 0.028 0.8381193 0.50351272 0.030 0.8381193 0.50351272 0.032 0.8381193 0.50351272 0.034 0.8352011 0.48749911 0.036 0.8326470 0.47340390 0.038 0.8267570 0.44688035 0.040 0.8248289 0.43893150 0.042 0.8248289 0.43893150 0.044 0.8248289 0.43893150 0.046 0.8248289 0.43893150 0.048 0.8248289 0.43893150 0.050 0.8231084 0.42467058 0.052 0.8174798 0.37478096 0.054 0.8138837 0.33679015 0.056 0.8118514 0.30751485 0.058 0.8118514 0.30751485 0.060 0.8118514 0.30751485 0.062 0.8118514 0.30751485 0.064 0.8118514 0.30751485 0.066 0.8099233 0.29697206 0.068 0.7971025 0.22226318 0.070 0.7958512 0.21465656 0.072 0.7958512 0.21465656 0.074 0.7958512 0.21465656 0.076 0.7689601 0.05701508 0.078 0.7593684 0.00000000 0.080 0.7593684 0.00000000 0.082 0.7593684 0.00000000 0.084 0.7593684 0.00000000 0.086 0.7593684 0.00000000 0.088 0.7593684 0.00000000 0.090 0.7593684 0.00000000 0.092 0.7593684 0.00000000 0.094 0.7593684 0.00000000 0.096 0.7593684 0.00000000 0.098 0.7593684 0.00000000 0.100 0.7593684 0.00000000 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.002.  Problem 4.2 - Selecting cp by Cross-Validation Fit a CART model to the training data using this value of cp.¬†What is the prediction accuracy on the test-set?\ncensusTree2 \u0026lt;- rpart(over50k ~ ., data = censusTrain, method=\u0026quot;class\u0026quot;, cp = 0.002) predictTree2 \u0026lt;- as.vector(predict(censusTree2, newdata = censusTest, type = \u0026quot;class\u0026quot;)) table(censusTest$over50k, predictTree2)  predictTree2 \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 9178 535 \u0026gt;50K 1240 1838 (9178 + 1838) / nrow(censusTest) [1] 0.8612306  Problem 4.3 - Selecting cp by Cross-Validation Compared to the original accuracy using the default value of cp, this new CART model is an improvement, and so we should clearly favor this new model over the old one ‚Äì or should we?\nPlot the CART tree for this model. How many splits are there?\nprp(censusTree2) 18   Conclusion This highlights one important trade-off in building predictive models. By tuning cp, we improved our accuracy by over 1%, but our tree became significantly more complicated. In some applications, such an improvement in accuracy would be worth the loss in interpretability. In others, we may prefer a less accurate model that is simpler to understand and describe over a more accurate ‚Äì but more complicated ‚Äì model.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"0071f1f170734f4498f4ebc12f22bda6","permalink":"/project/earnings/earnings/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/earnings/earnings/","section":"project","summary":"Predict how much a person earns","tags":["R","Data Analytics","Machine Learning"],"title":"Earnings Prediction From Census Data","type":"project"},{"authors":null,"categories":null,"content":" One of the earliest applications of predictive analytics methods applied so far was to automatically recognize letters, which post office machines use to sort mail. In this analysis, I‚Äôll build a model that uses statistics of images of four letters in the Roman alphabet ‚Äì A, B, P, and R ‚Äì to predict which letter a particular image corresponds to.\nNote, that this is a multiclass classification problem. We have mostly focused on binary classification problems (e.g., predicting whether an individual voted or not, whether the Supreme Court will affirm or reverse a case, whether or not a person is at risk for a certain disease, etc.). In this problem, we have more than two classifications that are possible for each observation.\nThe file letters_ABPR.csv contains 3116 observations, each of which corresponds to a certain image of one of the four letters A, B, P and R. The images came from 20 different fonts, which were then randomly distorted to produce the final images; each such distorted image is represented as a collection of pixels, each of which is ‚Äúon‚Äù or ‚Äúoff‚Äù.\nFor each such distorted image, we have available certain statistics of the image in terms of these pixels, as well as which of the four letters the image is. This data comes from the UCI ML Repository.\nThis dataset contains the following 17 variables:\n letter = the letter that the image corresponds to (A, B, P or R) xbox = the horizontal position of where the smallest box covering the letter shape begins. ybox = the vertical position of where the smallest box covering the letter shape begins. width = the width of this smallest box. height = the height of this smallest box. onpix = the total number of ‚Äúon‚Äù pixels in the character image xbar = the mean horizontal position of all of the ‚Äúon‚Äù pixels ybar = the mean vertical position of all of the ‚Äúon‚Äù pixels x2bar = the mean squared horizontal position of all of the ‚Äúon‚Äù pixels in the image y2bar = the mean squared vertical position of all of the ‚Äúon‚Äù pixels in the image xybar = the mean of the product of the horizontal and vertical position of all of the ‚Äúon‚Äù pixels in the image x2ybar = the mean of the product of the squared horizontal position and the vertical position of all of the ‚Äúon‚Äù pixels xy2bar = the mean of the product of the horizontal position and the squared vertical position of all of the ‚Äúon‚Äù pixels xedge = the mean number of edges (the number of times an ‚Äúoff‚Äù pixel is followed by an ‚Äúon‚Äù pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image xedgeycor = the mean of the product of the number of horizontal edges at each vertical position and the vertical position yedge = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image yedgexcor = the mean of the product of the number of vertical edges at each horizontal position and the horizontal position  Load the dataset letters \u0026lt;- read.csv(\u0026quot;letters_ABPR.csv\u0026quot;) summary(letters)  letter xbox ybox width A:789 Min. : 0.000 Min. : 0.000 Min. : 1.000 B:766 1st Qu.: 3.000 1st Qu.: 5.000 1st Qu.: 4.000 P:803 Median : 4.000 Median : 7.000 Median : 5.000 R:758 Mean : 3.915 Mean : 7.051 Mean : 5.186 3rd Qu.: 5.000 3rd Qu.: 9.000 3rd Qu.: 6.000 Max. :13.000 Max. :15.000 Max. :11.000 height onpix xbar ybar Min. : 0.000 Min. : 0.000 Min. : 3.000 Min. : 0.000 1st Qu.: 4.000 1st Qu.: 2.000 1st Qu.: 6.000 1st Qu.: 6.000 Median : 6.000 Median : 4.000 Median : 7.000 Median : 7.000 Mean : 5.276 Mean : 3.869 Mean : 7.469 Mean : 7.197 3rd Qu.: 7.000 3rd Qu.: 5.000 3rd Qu.: 8.000 3rd Qu.: 9.000 Max. :12.000 Max. :12.000 Max. :14.000 Max. :15.000 x2bar y2bar xybar x2ybar Min. : 0.000 Min. :0.000 Min. : 3.000 Min. : 0.00 1st Qu.: 3.000 1st Qu.:2.000 1st Qu.: 7.000 1st Qu.: 3.00 Median : 4.000 Median :4.000 Median : 8.000 Median : 5.00 Mean : 4.706 Mean :3.903 Mean : 8.491 Mean : 4.52 3rd Qu.: 6.000 3rd Qu.:5.000 3rd Qu.:10.000 3rd Qu.: 6.00 Max. :11.000 Max. :8.000 Max. :14.000 Max. :10.00 xy2bar xedge xedgeycor yedge Min. : 0.000 Min. : 0.000 Min. : 1.000 Min. : 0.0 1st Qu.: 6.000 1st Qu.: 2.000 1st Qu.: 7.000 1st Qu.: 3.0 Median : 7.000 Median : 2.000 Median : 8.000 Median : 4.0 Mean : 6.711 Mean : 2.913 Mean : 7.763 Mean : 4.6 3rd Qu.: 8.000 3rd Qu.: 4.000 3rd Qu.: 9.000 3rd Qu.: 6.0 Max. :14.000 Max. :10.000 Max. :13.000 Max. :12.0 yedgexcor Min. : 1.000 1st Qu.: 7.000 Median : 8.000 Mean : 8.418 3rd Qu.:10.000 Max. :13.000  str(letters) \u0026#39;data.frame\u0026#39;: 3116 obs. of 17 variables: $ letter : Factor w/ 4 levels \u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;P\u0026quot;,\u0026quot;R\u0026quot;: 2 1 4 2 3 4 4 1 3 3 ... $ xbox : int 4 1 5 5 3 8 2 3 8 6 ... $ ybox : int 2 1 9 9 6 10 6 7 14 10 ... $ width : int 5 3 5 7 4 8 4 5 7 8 ... $ height : int 4 2 7 7 4 6 4 5 8 8 ... $ onpix : int 4 1 6 10 2 6 3 3 4 7 ... $ xbar : int 8 8 6 9 4 7 6 12 5 8 ... $ ybar : int 7 2 11 8 14 7 7 2 10 5 ... $ x2bar : int 6 2 7 4 8 3 5 3 6 7 ... $ y2bar : int 6 2 3 4 1 5 5 2 3 5 ... $ xybar : int 7 8 7 6 11 8 6 10 12 7 ... $ x2ybar : int 6 2 3 8 6 4 5 2 5 6 ... $ xy2bar : int 6 8 9 6 3 8 7 9 4 6 ... $ xedge : int 2 1 2 6 0 6 3 2 4 3 ... $ xedgeycor: int 8 6 7 11 10 6 7 6 10 9 ... $ yedge : int 7 2 5 8 4 7 5 3 4 8 ... $ yedgexcor: int 10 7 11 7 8 7 8 8 8 9 ...  Problem 1.1 - Predicting B or not B Let‚Äôs warm up by attempting to predict just whether a letter is B or not. To begin, load the file letters_ABPR.csv into R, and call it letters. Then, create a new variable isB in the dataframe, which takes the value ‚ÄúTRUE‚Äù if the observation corresponds to the letter B, and ‚ÄúFALSE‚Äù if it does not.\nletters$isB \u0026lt;- as.factor(letters$letter == \u0026quot;B\u0026quot;) Now, split the dataset into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. The first argument to sample.split should be the dependent variable ‚Äúletters$isB‚Äù. Remember that TRUE values from sample.split should go in the training set.\nlibrary(caTools) set.seed(1000) lettersSplit = sample.split(letters$isB, SplitRatio = 0.5) lettersTrain = subset(letters, lettersSplit == TRUE) lettersTest = subset(letters, lettersSplit == FALSE) Before building models, let‚Äôs consider a baseline method that always predicts the most frequent outcome, which is ‚Äúnot B‚Äù.\nWhat is the accuracy of this baseline method on the test set?\ntable(letters$isB)  FALSE TRUE 2350 766  table(lettersTest$isB)  FALSE TRUE 1175 383  1175 / (1175 + 383) [1] 0.754172  Problem 1.2 - Predicting B or not B Now, build a classification tree to predict whether a letter is a B or not, using the training set to build the model. Remember to remove the variable ‚Äúletter‚Äù out of the model, as this is related to what we are trying to predict!\nlibrary(rpart) library(rpart.plot) CARTb \u0026lt;- rpart(isB ~ . - letter, data = lettersTrain, method=\u0026quot;class\u0026quot;) We are just using the default parameters in our CART model, so we don‚Äôt need to add the minbucket or cp arguments at all. We also added the argument method=‚Äúclass‚Äù since this is a classification problem.\nWhat is the accuracy of the CART model on the test-set? (Use type=‚Äúclass‚Äù when making predictions on the test set.)\nbPredict \u0026lt;- predict(CARTb, newdata = lettersTest, type = \u0026quot;class\u0026quot;) table(lettersTest$isB, bPredict)  bPredict FALSE TRUE FALSE 1118 57 TRUE 43 340 (1126 + 342) / nrow(lettersTest) [1] 0.9422336  Problem 1.3 - Predicting B or Not B Now, build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. Using all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). Using the default settings for ntree and nodesize (don‚Äôt include these arguments at all). Right before building the model, set the seed to 1000. (NOTE: You might get a slightly different answer on this problem, even if you set the random seed. This has to do with your operating system and the implementation of the random forest algorithm.)\nWhat is the accuracy of the model on the test set?\nbForestPredict \u0026lt;- predict(bForest, newdata = lettersTest, type = \u0026quot;class\u0026quot;) table(lettersTest$isB, bForestPredict)  bForestPredict FALSE TRUE FALSE 1163 12 TRUE 9 374 (1164 + 372) / nrow(lettersTest) [1] 0.9858793 Random forests tends to improve on CART in terms of predictive accuracy. Sometimes, this improvement can be quite significant, as it is here.\n Problem 2.1 - Predicting the letters A, B, P, R Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.\nAs we saw earlier, building a multiclass classification CART model is no harder than building the models for binary classification problems. Fortunately, building a random forest model is just as easy. The variable in our dataframe which we will be trying to predict is ‚Äúletter‚Äù.\nStart by converting letter in the original dataset (letters) to a factor by running the following code:\nletters$letter \u0026lt;- as.factor(letters$letter) Now, generate new training and testing sets of the letters dataframe using letters$letter as the first input to the sample.split function. Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)\nset.seed(2000) lettersAllSplit \u0026lt;- sample.split(letters$letter, SplitRatio = 0.5) lettersAllTrain \u0026lt;- subset(letters, lettersAllSplit == TRUE) lettersAllTest \u0026lt;- subset(letters, lettersAllSplit == FALSE) In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.\nWhat is the baseline accuracy on the testing set?\ntable(letters$letter)  A B P R 789 766 803 758  table(lettersAllTest$letter)  A B P R 395 383 401 379  401 / nrow(lettersAllTest) [1] 0.2573813 P is the most frequent class in the test set   Problem 2.2 - Predicting the letters A, B, P, R Now build a classification tree to predict ‚Äúletter‚Äù, using the training set to build our model. You should use all of the other variables as independent variables, except ‚ÄúisB‚Äù, since it is related to what we are trying to predict!\nJust use the default parameters in your CART model. Add the argument method=‚Äúclass‚Äù since this is a classification problem. Even though we have multiple classes here, nothing changes in how we build the model from the binary case.\nCARTletters \u0026lt;- rpart(letter ~ . - isB, data = lettersAllTrain, method=\u0026quot;class\u0026quot;) summary(CARTletters) Call: rpart(formula = letter ~ . - isB, data = lettersAllTrain, method = \u0026quot;class\u0026quot;) n= 1558 CP nsplit rel error xerror xstd 1 0.31920415 0 1.0000000 1.0346021 0.01442043 2 0.25865052 1 0.6807958 0.6323529 0.01704001 3 0.18685121 2 0.4221453 0.4238754 0.01585412 4 0.02595156 3 0.2352941 0.2370242 0.01299919 5 0.02076125 4 0.2093426 0.2162630 0.01253234 6 0.01730104 5 0.1885813 0.1980969 0.01209034 7 0.01384083 6 0.1712803 0.1894464 0.01186782 8 0.01211073 7 0.1574394 0.1678201 0.01127370 9 0.01000000 8 0.1453287 0.1608997 0.01107113 Variable importance ybar xedgeycor x2ybar xy2bar yedge y2bar xedge 17 16 14 12 11 8 7 xybar x2bar xbar 5 5 3 Node number 1: 1558 observations, complexity param=0.3192042 predicted class=P expected loss=0.7419769 P(node) =1 class counts: 394 383 402 379 probabilities: 0.253 0.246 0.258 0.243 left son=2 (1088 obs) right son=3 (470 obs) Primary splits: xedgeycor \u0026lt; 8.5 to the left, improve=293.2010, (0 missing) ybar \u0026lt; 5.5 to the left, improve=287.8322, (0 missing) xy2bar \u0026lt; 5.5 to the right, improve=278.1742, (0 missing) x2ybar \u0026lt; 2.5 to the left, improve=262.6356, (0 missing) yedge \u0026lt; 4.5 to the left, improve=177.0582, (0 missing) Surrogate splits: xy2bar \u0026lt; 5.5 to the right, agree=0.892, adj=0.643, (0 split) ybar \u0026lt; 8.5 to the left, agree=0.821, adj=0.406, (0 split) xedge \u0026lt; 1.5 to the right, agree=0.816, adj=0.391, (0 split) xybar \u0026lt; 10.5 to the left, agree=0.785, adj=0.287, (0 split) x2ybar \u0026lt; 6.5 to the left, agree=0.777, adj=0.262, (0 split) Node number 2: 1088 observations, complexity param=0.2586505 predicted class=A expected loss=0.6488971 P(node) =0.6983312 class counts: 382 338 13 355 probabilities: 0.351 0.311 0.012 0.326 left son=4 (344 obs) right son=5 (744 obs) Primary splits: ybar \u0026lt; 5.5 to the left, improve=275.7625, (0 missing) x2ybar \u0026lt; 2.5 to the left, improve=240.6702, (0 missing) y2bar \u0026lt; 2.5 to the left, improve=226.4519, (0 missing) yedge \u0026lt; 3.5 to the left, improve=215.2610, (0 missing) xedgeycor \u0026lt; 7.5 to the right, improve=171.4917, (0 missing) Surrogate splits: x2ybar \u0026lt; 2.5 to the left, agree=0.904, adj=0.698, (0 split) y2bar \u0026lt; 2.5 to the left, agree=0.892, adj=0.657, (0 split) yedge \u0026lt; 3.5 to the left, agree=0.881, adj=0.625, (0 split) x2bar \u0026lt; 2.5 to the left, agree=0.820, adj=0.430, (0 split) xbar \u0026lt; 9.5 to the right, agree=0.779, adj=0.302, (0 split) Node number 3: 470 observations, complexity param=0.01730104 predicted class=P expected loss=0.1723404 P(node) =0.3016688 class counts: 12 45 389 24 probabilities: 0.026 0.096 0.828 0.051 left son=6 (91 obs) right son=7 (379 obs) Primary splits: xybar \u0026lt; 7.5 to the left, improve=59.48719, (0 missing) xy2bar \u0026lt; 6.5 to the right, improve=54.86112, (0 missing) ybar \u0026lt; 7.5 to the left, improve=49.49367, (0 missing) yedge \u0026lt; 6.5 to the right, improve=48.42295, (0 missing) xedge \u0026lt; 5.5 to the left, improve=30.83057, (0 missing) Surrogate splits: xy2bar \u0026lt; 6.5 to the right, agree=0.936, adj=0.670, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.902, adj=0.495, (0 split) xedge \u0026lt; 5.5 to the right, agree=0.889, adj=0.429, (0 split) yedge \u0026lt; 6.5 to the right, agree=0.885, adj=0.407, (0 split) onpix \u0026lt; 6.5 to the right, agree=0.838, adj=0.165, (0 split) Node number 4: 344 observations predicted class=A expected loss=0.04360465 P(node) =0.2207959 class counts: 329 9 3 3 probabilities: 0.956 0.026 0.009 0.009 Node number 5: 744 observations, complexity param=0.1868512 predicted class=R expected loss=0.5268817 P(node) =0.4775353 class counts: 53 329 10 352 probabilities: 0.071 0.442 0.013 0.473 left son=10 (342 obs) right son=11 (402 obs) Primary splits: xedgeycor \u0026lt; 7.5 to the right, improve=139.70670, (0 missing) xy2bar \u0026lt; 7.5 to the left, improve= 92.43059, (0 missing) x2ybar \u0026lt; 5.5 to the right, improve= 81.07422, (0 missing) y2bar \u0026lt; 4.5 to the right, improve= 56.45671, (0 missing) yedgexcor \u0026lt; 10.5 to the left, improve= 52.58754, (0 missing) Surrogate splits: x2ybar \u0026lt; 5.5 to the right, agree=0.738, adj=0.430, (0 split) xy2bar \u0026lt; 6.5 to the left, agree=0.675, adj=0.292, (0 split) xedge \u0026lt; 2.5 to the left, agree=0.675, adj=0.292, (0 split) yedge \u0026lt; 5.5 to the right, agree=0.644, adj=0.225, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.625, adj=0.184, (0 split) Node number 6: 91 observations, complexity param=0.01384083 predicted class=B expected loss=0.5604396 P(node) =0.05840822 class counts: 10 40 20 21 probabilities: 0.110 0.440 0.220 0.231 left son=12 (55 obs) right son=13 (36 obs) Primary splits: x2bar \u0026lt; 3.5 to the right, improve=14.308240, (0 missing) xy2bar \u0026lt; 7.5 to the left, improve= 9.472092, (0 missing) yedge \u0026lt; 4.5 to the left, improve= 9.449763, (0 missing) x2ybar \u0026lt; 7.5 to the right, improve= 8.053076, (0 missing) yedgexcor \u0026lt; 6.5 to the right, improve= 7.478284, (0 missing) Surrogate splits: yedgexcor \u0026lt; 5.5 to the right, agree=0.736, adj=0.333, (0 split) x2ybar \u0026lt; 7.5 to the left, agree=0.725, adj=0.306, (0 split) yedge \u0026lt; 5.5 to the right, agree=0.725, adj=0.306, (0 split) xy2bar \u0026lt; 8.5 to the left, agree=0.714, adj=0.278, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.681, adj=0.194, (0 split) Node number 7: 379 observations predicted class=P expected loss=0.02638522 P(node) =0.2432606 class counts: 2 5 369 3 probabilities: 0.005 0.013 0.974 0.008 Node number 10: 342 observations, complexity param=0.02595156 predicted class=B expected loss=0.2192982 P(node) =0.2195122 class counts: 14 267 10 51 probabilities: 0.041 0.781 0.029 0.149 left son=20 (283 obs) right son=21 (59 obs) Primary splits: xy2bar \u0026lt; 7.5 to the left, improve=48.65030, (0 missing) xedge \u0026lt; 2.5 to the left, improve=33.98799, (0 missing) y2bar \u0026lt; 4.5 to the right, improve=27.13499, (0 missing) yedgexcor \u0026lt; 6.5 to the left, improve=15.49245, (0 missing) ybar \u0026lt; 8.5 to the left, improve=15.03303, (0 missing) Surrogate splits: xedge \u0026lt; 5.5 to the left, agree=0.871, adj=0.254, (0 split) yedgexcor \u0026lt; 4.5 to the right, agree=0.854, adj=0.153, (0 split) ybar \u0026lt; 9.5 to the left, agree=0.848, adj=0.119, (0 split) xbox \u0026lt; 6.5 to the left, agree=0.842, adj=0.085, (0 split) ybox \u0026lt; 11.5 to the left, agree=0.842, adj=0.085, (0 split) Node number 11: 402 observations, complexity param=0.02076125 predicted class=R expected loss=0.2512438 P(node) =0.2580231 class counts: 39 62 0 301 probabilities: 0.097 0.154 0.000 0.749 left son=22 (26 obs) right son=23 (376 obs) Primary splits: yedge \u0026lt; 2.5 to the left, improve=35.46191, (0 missing) x2ybar \u0026lt; 0.5 to the left, improve=34.14932, (0 missing) y2bar \u0026lt; 1.5 to the left, improve=33.87850, (0 missing) x2bar \u0026lt; 3.5 to the left, improve=19.57685, (0 missing) yedgexcor \u0026lt; 8.5 to the left, improve=19.07812, (0 missing) Surrogate splits: y2bar \u0026lt; 1.5 to the left, agree=0.993, adj=0.885, (0 split) x2ybar \u0026lt; 0.5 to the left, agree=0.993, adj=0.885, (0 split) Node number 12: 55 observations predicted class=B expected loss=0.3090909 P(node) =0.03530167 class counts: 1 38 13 3 probabilities: 0.018 0.691 0.236 0.055 Node number 13: 36 observations predicted class=R expected loss=0.5 P(node) =0.02310655 class counts: 9 2 7 18 probabilities: 0.250 0.056 0.194 0.500 Node number 20: 283 observations predicted class=B expected loss=0.08480565 P(node) =0.1816431 class counts: 3 259 8 13 probabilities: 0.011 0.915 0.028 0.046 Node number 21: 59 observations predicted class=R expected loss=0.3559322 P(node) =0.03786906 class counts: 11 8 2 38 probabilities: 0.186 0.136 0.034 0.644 Node number 22: 26 observations predicted class=A expected loss=0.03846154 P(node) =0.01668806 class counts: 25 0 0 1 probabilities: 0.962 0.000 0.000 0.038 Node number 23: 376 observations, complexity param=0.01211073 predicted class=R expected loss=0.2021277 P(node) =0.241335 class counts: 14 62 0 300 probabilities: 0.037 0.165 0.000 0.798 left son=46 (26 obs) right son=47 (350 obs) Primary splits: yedge \u0026lt; 7.5 to the right, improve=19.73450, (0 missing) x2ybar \u0026lt; 5.5 to the right, improve=16.32647, (0 missing) xybar \u0026lt; 8.5 to the right, improve=15.20779, (0 missing) xedge \u0026lt; 3.5 to the right, improve=14.35240, (0 missing) onpix \u0026lt; 4.5 to the right, improve=12.94437, (0 missing) Surrogate splits: xedgeycor \u0026lt; 4.5 to the left, agree=0.939, adj=0.115, (0 split) Node number 46: 26 observations predicted class=B expected loss=0.3076923 P(node) =0.01668806 class counts: 4 18 0 4 probabilities: 0.154 0.692 0.000 0.154 Node number 47: 350 observations predicted class=R expected loss=0.1542857 P(node) =0.224647 class counts: 10 44 0 296 probabilities: 0.029 0.126 0.000 0.846  prp(CARTletters) What is the test-set accuracy of our CART model? Use the argument type=‚Äúclass‚Äù when making predictions. (HINT: When you are computing the test-set accuracy using the confusion matrix, you want to add everything on the main diagonal and divide by the total number of observations in the test-set, which can be computed with nrow(test), where test is the name of our test-set).\nlettersPredict \u0026lt;- as.vector(predict(CARTletters, newdata = lettersAllTest, type = \u0026quot;class\u0026quot;)) length(lettersPredict) [1] 1558 lettersPredict  [1] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [18] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [35] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [52] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [69] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [86] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [103] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [120] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [137] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [154] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [171] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [188] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [205] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [222] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [239] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [256] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [273] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [290] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [307] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [324] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [341] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [358] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [375] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [392] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [409] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [426] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [443] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [460] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [477] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [494] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [511] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [528] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [545] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [562] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [579] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [596] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [613] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [630] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [647] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [664] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [681] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [698] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [715] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [732] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [749] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [766] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [783] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [800] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [817] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [834] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [851] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [868] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [885] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [902] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [919] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [936] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [953] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [970] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [987] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1004] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1021] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1038] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1055] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1072] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1089] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1106] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1123] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1140] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1157] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1174] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1191] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1208] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1225] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1242] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1259] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1276] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1293] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1310] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1327] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [1344] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1361] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1378] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1395] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1412] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1429] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1446] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1463] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1480] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1497] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1514] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [1531] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1548] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; nrow(lettersAllTest) [1] 1558 table(lettersAllTest$letter, lettersPredict)  lettersPredict A B P R A 348 4 0 43 B 8 318 12 45 P 2 21 363 15 R 10 24 5 340 (348 + 318 + 363 + 340) / nrow(lettersAllTest) [1] 0.8786906  Problem 2.3 - Predicting the letters A, B, P, R Now build a random forest model on the training data, using the same independent variables as in the previous problem ‚Äì again, don‚Äôt forget to remove the isB variable.\nJust use the default parameter values for ntree and node size (you don‚Äôt need to include these arguments at all). Set the seed to 1000 right before building our model. (Remember that you might get a slightly different result even if you set the random seed.)\nset.seed(1000) lettersForest \u0026lt;- randomForest(letter ~ . - isB, data = lettersAllTrain) What is the test-set accuracy of your random forest model?\nlettersForestPredict \u0026lt;- as.vector(predict(lettersForest, newdata = lettersAllTest, type = \u0026quot;class\u0026quot;)) lettersForestPredict  [1] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [18] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [35] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [52] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [69] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [86] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [103] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [120] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [137] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [154] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [171] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [188] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [205] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [222] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [239] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [256] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [273] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [290] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [307] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [324] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [341] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [358] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [375] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [392] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [409] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [426] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [443] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [460] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [477] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [494] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [511] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [528] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [545] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [562] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [579] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [596] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [613] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [630] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [647] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [664] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [681] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [698] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [715] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [732] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [749] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [766] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [783] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [800] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [817] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [834] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [851] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [868] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [885] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [902] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [919] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [936] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [953] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [970] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [987] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1004] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1021] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1038] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1055] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1072] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1089] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1106] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1123] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1140] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1157] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1174] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1191] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1208] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1225] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1242] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1259] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1276] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1293] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1310] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1327] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [1344] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1361] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1378] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1395] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1412] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1429] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1446] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1463] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1480] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1497] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1514] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [1531] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1548] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; table(lettersAllTest$letter, lettersForestPredict)  lettersForestPredict A B P R A 391 0 3 1 B 0 380 1 2 P 0 6 394 1 R 3 14 0 362 (390 + 380 + 393 + 364) / nrow(lettersAllTest) [1] 0.9801027  Conclusion You should find this value rather striking, for several reasons. The first is that it is significantly higher than the value for CART, highlighting the gain in accuracy that is possible from using random forest models.\nThe second is that while the accuracy of CART decreased significantly as we transitioned from the problem of predicting B or not B (a relatively simple problem) to the problem of predicting the four letters (certainly a harder problem), the accuracy of the random forest model decreased by a tiny amount.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"4066379bd08c6aafc0d70b3f118f9193","permalink":"/project/letter_recognition/letters/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/letter_recognition/letters/","section":"project","summary":"Predict which letter a particular image corresponds to","tags":["R","Data Analytics","Machine Learning"],"title":"Letter Reecognition","type":"project"},{"authors":null,"categories":null,"content":" In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is social, or extrinsic, pressure.\nTo quote the first paragraph of their 2008 research paper:\n ‚ÄúAmong the most striking features of a democratic political system is the participation of millions of voters in elections. Why do large numbers of people vote, despite the fact that ‚Ä¶‚Äùthe casting of a single vote is of no significance where there is a multitude of electors‚Äú? One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail to participate in elections. Voters‚Äô sense of civic duty has long been a leading explanation of voters turnout‚Ä¶‚Äù  In this analysis, I‚Äôll use both logistic regression and classification trees to analyze the data they collected.\nThe data The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a ‚Äúcontrol‚Äù group, and the rest were categorized into one of four ‚Äútreatment‚Äù groups. These five groups correspond to five binary variables in the dataset.\n‚ÄúCivic Duty‚Äù (variable civicduty) group members were sent a letter that simply said ‚ÄúDO YOUR CIVIC DUTY - VOTE!‚Äù ‚ÄúHawthorne Effect‚Äù (variable hawthorne) group members were sent a letter that had the ‚ÄúCivic Duty‚Äù message plus the additional message ‚ÄúYOU ARE BEING STUDIED‚Äù and they were informed that their voting behavior would be examined by means of public records. ‚ÄúSelf‚Äù (variable self) group members received the ‚ÄúCivic Duty‚Äù message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records. ‚ÄúNeighbors‚Äù (variable neighbors) group members were given the same message as that for the ‚ÄúSelf‚Äù group, except the message not only had the household voting records but, also that of neighbors - maximizing social pressure. ‚ÄúControl‚Äù (variable control) group members were not sent anything, and represented the typical voting situation.  Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).\n Problem 1.1 - Exploration and Logistic Regression We will first get familiar with the data.\nWhat proportion of people in this dataset voted in this election?\ngerber \u0026lt;- read.csv(\u0026quot;gerber.csv\u0026quot;) str(gerber) \u0026#39;data.frame\u0026#39;: 344084 obs. of 8 variables: $ sex : int 0 1 1 1 0 1 0 0 1 0 ... $ yob : int 1941 1947 1982 1950 1951 1959 1956 1981 1968 1967 ... $ voting : int 0 0 1 1 1 1 1 0 0 0 ... $ hawthorne: int 0 0 1 1 1 0 0 0 0 0 ... $ civicduty: int 1 1 0 0 0 0 0 0 0 0 ... $ neighbors: int 0 0 0 0 0 0 0 0 0 0 ... $ self : int 0 0 0 0 0 0 0 0 0 0 ... $ control : int 0 0 0 0 0 1 1 1 1 1 ... table(gerber$voting)  0 1 235388 108696  108696 / (235388 + 108696) [1] 0.3158996  Problem 1.2 - Exploration and Logistic Regression Which of the four ‚Äútreatment groups‚Äù had the largest percentage of people who actually voted (voting = 1)?\n# howthorne table(gerber$voting, gerber$hawthorne)  0 1 0 209500 25888 1 96380 12316 12316 / (25888 + 12316) [1] 0.3223746 # civicduty table(gerber$voting, gerber$civicduty)  0 1 0 209191 26197 1 96675 12021 12021 / (26197 + 12021) [1] 0.3145377 # neighbors table(gerber$voting, gerber$neighbors)  0 1 0 211625 23763 1 94258 14438 14438 / (23763 + 14438) [1] 0.3779482 # self table(gerber$voting, gerber$self)  0 1 0 210361 25027 1 95505 13191 13191 / (25027 + 13191) [1] 0.3451515 Neighbors   Problem 1.3 - Exploration and Logistic Regression Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Using all the data to build the model (NOT spliting the data into a training set and testing set).\nWhich of the following coefficients are significant in the logistic regression model?\nVotingLog \u0026lt;- glm(voting ~ civicduty + hawthorne + self + neighbors, data = gerber, family = binomial) summary(VotingLog)  Call: glm(formula = voting ~ civicduty + hawthorne + self + neighbors, family = binomial, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9744 -0.8691 -0.8389 1.4586 1.5590 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.863358 0.005006 -172.459 \u0026lt; 2e-16 *** civicduty 0.084368 0.012100 6.972 3.12e-12 *** hawthorne 0.120477 0.012037 10.009 \u0026lt; 2e-16 *** self 0.222937 0.011867 18.786 \u0026lt; 2e-16 *** neighbors 0.365092 0.011679 31.260 \u0026lt; 2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428090 on 344079 degrees of freedom AIC: 428100 Number of Fisher Scoring iterations: 4 All coefficients are significant   Problem 1.4 - Exploration and Logistic Regression Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don‚Äôt need to use the new data argument since we didn‚Äôt split our data.)\npredictVoting \u0026lt;- predict(VotingLog, type = \u0026quot;response\u0026quot;) table(gerber$voting, predictVoting \u0026gt; 0.3)  FALSE TRUE 0 134513 100875 1 56730 51966 (134513 + 51966) / nrow(gerber) [1] 0.5419578  Problem 1.5 - Exploration and Logistic Regression Using a threshold of 0.5, what is the accuracy of the logistic regression model?\ntable(gerber$voting, predictVoting \u0026gt; 0.5)  FALSE 0 235388 1 108696 (235388) / nrow(gerber) [1] 0.6841004 table(gerber$voting)  0 1 235388 108696  235388 / (235388 + 108696) [1] 0.6841004 0.6841004 =\u0026gt; equal to accuracy of threshold of 0.5   Problem 1.6 - Exploration and Logistic Regression Compare our previous two answers to the percentage of people who did not vote (the baseline accuracy) and computing the AUC of the model. What is happening here?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess ROCRpred = prediction(predictVoting, gerber$voting) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.5308461 Even though all of the variables are significant, this is a weak predictive model.   Problem 2.1 - Trees I‚Äôll now try out trees! Building a CART tree for voting using all data and the same four treatment variables we used before. Don‚Äôt set the option method=‚Äúclass‚Äù - we are actually going to create a regression tree here.\nWe are interested in building a tree to explore the fraction of people who vote, or the probability of voting.\nI‚Äôd like CART to split our groups if they have different probabilities of voting. If we used method=‚Äòclass‚Äô, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different).\nHowever, with regression trees, CART will split even if both groups have probability less than 50%. Leave all the parameters at their default values.\nlibrary(rpart) library(rpart.plot) CARTmodel \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors, data = gerber) # plot the tree. What happens, and if relevant, why? prp(CARTmodel) No variables are used (the tree is only a root node) - none of the variables make a big enough effect to be split on.   Problem 2.2 - Trees Now build the tree:\nCARTmodel2 \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0) prp(CARTmodel2) What do we observe about the order of the splits? #### Neighbor is the first split, civic duty is the last.\n Problem 2.3 - Trees Using only the CART tree plot, we note that the fraction (a number between 0 and 1) of ‚ÄúCivic Duty‚Äù people voted amounted to: #### 31%\n Problem 2.4 - Trees Building a new tree that includes the ‚Äúsex‚Äù variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.\nCARTmodel3 \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0) prp(CARTmodel3) In the control group, which gender is more likely to vote? #### Men (0)\nIn the ‚ÄúCivic Duty‚Äù group, which gender is more likely to vote? #### Men (0)\n Problem 3.1 - Interaction Terms We know trees can handle ‚Äúnonlinear‚Äù relationships, e.g. ‚Äúin the ‚ÄòCivic Duty‚Äô group and female‚Äù, but as we will see in the next few questions, it is possible to do the same for logistic regression.\nFirstly, let‚Äôs explore what trees can tell us. Let‚Äôs just focus on the ‚ÄúControl‚Äù treatment group. Creating a regression tree using just the ‚Äúcontrol‚Äù variable, then creating another tree with the ‚Äúcontrol‚Äù and ‚Äúsex‚Äù variables, both with cp=0.0.\nCARTcontrol \u0026lt;- rpart(voting ~ control, data = gerber, cp = 0.0) CARTcontrolAndSex \u0026lt;- rpart(voting ~ control + sex, data = gerber, cp = 0.0) In the ‚Äúcontrol‚Äù only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group?\nUsing the absolute value function to get an answer, i.e.¬†abs(Control Prediction - Non-Control Prediction). I‚Äôll add the argument ‚Äúdigits = 6‚Äù to the prp code to get a more accurate estimate.\nprp(CARTcontrol, digits = 6) abs(0.296638 - 0.34) [1] 0.043362 0.043362   Problem 3.2 - Interaction Terms Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group (being in any of the four treatment groups):\nprp(CARTcontrolAndSex, digits = 6) They are affected about the same (change in probability within 0.001 of each other).   Problem 3.3 - Interaction Terms Going back to logistic regression now, I‚Äôll build a model using ‚Äúsex‚Äù and ‚Äúcontrol‚Äù. Interpreting the coefficient for ‚Äúsex‚Äù:\nVotingControlAndSexLog \u0026lt;- glm(voting ~ control + sex, data = gerber, family = binomial) summary(VotingControlAndSexLog)  Call: glm(formula = voting ~ control + sex, family = binomial, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9220 -0.9012 -0.8290 1.4564 1.5717 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.635538 0.006511 -97.616 \u0026lt; 2e-16 *** control -0.200142 0.007364 -27.179 \u0026lt; 2e-16 *** sex -0.055791 0.007343 -7.597 3.02e-14 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428443 on 344081 degrees of freedom AIC: 428449 Number of Fisher Scoring iterations: 4 Coefficient is negative, reflecting that women are less likely to vote!   Problem 3.4 - Interaction Terms The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control).\nLogistic regression has attempted to do the same, although it wasn‚Äôt able to do as well because it can‚Äôt consider exactly the joint possibility of being a women and in the control group.\nI can quantify this precisely. By creating the following dataframe (this contains all of the possible values of sex and control), and evaluating our logistic regression using the predict function (where ‚ÄúLogModelSex‚Äù is the name of our logistic regression model that uses both control and sex):\nPossibilities \u0026lt;- data.frame(sex=c(0,0,1,1), control=c(0,1,0,1)) predict(VotingControlAndSexLog, newdata=Possibilities, type=\u0026quot;response\u0026quot;)  1 2 3 4 0.3462559 0.3024455 0.3337375 0.2908065  The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ).\nWhat is the absolute difference between the tree and the logistic regression for the (Woman, Control) case?\nThe answer contains five numbers after the decimal point.\nabs(0.290456 - 0.2908065) [1] 0.0003505 round(0.0003505, digits = 5) [1] 0.00035  Problem 3.5 - Interaction Terms So the difference is not too big for this dataset, but it‚Äôs there. I‚Äôm going to add a new term to our logistic regression now, that is the combination of the ‚Äúsex‚Äù and ‚Äúcontrol‚Äù variables - so if this new variable is 1, that means the person is a woman AND in the control group.\nLogModel2 \u0026lt;- glm(voting ~ sex + control + sex:control, data = gerber, family = \u0026quot;binomial\u0026quot;) How do I interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?\nsummary(LogModel2)  Call: glm(formula = voting ~ sex + control + sex:control, family = \u0026quot;binomial\u0026quot;, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9213 -0.9019 -0.8284 1.4573 1.5724 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.637471 0.007603 -83.843 \u0026lt; 2e-16 *** sex -0.051888 0.010801 -4.804 1.55e-06 *** control -0.196553 0.010356 -18.980 \u0026lt; 2e-16 *** sex:control -0.007259 0.014729 -0.493 0.622 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428442 on 344080 degrees of freedom AIC: 428450 Number of Fisher Scoring iterations: 4 If a person is a woman and in the control group, the chance that she voted goes down.   Problem 3.6 - Interaction Terms Run the same code as before to calculate the average for each group:\npredict(LogModel2, newdata=Possibilities, type=\u0026quot;response\u0026quot;)  1 2 3 4 0.3458183 0.3027947 0.3341757 0.2904558  Now, what is the difference between the logistic regression model and the CART model for the (Woman, Control) case?\nAgain, our answer has five numbers after the decimal point.\nabs(0.290456 - 0.2904558) [1] 2e-07 round(2e-07, digits = 5) [1] 0  Conclusion - Interaction Terms This example has shown that trees can capture nonlinear relationships that logistic regression cannot, but that we can get around this sometimes by using variables that are the combination of two variables.\nShould we always include all possible interaction terms of the independent variables when building a logistic regression model? #### No (because of overfitting)\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"bbef8a24f98dd4e6abfb460d2c2fbcdd","permalink":"/project/understanding_votes/votes/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/understanding_votes/votes/","section":"project","summary":"Why do people vote?","tags":["R","Data Analytics","Machine Learning"],"title":"Understanding Why People Vote?","type":"project"},{"authors":null,"categories":null,"content":" Wikipedia is a free online encyclopedia that anyone can edit and contribute to. It‚Äôs available in many languages and is growing all the time. On the English language version of Wikipedia:\n There are currently 4.7 million pages. There have been a total over 760 million edits (also called revisions) over its lifetime. There are approximately 130,000 edits per day.  One of the consequences of being editable by anyone is that some people vandalize pages.\nThis can take the form of removing content, adding promotional or inappropriate content, or more subtle shifts that change the meaning of the article.\nWith these many articles and edits per day it is difficult for humans to detect all instances of vandalism and revert (undo) them. As a result, Wikipedia uses bots - computer programs that automatically revert edits that look like vandalism. In this analysis I‚Äôll attempt to develop a vandalism detector that uses ML to distinguish between a valid edit and vandalism.\nThe data for this analysis is based on the revision history of the page Language. Wikipedia provides a history for each page that consists of the state of the page at each revision. Rather than manually considering each revision, a script was run that checked whether edits stayed or were reverted. If a change was eventually reverted then that revision is marked as vandalism. This may result in some misclassifications, but the script performs well enough for our needs.\nAs a result of this preprocessing, some common processing tasks have already been done, including lower-casing and punctuation removal. The columns in the dataset are:\n Vandal = 1 if this edit was vandalism, 0 if not. Minor = 1 if the user marked this edit as a ‚Äúminor edit‚Äù, 0 if not. Loggedin = 1 if the user made this edit while using a Wikipedia account, 0 if they did not. Added = The unique words added. Removed = The unique words removed.  Notice the repeated use of unique. The data we have available is not the traditional bag of words - rather it is the set of words that were removed or added. For example, if a word was removed multiple times in a revision it will only appear one time in the ‚ÄúRemoved‚Äù column.\nLoading the packages # install packages if necessary list.of.packages \u0026lt;- c(\u0026quot;SnowballC\u0026quot;) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages)) install.packages(new.packages) ## load packages library(tm) Loading required package: NLP library(SnowballC) library(caTools) # to use sample.split function. library(rpart) library(rpart.plot) library(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess Problem 1.1 - Bags of Words Load the data wiki.csv with the option stringsAsFactors=FALSE, calling the dataframe ‚Äúwiki‚Äù. Convert the ‚ÄúVandal‚Äù column to a factor using the code wiki\\(Vandal = as.factor(wiki\\)Vandal).\nwiki \u0026lt;- read.csv(\u0026quot;wiki.csv\u0026quot;, stringsAsFactors = FALSE) wiki$Vandal \u0026lt;- as.factor(wiki$Vandal) str(wiki) \u0026#39;data.frame\u0026#39;: 3876 obs. of 7 variables: $ X.1 : int 1 2 3 4 5 6 7 8 9 10 ... $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ Vandal : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ Minor : int 1 1 0 1 1 0 0 0 1 0 ... $ Loggedin: int 1 1 1 0 1 1 1 1 1 0 ... $ Added : chr \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethno\u0026quot;| __truncated__ \u0026quot; website external links\u0026quot; \u0026quot; \u0026quot; \u0026quot; afghanistan used iran mostly that farsiis is countries some xmlspacepreservepersian parts tajikestan region\u0026quot; ... $ Removed : chr \u0026quot; \u0026quot; \u0026quot; talklanguagetalk\u0026quot; \u0026quot; regarded as technologytechnologies human first\u0026quot; \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help all actions through ethnologue relat\u0026quot;| __truncated__ ... How many cases of vandalism were detected in the history of this page?\ntable(wiki$Vandal)  0 1 2061 1815  1815\n  Problem 1.2 - Bags of Words We will now use the bag of words approach to build a model. We have two columns of textual data, with different meanings. For example, adding rude words has a different meaning to removing rude words. The text already is lowercase and stripped of punctuation. So to pre-process the data, just complete the following four steps:\n# 1) Create the corpus for the Added column, and call it \u0026quot;corpusAdded\u0026quot;. corpusAdded \u0026lt;- Corpus(VectorSource(wiki$Added)) corpusAdded[[1]]$content [1] \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethnologue relationships linguistics regarded writing languages to other listing xmlspacepreservelanguages metaverse formal term philology common each including phonologyphonology often ten list humans affiliation see computer are speechpathologyspeech our what for ways dialects please artificial written body be of quite hypothesis found alone refers by about language profanity study programming priorities rosenfelders technologytechnologies makes or first among useful languagephilosophy one sounds use area create phrases mark their genetic basic families complete but sapirwhorfhypothesissapirwhorf with talklanguagetalk population animals this science up vocal can concepts called at and topics locations as numbers have in pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy most important meaningful a historicallinguisticsorphilologyhistorical semanticssemantics patterns the oral\u0026quot; # 2) Remove the English-language stopwords. corpusAdded \u0026lt;- tm_map(corpusAdded, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusAdded, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusAdded[[1]]$content [1] \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text actions human ethnologue relationships linguistics regarded writing languages listing xmlspacepreservelanguages metaverse formal term philology common including phonologyphonology often ten list humans affiliation see computer speechpathologyspeech ways dialects please artificial written body quite hypothesis found alone refers language profanity study programming priorities rosenfelders technologytechnologies makes first among useful languagephilosophy one sounds use area create phrases mark genetic basic families complete sapirwhorfhypothesissapirwhorf talklanguagetalk population animals science vocal can concepts called topics locations numbers pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy important meaningful historicallinguisticsorphilologyhistorical semanticssemantics patterns oral\u0026quot; # 3) Stem the words. corpusAdded \u0026lt;- tm_map(corpusAdded, stemDocument) Warning in tm_map.SimpleCorpus(corpusAdded, stemDocument): transformation drops documents corpusAdded[[1]]$content [1] \u0026quot;repres psycholinguisticspsycholinguist orthographyorthographi help text action human ethnologu relationship linguist regard write languag list xmlspacepreservelanguag metavers formal term philolog common includ phonologyphonolog often ten list human affili see comput speechpathologyspeech way dialect pleas artifici written bodi quit hypothesi found alon refer languag profan studi program prioriti rosenfeld technologytechnolog make first among use languagephilosophi one sound use area creat phrase mark genet basic famili complet sapirwhorfhypothesissapirwhorf talklanguagetalk popul anim scienc vocal can concept call topic locat number patholog differ develop 4000 thing idea group complex anim mathemat fair literatur httpwwwzompistcom philosophi import meaning historicallinguisticsorphilologyhistor semanticssemant pattern oral\u0026quot; # 4) Build the DocumentTermMatrix, and call it dtmAdded. dtmAdded = DocumentTermMatrix(corpusAdded) dtmAdded \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 6675)\u0026gt;\u0026gt; Non-/sparse entries: 15368/25856932 Sparsity : 100% Maximal term length: 784 Weighting : term frequency (tf) How many terms appear in dtmAdded? #### 6675\nIf the code length(stopwords(‚Äúenglish‚Äù)) does not return 174 for you, then please run the line of code in this file, which will store the standard stop words in a variable called sw. When removing stop words, use tm_map(corpusAdded, removeWords, sw) instead of tm_map(corpusAdded, removeWords, stopwords(‚Äúenglish‚Äù)).\nlength(stopwords(\u0026quot;english\u0026quot;)) [1] 174  Problem 1.3 - Bags of Words Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded.\nsparseAdded \u0026lt;- removeSparseTerms(dtmAdded, 0.997) sparseAdded \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 166)\u0026gt;\u0026gt; Non-/sparse entries: 2681/640735 Sparsity : 100% Maximal term length: 28 Weighting : term frequency (tf) How many terms appear in sparseAdded? #### 166\n Problem 1.4 - Bags of Words Convert sparseAdded to a dataframe called wordsAdded, and then prepend all the words with the letter A, by using the code:\nwordsAdded \u0026lt;- as.data.frame(as.matrix(sparseAdded)) colnames(wordsAdded) \u0026lt;- paste(\u0026quot;A\u0026quot;, colnames(wordsAdded)) str(wordsAdded) \u0026#39;data.frame\u0026#39;: 3876 obs. of 166 variables: $ A bodi : num 1 0 0 0 1 0 0 0 0 0 ... $ A call : num 1 0 0 0 1 0 0 0 0 0 ... $ A complet : num 1 0 0 0 1 0 0 0 0 0 ... $ A concept : num 1 0 0 0 1 0 0 0 0 0 ... $ A creat : num 1 0 0 0 1 0 0 0 0 0 ... $ A develop : num 1 0 0 0 1 0 0 0 0 0 ... $ A differ : num 1 0 0 0 1 0 0 0 0 0 ... $ A famili : num 1 0 0 0 1 0 0 0 0 0 ... $ A first : num 1 0 0 0 0 0 0 0 0 0 ... $ A group : num 1 0 0 0 1 0 0 0 0 0 ... $ A help : num 1 0 0 0 1 0 0 0 0 0 ... $ A idea : num 1 0 0 0 1 0 0 0 0 0 ... $ A import : num 1 0 0 0 1 0 0 0 0 0 ... $ A includ : num 1 0 0 0 1 0 0 0 0 0 ... $ A linguist : num 1 0 0 0 1 0 0 0 0 0 ... $ A make : num 1 0 0 0 1 0 0 0 0 0 ... $ A number : num 1 0 0 0 1 0 0 0 0 0 ... $ A pattern : num 1 0 0 0 1 0 0 0 0 0 ... $ A popul : num 1 0 0 0 1 0 0 0 0 0 ... $ A refer : num 1 0 0 0 1 0 0 0 0 0 ... $ A relationship : num 1 0 0 0 1 0 0 0 0 0 ... $ A repres : num 1 0 0 0 1 0 0 0 0 0 ... $ A sound : num 1 0 0 0 1 0 0 0 0 0 ... $ A studi : num 1 0 0 0 1 0 0 0 0 0 ... $ A ten : num 1 0 0 0 1 0 0 0 0 0 ... $ A use : num 2 0 0 1 2 0 0 0 0 0 ... $ A write : num 1 0 0 0 1 0 0 0 0 0 ... $ A part : num 0 0 0 1 0 0 0 0 0 0 ... $ A name : num 0 0 0 0 0 0 0 0 0 0 ... $ A communic : num 0 0 0 0 0 0 0 0 0 0 ... $ A defin : num 0 0 0 0 0 0 0 0 0 0 ... $ A express : num 0 0 0 0 0 0 0 0 0 0 ... $ A mean : num 0 0 0 0 0 0 0 0 0 0 ... $ A must : num 0 0 0 0 0 0 0 0 0 0 ... $ A set : num 0 0 0 0 0 0 0 0 0 0 ... $ A featur : num 0 0 0 0 0 0 0 0 0 0 ... $ A form : num 0 0 0 0 0 0 0 0 0 0 ... $ A symbol : num 0 0 0 0 0 0 0 0 0 0 ... $ A talk : num 0 0 0 0 0 0 0 0 0 0 ... $ A type : num 0 0 0 0 0 0 0 0 0 0 ... $ A learn : num 0 0 0 0 0 0 0 0 0 0 ... $ A general : num 0 0 0 0 0 0 0 0 0 0 ... $ A exampl : num 0 0 0 0 0 0 0 0 0 0 ... $ A provid : num 0 0 0 0 0 0 0 0 0 0 ... $ A speak : num 0 0 0 0 0 0 0 0 0 0 ... $ A thought : num 0 0 0 0 0 0 0 0 0 0 ... $ A cours : num 0 0 0 0 0 0 0 0 0 0 ... $ A german : num 0 0 0 0 0 0 0 0 0 0 ... $ A onlin : num 0 0 0 0 0 0 0 0 0 0 ... $ A biolog : num 0 0 0 0 0 0 0 0 0 0 ... $ A close : num 0 0 0 0 0 0 0 0 0 0 ... $ A communiti : num 0 0 0 0 0 0 0 0 0 0 ... $ A distinct : num 0 0 0 0 0 0 0 0 0 0 ... $ A dutch : num 0 0 0 0 0 0 0 0 0 0 ... $ A intellig : num 0 0 0 0 0 0 0 0 0 0 ... $ A least : num 0 0 0 0 0 0 0 0 0 0 ... $ A note : num 0 0 0 0 0 0 0 0 0 0 ... $ A parallel : num 0 0 0 0 0 0 0 0 0 0 ... $ A possibl : num 0 0 0 0 0 0 0 0 0 0 ... $ A principl : num 0 0 0 0 0 0 0 0 0 0 ... $ A sentenc : num 0 0 0 0 0 0 0 0 0 0 ... $ A standard : num 0 0 0 0 0 0 0 0 0 0 ... $ A stem : num 0 0 0 0 0 0 0 0 0 0 ... $ A understood : num 0 0 0 0 0 0 0 0 0 0 ... $ A utter : num 0 0 0 0 0 0 0 0 0 0 ... $ A arbitrari : num 0 0 0 0 0 0 0 0 0 0 ... $ A combin : num 0 0 0 0 0 0 0 0 0 0 ... $ A cultur : num 0 0 0 0 0 0 0 0 0 0 ... $ A discuss : num 0 0 0 0 0 0 0 0 0 0 ... $ A invent : num 0 0 0 0 0 0 0 0 0 0 ... $ A properti : num 0 0 0 0 0 0 0 0 0 0 ... $ A research : num 0 0 0 0 0 0 0 0 0 0 ... $ A unit : num 0 0 0 0 0 0 0 0 0 0 ... $ A work : num 0 0 0 0 0 0 0 0 0 0 ... $ A direct : num 0 0 0 0 0 0 0 0 0 0 ... $ A relat : num 0 0 0 0 0 0 0 0 0 0 ... $ A agre : num 0 0 0 0 0 0 0 0 0 0 ... $ A exist : num 0 0 0 0 0 0 0 0 0 0 ... $ A wide : num 0 0 0 0 0 0 0 0 0 0 ... $ A describ : num 0 0 0 0 0 0 0 0 0 0 ... $ A literari : num 0 0 0 0 0 0 0 0 0 0 ... $ A purpos : num 0 0 0 0 0 0 0 0 0 0 ... $ A fantasi : num 0 0 0 0 0 0 0 0 0 0 ... $ A promin : num 0 0 0 0 0 0 0 0 0 0 ... $ A manipul : num 0 0 0 0 0 0 0 0 0 0 ... $ A tolkien : num 0 0 0 0 0 0 0 0 0 0 ... $ A follow : num 0 0 0 0 0 0 0 0 0 0 ... $ A regular : num 0 0 0 0 0 0 0 0 0 0 ... $ A gestur : num 0 0 0 0 0 0 0 0 0 0 ... $ A similar : num 0 0 0 0 0 0 0 0 0 0 ... $ A imposs : num 0 0 0 0 0 0 0 0 0 0 ... $ A interact : num 0 0 0 0 0 0 0 0 0 0 ... $ A person : num 0 0 0 0 0 0 0 0 0 0 ... $ A reason : num 0 0 0 0 0 0 0 0 0 0 ... $ A writer : num 0 0 0 0 0 0 0 0 0 0 ... $ A phenomenon : num 0 0 0 0 0 0 0 0 0 0 ... $ A accord : num 0 0 0 0 0 0 0 0 0 0 ... $ A individu : num 0 0 0 0 0 0 0 0 0 0 ... $ A object : num 0 0 0 0 0 0 0 0 0 0 ... [list output truncated] Now repeat all of the steps we‚Äôve done so far (create a corpus, remove stop words, stem the document, create a sparse document term matrix, and convert it to a dataframe) to create a Removed bag-of-words dataframe, called wordsRemoved, except this time, prepend all of the words with the letter R:\n# 1) Create the corpus for the Removed column, and call it \u0026quot;corpusRemoved\u0026quot;. corpusRemoved \u0026lt;- Corpus(VectorSource(wiki$Removed)) corpusRemoved[[3]]$content [1] \u0026quot; regarded as technologytechnologies human first\u0026quot; # 2) Remove the English-language stopwords. corpusRemoved \u0026lt;- tm_map(corpusRemoved, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusRemoved, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusRemoved[[3]]$content [1] \u0026quot; regarded technologytechnologies human first\u0026quot; # 3) Stem the words. corpusRemoved \u0026lt;- tm_map(corpusRemoved, stemDocument) Warning in tm_map.SimpleCorpus(corpusRemoved, stemDocument): transformation drops documents corpusRemoved[[3]]$content [1] \u0026quot;regard technologytechnolog human first\u0026quot; # 4) Build the DocumentTermMatrix, and call it dtmRemoved. dtmRemoved \u0026lt;- DocumentTermMatrix(corpusRemoved) dtmRemoved \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 5403)\u0026gt;\u0026gt; Non-/sparse entries: 13293/20928735 Sparsity : 100% Maximal term length: 784 Weighting : term frequency (tf) # 5) Sparse document term matrix sparseRemoved \u0026lt;- removeSparseTerms(dtmRemoved, 0.997) sparseRemoved \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 162)\u0026gt;\u0026gt; Non-/sparse entries: 2552/625360 Sparsity : 100% Maximal term length: 28 Weighting : term frequency (tf) # 6) Create dataframe and preppend the colnames with the letter \u0026quot;R\u0026quot; wordsRemoved \u0026lt;- as.data.frame(as.matrix(sparseRemoved)) colnames(wordsRemoved) \u0026lt;- paste(\u0026quot;R\u0026quot;, colnames(wordsRemoved)) str(wordsRemoved) \u0026#39;data.frame\u0026#39;: 3876 obs. of 162 variables: $ R first : num 0 0 1 0 0 0 0 0 0 0 ... $ R bodi : num 0 0 0 1 0 0 0 0 0 0 ... $ R call : num 0 0 0 1 0 0 0 0 0 0 ... $ R complet : num 0 0 0 1 0 0 0 0 0 0 ... $ R concept : num 0 0 0 1 0 0 0 0 0 0 ... $ R creat : num 0 0 0 1 0 0 0 0 0 0 ... $ R develop : num 0 0 0 1 0 0 0 0 0 0 ... $ R differ : num 0 0 0 1 0 0 0 0 0 0 ... $ R famili : num 0 0 0 1 0 0 0 0 0 0 ... $ R group : num 0 0 0 1 0 0 0 0 0 0 ... $ R idea : num 0 0 0 1 0 0 0 0 0 0 ... $ R includ : num 0 0 0 1 0 0 0 0 0 0 ... $ R linguist : num 0 0 0 1 0 0 0 0 0 0 ... $ R make : num 0 0 0 1 0 0 0 0 0 0 ... $ R pattern : num 0 0 0 1 0 0 0 0 0 0 ... $ R popul : num 0 0 0 1 0 0 0 0 0 0 ... $ R quit : num 0 0 0 1 0 0 0 0 0 0 ... $ R refer : num 0 0 0 1 0 0 0 0 0 0 ... $ R relationship : num 0 0 0 1 0 0 0 0 0 0 ... $ R repres : num 0 0 0 1 0 0 0 0 0 0 ... $ R sound : num 0 0 0 1 0 0 0 0 0 0 ... $ R studi : num 0 0 0 1 0 0 0 0 0 0 ... $ R use : num 0 0 0 2 1 0 0 0 0 0 ... $ R part : num 0 0 0 0 1 0 0 0 0 0 ... $ R communic : num 0 0 0 0 0 0 0 0 0 0 ... $ R defin : num 0 0 0 0 0 0 0 0 0 0 ... $ R express : num 0 0 0 0 0 0 0 0 0 0 ... $ R mean : num 0 0 0 0 0 0 0 0 0 0 ... $ R must : num 0 0 0 0 0 0 0 0 0 0 ... $ R name : num 0 0 0 0 0 0 0 0 0 0 ... $ R featur : num 0 0 0 0 0 0 0 0 0 0 ... $ R set : num 0 0 0 0 0 0 0 0 0 0 ... $ R symbol : num 0 0 0 0 0 0 0 0 0 0 ... $ R type : num 0 0 0 0 0 0 0 0 0 0 ... $ R know : num 0 0 0 0 0 0 0 0 0 0 ... $ R method : num 0 0 0 0 0 0 0 0 0 0 ... $ R quot : num 0 0 0 0 0 0 0 0 0 0 ... $ R suggest : num 0 0 0 0 0 0 0 0 0 0 ... $ R want : num 0 0 0 0 0 0 0 0 0 0 ... $ R speak : num 0 0 0 0 0 0 0 0 0 0 ... $ R agre : num 0 0 0 0 0 0 0 0 0 0 ... $ R regular : num 0 0 0 0 0 0 0 0 0 0 ... $ R actual : num 0 0 0 0 0 0 0 0 0 0 ... $ R spanish : num 0 0 0 0 0 0 0 0 0 0 ... $ R cours : num 0 0 0 0 0 0 0 0 0 0 ... $ R onlin : num 0 0 0 0 0 0 0 0 0 0 ... $ R fuck : num 0 0 0 0 0 0 0 0 0 0 ... $ R person : num 0 0 0 0 0 0 0 0 0 0 ... $ R believ : num 0 0 0 0 0 0 0 0 0 0 ... $ R direct : num 0 0 0 0 0 0 0 0 0 0 ... $ R experiment : num 0 0 0 0 0 0 0 0 0 0 ... $ R will : num 0 0 0 0 0 0 0 0 0 0 ... $ R deriv : num 0 0 0 0 0 0 0 0 0 0 ... $ R gestur : num 0 0 0 0 0 0 0 0 0 0 ... $ R least : num 0 0 0 0 0 0 0 0 0 0 ... $ R logic : num 0 0 0 0 0 0 0 0 0 0 ... $ R mainlinguist : num 0 0 0 0 0 0 0 0 0 0 ... $ R possibl : num 0 0 0 0 0 0 0 0 0 0 ... $ R reason : num 0 0 0 0 0 0 0 0 0 0 ... $ R result : num 0 0 0 0 0 0 0 0 0 0 ... $ R standard : num 0 0 0 0 0 0 0 0 0 0 ... $ R thought : num 0 0 0 0 0 0 0 0 0 0 ... $ R tri : num 0 0 0 0 0 0 0 0 0 0 ... $ R say : num 0 0 0 0 0 0 0 0 0 0 ... $ R origin : num 0 0 0 0 0 0 0 0 0 0 ... $ R process : num 0 0 0 0 0 0 0 0 0 0 ... $ R languageenglish : num 0 0 0 0 0 0 0 0 0 0 ... $ R analog : num 0 0 0 0 0 0 0 0 0 0 ... $ R subject : num 0 0 0 0 0 0 0 0 0 0 ... $ R learn : num 0 0 0 0 0 0 0 0 0 0 ... $ R peopl : num 0 0 0 0 0 0 0 0 0 0 ... $ R discuss : num 0 0 0 0 0 0 0 0 0 0 ... $ R biologyanalog : num 0 0 0 0 0 0 0 0 0 0 ... $ R govern : num 0 0 0 0 0 0 0 0 0 0 ... $ R linguisticssent : num 0 0 0 0 0 0 0 0 0 0 ... $ R object : num 0 0 0 0 0 0 0 0 0 0 ... $ R sentenc : num 0 0 0 0 0 0 0 0 0 0 ... $ R verb : num 0 0 0 0 0 0 0 0 0 0 ... $ R compar : num 0 0 0 0 0 0 0 0 0 0 ... $ R get : num 0 0 0 0 0 0 0 0 0 0 ... $ R provid : num 0 0 0 0 0 0 0 0 0 0 ... $ R serv : num 0 0 0 0 0 0 0 0 0 0 ... $ R clear : num 0 0 0 0 0 0 0 0 0 0 ... $ R intern : num 0 0 0 0 0 0 0 0 0 0 ... $ R combin : num 0 0 0 0 0 0 0 0 0 0 ... $ R distinct : num 0 0 0 0 0 0 0 0 0 0 ... $ R relat : num 0 0 0 0 0 0 0 0 0 0 ... $ R map : num 0 0 0 0 0 0 0 0 0 0 ... $ R nation : num 0 0 0 0 0 0 0 0 0 0 ... $ R care : num 0 0 0 0 0 0 0 0 0 0 ... $ R geograph : num 0 0 0 0 0 0 0 0 0 0 ... $ R notion : num 0 0 0 0 0 0 0 0 0 0 ... $ R present : num 0 0 0 0 0 0 0 0 0 0 ... $ R appar : num 0 0 0 0 0 0 0 0 0 0 ... $ R close : num 0 0 0 0 0 0 0 0 0 0 ... $ R wide : num 0 0 0 0 0 0 0 0 0 0 ... $ R bigger : num 0 0 0 0 0 0 0 0 0 0 ... $ R follow : num 0 0 0 0 0 0 0 0 0 0 ... $ R recent : num 0 0 0 0 0 0 0 0 0 0 ... [list output truncated] How many words are in the wordsRemoved dataframe? #### 162\n Problem 1.5 - Bags of Words Combine the two dataframes into a dataframe called wikiWords with the following:\nwikiWords \u0026lt;- cbind(wordsAdded, wordsRemoved) The cbind function combines two sets of variables for the same observations into one dataframe. Then add the Vandal column. Set the random seed to 123 and then split the dataset using sample.split from the ‚ÄúcaTools‚Äù package to put 70% in the training set.\nwikiWords$Vandal \u0026lt;- wiki$Vandal set.seed(123) wikiSplit \u0026lt;- sample.split(wikiWords$Vandal, 0.7) wikiTrain \u0026lt;- subset(wikiWords, wikiSplit == TRUE) wikiTest \u0026lt;- subset(wikiWords, wikiSplit == FALSE) table(wikiTest$Vandal)  0 1 618 545  618 / (618 + 545) [1] 0.5313844  Problem 1.6 - Bags of Words Build a CART model to predict Vandal, using all of the other variables as independent variables. Use the training set to build the model and the default parameters (don‚Äôt set values for minbucket or cp).\nwikiTree \u0026lt;- rpart(Vandal ~ ., data = wikiTrain, method = \u0026quot;class\u0026quot;) What is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that if you add the argument type=‚Äúclass‚Äù when making predictions, the output of predict will automatically use a threshold of 0.5.)\nwikiPred \u0026lt;- as.vector(predict(wikiTree, newdata = wikiTest, type=\u0026quot;class\u0026quot;)) head(wikiPred) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest$Vandal, wikiPred)  wikiPred 0 1 0 614 4 1 526 19 (618 + 12) / nrow(wikiTest) [1] 0.5417025  Problem 1.7 - Bags of Words Plot the CART tree.\nHow many word stems does the CART model use?\nprp(wikiTree) 2   Problem 1.8 - Bags of Words Given the performance of the CART model relative to the baseline, what is the best explanation for these results? #### Although it beats the baseline, bag of words is not very predictive for this problem.\n Problem 2.1 - Problem-specific Knowledge We weren‚Äôt able to improve on the baseline using the raw textual information. More specifically, the words themselves were not useful. There are other options though, and in this section we will try two techniques - identifying a key class of words, and counting words. The key class of words we will use are website addresses. ‚ÄúWebsite addresses‚Äù (also known as URLs - Uniform Resource Locators) are comprised of two main parts. An example would be ‚Äúhttp://www.google.com‚Äù. The first part is the protocol, which is usually ‚Äúhttp‚Äù (HyperText Transfer Protocol). The second part is the address of the site, e.g. ‚Äúwww.google.com‚Äù.\nWe have stripped all punctuation so links to websites appear in the data as one word, e.g. ‚Äúhttpwwwgooglecom‚Äù. We hypothesize that given, given that a lot of vandalism seems to be adding links to promotional or irrelevant websites. The presence of a web address is a sign of vandalism. We can search for the presence of a web address in the words added by searching for ‚Äúhttp‚Äù in the Added column. The grepl function returns TRUE if a string is found in another string, e.g. * grepl(‚Äúcat‚Äù,‚Äúdogs and cats‚Äù,fixed=TRUE) # TRUE * grepl(‚Äúcat‚Äù,‚Äúdogs and rats‚Äù,fixed=TRUE) # FALSE\nCreate a copy of your dataframe from the previous question:\nwikiWords2 \u0026lt;- wikiWords # make a new column in wikiWords2 that is 1 if \u0026quot;http\u0026quot; was in Added: wikiWords2$HTTP \u0026lt;- ifelse(grepl(\u0026quot;http\u0026quot;, wiki$Added, fixed = TRUE), 1, 0) # based on this new column, how many revisions added a link? table(wikiWords2$HTTP)  0 1 3659 217  217   Problem 2.2 - Problem-Specific Knowledge In problem 1.5, you computed a vector called ‚Äúspl‚Äù that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets:\nwikiTrain2 \u0026lt;- subset(wikiWords2, wikiSplit == TRUE) wikiTest2 \u0026lt;- subset(wikiWords2, wikiSplit == FALSE) Then create a new CART model using this new variable as one of the independent variables.\nwikiTree2 \u0026lt;- rpart(Vandal ~ ., data = wikiTrain2, method = \u0026quot;class\u0026quot;) What is the new accuracy of the CART model on the test-set, using a threshold of 0.5?\nwikiPred2 \u0026lt;- as.vector(predict(wikiTree2, newdata = wikiTest2, type=\u0026quot;class\u0026quot;)) head(wikiPred2) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest2$Vandal, wikiPred2)  wikiPred2 0 1 0 605 13 1 481 64 (609 + 57) / nrow(wikiTest2) [1] 0.5726569  Problem 2.3 - Problem-Specific Knowledge Another possibility is that the number of words added and removed is predictive, perhaps more so than the actual words themselves. We already have a word count available in the form of the document-term matrices (DTMs). Sum the rows of dtmAdded and dtmRemoved and add them as new variables in our dataframe wikiWords2 (called NumWordsAdded and NumWordsRemoved):\nwikiWords2$NumWordsAdded \u0026lt;- rowSums(as.matrix(dtmAdded)) wikiWords2$NumWordsRemoved \u0026lt;- rowSums(as.matrix(dtmRemoved)) What is the average number of words added?\nmean(wikiWords2$NumWordsAdded) [1] 4.050052  Problem 2.4 - Problem-Specific Knowledge In problem 1.5, you computed a vector called ‚Äúspl‚Äù that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords2.\nCreate the CART model again (using the training set and the default parameters).\nwikiTrain2b \u0026lt;- subset(wikiWords2, wikiSplit == TRUE) wikiTest2b \u0026lt;- subset(wikiWords2, wikiSplit == FALSE) Then create a new CART model using this new variable as one of the independent variables.\nwikiTree2b \u0026lt;- rpart(Vandal ~ ., data = wikiTrain2b, method = \u0026quot;class\u0026quot;) What is the new accuracy of the CART model on the test-set?\nwikiPred2b \u0026lt;- as.vector(predict(wikiTree2b, newdata = wikiTest2b, type=\u0026quot;class\u0026quot;)) head(wikiPred2b) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest2b$Vandal, wikiPred2b)  wikiPred2b 0 1 0 514 104 1 297 248 (514 + 248) / nrow(wikiTest2b) [1] 0.6552021  Problem 3.1 - Using Non-Textual Data We have two pieces of ‚Äúmetadata‚Äù (data about data) that we haven‚Äôt yet used.\nMake a copy of wikiWords2, and call it wikiWords3:\nwikiWords3 \u0026lt;- wikiWords2 Then add the two original variables Minor and Loggedin to this new dataframe:\nwikiWords3$Minor \u0026lt;- wiki$Minor wikiWords3$Loggedin \u0026lt;- wiki$Loggedin In problem 1.5, you computed a vector called ‚Äúspl‚Äù that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords3.\nwikiTrain3 \u0026lt;- subset(wikiWords3, wikiSplit == TRUE) wikiTest3 \u0026lt;- subset(wikiWords3, wikiSplit == FALSE) Build a CART model using all the training data.\nWhat is the accuracy of the model on the test-set?\nThen create a new CART model using this new variable as one of the independentvariables.\nwikiTree3 \u0026lt;- rpart(Vandal ~ ., data = wikiTrain3, method = \u0026quot;class\u0026quot;) wikiPred3 \u0026lt;- as.vector(predict(wikiTree3, newdata = wikiTest3, type=\u0026quot;class\u0026quot;)) head(wikiPred3) [1] \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest3$Vandal, wikiPred3)  wikiPred3 0 1 0 595 23 1 304 241 (595 + 241) / nrow(wikiTest3) [1] 0.7188306  Problem 3.2 - Using Non-Textual Data There is a substantial difference in the accuracy of the model using the meta data. Is this because we made a more complicated model?\nPlot the CART tree. How many splits are there in the tree?\nprp(wikiTree3) 3   ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"ba2981075c05658c51efdfdb05674083","permalink":"/project/vandalism/vandalism/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/vandalism/vandalism/","section":"project","summary":"Developing a vandalism detector","tags":["R","Data Analytics","Machine Learning"],"title":"Vandalism Detection On Wikipedia","type":"project"},{"authors":null,"categories":null,"content":" In the lending industry, investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, if the borrower is unable to repay the loan, then the lender loses money. Therefore, lenders face the problem of predicting the risk of a borrower being unable to repay a loan.\nTo address this analysis, I‚Äôll use publicly available data from LendingClub.com, a website that connects borrowers and investors over the Internet. This dataset represents 9,578 3-year loans that were funded through the LendingClub.com platform between May 2007 and February 2010.\nThe binary dependent variable not.fully.paid indicates that the loan was not paid back in full (the borrower either defaulted or the loan was ‚Äúcharged off,‚Äù meaning the borrower was deemed unlikely to ever pay it back).\nTo predict this dependent variable, I‚Äôll use the following independent variables available to the investor when deciding whether to fund a loan:\n credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise. purpose: The purpose of the loan (takes values ‚Äúcredit_card‚Äù, ‚Äúdebt_consolidation‚Äù, ‚Äúeducational‚Äù, ‚Äúmajor_purchase‚Äù, ‚Äúsmall_business‚Äù, and ‚Äúall_other‚Äù). int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates. installment: The monthly installments ($) owed by the borrower if the loan is funded. log.annual.inc: The natural log of the self-reported annual income of the borrower. dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income). fico: The FICO credit score of the borrower. days.with.cr.line: The number of days the borrower has had a credit line. revol.bal: The borrower‚Äôs revolving balance (amount unpaid at the end of the credit card billing cycle). revol.util: The borrower‚Äôs revolving line utilization rate (the amount of the credit line used relative to total credit available). inq.last.6mths: The borrower‚Äôs number of inquiries by creditors in the last 6 months. delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years. pub.rec: The borrower‚Äôs number of derogatory public records (bankruptcy filings, tax liens, or judgments).  Problem 1.1 - Preparing the Dataset Load the dataset loans.csv into a dataframe called loans, and explore it using the str() and summary() functions.\nloans \u0026lt;- read.csv(\u0026quot;loans.csv\u0026quot;) str(loans) \u0026#39;data.frame\u0026#39;: 9578 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 3 2 3 3 2 2 3 1 5 3 ... $ int.rate : num 0.119 0.107 0.136 0.101 0.143 ... $ installment : num 829 228 367 162 103 ... $ log.annual.inc : num 11.4 11.1 10.4 11.4 11.3 ... $ dti : num 19.5 14.3 11.6 8.1 15 ... $ fico : int 737 707 682 712 667 727 667 722 682 707 ... $ days.with.cr.line: num 5640 2760 4710 2700 4066 ... $ revol.bal : int 28854 33623 3511 33667 4740 50807 3839 24220 69909 5630 ... $ revol.util : num 52.1 76.7 25.6 73.2 39.5 51 76.8 68.6 51.1 23 ... $ inq.last.6mths : int 0 0 1 1 0 0 0 0 1 1 ... $ delinq.2yrs : int 0 0 0 0 1 0 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 1 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 1 1 0 0 ... summary(loans)  credit.policy purpose int.rate Min. :0.000 all_other :2331 Min. :0.0600 1st Qu.:1.000 credit_card :1262 1st Qu.:0.1039 Median :1.000 debt_consolidation:3957 Median :0.1221 Mean :0.805 educational : 343 Mean :0.1226 3rd Qu.:1.000 home_improvement : 629 3rd Qu.:0.1407 Max. :1.000 major_purchase : 437 Max. :0.2164 small_business : 619 installment log.annual.inc dti fico Min. : 15.67 Min. : 7.548 Min. : 0.000 Min. :612.0 1st Qu.:163.77 1st Qu.:10.558 1st Qu.: 7.213 1st Qu.:682.0 Median :268.95 Median :10.928 Median :12.665 Median :707.0 Mean :319.09 Mean :10.932 Mean :12.607 Mean :710.8 3rd Qu.:432.76 3rd Qu.:11.290 3rd Qu.:17.950 3rd Qu.:737.0 Max. :940.14 Max. :14.528 Max. :29.960 Max. :827.0 NA\u0026#39;s :4 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : 0.00 Min. : 0.000 1st Qu.: 2820 1st Qu.: 3187 1st Qu.: 22.70 1st Qu.: 0.000 Median : 4140 Median : 8596 Median : 46.40 Median : 1.000 Mean : 4562 Mean : 16914 Mean : 46.87 Mean : 1.572 3rd Qu.: 5730 3rd Qu.: 18250 3rd Qu.: 71.00 3rd Qu.: 2.000 Max. :17640 Max. :1207359 Max. :119.00 Max. :33.000 NA\u0026#39;s :29 NA\u0026#39;s :62 NA\u0026#39;s :29 delinq.2yrs pub.rec not.fully.paid Min. : 0.0000 Min. :0.0000 Min. :0.0000 1st Qu.: 0.0000 1st Qu.:0.0000 1st Qu.:0.0000 Median : 0.0000 Median :0.0000 Median :0.0000 Mean : 0.1638 Mean :0.0621 Mean :0.1601 3rd Qu.: 0.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 Max. :13.0000 Max. :5.0000 Max. :1.0000 NA\u0026#39;s :29 NA\u0026#39;s :29  What proportion of the loans in the dataset were not paid in full?\ntable(loans$not.fully.paid)  0 1 8045 1533  1533 / (8045 + 1533) [1] 0.1600543  Problem 1.2 - Preparing the Dataset Which of the following variables has at least one missing observation?\n log.annual.inc days.with.cr.line revol.util inq.last.6mths delinq.2yrs pub.rec   Problem 1.3 - Preparing the Dataset Which of the following is the best reason to fill in the missing values for these variables instead of removing observations with missing data? (Hint: you can use the subset() function to build a dataframe with the observations missing at least one value. To test if a variable, for example pub.rec, is missing a value, use is.na(pub.rec).)\nloansNA \u0026lt;- subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec)) str(loansNA) \u0026#39;data.frame\u0026#39;: 62 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 1 4 3 3 6 2 1 4 1 1 ... $ int.rate : num 0.113 0.11 0.113 0.123 0.106 ... $ installment : num 98.7 52.4 263.2 23.4 182.4 ... $ log.annual.inc : num 10.53 10.53 10.71 9.85 11.26 ... $ dti : num 7.72 15.84 8.75 12.38 4.26 ... $ fico : int 677 682 682 662 697 667 687 687 722 752 ... $ days.with.cr.line: num 1680 1830 2490 1200 4141 ... $ revol.bal : int 0 0 0 0 0 0 0 0 0 0 ... $ revol.util : num NA NA NA NA NA NA NA NA NA NA ... $ inq.last.6mths : int 1 0 1 1 0 0 1 0 1 0 ... $ delinq.2yrs : int 0 0 1 0 0 0 0 0 0 0 ... $ pub.rec : int 0 0 0 0 1 0 0 0 0 0 ... $ not.fully.paid : int 1 0 1 0 0 1 0 0 0 0 ... summary(loansNA)  credit.policy purpose int.rate Min. :0.0000 all_other :41 Min. :0.0712 1st Qu.:0.0000 credit_card : 3 1st Qu.:0.0933 Median :0.0000 debt_consolidation: 8 Median :0.1122 Mean :0.3871 educational : 3 Mean :0.1187 3rd Qu.:1.0000 home_improvement : 1 3rd Qu.:0.1456 Max. :1.0000 major_purchase : 5 Max. :0.1913 small_business : 1 installment log.annual.inc dti fico Min. : 23.35 Min. : 8.294 Min. : 0.000 Min. :642.0 1st Qu.: 78.44 1st Qu.:10.096 1st Qu.: 5.147 1st Qu.:682.0 Median :145.91 Median :10.639 Median :10.000 Median :707.0 Mean :159.19 Mean :10.558 Mean : 9.184 Mean :711.5 3rd Qu.:192.73 3rd Qu.:11.248 3rd Qu.:11.540 3rd Qu.:740.8 Max. :859.57 Max. :13.004 Max. :22.720 Max. :802.0 NA\u0026#39;s :4 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : NA Min. :0.000 1st Qu.:1830 1st Qu.: 0 1st Qu.: NA 1st Qu.:0.000 Median :2580 Median : 0 Median : NA Median :1.000 Mean :3158 Mean : 5476 Mean :NaN Mean :1.182 3rd Qu.:4621 3rd Qu.: 0 3rd Qu.: NA 3rd Qu.:2.000 Max. :7890 Max. :290291 Max. : NA Max. :6.000 NA\u0026#39;s :29 NA\u0026#39;s :62 NA\u0026#39;s :29 delinq.2yrs pub.rec not.fully.paid Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 Median :0.0000 Median :0.0000 Median :0.0000 Mean :0.2121 Mean :0.0303 Mean :0.1935 3rd Qu.:0.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 Max. :4.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :29 NA\u0026#39;s :29  table(loansNA$not.fully.paid)  0 1 50 12  12 / (50 + 12) [1] 0.1935484 We want to be able to predict risk for all borrowers, instead of just the ones with all data reported.   Problem 1.4 - Preparing the Dataset For the rest of this problem, I‚Äôll be using a revised version of the dataset that has the missing values filled in with multiple imputation. To ensure everybody has the same dataframe going forward, you can either run the code below in your R console (if you haven‚Äôt already, run the code install.packages(‚Äúmice‚Äù) first), or you can download and load into R the dataset we created after running the imputation: loans_imputed.csv.\nIMPORTANT NOTE: On certain operating systems, the imputation results are not the same even if you set the random seed. If you decide to do the imputation yourself, please still read the provided imputed dataset (loans_imputed.csv) into R and compare your results, using the summary function. If the results are different, please make sure to use the data in loans_imputed.csv for the rest of the problem.\n library(mice) set.seed(144) vars.for.imputation = setdiff(names(loans), ‚Äúnot.fully.paid‚Äù) imputed = complete(mice(loans[vars.for.imputation])) loans[vars.for.imputation] = imputed  loans \u0026lt;- read.csv(\u0026quot;loans_imputed.csv\u0026quot;) Note, that to do this imputation, we set vars.for.imputation to all variables in the dataframe except for not.fully.paid, to impute the values using all of the other independent variables.\nWhat best describes the process we just used to handle missing values? #### We predicted missing variable values using the available independent variables for each observation.\n Problem 2.1 - Prediction Models Now that we have prepared the dataset, we need to split it into a training and testing set. To ensure everybody obtains the same split, set the random seed to 144 (even though you already did so earlier in the problem) and use the sample.split function to select the 70% of observations for the training set (the dependent variable for sample.split is not.fully.paid). Name the dataframes train and test.\nset.seed(144) split = sample.split(loans$not.fully.paid, SplitRatio = 0.7) train = subset(loans, split == TRUE) test = subset(loans, split == FALSE) Now, use logistic regression trained on the training set to predict the dependent variable not.fully.paid using all the independent variables.\nLoansLog \u0026lt;- glm(not.fully.paid ~ ., data = train, family = binomial) Which independent variables are significant in our model? (Significant variables have at least one star, or a Pr(\u0026gt;|z|) value less than 0.05.)\nsummary(LoansLog)  Call: glm(formula = not.fully.paid ~ ., family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -2.2049 -0.6205 -0.4951 -0.3606 2.6397 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.187e+00 1.554e+00 5.910 3.42e-09 *** credit.policy -3.368e-01 1.011e-01 -3.332 0.000861 *** purposecredit_card -6.141e-01 1.344e-01 -4.568 4.93e-06 *** purposedebt_consolidation -3.212e-01 9.183e-02 -3.498 0.000469 *** purposeeducational 1.347e-01 1.753e-01 0.768 0.442201 purposehome_improvement 1.727e-01 1.480e-01 1.167 0.243135 purposemajor_purchase -4.830e-01 2.009e-01 -2.404 0.016203 * purposesmall_business 4.120e-01 1.419e-01 2.905 0.003678 ** int.rate 6.110e-01 2.085e+00 0.293 0.769446 installment 1.275e-03 2.092e-04 6.093 1.11e-09 *** log.annual.inc -4.337e-01 7.148e-02 -6.067 1.30e-09 *** dti 4.638e-03 5.502e-03 0.843 0.399288 fico -9.317e-03 1.710e-03 -5.448 5.08e-08 *** days.with.cr.line 2.371e-06 1.588e-05 0.149 0.881343 revol.bal 3.085e-06 1.168e-06 2.641 0.008273 ** revol.util 1.839e-03 1.535e-03 1.199 0.230722 inq.last.6mths 8.437e-02 1.600e-02 5.275 1.33e-07 *** delinq.2yrs -8.320e-02 6.561e-02 -1.268 0.204762 pub.rec 3.300e-01 1.139e-01 2.898 0.003756 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 5896.6 on 6704 degrees of freedom Residual deviance: 5485.2 on 6686 degrees of freedom AIC: 5523.2 Number of Fisher Scoring iterations: 5  credit.policy purposecredit_card purposedebt_consolidation purposemajor_purchase purposesmall_business installment log.annual.inc fico revol.bal inq.last.6mths pub.rec   Problem 2.2 - Prediction Models Consider two loan applications, which are identical other than the fact that the borrower in Application A has FICO credit score 700 while the borrower in Application B has FICO credit score 710.\nLet‚Äôs Logit(A) be the log odds of loan A not being paid back in full, according to our logistic regression model, and define Logit(B) similarly for loan B.\nWhat is the value of Logit(A) - Logit(B)?\napplicationA \u0026lt;- train[1, ] applicationB \u0026lt;- applicationA applicationA$fico = 700 applicationB$fico = 710 applicationA  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 700 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 applicationB  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 710 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 applications \u0026lt;- rbind(applicationA, applicationB) applications  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 2 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 700 5639.958 28854 52.1 0 2 19.48 710 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 2 0 0 0 PredApplications \u0026lt;- predict(LoansLog, type = \u0026quot;response\u0026quot;, newdata = applications) PredApplications  1 2 0.1828795 0.1693660  PredApplications[1] - PredApplications[2]  1 0.01351347  CalcApplicationA \u0026lt;- 1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 700))) CalcApplicationB \u0026lt;- 1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 710))) CalcApplicationA - CalcApplicationB [1] 0.005902546 Now, let O(A) be the odds of loan A not being paid back in full, according to our logistic regression model, and define O(B) similarly for loan B.\nWhat is the value of O(A)/O(B)? (HINT: Use the mathematical rule that exp(A + B + C) = exp(A)exp(B)exp(C). Also, remember that exp() is the exponential function in R.)\nOddsApplicationA \u0026lt;- PredApplications[1] / (1 - PredApplications[1]) OddsApplicationA  1 0.2238097  OddsApplicationB \u0026lt;- PredApplications[2] / (1 - PredApplications[2]) OddsApplicationB  2 0.2038997  OddsApplicationA / OddsApplicationB  1 1.097646  1.097646 =\u0026gt; OK! After computing the logs, try log(odds) for previous problem\nlog(OddsApplicationA)  1 -1.496959  log(OddsApplicationB)  2 -1.590127  log(OddsApplicationA) - log(OddsApplicationB)  1 0.0931679   0.0931679 =\u0026gt; OK!   Problem 2.3 - Prediction Models Predict the probability of the test-set loans not being paid back in full (remember type=‚Äúresponse‚Äù for the predict function). Store these predicted probabilities in a variable named predicted.risk and add it to our test-set (we will use this variable in later parts of the problem). Compute the confusion matrix using a threshold of 0.5.\npredicted.risk \u0026lt;- predict(LoansLog, type = \u0026quot;response\u0026quot;, newdata = test) str(predicted.risk)  Named num [1:2873] 0.0771 0.1728 0.1087 0.1016 0.0681 ... - attr(*, \u0026quot;names\u0026quot;)= chr [1:2873] \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;10\u0026quot; \u0026quot;12\u0026quot; ... str(test) \u0026#39;data.frame\u0026#39;: 2873 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 2 3 3 3 1 3 1 2 7 2 ... $ int.rate : num 0.107 0.136 0.122 0.132 0.08 ... $ installment : num 228.2 366.9 84.1 253.6 188 ... $ log.annual.inc : num 11.1 10.4 10.2 11.8 11.2 ... $ dti : num 14.29 11.63 10 9.16 16.08 ... $ fico : int 707 682 707 662 772 662 772 797 712 682 ... $ days.with.cr.line: num 2760 4710 2730 4298 4889 ... $ revol.bal : int 33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ... $ revol.util : num 76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ... $ inq.last.6mths : int 0 1 1 2 1 0 0 0 0 3 ... $ delinq.2yrs : int 0 0 0 1 0 1 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 0 0 0 0 ... test$predicted.risk \u0026lt;- predicted.risk summary(test)  credit.policy purpose int.rate Min. :0.0000 all_other : 688 Min. :0.0600 1st Qu.:1.0000 credit_card : 411 1st Qu.:0.1028 Median :1.0000 debt_consolidation:1206 Median :0.1221 Mean :0.8047 educational : 93 Mean :0.1225 3rd Qu.:1.0000 home_improvement : 186 3rd Qu.:0.1393 Max. :1.0000 major_purchase : 105 Max. :0.2121 small_business : 184 installment log.annual.inc dti fico Min. : 15.67 Min. : 8.102 Min. : 0.00 Min. :612.0 1st Qu.:163.57 1st Qu.:10.560 1st Qu.: 7.16 1st Qu.:682.0 Median :267.74 Median :10.933 Median :12.85 Median :707.0 Mean :316.99 Mean :10.928 Mean :12.75 Mean :710.8 3rd Qu.:421.89 3rd Qu.:11.290 3rd Qu.:18.30 3rd Qu.:737.0 Max. :926.83 Max. :13.459 Max. :29.96 Max. :822.0 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : 0.00 Min. : 0.000 1st Qu.: 2795 1st Qu.: 3362 1st Qu.: 23.40 1st Qu.: 0.000 Median : 4140 Median : 8712 Median : 46.90 Median : 1.000 Mean : 4494 Mean : 17198 Mean : 47.02 Mean : 1.576 3rd Qu.: 5670 3rd Qu.: 18728 3rd Qu.: 70.40 3rd Qu.: 2.000 Max. :17640 Max. :1207359 Max. :108.80 Max. :24.000 delinq.2yrs pub.rec not.fully.paid predicted.risk Min. : 0.0000 Min. :0.00000 Min. :0.0000 Min. :0.02114 1st Qu.: 0.0000 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.09461 Median : 0.0000 Median :0.00000 Median :0.0000 Median :0.13697 Mean : 0.1605 Mean :0.05743 Mean :0.1601 Mean :0.15785 3rd Qu.: 0.0000 3rd Qu.:0.00000 3rd Qu.:0.0000 3rd Qu.:0.19658 Max. :13.0000 Max. :3.00000 Max. :1.0000 Max. :0.95373  table(test$not.fully.paid, test$predicted.risk \u0026gt; 0.5)  FALSE TRUE 0 2400 13 1 457 3 What is the accuracy of the logistic regression model?\n(2400 + 3) / nrow(test) [1] 0.8364079 What is the accuracy of the baseline model?\ntable(test$not.fully.paid)  0 1 2413 460  2413 / (2413 + 460) [1] 0.8398886  Problem 2.4 - Prediction Models Use the ROCR package to compute the test-set AUC.\nROCRpred = prediction(test$predicted.risk, test$not.fully.paid) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.6720995 The model has poor accuracy at the threshold 0.5. But, despite the poor accuracy, we will see later how an investor can still leverage this logistic regression model to make profitable investments.\n Problem 3.1 - A ‚ÄúSmart Baseline‚Äù In the previous problem, we built a logistic regression model that has an AUC significantly higher than the AUC of 0.5 that would be obtained by randomly ordering observations. However, LendingClub.com assigns the interest rate to a loan based on their estimate of that loan‚Äôs risk.\nThis variable, int.rate, is an independent variable in our dataset. In this part, we will investigate using the loan‚Äôs interest rate as a ‚Äúsmart baseline‚Äù to order the loans according to risk.\nUsing the training set, build a bivariate logistic regression model (aka a logistic regression model with a single independent variable) that predicts the dependent variable not.fully.paid using only the variable int.rate.\nLoansLog2 \u0026lt;- glm(not.fully.paid ~ int.rate, data = train, family = binomial) summary(LoansLog2)  Call: glm(formula = not.fully.paid ~ int.rate, family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -1.0547 -0.6271 -0.5442 -0.4361 2.2914 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -3.6726 0.1688 -21.76 \u0026lt;2e-16 *** int.rate 15.9214 1.2702 12.54 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 5896.6 on 6704 degrees of freedom Residual deviance: 5734.8 on 6703 degrees of freedom AIC: 5738.8 Number of Fisher Scoring iterations: 4 The variable int.rate is highly significant in the bivariate model, but it is not significant at the 0.05 level in the model trained with all the independent variables.\nWhat is the most likely explanation for this difference? #### int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.\n Problem 3.2 - A ‚ÄúSmart Baseline‚Äù Make test-set predictions for the bivariate model.\nWhat is the highest predicted probability of a loan not being paid in full on the testing set?\npredicted.risk2 \u0026lt;- predict(LoansLog2, type = \u0026quot;response\u0026quot;, newdata = test) max(predicted.risk2) [1] 0.426624 0.426624 With a logistic regression cut-off of 0.5, how many loans would be predicted as not being paid in full on the testing set?\ntable(test$not.fully.paid, predicted.risk2 \u0026gt; 0.5)  FALSE 0 2413 1 460 str(test) \u0026#39;data.frame\u0026#39;: 2873 obs. of 15 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 2 3 3 3 1 3 1 2 7 2 ... $ int.rate : num 0.107 0.136 0.122 0.132 0.08 ... $ installment : num 228.2 366.9 84.1 253.6 188 ... $ log.annual.inc : num 11.1 10.4 10.2 11.8 11.2 ... $ dti : num 14.29 11.63 10 9.16 16.08 ... $ fico : int 707 682 707 662 772 662 772 797 712 682 ... $ days.with.cr.line: num 2760 4710 2730 4298 4889 ... $ revol.bal : int 33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ... $ revol.util : num 76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ... $ inq.last.6mths : int 0 1 1 2 1 0 0 0 0 3 ... $ delinq.2yrs : int 0 0 0 1 0 1 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 0 0 0 0 ... $ predicted.risk : num 0.0771 0.1728 0.1087 0.1016 0.0681 ...  0   Problem 3.3 - A ‚ÄúSmart Baseline‚Äù What is the test-set AUC of the bivariate model?\nROCRpred2 = prediction(predicted.risk2, test$not.fully.paid) as.numeric(performance(ROCRpred2, \u0026quot;auc\u0026quot;)@y.values) [1] 0.6239081  Problem 4.1 - Computing the Profitability of an Investment While thus far we have predicted if a loan will be paid back or not, an investor needs to identify loans that are expected to be profitable.\nIf the loan is paid back in full, then the investor makes interest on the loan. However, if the loan is not paid back, the investor loses the money invested. Therefore, the investor should seek loans that best balance this risk and reward.\nTo compute interest revenue, consider a $c investment in a loan that has an annual interest rate r over a period of t years.\nUsing continuous compounding of interest, this investment pays back c * exp(rt) dollars by the end of the t years, where exp(rt) is e raised to the r*t power.\nHow much does a $10 investment with an annual interest rate of 6% pay back after 3 years, using continuous compounding of interest? Hint: remember to convert the percentage to a proportion before doing the math.\n10 * exp(0.06 * 3) [1] 11.97217  Problem 4.2 - Computing the Profitability of an Investment While the investment has value c * exp(rt) dollars after collecting interest, the investor had to pay $c for the investment.\nWhat is the profit to the investor if the investment is paid back in full?\n10 * exp(0.06 * 3) - 10 [1] 1.972174  Problem 4.3 - Computing the Profitability of an Investment Now, consider the case where the investor made a $c investment, but it was not paid back in full. Assume, conservatively, that no money was received from the borrower (often a lender will receive some but not all of the value of the loan, making this a pessimistic assumption of how much is received).\nWhat is the loss to the investor in this scenario? #### -10\n Problem 5.1 - A Simple Investment Strategy In the previous subproblem, we concluded that an investor who invested c dollars in a loan with interest rate r for t years makes c * (exp(rt) - 1) dollars of profit if the loan is paid back in full and -c dollars of profit if the loan is not paid back in full (pessimistically).\nIn order to evaluate the quality of an investment strategy, we need to compute this profit for each loan in the test-set.\nFor this variable, we will assume a $1 investment (aka c=1). To create the variable, we first assign to the profit for a fully paid loan, exp(rt)-1, to every observation, and we then replace this value with -1 in the cases where the loan was not paid in full.\nAll the loans in our dataset are 3-year loans, meaning t=3 in our calculations.\ntest$profit = exp(test$int.rate*3) - 1 test$profit[test$not.fully.paid == 1] = -1 What is the maximum profit of a $10 investment in any loan in the testing set?\nmax(test$profit) * 10 [1] 8.894769  Problem 6.1 - An Investment Strategy Based on Risk A simple investment strategy of equally investing in all the loans would yield profit $20.94 for a $100 investment. But this simple investment strategy does not leverage the prediction model we built earlier in this problem.\nAs stated earlier, investors seek loans that balance reward with risk, in that they simultaneously have high interest rates and a low risk of not being paid back.\nTo meet this objective, I‚Äôll analyze an investment strategy in which the investor only purchases loans with a high interest rate (a rate of at least 15%), but amongst these loans selects the ones with the lowest predicted risk of not being paid back in full.\nWe will model an investor who invests $1 in each of the most promising 100 loans.\nFirst, use the subset() function to build a dataframe called highInterest consisting of the test-set loans with an interest rate of at least 15%.\nhighInterest \u0026lt;- subset(test, int.rate \u0026gt;= 0.15) What is the average profit of a $1 investment in one of these high-interest loans?\nmean(highInterest$profit) [1] 0.2251015 What proportion of the high-interest loans were not paid back in full?\ntable(highInterest$not.fully.paid)  0 1 327 110  110 / (327 + 110) [1] 0.2517162  Problem 6.2 - An Investment Strategy Based on Risk Next, I‚Äôll determine the 100th smallest predicted probability of not paying in full by sorting the predicted risks in increasing order and selecting the 100th element of this sorted list.\ncutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100] cutoff [1] 0.1763305 Use the subset() function to build a dataframe called selectedLoans consisting of the high-interest loans with predicted risk not exceeding the cut-off we just computed. Check to make sure you have selected 100 loans for investment.\nselectedLoans \u0026lt;- subset(highInterest, highInterest$predicted.risk \u0026lt;= cutoff) summary(selectedLoans)  credit.policy purpose int.rate installment Min. :0.00 all_other : 8 Min. :0.1501 Min. : 48.79 1st Qu.:1.00 credit_card :17 1st Qu.:0.1533 1st Qu.:176.16 Median :1.00 debt_consolidation:60 Median :0.1570 Median :309.37 Mean :0.93 educational : 1 Mean :0.1610 Mean :358.30 3rd Qu.:1.00 home_improvement : 1 3rd Qu.:0.1645 3rd Qu.:473.10 Max. :1.00 major_purchase : 7 Max. :0.2052 Max. :907.60 small_business : 6 log.annual.inc dti fico days.with.cr.line Min. : 9.575 Min. : 0.00 Min. :642.0 Min. : 1140 1st Qu.:10.776 1st Qu.: 6.05 1st Qu.:662.0 1st Qu.: 2162 Median :11.127 Median :12.35 Median :672.0 Median : 3630 Mean :11.203 Mean :12.19 Mean :680.5 Mean : 3911 3rd Qu.:11.670 3rd Qu.:18.23 3rd Qu.:692.0 3rd Qu.: 5010 Max. :13.305 Max. :28.15 Max. :782.0 Max. :13170 revol.bal revol.util inq.last.6mths delinq.2yrs Min. : 0 Min. : 0.00 Min. : 0.00 Min. :0.00 1st Qu.: 3768 1st Qu.:45.92 1st Qu.: 0.00 1st Qu.:0.00 Median : 9691 Median :71.65 Median : 0.00 Median :0.00 Mean : 19923 Mean :65.79 Mean : 0.89 Mean :0.33 3rd Qu.: 24534 3rd Qu.:93.80 3rd Qu.: 1.00 3rd Qu.:0.00 Max. :168496 Max. :99.70 Max. :10.00 Max. :4.00 pub.rec not.fully.paid predicted.risk profit Min. :0.00 Min. :0.00 Min. :0.06871 Min. :-1.0000 1st Qu.:0.00 1st Qu.:0.00 1st Qu.:0.13596 1st Qu.: 0.5823 Median :0.00 Median :0.00 Median :0.15327 Median : 0.5992 Mean :0.03 Mean :0.19 Mean :0.14794 Mean : 0.3128 3rd Qu.:0.00 3rd Qu.:0.00 3rd Qu.:0.16514 3rd Qu.: 0.6317 Max. :1.00 Max. :1.00 Max. :0.17633 Max. : 0.8508  str(selectedLoans) \u0026#39;data.frame\u0026#39;: 100 obs. of 16 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 7 2 3 1 3 5 2 3 2 3 ... $ int.rate : num 0.15 0.153 0.158 0.159 0.156 ... $ installment : num 225 444 420 246 245 ... $ log.annual.inc : num 12.3 11 11.5 11.5 10.8 ... $ dti : num 6.45 19.52 18.55 24.19 2.72 ... $ fico : int 677 667 667 667 672 702 667 672 662 682 ... $ days.with.cr.line: num 6240 2701 4560 5376 3010 ... $ revol.bal : int 56411 33074 34841 590 3273 4980 15977 16473 22783 87502 ... $ revol.util : num 75.3 68.8 89.6 84.3 69.6 55.3 83.6 94.1 93.7 96.4 ... $ inq.last.6mths : int 0 2 0 0 1 1 0 2 3 0 ... $ delinq.2yrs : int 0 0 0 0 0 0 0 2 1 1 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 1 0 0 0 1 0 0 0 0 0 ... $ predicted.risk : num 0.164 0.169 0.158 0.162 0.147 ... $ profit : num -1 0.584 0.604 0.61 -1 ... What is the profit of the investor, who invested $1 in each of these 100 loans?\nsum(selectedLoans$profit) [1] 31.27825 How many of 100 selected loans were not paid back in full?\ntable(selectedLoans$not.fully.paid)  0 1 81 19  19   Conclusion We have now seen how analytics can be used to select a subset of the high-interest loans that were paid back at only a slightly lower rate than average, resulting in a significant increase in the profit from our investor‚Äôs $100 investment. Although the logistic regression models developed in this analysis did not have large AUC values, we see that they still provided the edge needed to improve the profitability of an investment portfolio.\nWe conclude with a note of warning. Throughout this analysis I‚Äôve assume that the loans we invest in will perform in the same way as the loans we used to train our model, even though our training set covers a relatively short period of time. If there is an economic shock like a large financial downturn, default rates might be significantly higher than those observed in the training set and we might end up losing money instead of profiting. Investors must pay careful attention to such risk when making investment decisions.\n ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"71ca679d3bab7188324ce724fa8dc839","permalink":"/project/loan_repayment/loan_repayments/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/loan_repayment/loan_repayments/","section":"project","summary":"Predict the risk of a borrower being unable to repay a loan","tags":["R","Data Analytics","Machine Learning"],"title":"Predict Loan Repayments","type":"project"},{"authors":null,"categories":null,"content":" In many criminal justice systems around the world, inmates deemed not to be a threat to society are released from prison under the parole system prior to completing their sentence. They are still considered to be serving their sentence while on parole, and they can be returned to prison if they violate the terms of their parole.\nParole boards are charged with identifying which inmates are good candidates for release on parole. They seek to release inmates who will not commit additional crimes after release. In this analysis, I‚Äôll build and validate a model that predicts if an inmate will violate the terms of his or her parole.\nSuch a model could be useful to a parole board when deciding to approve or deny an application for parole.\nFor this prediction task, I‚Äôll use data from the U.S 2004 National Corrections Reporting Program, a nationwide census of parole releases that occurred during 2004.\nI‚Äôve limited my focus to parolees who served no more than 6 months in prison and whose maximum sentence for all charges did not exceed 18 months.\nThe dataset contains all such parolees who either successfully completed their term of parole during 2004 or those who violated the terms of their parole during that year. The dataset contains the following variables:\n male: 1 if the parolee is male, 0 if female race: 1 if the parolee is white, 2 otherwise age: the parolee‚Äôs age (in years) when he or she was released from prison state: a code for the parolee‚Äôs state. 2 is Kentucky, 3 is Louisiana, 4 is Virginia, and 1 is any other state. The three states were selected due to having a high representation in the dataset. time.served: the number of months the parolee served in prison (limited by the inclusion criteria to not exceed 6 months). max.sentence: the maximum sentence length for all charges, in months (limited by the inclusion criteria to not exceed 18 months). multiple.offenses: 1 if the parolee was incarcerated for multiple offenses, 0 otherwise. crime: a code for the parolee‚Äôs main crime leading to incarceration. 2 is larceny, 3 is drug-related crime, 4 is driving-related crime, and 1 is any other crime. violator: 1 if the parolee violated the parole, and 0 if the parolee completed the parole without violation.  Loading the Dataset \u0026amp; EDA parole \u0026lt;- read.csv(\u0026quot;parole.csv\u0026quot;) str(parole) \u0026#39;data.frame\u0026#39;: 675 obs. of 9 variables: $ male : int 1 0 1 1 1 1 1 0 0 1 ... $ race : int 1 1 2 1 2 2 1 1 1 2 ... $ age : num 33.2 39.7 29.5 22.4 21.6 46.7 31 24.6 32.6 29.1 ... $ state : int 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.5 5.4 5.6 5.7 5.4 6 6 4.8 4.5 4.7 ... $ max.sentence : int 18 12 12 18 12 18 18 12 13 12 ... $ multiple.offenses: int 0 0 0 0 0 0 0 0 0 0 ... $ crime : int 4 3 3 1 1 4 3 1 3 2 ... $ violator : int 0 0 0 0 0 0 0 0 0 0 ... summary(parole)  male race age state Min. :0.0000 Min. :1.000 Min. :18.40 Min. :1.000 1st Qu.:1.0000 1st Qu.:1.000 1st Qu.:25.35 1st Qu.:2.000 Median :1.0000 Median :1.000 Median :33.70 Median :3.000 Mean :0.8074 Mean :1.424 Mean :34.51 Mean :2.887 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:42.55 3rd Qu.:4.000 Max. :1.0000 Max. :2.000 Max. :67.00 Max. :4.000 time.served max.sentence multiple.offenses crime Min. :0.000 Min. : 1.00 Min. :0.0000 Min. :1.000 1st Qu.:3.250 1st Qu.:12.00 1st Qu.:0.0000 1st Qu.:1.000 Median :4.400 Median :12.00 Median :1.0000 Median :2.000 Mean :4.198 Mean :13.06 Mean :0.5363 Mean :2.059 3rd Qu.:5.200 3rd Qu.:15.00 3rd Qu.:1.0000 3rd Qu.:3.000 Max. :6.000 Max. :18.00 Max. :1.0000 Max. :4.000 violator Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.1156 3rd Qu.:0.0000 Max. :1.0000  How many parolees are contained in the dataset? #### 675\n Problem 1.1 - Preparing the Dataset Which variables in this dataset are unordered factors with at least three levels? #### state, crime\n Problem 1.2 - Preparing the Dataset In the last subproblem, we identified variables that are unordered factors with at least 3 levels, so we need to convert them to factors for our prediction problem.\nUsing the as.factor() function, we convert these variables to factors. Keep in mind that we are not changing the values, just the way R understands them (the values are still numbers).\nparole$state \u0026lt;- as.factor(parole$state) parole$crime \u0026lt;- as.factor(parole$crime) How does the output of summary() change for a factor variable as compared to a numerical variable?\nsummary(parole)  male race age state time.served Min. :0.0000 Min. :1.000 Min. :18.40 1:143 Min. :0.000 1st Qu.:1.0000 1st Qu.:1.000 1st Qu.:25.35 2:120 1st Qu.:3.250 Median :1.0000 Median :1.000 Median :33.70 3: 82 Median :4.400 Mean :0.8074 Mean :1.424 Mean :34.51 4:330 Mean :4.198 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:42.55 3rd Qu.:5.200 Max. :1.0000 Max. :2.000 Max. :67.00 Max. :6.000 max.sentence multiple.offenses crime violator Min. : 1.00 Min. :0.0000 1:315 Min. :0.0000 1st Qu.:12.00 1st Qu.:0.0000 2:106 1st Qu.:0.0000 Median :12.00 Median :1.0000 3:153 Median :0.0000 Mean :13.06 Mean :0.5363 4:101 Mean :0.1156 3rd Qu.:15.00 3rd Qu.:1.0000 3rd Qu.:0.0000 Max. :18.00 Max. :1.0000 Max. :1.0000  The output becomes similar to that of the table() function applied to that variable   Problem 2.1 - Splitting into a Training and Testing Set To ensure consistent training/testing set splits, run the following 5 lines of code (do not include the line numbers at the beginning):\nset.seed(144) # 70% to the training set, 30% to the testing set split = sample.split(parole$violator, SplitRatio = 0.7) train = subset(parole, split == TRUE) test = subset(parole, split == FALSE) Roughly what proportion of parolees have been allocated to the training and testing sets?\nstr(train) \u0026#39;data.frame\u0026#39;: 473 obs. of 9 variables: $ male : int 1 1 1 1 1 0 0 1 1 1 ... $ race : int 1 1 2 2 1 1 2 1 1 1 ... $ age : num 33.2 22.4 21.6 46.7 31 32.6 28.4 20.5 30.1 37.8 ... $ state : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.5 5.7 5.4 6 6 4.5 4.5 5.9 5.3 5.3 ... $ max.sentence : int 18 18 12 18 18 13 12 12 16 8 ... $ multiple.offenses: int 0 0 0 0 0 0 1 0 0 0 ... $ crime : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 4 1 1 4 3 3 1 1 3 3 ... $ violator : int 0 0 0 0 0 0 0 0 0 0 ... 473 / 675 [1] 0.7007407 str(test) \u0026#39;data.frame\u0026#39;: 202 obs. of 9 variables: $ male : int 0 1 0 1 1 1 1 1 1 1 ... $ race : int 1 2 1 2 2 1 1 2 1 1 ... $ age : num 39.7 29.5 24.6 29.1 24.5 32.8 36.7 36.5 33.5 37.3 ... $ state : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.4 5.6 4.8 4.7 6 5.9 0.9 3.9 4.2 4.6 ... $ max.sentence : int 12 12 12 12 16 16 16 12 12 12 ... $ multiple.offenses: int 0 0 0 0 0 0 0 1 1 1 ... $ crime : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 3 3 1 2 3 3 3 4 1 1 ... $ violator : int 0 0 0 0 0 0 0 1 1 1 ... 202 / 675  [1] 0.2992593  Problem 2.2 - Splitting into a Training and Testing Set Now, suppose you re-ran lines [1]-[5] of Problem 3.1. What would you expect? #### The exact same training/testing set split as the first execution of [1]-[5]\nIf you instead ONLY re-ran lines [3]-[5], what would you expect? #### A different training/testing set split from the first execution of [1]-[5]\nIf you instead called set.seed() with a different number and then re-ran lines [3]-[5] of Problem 3.1, what would you expect? #### A different training/testing set split from the first execution of [1]-[5]\n?sample.split  Problem 3.1 - Building a Logistic Regression Model If you tested other training/testing set splits in the previous section, please re-run the original 5 lines of code to obtain the original split. Using glm (and remembering the parameter family=‚Äúbinomial‚Äù), train a logistic regression model on the training set. Your dependent variable is ‚Äúviolator‚Äù, and you should use all of the other variables as independent variables.\nWhat variables are significant in this model? Significant variables should have a least one star, or should have a probability less than 0.05 (the column Pr(\u0026gt;|z|) in the summary output).\nParoleViolatorLog \u0026lt;- glm(violator ~ ., data = train, family = binomial) summary(ParoleViolatorLog)  Call: glm(formula = violator ~ ., family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -1.7041 -0.4236 -0.2719 -0.1690 2.8375 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -4.2411574 1.2938852 -3.278 0.00105 ** male 0.3869904 0.4379613 0.884 0.37690 race 0.8867192 0.3950660 2.244 0.02480 * age -0.0001756 0.0160852 -0.011 0.99129 state2 0.4433007 0.4816619 0.920 0.35739 state3 0.8349797 0.5562704 1.501 0.13335 state4 -3.3967878 0.6115860 -5.554 2.79e-08 *** time.served -0.1238867 0.1204230 -1.029 0.30359 max.sentence 0.0802954 0.0553747 1.450 0.14705 multiple.offenses 1.6119919 0.3853050 4.184 2.87e-05 *** crime2 0.6837143 0.5003550 1.366 0.17180 crime3 -0.2781054 0.4328356 -0.643 0.52054 crime4 -0.0117627 0.5713035 -0.021 0.98357 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 340.04 on 472 degrees of freedom Residual deviance: 251.48 on 460 degrees of freedom AIC: 277.48 Number of Fisher Scoring iterations: 6 race, state4, multiple.offenses   Problem 3.2 - Building a Logistic Regression Model What can we say based on the coefficient of the multiple.offenses variable? The following two properties might be useful to you when exploring this question:\nIf we have a coefficient c for a variable, then that means the log odds (or Logit) are increased by c for a unit increase in the variable. If we have a coefficient c for a variable, then that means the odds are multiplied by e^c for a unit increase in the variable.  exp(1.6119919) [1] 5.012786 Our model predicts that a parolee who committed multiple offenses has 5.01 times higher odds of being a violator than a parolee who did not commit multiple offenses but, is otherwise identical.\n Problem 3.3 - Building a Logistic Regression Model Consider a parolee who is male, of white race, aged 50 years at prison release, from the state of Maryland, served 3 months, had a maximum sentence of 12 months, did not commit multiple offenses, and committed a larceny.\nExplore the following questions based on the model‚Äôs predictions for this individual. (HINT: We should use the coefficients of our model, the Logistic Response Function, and the Odds equation to solve this problem.) According to the model, what are the odds this individual is a violator?\nexp(-4.2411574 + # intercept 0.3869904 * 1 + # male 0.8867192 * 1 + # white race -0.0001756 * 50 + # aged 50 0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland -0.1238867 * 3 + # served 3 months 0.0802954 * 12 + # max sentence of 12 months 1.6119919 * 0 + # did not commit multiple offenses 0.6837143*1 + -0.2781054*0 + -0.0117627*0 ) [1] 0.1825687 ## 0.1825687 # according to the model, what is the probability this individual is a violator? 1 / (1 + exp(-1 * (-4.2411574 + # intercept 0.3869904 * 1 + # male 0.8867192 * 1 + # white race -0.0001756 * 50 + # aged 50 0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland -0.1238867 * 3 + # served 3 months 0.0802954 * 12 + # max sentence of 12 months 1.6119919 * 0 + # did not commit multiple offenses 0.6837143*1 + -0.2781054*0 + -0.0117627*0 ))) [1] 0.1543832 ## Logistic Response Function -\u0026gt; P(y = 1) = 0.1543832  Problem 4.1 - Evaluating the Model on the Testing Set Use the predict() function to obtain the model‚Äôs predicted probabilities for parolees in the testing set, remembering to pass type=‚Äúresponse‚Äù.\nWhat is the maximum predicted probability of a violation?\nParolePredTest \u0026lt;- predict(ParoleViolatorLog, type = \u0026quot;response\u0026quot;, newdata = test) max(ParolePredTest) [1] 0.9072791  Problem 4.2 - Evaluating the Model on the Testing Set In the following questions, evaluate the model‚Äôs predictions on the test-set using a threshold of 0.5.\ntable(test$violator, ParolePredTest \u0026gt; 0.5)  FALSE TRUE 0 167 12 1 11 12 # what is the model\u0026#39;s sensitivity? 12 / (11 + 12) # TP / (TP + FN) [1] 0.5217391 # what is the model\u0026#39;s specificity? 167 / (167 + 12) # TN / (TN + FP) [1] 0.9329609 # what is the model\u0026#39;s accuracy? (167 + 12) / nrow(test) # (TN + TP) / N [1] 0.8861386  Problem 4.3 - Evaluating the Model on the Testing Set What is the accuracy of a simple model that predicts that every parolee is a non-violator?\ntable(test$violator)  0 1 179 23  179 / (179 + 23) [1] 0.8861386  Problem 4.4 - Evaluating the Model on the Testing Set Consider a parole board using the model to predict whether parolees will be violators or not.\nThe job of a parole board is to make sure that a prisoner is ready to be released into free society, and therefore parole boards tend to be particularily concerned about releasing prisoners who will violate their parole.\nWhich of the following most likely describes their preferences and best course of action? #### The board assigns more cost to a false negative than a false positive, and should therefore use a logistic regression cut-off less than 0.5.\n Problem 4.5 - Evaluating the Model on the Testing Set Which of the following is the most accurate assessment of the value of the logistic regression model with a cut-off 0.5 to a parole board, based on the model‚Äôs accuracy as compared to the simple baseline model? #### The model is likely of value to the board, and using a different logistic regression cut-off is likely to improve the model‚Äôs value.\n Problem 4.6 - Evaluating the Model on the Testing Set Using the ROCR package, what is the AUC value for the model?\nROCRpred = prediction(ParolePredTest, test$violator) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.8945834  Problem 4.7 - Evaluating the Model on the Testing Set Describe the meaning of AUC in this context. #### The probability the model can correctly differentiate between a randomly selected parole violator and a randomly selected parole non-violator.\n Problem 5.1 - Identifying Bias in Observational Data Our goal has been to predict the outcome of a parole decision, and we used a publicly available dataset of parole releases for predictions.\nIn this final problem, we‚Äôll evaluate a potential source of bias associated with our analysis. It is always important to evaluate a dataset for possible sources of bias.\nThe dataset contains all individuals released from parole in 2004, either due to completing their parole term or violating the terms of their parole. However, it does not contain parolees who neither violated their parole nor completed their term in 2004, causing non-violators to be underrepresented.\nThis is called ‚Äúselection bias‚Äù or ‚Äúselecting on the dependent variable,‚Äù because only a subset of all relevant parolees were included in our analysis, based on our dependent variable in this analysis (parole violation).\nHow could we improve our dataset to best address selection bias? #### We should use a dataset tracking a group of parolees from the start of their parole until either they violated parole or they completed their term.\n ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"a08232a5d5fe79172edf0f14ceff9d3c","permalink":"/project/parole_violators/parole_violators/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/parole_violators/parole_violators/","section":"project","summary":"Predict if an inmate will violate his or her terms of parole","tags":["R","Data Analytics","Machine Learning"],"title":"Predict Parole Violators","type":"project"},{"authors":null,"categories":null,"content":" Popularity of music records\nThe music industry has a well-developed market with a global annual revenue around $15 billion. The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales.\nArtists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale. A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.\nUnfortunately, the success of an artist‚Äôs release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads, while another single may turn out quite unpopular, and therefore unprofitable.\nKnowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success.\nHow can we use analytics to predict the popularity of a song? In this project, we challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.\nTaking an analytics approach, we aim to use information about a song‚Äôs properties to predict its popularity. The dataset songs.csv consists of all songs which made it to the Top 10 of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn‚Äôt make the Top 10. This data comes from three sources: Wikipedia, Billboard.com, and EchoNest.\nThe variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes: time signature, loudness, key, pitch, tempo, and timbre.\nHere‚Äôs a detailed description of the variables:\n year = the year the song was released songtitle = the title of the song artistname = the name of the artist of the song songID and artistID = identifying variables for the song and artist timesignature and timesignature_confidence = a variable estimating the time signature of the song, and the confidence in the estimate loudness = a continuous variable indicating the average amplitude of the audio in decibels tempo and tempo_confidence = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate key and key_confidence = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate energy = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness pitch = a continuous variable that indicates the pitch of the song timbre_0_min, timbre_0_max, timbre_1_min, timbre_1_max, . . . , timbre_11_min, and timbre_11_max = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables) Top10 = a binary variable indicating whether or not the song made it to the Top 10 of the Billboard Hot 100 Chart (1 if it was in the top 10, and 0 if it was not)  Problem 1.1 - Understanding the Data Use the read.csv function to load the dataset ‚Äúsongs.csv‚Äù into R. How many observations (songs) are from the year 2010?\nsongs \u0026lt;- read.csv(\u0026quot;songs.csv\u0026quot;) str(songs) \u0026#39;data.frame\u0026#39;: 7574 obs. of 39 variables: $ year : int 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ... $ songtitle : Factor w/ 7141 levels \u0026quot;ÕÑ l\u0026#39;or_e des bois\u0026quot;,..: 6204 5522 241 3115 48 608 255 4419 2886 6756 ... $ artistname : Factor w/ 1032 levels \u0026quot;50 Cent\u0026quot;,\u0026quot;98 Degrees\u0026quot;,..: 3 3 3 3 3 3 3 3 3 12 ... $ songID : Factor w/ 7549 levels \u0026quot;SOAACNI1315CD4AC42\u0026quot;,..: 595 5439 5252 1716 3431 1020 1831 3964 6904 2473 ... $ artistID : Factor w/ 1047 levels \u0026quot;AR00B1I1187FB433EB\u0026quot;,..: 671 671 671 671 671 671 671 671 671 507 ... $ timesignature : int 3 4 4 4 4 4 4 4 4 4 ... $ timesignature_confidence: num 0.853 1 1 1 0.788 1 0.968 0.861 0.622 0.938 ... $ loudness : num -4.26 -4.05 -3.57 -3.81 -4.71 ... $ tempo : num 91.5 140 160.5 97.5 140.1 ... $ tempo_confidence : num 0.953 0.921 0.489 0.794 0.286 0.347 0.273 0.83 0.018 0.929 ... $ key : int 11 10 2 1 6 4 10 5 9 11 ... $ key_confidence : num 0.453 0.469 0.209 0.632 0.483 0.627 0.715 0.423 0.751 0.602 ... $ energy : num 0.967 0.985 0.99 0.939 0.988 ... $ pitch : num 0.024 0.025 0.026 0.013 0.063 0.038 0.026 0.033 0.027 0.004 ... $ timbre_0_min : num 0.002 0 0.003 0 0 ... $ timbre_0_max : num 57.3 57.4 57.4 57.8 56.9 ... $ timbre_1_min : num -6.5 -37.4 -17.2 -32.1 -223.9 ... $ timbre_1_max : num 171 171 171 221 171 ... $ timbre_2_min : num -81.7 -149.6 -72.9 -138.6 -147.2 ... $ timbre_2_max : num 95.1 180.3 157.9 173.4 166 ... $ timbre_3_min : num -285 -380.1 -204 -73.5 -128.1 ... $ timbre_3_max : num 259 384 251 373 389 ... $ timbre_4_min : num -40.4 -48.7 -66 -55.6 -43.9 ... $ timbre_4_max : num 73.6 100.4 152.1 119.2 99.3 ... $ timbre_5_min : num -104.7 -87.3 -98.7 -77.5 -96.1 ... $ timbre_5_max : num 183.1 42.8 141.4 141.2 38.3 ... $ timbre_6_min : num -88.8 -86.9 -88.9 -70.8 -110.8 ... $ timbre_6_max : num 73.5 75.5 66.5 64.5 72.4 ... $ timbre_7_min : num -71.1 -65.8 -67.4 -63.7 -55.9 ... $ timbre_7_max : num 82.5 106.9 80.6 96.7 110.3 ... $ timbre_8_min : num -52 -61.3 -59.8 -78.7 -56.5 ... $ timbre_8_max : num 39.1 35.4 46 41.1 37.6 ... $ timbre_9_min : num -35.4 -81.9 -46.3 -49.2 -48.6 ... $ timbre_9_max : num 71.6 74.6 59.9 95.4 67.6 ... $ timbre_10_min : num -126.4 -103.8 -108.3 -102.7 -52.8 ... $ timbre_10_max : num 18.7 121.9 33.3 46.4 22.9 ... $ timbre_11_min : num -44.8 -38.9 -43.7 -59.4 -50.4 ... $ timbre_11_max : num 26 22.5 25.7 37.1 32.8 ... $ Top10 : int 0 0 0 0 0 0 0 0 0 1 ... summary(songs)  year songtitle artistname Min. :1990 Intro : 15 Various artists: 162 1st Qu.:1997 Forever : 8 Anal Cunt : 49 Median :2002 Home : 7 Various Artists: 44 Mean :2001 Goodbye : 6 Tori Amos : 41 3rd Qu.:2006 Again : 5 Eels : 37 Max. :2010 Beautiful: 5 Napalm Death : 37 (Other) :7528 (Other) :7204 songID artistID timesignature SOALSZJ1370F1A7C75: 2 ARAGWS81187FB3F768: 222 Min. :0.000 SOANPAC13936E0B640: 2 ARL14X91187FB4CF14: 49 1st Qu.:4.000 SOBDGMX12B0B80808E: 2 AR4KS8C1187FB4CF3D: 41 Median :4.000 SOBUDCZ12A58A80013: 2 AR0JZZ01187B9B2C99: 37 Mean :3.894 SODFRLK13134387FB5: 2 ARZGTK71187B9AC7F5: 37 3rd Qu.:4.000 SOEJPOK12A6D4FAFE4: 2 AR95XYH1187FB53951: 31 Max. :7.000 (Other) :7562 (Other) :7157 timesignature_confidence loudness tempo Min. :0.0000 Min. :-42.451 Min. : 0.00 1st Qu.:0.8193 1st Qu.:-10.847 1st Qu.: 88.86 Median :0.9790 Median : -7.649 Median :103.27 Mean :0.8533 Mean : -8.817 Mean :107.35 3rd Qu.:1.0000 3rd Qu.: -5.640 3rd Qu.:124.80 Max. :1.0000 Max. : 1.305 Max. :244.31 tempo_confidence key key_confidence energy Min. :0.0000 Min. : 0.000 Min. :0.0000 Min. :0.00002 1st Qu.:0.3720 1st Qu.: 2.000 1st Qu.:0.2040 1st Qu.:0.50014 Median :0.7015 Median : 6.000 Median :0.4515 Median :0.71816 Mean :0.6229 Mean : 5.385 Mean :0.4338 Mean :0.67547 3rd Qu.:0.8920 3rd Qu.: 9.000 3rd Qu.:0.6460 3rd Qu.:0.88740 Max. :1.0000 Max. :11.000 Max. :1.0000 Max. :0.99849 pitch timbre_0_min timbre_0_max timbre_1_min Min. :0.00000 Min. : 0.000 Min. :12.58 Min. :-333.72 1st Qu.:0.00300 1st Qu.: 0.000 1st Qu.:53.12 1st Qu.:-160.12 Median :0.00700 Median : 0.027 Median :55.53 Median :-107.75 Mean :0.01082 Mean : 4.123 Mean :54.46 Mean :-110.79 3rd Qu.:0.01400 3rd Qu.: 2.772 3rd Qu.:57.08 3rd Qu.: -59.71 Max. :0.54100 Max. :48.353 Max. :64.01 Max. : 123.73 timbre_1_max timbre_2_min timbre_2_max timbre_3_min Min. :-74.37 Min. :-324.86 Min. : -0.832 Min. :-495.36 1st Qu.:171.13 1st Qu.:-167.64 1st Qu.:100.519 1st Qu.:-226.87 Median :194.40 Median :-136.60 Median :129.908 Median :-170.61 Mean :212.34 Mean :-136.89 Mean :136.673 Mean :-186.11 3rd Qu.:239.24 3rd Qu.:-106.51 3rd Qu.:166.121 3rd Qu.:-131.56 Max. :549.97 Max. : 34.57 Max. :397.095 Max. : -21.55 timbre_3_max timbre_4_min timbre_4_max timbre_5_min Min. : 12.85 Min. :-207.07 Min. : -0.651 Min. :-262.48 1st Qu.:127.14 1st Qu.: -77.69 1st Qu.: 83.966 1st Qu.:-113.58 Median :189.50 Median : -63.83 Median :107.422 Median : -95.47 Mean :211.81 Mean : -65.28 Mean :108.227 Mean :-104.00 3rd Qu.:290.72 3rd Qu.: -51.34 3rd Qu.:130.286 3rd Qu.: -81.02 Max. :499.62 Max. : 51.43 Max. :257.801 Max. : -42.17 timbre_5_max timbre_6_min timbre_6_max timbre_7_min Min. :-22.41 Min. :-152.170 Min. : 12.70 Min. :-214.791 1st Qu.: 84.64 1st Qu.: -94.792 1st Qu.: 59.04 1st Qu.:-101.171 Median :119.90 Median : -80.418 Median : 70.47 Median : -81.797 Mean :127.04 Mean : -80.944 Mean : 72.17 Mean : -84.313 3rd Qu.:162.34 3rd Qu.: -66.521 3rd Qu.: 83.19 3rd Qu.: -64.301 Max. :350.94 Max. : 4.503 Max. :208.39 Max. : 5.153 timbre_7_max timbre_8_min timbre_8_max timbre_9_min Min. : 15.70 Min. :-158.756 Min. :-25.95 Min. :-149.51 1st Qu.: 76.50 1st Qu.: -73.051 1st Qu.: 40.58 1st Qu.: -70.28 Median : 94.63 Median : -62.661 Median : 49.22 Median : -58.65 Mean : 95.65 Mean : -63.704 Mean : 50.06 Mean : -59.52 3rd Qu.:112.71 3rd Qu.: -52.983 3rd Qu.: 58.46 3rd Qu.: -47.70 Max. :214.82 Max. : -2.382 Max. :144.99 Max. : 1.14 timbre_9_max timbre_10_min timbre_10_max timbre_11_min Min. : 8.415 Min. :-208.82 Min. : -6.359 Min. :-145.599 1st Qu.: 53.037 1st Qu.:-105.13 1st Qu.: 39.196 1st Qu.: -58.058 Median : 65.935 Median : -83.07 Median : 50.895 Median : -50.892 Mean : 68.028 Mean : -87.34 Mean : 55.521 Mean : -50.868 3rd Qu.: 81.267 3rd Qu.: -64.52 3rd Qu.: 66.593 3rd Qu.: -43.292 Max. :161.518 Max. : -10.64 Max. :192.417 Max. : -6.497 timbre_11_max Top10 Min. : 7.20 Min. :0.0000 1st Qu.: 38.98 1st Qu.:0.0000 Median : 46.44 Median :0.0000 Mean : 47.49 Mean :0.1477 3rd Qu.: 55.03 3rd Qu.:0.0000 Max. :110.27 Max. :1.0000  table(songs$year)  1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 328 196 186 324 198 258 178 329 380 357 363 282 518 434 479 2005 2006 2007 2008 2009 2010 392 479 622 415 483 373  373\n Problem 1.2 - Understanding the Data How many songs does the dataset include for which the artist name is ‚ÄúMichael Jackson‚Äù?\nnrow(subset(songs, artistname == \u0026quot;Michael Jackson\u0026quot;)) [1] 18  Problem 1.3 - Understanding the Data Which of these songs by Michael Jackson made it to the Top 10? Select all that apply.\nsubset(songs, artistname == \u0026quot;Michael Jackson\u0026quot; \u0026amp; Top10 == 1, select = c(artistname, songtitle))  artistname songtitle 4329 Michael Jackson You Rock My World 6207 Michael Jackson You Are Not Alone 6210 Michael Jackson Black or White 6218 Michael Jackson Remember the Time 6915 Michael Jackson In The Closet You Rock My World, You Are Not Alone\n Problem 1.4 - Understanding the Data The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). What are the values of this variable that occur in our dataset?\nsummary(songs$timesignature)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 4.000 4.000 3.894 4.000 7.000  table(songs$timesignature)  0 1 3 4 5 7 10 143 503 6787 112 19  Which timesignature value is the most frequent among songs in our dataset? #### 4\n Problem 1.5 - Understanding the Data Out of all of the songs in our dataset, the song with the highest tempo is one of the following songs.\nWhich one is it?\nsummary(songs$tempo)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00 88.86 103.27 107.35 124.80 244.31  which.max(songs$tempo) [1] 6206 songs$tempo[6206] [1] 244.307 nrow(subset(songs, tempo == 244.307)) [1] 1 songs$songtitle[6206] [1] Wanna Be Startin\u0026#39; Somethin\u0026#39; 7141 Levels: ÕÑ l\u0026#39;or_e des bois _\\x84_ _\\x84\\x8d ... Zumbi Wanna Be Startin‚Äô Somethin‚Äô\n Problem 2.1 - Creating Our Prediction Model We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set ‚ÄúSongsTrain‚Äù consisting of all the observations up to and including 2009 song releases, and a testing set ‚ÄúSongsTest‚Äù, consisting of the 2010 song releases.\nHow many observations (songs) are in the training set?\nSongsTrain \u0026lt;- subset(songs, year \u0026lt;= 2009) SongsTest \u0026lt;- subset(songs, year == 2010) nrow(songs) [1] 7574 nrow(SongsTrain) + nrow(SongsTest) [1] 7574  Problem 2.2 - Creating our Prediction Model In this problem, our outcome variable is ‚ÄúTop10‚Äù - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart.\nSince the outcome variable is binary, we will build a logistic regression model. We‚Äôll start by using all song attributes as our independent variables, which we‚Äôll call Model 1. We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model.\nSo we won‚Äôt use the variables ‚Äúyear‚Äù, ‚Äúsongtitle‚Äù, ‚Äúartistname‚Äù, ‚ÄúsongID‚Äù or ‚ÄúartistID‚Äù. We have seen in the lecture that, to build the logistic regression model, we would normally explicitly input the formula including all the independent variables in R. However, in this case, this is a tedious amount of work since we have a large number of independent variables. There is a nice trick to avoid doing so. Let‚Äôs suppose that, except for the outcome variable Top10, all other variables in the training set are inputs to Model 1. Then, we can use the formula SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial) to build our model. Notice that the ‚Äú.‚Äù is used in place of enumerating all the independent variables. (Also, keep in mind that you can choose to put quotes around binomial, or leave out the quotes. R can understand this argument either way.) However, in our case, we want to exclude some of the variables in our dataset from being used as independent variables (‚Äúyear‚Äù, ‚Äúsongtitle‚Äù, ‚Äúartistname‚Äù, ‚ÄúsongID‚Äù, and ‚ÄúartistID‚Äù).\nTo do this, we can use the following trick. First define a vector of variable names called nonvars - these are the variables that we won‚Äôt use in our model.\nnonvars = c(\u0026quot;year\u0026quot;, \u0026quot;songtitle\u0026quot;, \u0026quot;artistname\u0026quot;, \u0026quot;songID\u0026quot;, \u0026quot;artistID\u0026quot;) To remove these variables from our training and testing sets.\nSongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ] SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ] Now, use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. You should use SongsTrain to build the model.\nLooking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?\nSongsLog1 \u0026lt;- glm(Top10 ~ ., data = SongsTrain, family=binomial) summary(SongsLog1)  Call: glm(formula = Top10 ~ ., family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -1.9220 -0.5399 -0.3459 -0.1845 3.0770 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 1.470e+01 1.806e+00 8.138 4.03e-16 *** timesignature 1.264e-01 8.674e-02 1.457 0.145050 timesignature_confidence 7.450e-01 1.953e-01 3.815 0.000136 *** loudness 2.999e-01 2.917e-02 10.282 \u0026lt; 2e-16 *** tempo 3.634e-04 1.691e-03 0.215 0.829889 tempo_confidence 4.732e-01 1.422e-01 3.329 0.000873 *** key 1.588e-02 1.039e-02 1.529 0.126349 key_confidence 3.087e-01 1.412e-01 2.187 0.028760 * energy -1.502e+00 3.099e-01 -4.847 1.25e-06 *** pitch -4.491e+01 6.835e+00 -6.570 5.02e-11 *** timbre_0_min 2.316e-02 4.256e-03 5.441 5.29e-08 *** timbre_0_max -3.310e-01 2.569e-02 -12.882 \u0026lt; 2e-16 *** timbre_1_min 5.881e-03 7.798e-04 7.542 4.64e-14 *** timbre_1_max -2.449e-04 7.152e-04 -0.342 0.732087 timbre_2_min -2.127e-03 1.126e-03 -1.889 0.058843 . timbre_2_max 6.586e-04 9.066e-04 0.726 0.467571 timbre_3_min 6.920e-04 5.985e-04 1.156 0.247583 timbre_3_max -2.967e-03 5.815e-04 -5.103 3.34e-07 *** timbre_4_min 1.040e-02 1.985e-03 5.237 1.63e-07 *** timbre_4_max 6.110e-03 1.550e-03 3.942 8.10e-05 *** timbre_5_min -5.598e-03 1.277e-03 -4.385 1.16e-05 *** timbre_5_max 7.736e-05 7.935e-04 0.097 0.922337 timbre_6_min -1.686e-02 2.264e-03 -7.445 9.66e-14 *** timbre_6_max 3.668e-03 2.190e-03 1.675 0.093875 . timbre_7_min -4.549e-03 1.781e-03 -2.554 0.010661 * timbre_7_max -3.774e-03 1.832e-03 -2.060 0.039408 * timbre_8_min 3.911e-03 2.851e-03 1.372 0.170123 timbre_8_max 4.011e-03 3.003e-03 1.336 0.181620 timbre_9_min 1.367e-03 2.998e-03 0.456 0.648356 timbre_9_max 1.603e-03 2.434e-03 0.659 0.510188 timbre_10_min 4.126e-03 1.839e-03 2.244 0.024852 * timbre_10_max 5.825e-03 1.769e-03 3.292 0.000995 *** timbre_11_min -2.625e-02 3.693e-03 -7.108 1.18e-12 *** timbre_11_max 1.967e-02 3.385e-03 5.811 6.21e-09 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4759.2 on 7167 degrees of freedom AIC: 4827.2 Number of Fisher Scoring iterations: 6 AIC: 4827.2\n Problem 2.3 - Creating Our Prediction Model Let‚Äôs now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). What does the model suggest? #### The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10\n Problem 2.4 - Creating Our Prediction Model In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex.\nWhat does Model 1 suggest in terms of complexity? #### Mainstream listeners tend to prefer less complex songs\n Problem 2.5 - Creating Our Prediction Model Songs with heavier instrumentation tend to be louder (have higher values in the variable ‚Äúloudness‚Äù) and more energetic (have higher values in the variable ‚Äúenergy‚Äù). By inspecting the coefficient of the variable ‚Äúloudness‚Äù, what does Model 1 suggest? #### Mainstream listeners prefer songs with heavy instrumentation\nBy inspecting the coefficient of the variable ‚Äúenergy‚Äù, do we draw the same conclusions as above? #### No\n Problem 3.1 - Beware of Multicollinearity Issues! What is the correlation between the variables ‚Äúloudness‚Äù and ‚Äúenergy‚Äù in the training set?\ncor(SongsTrain$loudness, SongsTrain$energy) [1] 0.7399067 Given that these two variables are highly correlated, Model 1 suffers from multicollinearity. To avoid this issue, we will omit one of these two variables and re-run the logistic regression.\nIn the rest of this problem, we‚Äôll build two variations of our original model: Model 2, in which we keep ‚Äúenergy‚Äù and omit ‚Äúloudness‚Äù, and Model 3, in which we keep ‚Äúloudness‚Äù and omit ‚Äúenergy‚Äù.\n Problem 3.2 - Beware of Multicollinearity Issues! Create Model 2, which is Model 1 without the independent variable ‚Äúloudness‚Äù.\nSongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial) We just subtracted the variable loudness. We couldn‚Äôt do this with the variables ‚Äúsongtitle‚Äù and ‚Äúartistname‚Äù, because they are not numeric variables, and we might get different values in the test-set that the training set has never seen. But this approach (subtracting the variable from the model formula) will always work when you want to remove numeric variables.\nLook at the summary of SongsLog2, and inspect the coefficient of the variable ‚Äúenergy‚Äù. What do you observe?\nsummary(SongsLog2)  Call: glm(formula = Top10 ~ . - loudness, family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -2.0983 -0.5607 -0.3602 -0.1902 3.3107 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -2.241e+00 7.465e-01 -3.002 0.002686 ** timesignature 1.625e-01 8.734e-02 1.860 0.062873 . timesignature_confidence 6.885e-01 1.924e-01 3.578 0.000346 *** tempo 5.521e-04 1.665e-03 0.332 0.740226 tempo_confidence 5.497e-01 1.407e-01 3.906 9.40e-05 *** key 1.740e-02 1.026e-02 1.697 0.089740 . key_confidence 2.954e-01 1.394e-01 2.118 0.034163 * energy 1.813e-01 2.608e-01 0.695 0.486991 pitch -5.150e+01 6.857e+00 -7.511 5.87e-14 *** timbre_0_min 2.479e-02 4.240e-03 5.847 5.01e-09 *** timbre_0_max -1.007e-01 1.178e-02 -8.551 \u0026lt; 2e-16 *** timbre_1_min 7.143e-03 7.710e-04 9.265 \u0026lt; 2e-16 *** timbre_1_max -7.830e-04 7.064e-04 -1.108 0.267650 timbre_2_min -1.579e-03 1.109e-03 -1.424 0.154531 timbre_2_max 3.889e-04 8.964e-04 0.434 0.664427 timbre_3_min 6.500e-04 5.949e-04 1.093 0.274524 timbre_3_max -2.462e-03 5.674e-04 -4.339 1.43e-05 *** timbre_4_min 9.115e-03 1.952e-03 4.670 3.02e-06 *** timbre_4_max 6.306e-03 1.532e-03 4.115 3.87e-05 *** timbre_5_min -5.641e-03 1.255e-03 -4.495 6.95e-06 *** timbre_5_max 6.937e-04 7.807e-04 0.889 0.374256 timbre_6_min -1.612e-02 2.235e-03 -7.214 5.45e-13 *** timbre_6_max 3.814e-03 2.157e-03 1.768 0.076982 . timbre_7_min -5.102e-03 1.755e-03 -2.907 0.003644 ** timbre_7_max -3.158e-03 1.811e-03 -1.744 0.081090 . timbre_8_min 4.488e-03 2.810e-03 1.597 0.110254 timbre_8_max 6.423e-03 2.950e-03 2.177 0.029497 * timbre_9_min -4.282e-04 2.955e-03 -0.145 0.884792 timbre_9_max 3.525e-03 2.377e-03 1.483 0.138017 timbre_10_min 2.993e-03 1.804e-03 1.660 0.097004 . timbre_10_max 7.367e-03 1.731e-03 4.255 2.09e-05 *** timbre_11_min -2.837e-02 3.630e-03 -7.815 5.48e-15 *** timbre_11_max 1.829e-02 3.341e-03 5.476 4.34e-08 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4871.8 on 7168 degrees of freedom AIC: 4937.8 Number of Fisher Scoring iterations: 6 Model 2 suggests that songs with high energy levels tend to be more popular. This contradicts our observation in Model 1.\n Problem 3.3 - Beware of Multicollinearity Issues! Now, create Model 3, which should be exactly like Model 1, but without the variable ‚Äúenergy‚Äù.\nSongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial) Look at the summary of Model 3 and inspect the coefficient of the variable ‚Äúloudness‚Äù. Remembering that higher loudness and energy both occur in songs with heavier instrumentation, do we make the same observation about the popularity of heavy instrumentation as we did with Model 2?\nsummary(SongsLog3)  Call: glm(formula = Top10 ~ . - energy, family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -1.9182 -0.5417 -0.3481 -0.1874 3.4171 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 1.196e+01 1.714e+00 6.977 3.01e-12 *** timesignature 1.151e-01 8.726e-02 1.319 0.187183 timesignature_confidence 7.143e-01 1.946e-01 3.670 0.000242 *** loudness 2.306e-01 2.528e-02 9.120 \u0026lt; 2e-16 *** tempo -6.460e-04 1.665e-03 -0.388 0.698107 tempo_confidence 3.841e-01 1.398e-01 2.747 0.006019 ** key 1.649e-02 1.035e-02 1.593 0.111056 key_confidence 3.394e-01 1.409e-01 2.409 0.015984 * pitch -5.328e+01 6.733e+00 -7.914 2.49e-15 *** timbre_0_min 2.205e-02 4.239e-03 5.200 1.99e-07 *** timbre_0_max -3.105e-01 2.537e-02 -12.240 \u0026lt; 2e-16 *** timbre_1_min 5.416e-03 7.643e-04 7.086 1.38e-12 *** timbre_1_max -5.115e-04 7.110e-04 -0.719 0.471928 timbre_2_min -2.254e-03 1.120e-03 -2.012 0.044190 * timbre_2_max 4.119e-04 9.020e-04 0.457 0.647915 timbre_3_min 3.179e-04 5.869e-04 0.542 0.588083 timbre_3_max -2.964e-03 5.758e-04 -5.147 2.64e-07 *** timbre_4_min 1.105e-02 1.978e-03 5.585 2.34e-08 *** timbre_4_max 6.467e-03 1.541e-03 4.196 2.72e-05 *** timbre_5_min -5.135e-03 1.269e-03 -4.046 5.21e-05 *** timbre_5_max 2.979e-04 7.855e-04 0.379 0.704526 timbre_6_min -1.784e-02 2.246e-03 -7.945 1.94e-15 *** timbre_6_max 3.447e-03 2.182e-03 1.580 0.114203 timbre_7_min -5.128e-03 1.768e-03 -2.900 0.003733 ** timbre_7_max -3.394e-03 1.820e-03 -1.865 0.062208 . timbre_8_min 3.686e-03 2.833e-03 1.301 0.193229 timbre_8_max 4.658e-03 2.988e-03 1.559 0.119022 timbre_9_min -9.318e-05 2.957e-03 -0.032 0.974859 timbre_9_max 1.342e-03 2.424e-03 0.554 0.579900 timbre_10_min 4.050e-03 1.827e-03 2.217 0.026637 * timbre_10_max 5.793e-03 1.759e-03 3.294 0.000988 *** timbre_11_min -2.638e-02 3.683e-03 -7.162 7.96e-13 *** timbre_11_max 1.984e-02 3.365e-03 5.896 3.74e-09 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4782.7 on 7168 degrees of freedom AIC: 4848.7 Number of Fisher Scoring iterations: 6 In the remainder of this problem, we‚Äôll just use Model 3.\n Problem 4.1 - Validating Our Model Make predictions on the test-set using Model 3. What is the accuracy of Model 3 on the test-set, using a threshold of 0.45? (Compute the accuracy as a number between 0 and 1.)\npredSongsTest = predict(SongsLog3, type=\u0026quot;response\u0026quot;, newdata = SongsTest) table(SongsTest$Top10, predSongsTest \u0026gt; 0.45)  FALSE TRUE 0 309 5 1 40 19 (309 + 19) / nrow(SongsTest) [1] 0.8793566  Problem 4.2 - Validating Our Model Let‚Äôs check if there‚Äôs any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs.\nWhat would the accuracy of the baseline model be on the test-set?\ntable(SongsTest$Top10)  0 1 314 59  314/(314 + 59) [1] 0.8418231  Problem 4.3 - Validating Our Model It seems that Model 3 gives us a small improvement over the baseline model. Still, does it create an edge? Let‚Äôs view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. The company‚Äôs objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular.\nA competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer.\nHow many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?\ntable(SongsTest$Top10, predSongsTest \u0026gt; 0.45)  FALSE TRUE 0 309 5 1 40 19 19\nHow many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45? #### 5\n Problem 4.4 - Validating Our Model # what is the sensitivity of Model 3 on the test set, using a threshold of 0.45? 19 / (40 + 19) [1] 0.3220339 # what is the specificity of Model 3 on the test set, using a threshold of 0.45? 309 / (309 + 5) [1] 0.9840764  Conclusions  Model 3 favors specificity over sensitivity. Model 3 provides conservative predictions, and predicts that a song will make it to the Top 10 very rarely. So while it detects less than half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.   ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"64f145759eb69d7479b4cbb5964be4cc","permalink":"/project/music/music/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/music/music/","section":"project","summary":"Predict whether a song will reach a spot in the Top 10, of the Billboard Hot 100 Chart","tags":["regression","R","Machine Learning"],"title":"Predict Popular Songs","type":"project"},{"authors":null,"categories":null,"content":" Flu epidemics constitute a major public health concern causing respiratory illnesses, hospitalizations, and deaths. According to the National Vital Statistics Reports published in October 2012, influenza ranked as the eighth leading cause of death in 2011 in the U.S. Each year, 250,000 to 500,000 deaths are attributed to influenza related diseases throughout the world.\nThe U.S. Centers for Disease Control and Prevention (CDC) and the European Influenza Surveillance Scheme (EISS) detect influenza activity through virologic and clinical data, including Influenza-like Illness (ILI) physician visits. Reporting national and regional data, however, are published with a 1-2 week lag.\nThe Google Flu Trends project was initiated to see if faster reporting can be made possible by considering flu-related online search queries ‚Äì data that is available almost immediately.\nI would like to estimate influenza-like illness (ILI) activity using Google web search logs. Fortunately, one can easily access this data online:\n ILI Data - The CDC publishes on its website the official regional and state-level percentage of patient visits to healthcare providers for ILI purposes on a weekly basis. Google Search Queries - Google Trends allows public retrieval of weekly counts for every query searched by users around the world.  For each location, the counts are normalized by dividing the count for each query in a particular week by the total number of online search queries submitted in that location during the week. Then, the values are adjusted to be between 0 and 1.\nThe csv file FluTrain.csv aggregates this data from January 1, 2004 until December 31, 2011 as follows:\n ‚ÄúWeek‚Äù - The range of dates represented by this observation, in year/month/day format. ‚ÄúILI‚Äù - This column lists the percentage of ILI-related physician visits for the corresponding week. ‚ÄúQueries‚Äù - This column lists the fraction of queries that are ILI-related for the corresponding week, adjusted to be between 0 and 1 (higher values correspond to more ILI-related search queries).  Before applying analytics tools on the training set, we first need to understand the data at hand. Looking at the time period 2004-2011, which week corresponds to the highest percentage of ILI-related physician visits?\nLoading the data FluTrain \u0026lt;- read.csv(\u0026quot;FluTrain.csv\u0026quot;) summary(FluTrain)  Week ILI Queries 2004-01-04 - 2004-01-10: 1 Min. :0.5341 Min. :0.04117 2004-01-11 - 2004-01-17: 1 1st Qu.:0.9025 1st Qu.:0.15671 2004-01-18 - 2004-01-24: 1 Median :1.2526 Median :0.28154 2004-01-25 - 2004-01-31: 1 Mean :1.6769 Mean :0.28603 2004-02-01 - 2004-02-07: 1 3rd Qu.:2.0587 3rd Qu.:0.37849 2004-02-08 - 2004-02-14: 1 Max. :7.6189 Max. :1.00000 (Other) :411  str(FluTrain) \u0026#39;data.frame\u0026#39;: 417 obs. of 3 variables: $ Week : Factor w/ 417 levels \u0026quot;2004-01-04 - 2004-01-10\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ ILI : num 2.42 1.81 1.71 1.54 1.44 ... $ Queries: num 0.238 0.22 0.226 0.238 0.224 ...  Problem 1.1 - EDA Select the day of the month corresponding to the start of this week?\nFluTrain[which.max(FluTrain$ILI),]  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 Which week corresponds to the highest percentage of ILI-related query fraction?\nFluTrain[which.max(FluTrain$Queries),]  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 subset(FluTrain, Queries == 1)  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 October 18, 2009\n Problem 1.2 - EDA Let‚Äôs now understand the data at a high level. Plot the histogram of the dependent variable, ILI.\nWhat best describes the distribution of values of ILI?\nhist(FluTrain$ILI) Most of the ILI values are small, with a relatively small number of much larger values (in statistics, this sort of data is called ‚Äúskew right‚Äù).   Problem 1.3 - EDA When handling a skewed dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself ‚Äì this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models.\nIn this problem, I‚Äôll predict the natural log of the ILI variable, which can be computed using the log() function. Plot the natural logarithm of ILI versus Queries.\nplot(log(FluTrain$ILI), FluTrain$Queries) plot(FluTrain$Queries, log(FluTrain$ILI)) What does the plot suggest? #### There is a positive, linear relationship between log(ILI) and Queries.\n Problem 2.1 - Linear Regression Model Based on the plot we just made, it seems that a linear regression model could be a good modeling choice. Based on our understanding of the data from the previous subproblem, which model best describes our estimation problem? #### log(ILI) = intercept + coefficient x Queries, where the coefficient is positive.\n Problem 2.2 - Linear Regression Model Let‚Äôs call the regression model from the previous problem (Problem 2.1). FluTrend1 and run it. Hint: to take the logarithm of a variable Var in a regression equation, you simply use log(Var) when specifying the formula to the lm() function.\nFluTrend1 \u0026lt;- lm(log(ILI) ~ Queries, data = FluTrain) What is the training set R-squared value for FluTrend1 model (the ‚ÄúMultiple R-squared‚Äù)?\nsummary(FluTrend1)  Call: lm(formula = log(ILI) ~ Queries, data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.76003 -0.19696 -0.01657 0.18685 1.06450 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.49934 0.03041 -16.42 \u0026lt;2e-16 *** Queries 2.96129 0.09312 31.80 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.2995 on 415 degrees of freedom Multiple R-squared: 0.709, Adjusted R-squared: 0.7083 F-statistic: 1011 on 1 and 415 DF, p-value: \u0026lt; 2.2e-16 0.709\n Problem 2.3 - Linear Regression Model For a single variable linear regression model, there is a direct relationship between the R-squared and the correlation between the independent and the dependent variables.\nWhat is the relationship we infer from our problem? (Don‚Äôt forget that you can use the cor function to compute the correlation between two variables.)\ncorILIQueries \u0026lt;- cor(log(FluTrain$ILI), FluTrain$Queries) cor(FluTrain$ILI, FluTrain$Queries) [1] 0.8142115 corILIQueries^2 [1] 0.7090201 log(1/corILIQueries) [1] 0.1719357 exp(-0.5 * corILIQueries) [1] 0.6563792 Note = R-squared = Correlation^2\nNote that the ‚Äúexp‚Äù function stands for the exponential function. The exponential can be computed in R using the function exp().\n Problem 3.1 - Performance on the Test Set The file provides the 2012 weekly data of the ILI-related search queries and the observed weekly percentage of ILI-related physician visits.\nLoad this data into a dataframe called FluTest.\nFluTest \u0026lt;- read.csv(\u0026quot;FluTest.csv\u0026quot;) Normally, we would obtain test-set predictions from the model FluTrend1 using the code PredTest1 = predict(FluTrend1, newdata=FluTest) However, the dependent variable in our model is log(ILI), so PredTest1 would contain predictions of the log(ILI) value.\nWe are instead interested in obtaining predictions of the ILI value. We can convert from predictions of log(ILI) to predictions of ILI via exponentiation, or the exp() function. The new code, which predicts the ILI value.\nPredTest1 = exp(predict(FluTrend1, newdata=FluTest)) What is our estimate for the percentage of ILI-related physician visits for the week of March 11, 2012? (HINT: You can either just output FluTest$Week to find which element corresponds to March 11, 2012, or you can use the ‚Äúwhich‚Äù function in R. To learn more about the which function, type ?which in your R console.)\nFluTest$Week  [1] 2012-01-01 - 2012-01-07 2012-01-08 - 2012-01-14 [3] 2012-01-15 - 2012-01-21 2012-01-22 - 2012-01-28 [5] 2012-01-29 - 2012-02-04 2012-02-05 - 2012-02-11 [7] 2012-02-12 - 2012-02-18 2012-02-19 - 2012-02-25 [9] 2012-02-26 - 2012-03-03 2012-03-04 - 2012-03-10 [11] 2012-03-11 - 2012-03-17 2012-03-18 - 2012-03-24 [13] 2012-03-25 - 2012-03-31 2012-04-01 - 2012-04-07 [15] 2012-04-08 - 2012-04-14 2012-04-15 - 2012-04-21 [17] 2012-04-22 - 2012-04-28 2012-04-29 - 2012-05-05 [19] 2012-05-06 - 2012-05-12 2012-05-13 - 2012-05-19 [21] 2012-05-20 - 2012-05-26 2012-05-27 - 2012-06-02 [23] 2012-06-03 - 2012-06-09 2012-06-10 - 2012-06-16 [25] 2012-06-17 - 2012-06-23 2012-06-24 - 2012-06-30 [27] 2012-07-01 - 2012-07-07 2012-07-08 - 2012-07-14 [29] 2012-07-15 - 2012-07-21 2012-07-22 - 2012-07-28 [31] 2012-07-29 - 2012-08-04 2012-08-05 - 2012-08-11 [33] 2012-08-12 - 2012-08-18 2012-08-19 - 2012-08-25 [35] 2012-08-26 - 2012-09-01 2012-09-02 - 2012-09-08 [37] 2012-09-09 - 2012-09-15 2012-09-16 - 2012-09-22 [39] 2012-09-23 - 2012-09-29 2012-09-30 - 2012-10-06 [41] 2012-10-07 - 2012-10-13 2012-10-14 - 2012-10-20 [43] 2012-10-21 - 2012-10-27 2012-10-28 - 2012-11-03 [45] 2012-11-04 - 2012-11-10 2012-11-11 - 2012-11-17 [47] 2012-11-18 - 2012-11-24 2012-11-25 - 2012-12-01 [49] 2012-12-02 - 2012-12-08 2012-12-09 - 2012-12-15 [51] 2012-12-16 - 2012-12-22 2012-12-23 - 2012-12-29 52 Levels: 2012-01-01 - 2012-01-07 ... 2012-12-23 - 2012-12-29 FluTest[11, ]  Week ILI Queries 11 2012-03-11 - 2012-03-17 2.293422 0.4329349 PredTest1[11]  11 2.187378  2.293422\n Problem 3.2 - Performance on the Test Set What is the relative error betweeen the estimate (our prediction) and the observed value for the week of March 11, 2012? Note that the relative error is calculated as (Observed ILI - Estimated ILI)/Observed ILI.\n(FluTest[11, 2] - PredTest1[11]) / FluTest[11, 2]  11 0.04623827   Problem 3.3 - Performance on the Test Set What is the Root Mean Square Error (RMSE) between our estimates and the actual observations for the percentage of ILI-related physician visits, on the test-set?\nFluTestSSE = sum((PredTest1 - FluTest$ILI)^2) FluTestRMSE = sqrt(FluTestSSE/nrow(FluTest)) FluTestRMSE [1] 0.7490645  Problem 4.1 - Training a Time Series Model The observations in this dataset are consecutive weekly measurements of the dependent and independent variables. This sort of dataset is called a ‚Äútime series.‚Äù\nOften, statistical models can be improved by predicting the current value of the dependent variable using the value of the dependent variable from earlier weeks. In our models, this means we will predict the ILI variable in the current week using values of the ILI variable from previous weeks.\nFirst, we need to decide the amount of time to lag the observations. Because the ILI variable is reported with a 1- or 2-week lag, a decision maker cannot rely on the previous week‚Äôs ILI value to predict the current week‚Äôs value. Instead, the decision maker will only have data available from 2 or more weeks ago.\nWe will build a variable called ILILag2 that contains the ILI value from 2 weeks before the current observation.\nTo do so, we‚Äôll use the ‚Äúzoo‚Äù package, which provides a number of helpful methods for time series models. While many functions are built into R, you need to add new packages to use some functions. New packages can be installed and loaded easily in R. Run the following two codes to install and load the zoo package.\nIn the first code, you will be prompted to select a CRAN mirror to use for your download. Select a mirror near you geographically. install.packages(‚Äúzoo‚Äù)\nAfter installing and loading the zoo package, create the ILILag2 variable in the training set.\nILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE) FluTrain$ILILag2 = coredata(ILILag2) The value of -2 passed to lag means to return 2 observations before the current one; a positive value would have returned future observations. The parameter na.pad=TRUE means to add missing values for the first two weeks of our dataset, where we can‚Äôt compute the data from 2 weeks earlier.\n?lag ?coredata ILILag2  1 2 3 4 5 6 7 NA NA 2.4183312 1.8090560 1.7120239 1.5424951 1.4378683 8 9 10 11 12 13 14 1.3242740 1.3072567 1.0369770 1.0103204 1.0524925 1.0200901 0.9244187 15 16 17 18 19 20 21 0.7906450 0.8026098 0.8361300 0.7924358 0.6835877 0.7574523 0.7885854 22 23 24 25 26 27 28 0.8121710 0.8044629 0.8777009 0.7414530 0.6610222 0.7151092 0.5622412 29 30 31 32 33 34 35 0.7868082 0.8606578 0.6899440 0.7796912 0.6281439 0.9024586 0.8064432 36 37 38 39 40 41 42 0.8748878 0.9932130 0.8761408 0.9480916 0.9269426 0.9716430 0.8971591 43 44 45 46 47 48 49 1.0224828 1.0629632 1.1469570 1.2049501 1.3051655 1.2869916 1.5946756 50 51 52 53 54 55 56 1.3971432 1.4499567 1.6174545 2.1911192 2.5664893 2.1764491 2.2017121 57 58 59 60 61 62 63 2.5301211 3.0652381 3.9806083 4.5956803 4.7519706 4.1796206 3.4535851 64 65 66 67 68 69 70 3.1585224 2.6732010 2.3516104 1.8924285 1.5249048 1.4113441 1.2506826 71 72 73 74 75 76 77 1.2070250 1.0789550 1.1452080 1.0612426 1.0567977 1.2519310 1.0141893 78 79 80 81 82 83 84 1.0419693 0.9540274 0.8482299 0.8418715 0.7308936 0.7134316 0.6706772 85 86 87 88 89 90 91 0.6892776 0.7049290 0.6159033 0.6094256 0.6802587 0.7754884 0.6834214 92 93 94 95 96 97 98 0.7810748 0.8069435 1.0763468 1.0586890 1.1152326 1.1238125 1.2548892 99 100 101 102 103 104 105 1.3366090 1.3786364 1.6082900 1.4831056 1.6537399 2.0067892 2.5685716 106 107 108 109 110 111 112 3.0527762 2.4250373 2.0019506 2.0586902 2.2127697 2.3222001 2.4927920 113 114 115 116 117 118 119 2.7948942 2.9691114 2.8395905 2.7779902 2.4728693 2.1806146 2.0167951 120 121 122 123 124 125 126 1.6410133 1.3582865 1.1427983 1.0403125 0.9643469 0.9379817 0.9474493 127 128 129 130 131 132 133 0.8919182 0.8646427 0.9703199 0.8443901 0.7748704 0.8213725 0.8727445 134 135 136 137 138 139 140 0.9226345 0.8994868 0.8430824 0.8818244 0.8171452 0.8715001 0.7386205 141 142 143 144 145 146 147 0.7979660 1.0139373 0.8809358 0.9433663 0.8915462 1.2032228 1.0578822 148 149 150 151 152 153 154 1.1305354 1.1255230 1.2080820 1.3495244 1.4689004 1.8276716 1.6656012 155 156 157 158 159 160 161 1.8596834 2.3889130 2.7897759 3.1154858 2.2694245 1.8635464 1.9998635 162 163 164 165 166 167 168 2.4406044 2.8301821 3.1234256 3.2701949 3.1775688 2.7236366 2.5020140 169 170 171 172 173 174 175 2.4271992 1.9604132 1.5913980 1.3697835 1.3631668 1.1736951 1.0635756 176 177 178 179 180 181 182 0.9697111 0.9653617 0.8567489 0.8633465 0.9353695 0.7455694 0.7404281 183 184 185 186 187 188 189 0.6728965 0.6662820 0.6627473 0.5456190 0.5862306 0.6606867 0.5340928 190 191 192 193 194 195 196 0.5855491 0.6180750 0.6874647 0.7156961 0.8293131 0.8009115 0.9184839 197 198 199 200 201 202 203 0.8142590 1.0719708 1.2178574 1.2457554 1.3598449 1.4467085 1.5328638 204 205 206 207 208 209 210 1.6665324 1.9748773 1.6730547 1.6340509 1.7459475 1.9364319 2.4890534 211 212 213 214 215 216 217 2.2540484 2.0914715 2.3593428 3.3233143 4.4338100 5.3454714 5.4225751 218 219 220 221 222 223 224 5.3030330 4.2445550 3.6280001 3.0346275 2.5359536 2.0573015 1.7415035 225 226 227 228 229 230 231 1.4065217 1.2686070 1.0771887 0.9934452 0.9112119 0.9721091 0.9932575 232 233 234 235 236 237 238 1.0913202 0.8884460 0.8876915 0.8831874 0.8267564 0.7832014 0.7806103 239 240 241 242 243 244 245 0.7690726 0.7212979 0.7525273 0.7527210 0.7927660 0.7438962 0.8141663 246 247 248 249 250 251 252 0.8384009 0.8511236 1.1097575 1.0311436 1.0228436 1.0301739 1.0124478 253 254 255 256 257 258 259 1.0835911 1.1657765 1.1912964 1.2807470 1.2705251 1.5957825 1.4584994 260 261 262 263 264 265 266 1.4992072 1.6298157 2.1556121 2.0205270 1.5456623 1.6422367 1.9652378 267 268 269 270 271 272 273 2.3436784 2.8605744 3.3421049 3.2056588 3.1004908 2.9581850 2.4638058 274 275 276 277 278 279 280 2.1927224 1.8739459 1.6481690 1.4987776 1.2923267 1.2716411 2.9815890 281 282 283 284 285 286 287 2.4370224 2.2813011 3.8157199 4.2131523 3.1783224 2.5097162 2.0663177 288 289 290 291 292 293 294 1.7180460 1.5596467 1.3085629 1.1869460 1.1379623 1.1500523 1.1126189 295 296 297 298 299 300 301 1.1614188 1.6410714 2.4716598 3.7196936 3.9497480 4.0875636 4.0189724 302 303 304 305 306 307 308 4.6036164 5.6608671 6.8152222 7.6188921 7.3883586 6.3392723 4.9434950 309 310 311 312 313 314 315 3.8099612 3.4410588 2.6677306 2.4718250 2.3449995 2.7143498 2.6766718 316 317 318 319 320 321 322 1.9828382 1.8274862 1.9260563 1.9249472 2.0887684 2.0343408 1.9764946 323 324 325 326 327 328 329 1.9936177 1.8538260 1.8673036 1.6998677 1.4974082 1.4511188 1.2071478 330 331 332 333 334 335 336 1.1741508 1.1620668 1.1721343 1.1216765 1.1498116 1.1332758 1.0817133 337 338 339 340 341 342 343 1.1995860 0.9528083 0.9160321 0.9265822 0.8696197 0.9031331 0.7737757 344 345 346 347 348 349 350 0.7427744 0.7309345 0.7868818 0.7630507 0.8410432 0.7915728 0.9127318 351 352 353 354 355 356 357 1.0339765 0.9340091 1.0818888 1.0656260 1.1350529 1.2525629 1.2456956 358 359 360 361 362 363 364 1.2677380 1.4372295 1.5334125 1.6944544 1.9915024 1.8130453 2.0142579 365 366 367 368 369 370 371 2.5565913 3.3818486 3.4317231 2.6915111 2.9106289 3.4923189 4.0036963 372 373 374 375 376 377 378 4.4353368 4.2421482 4.3971861 3.9025565 3.1507275 2.7242234 2.3333563 379 380 381 382 383 384 385 1.9250003 1.7524260 1.5770365 1.3576558 1.3122310 1.1493747 1.1145057 386 387 388 389 390 391 392 1.1098449 1.0524026 1.0353647 1.1177658 0.9829495 0.9251944 0.8355311 393 394 395 396 397 398 399 0.8323927 0.8555910 0.7069494 0.6943868 0.6879762 0.6447430 0.6753299 400 401 402 403 404 405 406 0.7282297 0.8065263 0.8604084 0.9360754 0.9666827 0.9960071 1.1084635 407 408 409 410 411 412 413 1.2030858 1.2369566 1.2525865 1.3054612 1.4528432 1.4408922 1.4622115 414 415 416 417 1.6554147 1.4657230 1.5181061 1.6639544  How many values are missing in the new ILILag2 variable?\nsum(is.na(FluTrain$ILILag2)) [1] 2  Problem 4.2 - Training a Time Series Model Use the plot() function to plot the log of ILILag2 against the log of ILI.\nWhich best describes the relationship between these two variables?\nplot(log(FluTrain$ILILag2), log(FluTrain$ILI)) There is a strong positive relationship between log(ILILag2) and log(ILI).\n Problem 4.3 - Training a Time Series Model Train a linear regression model on the FluTrain dataset to predict the log of the ILI variable using the Queries variable as well as the log of the ILILag2 variable. Call this model FluTrend2.\nFluTrend2 \u0026lt;- lm(log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Which coefficients are significant at the p=0.05 level in this regression model?\nsummary(FluTrend2)  Call: lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.52209 -0.11082 -0.01819 0.08143 0.76785 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.24064 0.01953 -12.32 \u0026lt;2e-16 *** Queries 1.25578 0.07910 15.88 \u0026lt;2e-16 *** log(ILILag2) 0.65569 0.02251 29.14 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.1703 on 412 degrees of freedom (2 observations deleted due to missingness) Multiple R-squared: 0.9063, Adjusted R-squared: 0.9059 F-statistic: 1993 on 2 and 412 DF, p-value: \u0026lt; 2.2e-16 All are significant at p\u0026lt;0.05\nWhat is the R^2 value of the FluTrend2 model? #### 0.9063\n Problem 4.4 - Training a Time Series Model On the basis of R-squared value and significance of coefficients, which statement is the most accurate?\nsummary(FluTrend1)  Call: lm(formula = log(ILI) ~ Queries, data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.76003 -0.19696 -0.01657 0.18685 1.06450 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.49934 0.03041 -16.42 \u0026lt;2e-16 *** Queries 2.96129 0.09312 31.80 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.2995 on 415 degrees of freedom Multiple R-squared: 0.709, Adjusted R-squared: 0.7083 F-statistic: 1011 on 1 and 415 DF, p-value: \u0026lt; 2.2e-16 summary(FluTrend2)  Call: lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.52209 -0.11082 -0.01819 0.08143 0.76785 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.24064 0.01953 -12.32 \u0026lt;2e-16 *** Queries 1.25578 0.07910 15.88 \u0026lt;2e-16 *** log(ILILag2) 0.65569 0.02251 29.14 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.1703 on 412 degrees of freedom (2 observations deleted due to missingness) Multiple R-squared: 0.9063, Adjusted R-squared: 0.9059 F-statistic: 1993 on 2 and 412 DF, p-value: \u0026lt; 2.2e-16 FluTrend2 is a stronger model than FluTrend1 on the training set, due to it‚Äôs higher R^2 value.   Problem 5.1 - Evaluating the Time Series Model in the Test Set So far, we have only added the ILILag2 variable to the FluTrain dataframe. To make predictions with our FluTrend2 model, we‚Äôll also need to add ILILag2 to the FluTest dataframe (note that adding variables before splitting into a training and testing set can prevent this duplication of effort).\nModifying the code from the previous subproblem to add an ILILag2 variable to the FluTest dataframe.\nHow many missing values are there in this new variable?\nTest_ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE) FluTest$ILILag2 = coredata(Test_ILILag2) sum(is.na(FluTest$ILILag2)) [1] 2  Problem 5.2 - Evaluating the Time Series Model in the Test Set In this problem, the training and testing sets are split sequentially ‚Äì the training set contains all observations from 2004-2011 and the testing set contains all observations from 2012.\nThere is no time gap between the two datasets, meaning the first observation in FluTest was recorded one week after the last observation in FluTrain. From this, we can identify how to fill in the missing values for the ILILag2 variable in FluTest. Which value should be used to fill in the ILILag2 variable for the first observation in FluTest?\nThe ILI value of the second-to-last observation in the FluTrain dataframe. Which value should be used to fill in the ILILag2 variable for the second observation in FluTest? #### The ILI value of the last observation in the FluTrain dataframe.\n Problem 5.3 - Evaluating the Time Series Model in the Test Set Fill in the missing values for ILILag2 in FluTest. In terms of syntax, you could set the value of ILILag2 in row ‚Äúx‚Äù of the FluTest dataframe to the value of ILI in row ‚Äúy‚Äù of the FluTrain dataframe with ‚ÄúFluTest\\(ILILag2[x] = FluTrain\\)ILI[y]‚Äù.\nUse the answer to the previous questions to determine the appropriate values of ‚Äúx‚Äù and ‚Äúy‚Äù. It may be helpful to check the total number of rows in FluTrain using str(FluTrain) or nrow(FluTrain).\nnrow(FluTrain) [1] 417 FluTest$ILILag2[1] = FluTrain$ILI[416] FluTest$ILILag2[2] = FluTrain$ILI[417] What is the new value of the ILILag2 variable in the first row of FluTest?\nFluTrain$ILI[416] [1] 1.852736 FluTest$ILILag2[1] [1] 1.852736 What is the new value of the ILILag2 variable in the second row of FluTest?\nFluTrain$ILI[417] [1] 2.12413 FluTest$ILILag2[2] [1] 2.12413  Problem 5.4 - Evaluating the Time Series Model in the Test Set Obtain test-set predictions of the ILI variable from the FluTrend2 model, again remembering to call the exp() function on the result of the predict() function to obtain predictions for ILI instead of log(ILI).\nWhat is the test-set RMSE of the FluTrend2 model?\nPredTest2 = exp(predict(FluTrend2, newdata=FluTest)) FluTestSSE2 = sum((PredTest2 - FluTest$ILI)^2) FluTestRMSE2 = sqrt(FluTestSSE2/nrow(FluTest)) FluTestRMSE2 [1] 0.2942029  Problem 5.5 - Evaluating the Time Series Model in the Test Set Which model obtained the best test-set RMSE? #### FluTrend2 (less RMSE is better)\nConclusion In this analysis, I‚Äôve used a simple time series model with a single lag term. ARIMA models are a more general form of the model we built, which can include multiple lag terms as well as more complicated combinations of previous values of the dependent variable.\n  ","date":1554508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554508800,"objectID":"fc1cd3da47244505d6c2a5717a1bee4f","permalink":"/project/flu_epidemics/flu_epidemics/","publishdate":"2019-04-06T00:00:00Z","relpermalink":"/project/flu_epidemics/flu_epidemics/","section":"project","summary":"The Google Flu Trends project","tags":["R","Data Analytics","Machine Learning"],"title":"Detecting Flu Epidemics via Search Engine Query Data","type":"project"},{"authors":null,"categories":null,"content":" The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science.\nThe test provides a quantitative way to compare the performance of students from different parts of the world.\nIn this analysis, I‚Äôll predict the reading scores of students from the USA on the 2009 PISA exam.\nThe datasets contain information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data we are bound by the NCES data use agreement, which prohibits any attempt to determine the identity of any student in the datasets.\nEach row in the datasets represents one student taking the exam. The datasets have the following variables:\n grade: The grade in school of the student (most 15-year-olds in America are in 10th grade) male: Whether the student is male (1/0) raceeth: The race/ethnicity composite of the student preschool: Whether the student attended preschool (1/0) expectBachelors: Whether the student expects to obtain a bachelor‚Äôs degree (1/0) motherHS: Whether the student‚Äôs mother completed high school (1/0) motherBachelors: Whether the student‚Äôs mother obtained a bachelor‚Äôs degree (1/0) motherWork: Whether the student‚Äôs mother has part-time or full-time work (1/0) fatherHS: Whether the student‚Äôs father completed high school (1/0) fatherBachelors: Whether the student‚Äôs father obtained a bachelor‚Äôs degree (1/0) fatherWork: Whether the student‚Äôs father has part-time or full-time work (1/0) selfBornUS: Whether the student was born in the United States of America (1/0) motherBornUS: Whether the student‚Äôs mother was born in the United States of America (1/0) fatherBornUS: Whether the student‚Äôs father was born in the United States of America (1/0) englishAtHome: Whether the student speaks English at home (1/0) computerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0) read30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0) minutesPerWeekEnglish: The number of minutes per week the student spend in English class studentsInEnglish: The number of students in this student‚Äôs English class at school schoolHasLibrary: Whether this student‚Äôs school has a library (1/0) publicSchool: Whether this student attends a public school (1/0) urban: Whether this student‚Äôs school is in an urban area (1/0) schoolSize: The number of students in this student‚Äôs school readingScore: The student‚Äôs reading score, on a 1000-point scale  Problem 1.1 - Dataset size Load the training and testing sets using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.\npisaTrain \u0026lt;- read.csv(\u0026quot;pisa2009train.csv\u0026quot;) pisaTest \u0026lt;- read.csv(\u0026quot;pisa2009test.csv\u0026quot;) str(pisaTrain) \u0026#39;data.frame\u0026#39;: 3663 obs. of 24 variables: $ grade : int 11 11 9 10 10 10 10 10 9 10 ... $ male : int 1 1 1 0 1 1 0 0 0 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: NA 7 7 3 4 3 2 7 7 5 ... $ preschool : int NA 0 1 1 1 1 0 1 1 1 ... $ expectBachelors : int 0 0 1 1 0 1 1 1 0 1 ... $ motherHS : int NA 1 1 0 1 NA 1 1 1 1 ... $ motherBachelors : int NA 1 1 0 0 NA 0 0 NA 1 ... $ motherWork : int 1 1 1 1 1 1 1 0 1 1 ... $ fatherHS : int NA 1 1 1 1 1 NA 1 0 0 ... $ fatherBachelors : int NA 0 NA 0 0 0 NA 0 NA 0 ... $ fatherWork : int 1 1 1 1 0 1 NA 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 1 0 1 1 1 ... $ motherBornUS : int 0 1 1 1 1 1 1 1 1 1 ... $ fatherBornUS : int 0 1 1 1 0 1 NA 1 1 1 ... $ englishAtHome : int 0 1 1 1 1 1 1 1 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 1 1 1 1 1 ... $ read30MinsADay : int 0 1 0 1 1 0 0 1 0 0 ... $ minutesPerWeekEnglish: int 225 450 250 200 250 300 250 300 378 294 ... $ studentsInEnglish : int NA 25 28 23 35 20 28 30 20 24 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 0 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 1 ... $ urban : int 1 0 0 1 1 0 1 0 1 0 ... $ schoolSize : int 673 1173 1233 2640 1095 227 2080 1913 502 899 ... $ readingScore : num 476 575 555 458 614 ... summary(pisaTrain)  grade male raceeth Min. : 8.00 Min. :0.0000 White :2015 1st Qu.:10.00 1st Qu.:0.0000 Hispanic : 834 Median :10.00 Median :1.0000 Black : 444 Mean :10.09 Mean :0.5111 Asian : 143 3rd Qu.:10.00 3rd Qu.:1.0000 More than one race: 124 Max. :12.00 Max. :1.0000 (Other) : 68 NA\u0026#39;s : 35 preschool expectBachelors motherHS motherBachelors Min. :0.0000 Min. :0.0000 Min. :0.00 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:1.00 1st Qu.:0.0000 Median :1.0000 Median :1.0000 Median :1.00 Median :0.0000 Mean :0.7228 Mean :0.7859 Mean :0.88 Mean :0.3481 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.00 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00 Max. :1.0000 NA\u0026#39;s :56 NA\u0026#39;s :62 NA\u0026#39;s :97 NA\u0026#39;s :397 motherWork fatherHS fatherBachelors fatherWork Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :0.0000 Median :1.0000 Mean :0.7345 Mean :0.8593 Mean :0.3319 Mean :0.8531 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :93 NA\u0026#39;s :245 NA\u0026#39;s :569 NA\u0026#39;s :233 selfBornUS motherBornUS fatherBornUS englishAtHome Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Mean :0.9313 Mean :0.7725 Mean :0.7668 Mean :0.8717 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :69 NA\u0026#39;s :71 NA\u0026#39;s :113 NA\u0026#39;s :71 computerForSchoolwork read30MinsADay minutesPerWeekEnglish Min. :0.0000 Min. :0.0000 Min. : 0.0 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 225.0 Median :1.0000 Median :0.0000 Median : 250.0 Mean :0.8994 Mean :0.2899 Mean : 266.2 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.: 300.0 Max. :1.0000 Max. :1.0000 Max. :2400.0 NA\u0026#39;s :65 NA\u0026#39;s :34 NA\u0026#39;s :186 studentsInEnglish schoolHasLibrary publicSchool urban Min. : 1.0 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:20.0 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:0.0000 Median :25.0 Median :1.0000 Median :1.0000 Median :0.0000 Mean :24.5 Mean :0.9676 Mean :0.9339 Mean :0.3849 3rd Qu.:30.0 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :75.0 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :249 NA\u0026#39;s :143 schoolSize readingScore Min. : 100 Min. :168.6 1st Qu.: 712 1st Qu.:431.7 Median :1212 Median :499.7 Mean :1369 Mean :497.9 3rd Qu.:1900 3rd Qu.:566.2 Max. :6694 Max. :746.0 NA\u0026#39;s :162  Number of students in the training set is 3663\n Problem 1.2 - Summarizing the dataset Using tapply() on pisaTrain, what is the average reading test score of males?\ntapply(pisaTrain$readingScore, pisaTrain$male, mean)  0 1 512.9406 483.5325  Males reading score, 483.5325 and Females reading score is 512.9406\n Problem 1.3 - Locating missing values Which variables are missing data in at least one observation in the training set?\nsummary(pisaTrain)  grade male raceeth Min. : 8.00 Min. :0.0000 White :2015 1st Qu.:10.00 1st Qu.:0.0000 Hispanic : 834 Median :10.00 Median :1.0000 Black : 444 Mean :10.09 Mean :0.5111 Asian : 143 3rd Qu.:10.00 3rd Qu.:1.0000 More than one race: 124 Max. :12.00 Max. :1.0000 (Other) : 68 NA\u0026#39;s : 35 preschool expectBachelors motherHS motherBachelors Min. :0.0000 Min. :0.0000 Min. :0.00 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:1.00 1st Qu.:0.0000 Median :1.0000 Median :1.0000 Median :1.00 Median :0.0000 Mean :0.7228 Mean :0.7859 Mean :0.88 Mean :0.3481 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.00 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00 Max. :1.0000 NA\u0026#39;s :56 NA\u0026#39;s :62 NA\u0026#39;s :97 NA\u0026#39;s :397 motherWork fatherHS fatherBachelors fatherWork Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :0.0000 Median :1.0000 Mean :0.7345 Mean :0.8593 Mean :0.3319 Mean :0.8531 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :93 NA\u0026#39;s :245 NA\u0026#39;s :569 NA\u0026#39;s :233 selfBornUS motherBornUS fatherBornUS englishAtHome Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Mean :0.9313 Mean :0.7725 Mean :0.7668 Mean :0.8717 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :69 NA\u0026#39;s :71 NA\u0026#39;s :113 NA\u0026#39;s :71 computerForSchoolwork read30MinsADay minutesPerWeekEnglish Min. :0.0000 Min. :0.0000 Min. : 0.0 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 225.0 Median :1.0000 Median :0.0000 Median : 250.0 Mean :0.8994 Mean :0.2899 Mean : 266.2 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.: 300.0 Max. :1.0000 Max. :1.0000 Max. :2400.0 NA\u0026#39;s :65 NA\u0026#39;s :34 NA\u0026#39;s :186 studentsInEnglish schoolHasLibrary publicSchool urban Min. : 1.0 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:20.0 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:0.0000 Median :25.0 Median :1.0000 Median :1.0000 Median :0.0000 Mean :24.5 Mean :0.9676 Mean :0.9339 Mean :0.3849 3rd Qu.:30.0 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :75.0 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :249 NA\u0026#39;s :143 schoolSize readingScore Min. : 100 Min. :168.6 1st Qu.: 712 1st Qu.:431.7 Median :1212 Median :499.7 Mean :1369 Mean :497.9 3rd Qu.:1900 3rd Qu.:566.2 Max. :6694 Max. :746.0 NA\u0026#39;s :162  raceeth, preschool, expectBachelors, motherHS, motherBachelors, motherWork, fatherHS, fatherBachelors, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, computerForSchoolWork, read30MinsADay, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, schoolSize   Problem 1.4 - Removing missing values Linear regression discards observations with missing data, so I‚Äôll remove all such observations from the training and testing sets. Later, we‚Äôll learn about imputation, which deals with missing data by filling in missing values with plausible information.\nRemoving observations with missing value from pisaTrain and pisaTest:\npisaTrain = na.omit(pisaTrain) pisaTest = na.omit(pisaTest) How many observations are now in the training set?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ... 2414\nHow many observations are now in the testing set?\nstr(pisaTest) \u0026#39;data.frame\u0026#39;: 990 obs. of 24 variables: $ grade : int 10 10 10 10 11 10 10 10 10 10 ... $ male : int 0 0 0 0 0 1 0 1 1 0 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 7 1 7 7 4 7 4 7 4 ... $ preschool : int 1 1 1 1 0 1 0 1 1 1 ... $ expectBachelors : int 0 1 0 0 0 1 1 0 1 1 ... $ motherHS : int 1 1 1 1 1 1 1 1 1 1 ... $ motherBachelors : int 1 0 0 0 1 1 0 0 1 0 ... $ motherWork : int 1 0 0 1 1 1 0 1 1 1 ... $ fatherHS : int 1 1 1 1 1 1 1 1 1 1 ... $ fatherBachelors : int 0 1 0 0 1 0 0 0 1 1 ... $ fatherWork : int 0 1 0 1 1 1 1 0 1 1 ... $ selfBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ motherBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ fatherBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ englishAtHome : int 1 1 1 1 1 1 1 1 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 1 1 1 1 1 ... $ read30MinsADay : int 0 0 1 1 1 1 0 0 0 1 ... $ minutesPerWeekEnglish: int 240 240 240 270 270 350 350 360 350 360 ... $ studentsInEnglish : int 30 30 30 35 30 25 27 28 25 27 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 1 ... $ urban : int 0 0 0 0 0 0 0 0 0 0 ... $ schoolSize : int 808 808 808 808 808 899 899 899 899 899 ... $ readingScore : num 355 454 405 665 605 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 2 3 4 6 12 16 17 19 22 23 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; ... 990\n Problem 2.1 - Factor variables Factor variables are variables that take on a discrete set of values. This is an unordered factor because there isn‚Äôt any natural ordering between the levels.\nAn ordered factor has a natural ordering between the levels (an example would be the classifications ‚Äúlarge,‚Äù ‚Äúmedium,‚Äù and ‚Äúsmall‚Äù).\nWhich of the following variables is an unordered factor with at least 3 levels?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ... raceeth Which of the following variables is an ordered factor with at least 3 levels? #### grade\n  Problem 2.2 - Unordered factors in regression models To include unordered factors in a linear regression model, we define one level as the ‚Äúreference level‚Äù and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.\nAs an example, consider the unordered factor variable ‚Äúcolor‚Äù, with levels ‚Äúred‚Äù, ‚Äúgreen‚Äù, and ‚Äúblue‚Äù. If ‚Äúgreen‚Äù were the reference level, then we would add binary variables ‚Äúcolored‚Äù and ‚Äúcolorblue‚Äù to a linear regression problem. All red examples would have colored=1 and colorblue=0. All blue examples would have colored=0 and colorblue=1. All green examples would have colored=0 and colorblue=0.\nNow, consider the variable ‚Äúraceeth‚Äù in our problem, which has levels ‚ÄúAmerican Indian/Alaska Native‚Äù, ‚ÄúAsian‚Äù, ‚ÄúBlack‚Äù, ‚ÄúHispanic‚Äù,\n‚ÄúMore than one race‚Äù, ‚ÄúNative Hawaiian/Other Pacific Islander‚Äù, and ‚ÄúWhite‚Äù.\nBecause it‚Äôs the most common in our population, we will select White as the reference level.\nWhich binary variables will be included in the regression model?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ...  raceethAmerican Indian/Alaska Native raceethAsian raceethBlack raceethHispanic raceethMore than one race raceethNative Hawaiian/Other Pacific Islander   Problem 2.3 - Example unordered factors Consider again adding our unordered factor race to the regression model with reference level ‚ÄúWhite‚Äù. For a student who is Asian, which binary variables would be set to 0. All remaining variables will be set to 1. (all except raceethAsian) #### all\n Problem 3.1 - Building a model Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() ‚Äì you can see this when you run str(pisaTrain) or str(pisaTest).\nHowever, by default R selects the first level alphabetically (‚ÄúAmerican Indian/Alaska Native‚Äù) as the reference level of our factor instead of the most common level (‚ÄúWhite‚Äù).\nLet‚Äôs Set the reference level of the factor.\npisaTrain$raceeth = relevel(pisaTrain$raceeth, \u0026quot;White\u0026quot;) pisaTest$raceeth = relevel(pisaTest$raceeth, \u0026quot;White\u0026quot;) Now, building a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables. It would be time-consuming to type all the variables, but R provides the shorthand notation ‚ÄúreadingScore ~ .‚Äù to mean ‚Äúpredict readingScore using all the other variables in the dataframe.‚Äù The period is used to replace listing out all of the independent variables.\nAs an example, if our dependent variable is called ‚ÄúY‚Äù, our independent variables are called ‚ÄúX1‚Äù, ‚ÄúX2‚Äù, and ‚ÄúX3‚Äù, and our training dataset is called ‚ÄúTrain‚Äù, instead of the regular notation: LinReg = lm(Y ~ X1 + X2 + X3, data = Train)\nYou would use the following code to build our model: LinReg = lm(Y ~ ., data = Train)\nlmScore \u0026lt;- lm(readingScore ~ ., data = pisaTrain) What is the Multiple R-squared value of lmScore on the training set?\nsummary(lmScore)  Call: lm(formula = readingScore ~ ., data = pisaTrain) Residuals: Min 1Q Median 3Q Max -247.44 -48.86 1.86 49.77 217.18 Coefficients: Estimate Std. Error (Intercept) 143.766333 33.841226 grade 29.542707 2.937399 male -14.521653 3.155926 raceethAmerican Indian/Alaska Native -67.277327 16.786935 raceethAsian -4.110325 9.220071 raceethBlack -67.012347 5.460883 raceethHispanic -38.975486 5.177743 raceethMore than one race -16.922522 8.496268 raceethNative Hawaiian/Other Pacific Islander -5.101601 17.005696 preschool -4.463670 3.486055 expectBachelors 55.267080 4.293893 motherHS 6.058774 6.091423 motherBachelors 12.638068 3.861457 motherWork -2.809101 3.521827 fatherHS 4.018214 5.579269 fatherBachelors 16.929755 3.995253 fatherWork 5.842798 4.395978 selfBornUS -3.806278 7.323718 motherBornUS -8.798153 6.587621 fatherBornUS 4.306994 6.263875 englishAtHome 8.035685 6.859492 computerForSchoolwork 22.500232 5.702562 read30MinsADay 34.871924 3.408447 minutesPerWeekEnglish 0.012788 0.010712 studentsInEnglish -0.286631 0.227819 schoolHasLibrary 12.215085 9.264884 publicSchool -16.857475 6.725614 urban -0.110132 3.962724 schoolSize 0.006540 0.002197 t value Pr(\u0026gt;|t|) (Intercept) 4.248 2.24e-05 *** grade 10.057 \u0026lt; 2e-16 *** male -4.601 4.42e-06 *** raceethAmerican Indian/Alaska Native -4.008 6.32e-05 *** raceethAsian -0.446 0.65578 raceethBlack -12.271 \u0026lt; 2e-16 *** raceethHispanic -7.528 7.29e-14 *** raceethMore than one race -1.992 0.04651 * raceethNative Hawaiian/Other Pacific Islander -0.300 0.76421 preschool -1.280 0.20052 expectBachelors 12.871 \u0026lt; 2e-16 *** motherHS 0.995 0.32001 motherBachelors 3.273 0.00108 ** motherWork -0.798 0.42517 fatherHS 0.720 0.47147 fatherBachelors 4.237 2.35e-05 *** fatherWork 1.329 0.18393 selfBornUS -0.520 0.60331 motherBornUS -1.336 0.18182 fatherBornUS 0.688 0.49178 englishAtHome 1.171 0.24153 computerForSchoolwork 3.946 8.19e-05 *** read30MinsADay 10.231 \u0026lt; 2e-16 *** minutesPerWeekEnglish 1.194 0.23264 studentsInEnglish -1.258 0.20846 schoolHasLibrary 1.318 0.18749 publicSchool -2.506 0.01226 * urban -0.028 0.97783 schoolSize 2.977 0.00294 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 73.81 on 2385 degrees of freedom Multiple R-squared: 0.3251, Adjusted R-squared: 0.3172 F-statistic: 41.04 on 28 and 2385 DF, p-value: \u0026lt; 2.2e-16 0.3251\nNote, that this R-squared is lower than the ones prevously observed. This does not necessarily imply that the model is of poor quality. More often than not, it simply means that the prediction problem at hand (predicting a student‚Äôs test score based on demographic and school-related variables) is more difficult than other prediction problems (like predicting a team‚Äôs number of wins from their runs scored and allowed, or predicting the quality of wine from weather conditions).\n Problem 3.2 - Computing the root-mean squared error of the model What is the training-set root mean squared error (RMSE) of lmScore?\nlmScoreSSE \u0026lt;- sum(lmScore$residuals^2) lmScoreSSE [1] 12993365 sqrt(lmScoreSSE/nrow(pisaTrain)) [1] 73.36555  Problem 3.3 - Comparing predictions for similar students Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9.\nWhat is the predicted reading score of student A minus the predicted reading score of student B?\npisaPred \u0026lt;- pisaTest[1,] pisaPred \u0026lt;- rbind(pisaPred, pisaTest[1,]) pisaPred[1,1] \u0026lt;- 11 ## grade 11 for student A pisaPred[2,1] \u0026lt;- 9 ## grade 9 for student B pisaPred  grade male raceeth preschool expectBachelors motherHS motherBachelors 1 11 0 White 1 0 1 1 2 9 0 White 1 0 1 1 motherWork fatherHS fatherBachelors fatherWork selfBornUS motherBornUS 1 1 1 0 0 1 1 2 1 1 0 0 1 1 fatherBornUS englishAtHome computerForSchoolwork read30MinsADay 1 1 1 1 0 2 1 1 1 0 minutesPerWeekEnglish studentsInEnglish schoolHasLibrary publicSchool 1 240 30 1 1 2 240 30 1 1 urban schoolSize readingScore 1 0 808 355.24 2 0 808 355.24 predictedScores \u0026lt;- predict(lmScore, pisaPred) predictedScores  1 2 501.5294 442.4440  predictedScores[1] - predictedScores[2]  1 59.08541  59.08541 ~ 59.09\n Problem 3.4 - Interpreting model coefficients What is the meaning of the coefficient associated with variable raceethAsian?\nsummary(lmScore)  Call: lm(formula = readingScore ~ ., data = pisaTrain) Residuals: Min 1Q Median 3Q Max -247.44 -48.86 1.86 49.77 217.18 Coefficients: Estimate Std. Error (Intercept) 143.766333 33.841226 grade 29.542707 2.937399 male -14.521653 3.155926 raceethAmerican Indian/Alaska Native -67.277327 16.786935 raceethAsian -4.110325 9.220071 raceethBlack -67.012347 5.460883 raceethHispanic -38.975486 5.177743 raceethMore than one race -16.922522 8.496268 raceethNative Hawaiian/Other Pacific Islander -5.101601 17.005696 preschool -4.463670 3.486055 expectBachelors 55.267080 4.293893 motherHS 6.058774 6.091423 motherBachelors 12.638068 3.861457 motherWork -2.809101 3.521827 fatherHS 4.018214 5.579269 fatherBachelors 16.929755 3.995253 fatherWork 5.842798 4.395978 selfBornUS -3.806278 7.323718 motherBornUS -8.798153 6.587621 fatherBornUS 4.306994 6.263875 englishAtHome 8.035685 6.859492 computerForSchoolwork 22.500232 5.702562 read30MinsADay 34.871924 3.408447 minutesPerWeekEnglish 0.012788 0.010712 studentsInEnglish -0.286631 0.227819 schoolHasLibrary 12.215085 9.264884 publicSchool -16.857475 6.725614 urban -0.110132 3.962724 schoolSize 0.006540 0.002197 t value Pr(\u0026gt;|t|) (Intercept) 4.248 2.24e-05 *** grade 10.057 \u0026lt; 2e-16 *** male -4.601 4.42e-06 *** raceethAmerican Indian/Alaska Native -4.008 6.32e-05 *** raceethAsian -0.446 0.65578 raceethBlack -12.271 \u0026lt; 2e-16 *** raceethHispanic -7.528 7.29e-14 *** raceethMore than one race -1.992 0.04651 * raceethNative Hawaiian/Other Pacific Islander -0.300 0.76421 preschool -1.280 0.20052 expectBachelors 12.871 \u0026lt; 2e-16 *** motherHS 0.995 0.32001 motherBachelors 3.273 0.00108 ** motherWork -0.798 0.42517 fatherHS 0.720 0.47147 fatherBachelors 4.237 2.35e-05 *** fatherWork 1.329 0.18393 selfBornUS -0.520 0.60331 motherBornUS -1.336 0.18182 fatherBornUS 0.688 0.49178 englishAtHome 1.171 0.24153 computerForSchoolwork 3.946 8.19e-05 *** read30MinsADay 10.231 \u0026lt; 2e-16 *** minutesPerWeekEnglish 1.194 0.23264 studentsInEnglish -1.258 0.20846 schoolHasLibrary 1.318 0.18749 publicSchool -2.506 0.01226 * urban -0.028 0.97783 schoolSize 2.977 0.00294 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 73.81 on 2385 degrees of freedom Multiple R-squared: 0.3251, Adjusted R-squared: 0.3172 F-statistic: 41.04 on 28 and 2385 DF, p-value: \u0026lt; 2.2e-16 Predicted difference in the reading score between an Asian student and a white student who is otherwise identical.   Problem 3.5 - Identifying variables lacking statistical significance Based on the significance codes, which variables are candidates for removal from the model? (We‚Äôll assume that the factor variable raceeth should only be removed if none of its levels are significant.)\n preschool, motherHS, motherWork, fatherHS, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, urban   Problem 4.1 - Predicting on unseen data Using the ‚Äúpredict‚Äù function and supplying the ‚Äúnewdata‚Äù argument, use the lmScore model to predict the reading scores of students in pisaTest. Call this vector of predictions ‚ÄúpredTest‚Äù. Do not change the variables in the model (for example, do not remove variables that we found were not significant in the previous part of this problem). Use the summary function to describe the test-set predictions.\nWhat is the range between the max and min predicted reading score on the test-set?\npredTest \u0026lt;- predict(lmScore, newdata = pisaTest) summary(predTest)  Min. 1st Qu. Median Mean 3rd Qu. Max. 353.2 482.0 524.0 516.7 555.7 637.7  637.7 - 353.2\n Problem 4.2 - Test set SSE and RMSE What is the sum of squared errors (SSE) of lmScore on the testing set?\ntest_set_SSE = sum((predTest - pisaTest$readingScore)^2) test_set_SSE [1] 5762082 What is the root mean squared error (RMSE) of lmScore on the testing set?\ntest_set_RMSE = sqrt(test_set_SSE/nrow(pisaTest)) test_set_RMSE [1] 76.29079  Problem 4.3 - Baseline prediction and test-set SSE What is the predicted test score used in the baseline model?\nmean(pisaTrain$readingScore) [1] 517.9629 What is the sum of squared errors of the baseline model on the testing set? HINT: We call the sum of squared errors for the baseline model the total sum of squares (SST).\ntest_set_SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2) test_set_SST [1] 7802354  Problem 4.4 - Test-set R-squared What is the test-set R-squared value of lmScore?\n1 - test_set_SSE/test_set_SST [1] 0.2614944  ","date":1554508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554508800,"objectID":"ee363738541f1a895e2e849df95cac70","permalink":"/project/pisa2009/pisa/","publishdate":"2019-04-06T00:00:00Z","relpermalink":"/project/pisa2009/pisa/","section":"project","summary":"Predict reading scores of students in the 2009 PISA exam","tags":["R","Data Analytics","Machine Learning"],"title":"Reading Test Scores","type":"project"},{"authors":null,"categories":null,"content":" There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.\nIn this analysis, I‚Äôll attempt to study the relationship between average global temperature and several other factors.\nThe file climate_change.csv contains climate data from May 1983 to December 2008. The available variables include:\n Year: the observation year. Month: the observation month. Temp: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia. CO2, N2O, CH4, CFC.11, CFC.12: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.  CO2, N2O and CH4 are expressed in ppmv (parts per million by volume ‚Äì i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere) CFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume).  Aerosols: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun‚Äôs energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA. TSI: the total solar irradiance (TSI) in W/m2 (the rate at which the sun‚Äôs energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website. MEI: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.  We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv.\nclimate \u0026lt;- read.csv(\u0026quot;climate_change.csv\u0026quot;) str(climate) \u0026#39;data.frame\u0026#39;: 308 obs. of 11 variables: $ Year : int 1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ... $ Month : int 5 6 7 8 9 10 11 12 1 2 ... $ MEI : num 2.556 2.167 1.741 1.13 0.428 ... $ CO2 : num 346 346 344 342 340 ... $ CH4 : num 1639 1634 1633 1631 1648 ... $ N2O : num 304 304 304 304 304 ... $ CFC.11 : num 191 192 193 194 194 ... $ CFC.12 : num 350 352 354 356 357 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ... $ Temp : num 0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ... summary(climate)  Year Month MEI CO2 Min. :1983 Min. : 1.000 Min. :-1.6350 Min. :340.2 1st Qu.:1989 1st Qu.: 4.000 1st Qu.:-0.3987 1st Qu.:353.0 Median :1996 Median : 7.000 Median : 0.2375 Median :361.7 Mean :1996 Mean : 6.552 Mean : 0.2756 Mean :363.2 3rd Qu.:2002 3rd Qu.:10.000 3rd Qu.: 0.8305 3rd Qu.:373.5 Max. :2008 Max. :12.000 Max. : 3.0010 Max. :388.5 CH4 N2O CFC.11 CFC.12 Min. :1630 Min. :303.7 Min. :191.3 Min. :350.1 1st Qu.:1722 1st Qu.:308.1 1st Qu.:246.3 1st Qu.:472.4 Median :1764 Median :311.5 Median :258.3 Median :528.4 Mean :1750 Mean :312.4 Mean :252.0 Mean :497.5 3rd Qu.:1787 3rd Qu.:317.0 3rd Qu.:267.0 3rd Qu.:540.5 Max. :1814 Max. :322.2 Max. :271.5 Max. :543.8 TSI Aerosols Temp Min. :1365 Min. :0.00160 Min. :-0.2820 1st Qu.:1366 1st Qu.:0.00280 1st Qu.: 0.1217 Median :1366 Median :0.00575 Median : 0.2480 Mean :1366 Mean :0.01666 Mean : 0.2568 3rd Qu.:1366 3rd Qu.:0.01260 3rd Qu.: 0.4073 Max. :1367 Max. :0.14940 Max. : 0.7390  ML Workflow Then, split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.\nclimate_train \u0026lt;- subset(climate, Year \u0026lt;= 2006) climate_test \u0026lt;- subset(climate, Year \u0026gt; 2006) str(climate_train) \u0026#39;data.frame\u0026#39;: 284 obs. of 11 variables: $ Year : int 1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ... $ Month : int 5 6 7 8 9 10 11 12 1 2 ... $ MEI : num 2.556 2.167 1.741 1.13 0.428 ... $ CO2 : num 346 346 344 342 340 ... $ CH4 : num 1639 1634 1633 1631 1648 ... $ N2O : num 304 304 304 304 304 ... $ CFC.11 : num 191 192 193 194 194 ... $ CFC.12 : num 350 352 354 356 357 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ... $ Temp : num 0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ... summary(climate_train)  Year Month MEI CO2 Min. :1983 Min. : 1.000 Min. :-1.5860 Min. :340.2 1st Qu.:1989 1st Qu.: 4.000 1st Qu.:-0.3230 1st Qu.:352.3 Median :1995 Median : 7.000 Median : 0.3085 Median :359.9 Mean :1995 Mean : 6.556 Mean : 0.3419 Mean :361.4 3rd Qu.:2001 3rd Qu.:10.000 3rd Qu.: 0.8980 3rd Qu.:370.6 Max. :2006 Max. :12.000 Max. : 3.0010 Max. :385.0 CH4 N2O CFC.11 CFC.12 Min. :1630 Min. :303.7 Min. :191.3 Min. :350.1 1st Qu.:1716 1st Qu.:307.7 1st Qu.:249.6 1st Qu.:462.5 Median :1759 Median :310.8 Median :260.4 Median :522.1 Mean :1746 Mean :311.7 Mean :252.5 Mean :494.2 3rd Qu.:1782 3rd Qu.:316.1 3rd Qu.:267.4 3rd Qu.:541.0 Max. :1808 Max. :320.5 Max. :271.5 Max. :543.8 TSI Aerosols Temp Min. :1365 Min. :0.00160 Min. :-0.2820 1st Qu.:1366 1st Qu.:0.00270 1st Qu.: 0.1180 Median :1366 Median :0.00620 Median : 0.2325 Mean :1366 Mean :0.01772 Mean : 0.2478 3rd Qu.:1366 3rd Qu.:0.01400 3rd Qu.: 0.4065 Max. :1367 Max. :0.14940 Max. : 0.7390  str(climate_test) \u0026#39;data.frame\u0026#39;: 24 obs. of 11 variables: $ Year : int 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... $ Month : int 1 2 3 4 5 6 7 8 9 10 ... $ MEI : num 0.974 0.51 0.074 -0.049 0.183 ... $ CO2 : num 383 384 385 386 387 ... $ CH4 : num 1800 1803 1803 1802 1796 ... $ N2O : num 321 321 321 321 320 ... $ CFC.11 : num 248 248 248 248 247 ... $ CFC.12 : num 539 539 539 539 538 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0054 0.0051 0.0045 0.0045 0.0041 0.004 0.004 0.0041 0.0042 0.0041 ... $ Temp : num 0.601 0.498 0.435 0.466 0.372 0.382 0.394 0.358 0.402 0.362 ... summary(climate_test)  Year Month MEI CO2 Min. :2007 Min. : 1.00 Min. :-1.6350 Min. :380.9 1st Qu.:2007 1st Qu.: 3.75 1st Qu.:-1.0437 1st Qu.:383.1 Median :2008 Median : 6.50 Median :-0.5305 Median :384.5 Mean :2008 Mean : 6.50 Mean :-0.5098 Mean :384.7 3rd Qu.:2008 3rd Qu.: 9.25 3rd Qu.:-0.0360 3rd Qu.:386.1 Max. :2008 Max. :12.00 Max. : 0.9740 Max. :388.5 CH4 N2O CFC.11 CFC.12 Min. :1772 Min. :320.3 Min. :244.1 Min. :534.9 1st Qu.:1792 1st Qu.:320.6 1st Qu.:244.6 1st Qu.:535.1 Median :1798 Median :321.3 Median :246.2 Median :537.0 Mean :1797 Mean :321.1 Mean :245.9 Mean :536.7 3rd Qu.:1804 3rd Qu.:321.4 3rd Qu.:246.6 3rd Qu.:537.4 Max. :1814 Max. :322.2 Max. :248.4 Max. :539.2 TSI Aerosols Temp Min. :1366 Min. :0.003100 Min. :0.074 1st Qu.:1366 1st Qu.:0.003600 1st Qu.:0.307 Median :1366 Median :0.004100 Median :0.380 Mean :1366 Mean :0.004071 Mean :0.363 3rd Qu.:1366 3rd Qu.:0.004500 3rd Qu.:0.414 Max. :1366 Max. :0.005400 Max. :0.601  Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.\nfit.climate \u0026lt;- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) summary(fit.climate)  Call: lm(formula = Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.25888 -0.05913 -0.00082 0.05649 0.32433 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.246e+02 1.989e+01 -6.265 1.43e-09 *** MEI 6.421e-02 6.470e-03 9.923 \u0026lt; 2e-16 *** CO2 6.457e-03 2.285e-03 2.826 0.00505 ** CH4 1.240e-04 5.158e-04 0.240 0.81015 N2O -1.653e-02 8.565e-03 -1.930 0.05467 . CFC.11 -6.631e-03 1.626e-03 -4.078 5.96e-05 *** CFC.12 3.808e-03 1.014e-03 3.757 0.00021 *** TSI 9.314e-02 1.475e-02 6.313 1.10e-09 *** Aerosols -1.538e+00 2.133e-01 -7.210 5.41e-12 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09171 on 275 degrees of freedom Multiple R-squared: 0.7509, Adjusted R-squared: 0.7436 F-statistic: 103.6 on 8 and 275 DF, p-value: \u0026lt; 2.2e-16 The model R2 (the ‚ÄúMultiple R-squared‚Äù value) is 0.7509\n Creating Our First Model Which variables are significant in the model? We will consider a variable signficant only if the p-value is below 0.05. #### MEI, CO2, CFC.11, CFC.12, TSI, Aerosols\n Understanding the Model Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are negative, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.\nWhich of the following is the simplest correct explanation for this contradiction? #### All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the dataset.\nCompute the correlations between all the variables in the training set.\ncor(climate_train)  Year Month MEI CO2 CH4 Year 1.00000000 -0.0279419602 -0.0369876842 0.98274939 0.91565945 Month -0.02794196 1.0000000000 0.0008846905 -0.10673246 0.01856866 MEI -0.03698768 0.0008846905 1.0000000000 -0.04114717 -0.03341930 CO2 0.98274939 -0.1067324607 -0.0411471651 1.00000000 0.87727963 CH4 0.91565945 0.0185686624 -0.0334193014 0.87727963 1.00000000 N2O 0.99384523 0.0136315303 -0.0508197755 0.97671982 0.89983864 CFC.11 0.56910643 -0.0131112236 0.0690004387 0.51405975 0.77990402 CFC.12 0.89701166 0.0006751102 0.0082855443 0.85268963 0.96361625 TSI 0.17030201 -0.0346061935 -0.1544919227 0.17742893 0.24552844 Aerosols -0.34524670 0.0148895406 0.3402377871 -0.35615480 -0.26780919 Temp 0.78679714 -0.0998567411 0.1724707512 0.78852921 0.70325502 N2O CFC.11 CFC.12 TSI Aerosols Year 0.99384523 0.56910643 0.8970116635 0.17030201 -0.34524670 Month 0.01363153 -0.01311122 0.0006751102 -0.03460619 0.01488954 MEI -0.05081978 0.06900044 0.0082855443 -0.15449192 0.34023779 CO2 0.97671982 0.51405975 0.8526896272 0.17742893 -0.35615480 CH4 0.89983864 0.77990402 0.9636162478 0.24552844 -0.26780919 N2O 1.00000000 0.52247732 0.8679307757 0.19975668 -0.33705457 CFC.11 0.52247732 1.00000000 0.8689851828 0.27204596 -0.04392120 CFC.12 0.86793078 0.86898518 1.0000000000 0.25530281 -0.22513124 TSI 0.19975668 0.27204596 0.2553028138 1.00000000 0.05211651 Aerosols -0.33705457 -0.04392120 -0.2251312440 0.05211651 1.00000000 Temp 0.77863893 0.40771029 0.6875575483 0.24338269 -0.38491375 Temp Year 0.78679714 Month -0.09985674 MEI 0.17247075 CO2 0.78852921 CH4 0.70325502 N2O 0.77863893 CFC.11 0.40771029 CFC.12 0.68755755 TSI 0.24338269 Aerosols -0.38491375 Temp 1.00000000 The following independent variables is N2O, highly correlated with (absolute correlation greater than 0.7)? #### CO2, CH4, CFC.12\nThe following independent variables is CFC.11, highly correlated with? #### CH4, CFC.12\n Simplifying the Model Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Note, using the training set to build the model.\nfit.climate.2 \u0026lt;- lm(Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train) summary(fit.climate.2)  Call: lm(formula = Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.27916 -0.05975 -0.00595 0.05672 0.34195 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.162e+02 2.022e+01 -5.747 2.37e-08 *** MEI 6.419e-02 6.652e-03 9.649 \u0026lt; 2e-16 *** N2O 2.532e-02 1.311e-03 19.307 \u0026lt; 2e-16 *** TSI 7.949e-02 1.487e-02 5.344 1.89e-07 *** Aerosols -1.702e+00 2.180e-01 -7.806 1.19e-13 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09547 on 279 degrees of freedom Multiple R-squared: 0.7261, Adjusted R-squared: 0.7222 F-statistic: 184.9 on 4 and 279 DF, p-value: \u0026lt; 2.2e-16 The coefficient of N2O in this reduced model is 2.532e-02\n(How does this compare to the coefficient in the previous model with all of the variables?) The model R2 is 0.7261\n Automatically Building the Model We have many variables in this analysis, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2.\nThis trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.\nThe step function has one argument - the name of the initial model. It returns a simplified model. Using the step function in R to derive a new model, with the full model as the initial model (HINT: If your initial full model was called ‚ÄúclimateLM‚Äù, you could create a new model with the step function by typing step(climateLM). Be sure to save your new model to a variable name so that you can look at the summary. For more information about the step function, type? step in your R console.)\nfit.climate.step \u0026lt;- step(fit.climate) Start: AIC=-1348.16 Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols Df Sum of Sq RSS AIC - CH4 1 0.00049 2.3135 -1350.1 \u0026lt;none\u0026gt; 2.3130 -1348.2 - N2O 1 0.03132 2.3443 -1346.3 - CO2 1 0.06719 2.3802 -1342.0 - CFC.12 1 0.11874 2.4318 -1335.9 - CFC.11 1 0.13986 2.4529 -1333.5 - TSI 1 0.33516 2.6482 -1311.7 - Aerosols 1 0.43727 2.7503 -1301.0 - MEI 1 0.82823 3.1412 -1263.2 Step: AIC=-1350.1 Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols Df Sum of Sq RSS AIC \u0026lt;none\u0026gt; 2.3135 -1350.1 - N2O 1 0.03133 2.3448 -1348.3 - CO2 1 0.06672 2.3802 -1344.0 - CFC.12 1 0.13023 2.4437 -1336.5 - CFC.11 1 0.13938 2.4529 -1335.5 - TSI 1 0.33500 2.6485 -1313.7 - Aerosols 1 0.43987 2.7534 -1302.7 - MEI 1 0.83118 3.1447 -1264.9 summary(fit.climate.step)  Call: lm(formula = Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.25770 -0.05994 -0.00104 0.05588 0.32203 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.245e+02 1.985e+01 -6.273 1.37e-09 *** MEI 6.407e-02 6.434e-03 9.958 \u0026lt; 2e-16 *** CO2 6.402e-03 2.269e-03 2.821 0.005129 ** N2O -1.602e-02 8.287e-03 -1.933 0.054234 . CFC.11 -6.609e-03 1.621e-03 -4.078 5.95e-05 *** CFC.12 3.868e-03 9.812e-04 3.942 0.000103 *** TSI 9.312e-02 1.473e-02 6.322 1.04e-09 *** Aerosols -1.540e+00 2.126e-01 -7.244 4.36e-12 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09155 on 276 degrees of freedom Multiple R-squared: 0.7508, Adjusted R-squared: 0.7445 F-statistic: 118.8 on 7 and 276 DF, p-value: \u0026lt; 2.2e-16 R2 value of the model produced by the step function is 0.7508\nWhich of the following variable(s) were eliminated from the full model by the step function? #### It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).\n Testing on Unseen Data We have developed an understanding of how well we can fit a linear regression to the training data, but does the model quality hold when applied to unseen data?\nUsing the model produced from the step function, calculate temperature predictions for the testing dataset, using the predict function.\nTempPredictions \u0026lt;- predict(fit.climate.step, newdata = climate_test) climate.SSE = sum((TempPredictions - climate_test$Temp)^2) climate.SST = sum((climate_test$Temp - mean(climate_train$Temp))^2) 1 - climate.SSE/climate.SST [1] 0.6286051 Testing set R2 is 0.6286\n ","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"8a0aae4ba257ba1a8eaa06545f983374","permalink":"/project/climate_change/climate_change/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/project/climate_change/climate_change/","section":"project","summary":"Study the relationship between average global temperature \u0026 several other factors","tags":["R","Data Analytics"],"title":"Climate Change","type":"project"},{"authors":null,"categories":["R"],"content":" Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.  Originally Written by Mar√≠a Carina Rold√°n, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014.  Taken from the link Reading in a csv file sales_data \u0026lt;- read.csv(\u0026#39;sales_data_sample.csv\u0026#39;) sales_data is a ‚Äòdata.frame‚Äô. It is the main way that R deals with tables of data. Click on the arrow next to sales_data in the Environment pane to see the data types of each column Click on sales_data in the Environment pane to see the table. You can also type View(sales_data) to do this.\nRun summary to see a summary of the data.\nsummary(sales_data)  ORDERNUMBER QUANTITYORDERED PRICEEACH ORDERLINENUMBER Min. :10100 Min. : 6.00 Min. : 26.88 Min. : 1.000 1st Qu.:10180 1st Qu.:27.00 1st Qu.: 68.86 1st Qu.: 3.000 Median :10262 Median :35.00 Median : 95.70 Median : 6.000 Mean :10259 Mean :35.09 Mean : 83.66 Mean : 6.466 3rd Qu.:10334 3rd Qu.:43.00 3rd Qu.:100.00 3rd Qu.: 9.000 Max. :10425 Max. :97.00 Max. :100.00 Max. :18.000 SALES ORDERDATE STATUS Min. : 482.1 11/14/2003 0:00: 38 Cancelled : 60 1st Qu.: 2203.4 11/24/2004 0:00: 35 Disputed : 14 Median : 3184.8 11/12/2003 0:00: 34 In Process: 41 Mean : 3553.9 11/17/2004 0:00: 32 On Hold : 44 3rd Qu.: 4508.0 11/4/2004 0:00 : 29 Resolved : 47 Max. :14082.8 10/16/2004 0:00: 28 Shipped :2617 (Other) :2627 QTR_ID MONTH_ID YEAR_ID PRODUCTLINE Min. :1.000 Min. : 1.000 Min. :2003 Classic Cars :967 1st Qu.:2.000 1st Qu.: 4.000 1st Qu.:2003 Motorcycles :331 Median :3.000 Median : 8.000 Median :2004 Planes :306 Mean :2.718 Mean : 7.092 Mean :2004 Ships :234 3rd Qu.:4.000 3rd Qu.:11.000 3rd Qu.:2004 Trains : 77 Max. :4.000 Max. :12.000 Max. :2005 Trucks and Buses:301 Vintage Cars :607 MSRP PRODUCTCODE CUSTOMERNAME Min. : 33.0 S18_3232: 52 Euro Shopping Channel : 259 1st Qu.: 68.0 S10_1949: 28 Mini Gifts Distributors Ltd.: 180 Median : 99.0 S10_4962: 28 Australian Collectors, Co. : 55 Mean :100.7 S12_1666: 28 La Rochelle Gifts : 53 3rd Qu.:124.0 S18_1097: 28 AV Stores, Co. : 51 Max. :214.0 S18_2432: 28 Land of Toys Inc. : 49 (Other) :2631 (Other) :2176 PHONE ADDRESSLINE1 (91) 555 94 44: 259 C/ Moralzarzal, 86 : 259 4155551450 : 180 5677 Strong St. : 180 03 9520 4555 : 55 636 St Kilda Road : 55 40.67.8555 : 53 67, rue des Cinquante Otages: 53 (171) 555-1555: 51 Fauntleroy Circus : 51 6175558555 : 51 897 Long Airport Avenue : 49 (Other) :2174 (Other) :2176 ADDRESSLINE2 CITY STATE POSTALCODE :2521 Madrid : 304 :1486 28034 : 259 Level 3 : 55 San Rafael : 180 CA : 416 97562 : 205 Suite 400: 48 NYC : 152 MA : 190 10022 : 152 Level 15 : 46 Singapore : 79 NY : 178 94217 : 89 Level 6 : 46 Paris : 70 NSW : 92 : 76 2nd Floor: 36 San Francisco: 62 Victoria: 78 50553 : 61 (Other) : 71 (Other) :1976 (Other) : 383 (Other):1981 COUNTRY TERRITORY CONTACTLASTNAME CONTACTFIRSTNAME USA :1004 APAC : 221 Freyre : 259 Diego : 259 Spain : 342 EMEA :1407 Nelson : 204 Valarie: 257 France : 314 Japan: 121 Young : 115 Julie : 117 Australia: 185 NA\u0026#39;s :1074 Frick : 91 Michael: 84 UK : 144 Brown : 88 Sue : 84 Italy : 113 Yu : 80 Juri : 60 (Other) : 721 (Other):1986 (Other):1962 DEALSIZE Large : 157 Medium:1384 Small :1282  Or can run summary on individual columns.\nsummary(sales_data$PRICEEACH)  Min. 1st Qu. Median Mean 3rd Qu. Max. 26.88 68.86 95.70 83.66 100.00 100.00  summary(sales_data$PRODUCTLINE)  Classic Cars Motorcycles Planes Ships 967 331 306 234 Trains Trucks and Buses Vintage Cars 77 301 607  max(sales_data$SALES) [1] 14082.8 mean(sales_data$SALES) [1] 3553.889 min(sales_data$SALES) [1] 482.13 sd(sales_data$SALES) [1] 1841.865 We will now go through select(), arrange(), filter(), mutate(), group_by(), summarise()\nselect() function select specific columns first argument is always the dataset, and each argument after is the fields you want  select(sales_data, QUANTITYORDERED, PRICEEACH)  QUANTITYORDERED PRICEEACH 1 30 95.70 2 34 81.35 3 41 94.74 4 45 83.26 5 49 100.00 6 36 96.66 7 29 86.13 8 48 100.00 9 22 98.57 10 41 100.00 11 37 100.00 12 23 100.00 13 28 100.00 14 34 100.00 15 45 92.83 16 36 100.00 17 23 100.00 18 41 100.00 19 46 94.74 20 42 100.00 21 41 100.00 22 20 72.55 23 21 34.91 24 42 76.36 25 24 100.00 26 66 100.00 27 26 100.00 28 29 100.00 29 38 100.00 30 37 100.00 31 45 100.00 32 21 100.00 33 34 100.00 34 23 100.00 35 42 100.00 36 47 100.00 37 35 100.00 38 29 100.00 39 34 100.00 40 32 100.00 41 21 100.00 42 34 100.00 43 37 100.00 44 47 100.00 45 48 100.00 46 40 100.00 47 26 100.00 48 30 100.00 49 32 100.00 50 41 100.00 51 36 100.00 52 24 100.00 53 23 100.00 54 50 100.00 55 39 99.91 56 29 96.34 57 27 100.00 58 37 100.00 59 37 100.00 60 27 100.00 61 42 100.00 62 38 96.34 63 24 100.00 64 23 100.00 65 47 100.00 66 22 100.00 67 44 100.00 68 40 100.00 69 22 100.00 70 47 100.00 71 39 96.34 72 34 100.00 73 45 100.00 74 20 100.00 75 40 68.92 76 26 51.15 77 39 100.00 78 50 44.51 79 45 100.00 80 45 100.00 81 27 100.00 82 46 100.00 83 31 100.00 84 33 100.00 85 22 100.00 86 20 100.00 87 41 100.00 88 45 100.00 89 49 100.00 90 34 100.00 91 49 100.00 92 39 100.00 93 43 100.00 94 41 100.00 95 36 100.00 96 27 100.00 97 29 100.00 98 20 100.00 99 37 100.00 100 26 100.00 101 39 76.67 102 22 100.00 103 22 100.00 104 21 86.77 105 66 100.00 106 56 100.00 107 50 100.00 108 46 100.00 109 33 100.00 110 49 100.00 111 32 100.00 112 44 100.00 113 24 100.00 114 26 100.00 115 45 100.00 116 39 100.00 117 49 100.00 118 20 100.00 119 27 100.00 120 30 100.00 121 25 100.00 122 24 100.00 123 22 100.00 124 33 100.00 125 47 64.93 126 25 48.05 127 26 75.47 128 48 54.68 129 39 100.00 130 34 100.00 131 32 100.00 132 64 100.00 133 19 100.00 134 42 100.00 135 31 100.00 136 22 100.00 137 26 100.00 138 20 100.00 139 21 100.00 140 33 100.00 141 28 100.00 142 26 100.00 143 31 100.00 144 48 100.00 145 50 100.00 146 28 100.00 147 26 100.00 148 32 100.00 149 44 100.00 150 30 100.00 151 38 100.00 152 40 100.00 153 46 61.99 154 26 100.00 155 27 100.00 156 43 100.00 157 35 65.63 158 37 100.00 159 37 46.90 160 27 100.00 161 38 100.00 162 33 100.00 163 42 100.00 164 42 100.00 165 48 100.00 166 41 100.00 167 30 100.00 168 27 100.00 169 21 100.00 170 20 100.00 171 41 100.00 172 27 100.00 173 28 100.00 174 24 100.00 175 44 100.00 176 50 100.00 177 21 100.00 178 33 100.00 179 33 100.00 180 31 100.00 181 41 71.47 182 45 79.65 183 33 85.39 184 45 76.00 185 26 99.04 186 12 100.00 187 41 100.00 188 33 100.00 189 46 100.00 190 33 100.00 191 20 100.00 192 44 100.00 193 33 100.00 194 21 100.00 195 47 100.00 196 46 100.00 197 32 100.00 198 42 100.00 199 44 100.00 200 35 100.00 201 41 100.00 202 46 100.00 203 31 100.00 204 38 100.00 205 42 64.00 206 33 57.22 207 48 52.36 208 42 100.00 209 32 100.00 210 34 100.00 211 33 69.12 212 36 100.00 213 27 100.00 214 21 100.00 215 21 100.00 216 38 100.00 217 30 100.00 218 49 100.00 219 43 100.00 220 41 100.00 221 38 100.00 222 28 100.00 223 43 100.00 224 25 100.00 225 38 100.00 226 41 100.00 227 28 100.00 228 25 100.00 229 41 100.00 230 39 100.00 231 21 100.00 232 27 100.00 233 33 99.21 234 29 100.00 235 49 100.00 236 49 100.00 237 20 100.00 238 39 63.20 239 40 100.00 240 49 100.00 241 21 100.00 242 50 100.00 243 20 100.00 244 49 100.00 245 38 100.00 246 35 100.00 247 40 100.00 248 28 100.00 249 25 100.00 250 36 100.00 251 43 100.00 252 32 100.00 253 46 100.00 254 48 100.00 255 43 100.00 256 49 100.00 257 24 100.00 258 26 100.00 259 30 100.00 260 24 100.00 261 55 100.00 262 22 100.00 263 49 78.92 264 44 100.00 265 66 100.00 266 21 100.00 267 34 100.00 268 43 100.00 269 46 100.00 270 33 100.00 271 42 100.00 272 34 100.00 273 47 100.00 274 33 100.00 275 24 100.00 276 26 100.00 277 30 100.00 278 43 100.00 279 25 100.00 280 27 100.00 281 27 100.00 282 24 100.00 283 34 100.00 284 46 100.00 285 27 54.33 286 33 100.00 287 47 100.00 288 49 55.34 289 40 100.00 290 37 100.00 291 47 100.00 292 45 100.00 293 37 99.82 294 48 100.00 295 31 100.00 296 46 100.00 297 47 100.00 298 28 100.00 299 40 100.00 300 20 100.00 301 39 100.00 302 25 99.82 303 29 100.00 304 22 100.00 305 22 100.00 306 47 100.00 307 45 100.00 308 29 100.00 309 24 100.00 310 35 100.00 311 46 83.63 312 44 95.93 313 34 96.73 314 35 100.00 315 25 72.38 316 10 100.00 317 29 100.00 318 39 100.00 319 42 100.00 320 46 100.00 321 49 100.00 322 27 100.00 323 50 100.00 324 43 100.00 325 38 100.00 326 20 100.00 327 27 100.00 328 49 100.00 329 27 100.00 330 39 100.00 331 24 100.00 332 45 100.00 333 20 100.00 334 36 100.00 335 24 100.00 336 49 63.38 337 26 100.00 338 49 62.09 339 34 100.00 340 34 95.35 341 33 100.00 342 22 100.00 343 39 89.38 344 32 63.84 345 24 75.01 346 21 63.84 347 24 73.42 348 36 63.84 349 20 81.40 350 30 64.64 351 44 82.99 352 28 92.57 353 37 77.41 354 20 74.21 355 25 90.17 356 35 76.61 357 38 83.79 358 41 69.43 359 22 76.61 360 49 81.40 361 38 73.42 362 33 100.00 363 36 93.56 364 34 81.62 365 24 67.83 366 36 70.26 367 34 90.17 368 41 100.00 369 46 100.00 370 24 100.00 371 21 100.00 372 24 100.00 373 48 100.00 374 26 100.00 375 37 100.00 376 49 100.00 377 34 99.54 378 48 100.00 379 36 100.00 380 46 100.00 381 46 100.00 382 31 97.17 383 41 100.00 384 21 100.00 385 38 100.00 386 45 100.00 387 26 58.38 388 38 100.00 389 48 100.00 390 42 64.16 391 49 35.71 392 32 66.58 393 54 100.00 394 33 100.00 395 36 100.00 396 20 100.00 397 29 97.89 398 33 97.89 399 50 100.00 400 41 100.00 401 36 100.00 402 27 100.00 403 47 100.00 404 33 100.00 405 21 100.00 406 21 93.28 407 41 100.00 408 40 100.00 409 28 100.00 410 23 100.00 411 23 100.00 412 25 100.00 413 24 100.00 414 39 64.74 415 55 75.20 416 46 88.45 417 50 100.00 418 47 100.00 419 97 93.28 420 32 100.00 421 35 100.00 422 49 100.00 423 38 100.00 424 32 100.00 425 34 100.00 426 36 99.17 427 48 93.34 428 21 96.84 429 21 93.34 430 34 100.00 431 46 100.00 432 32 100.00 433 29 100.00 434 41 100.00 435 43 96.84 436 24 100.00 437 41 100.00 438 46 98.00 439 32 100.00 440 22 100.00 441 29 40.25 442 42 49.60 443 39 98.00 444 27 100.00 445 48 98.00 446 29 85.10 447 27 100.00 448 54 100.00 449 26 100.00 450 34 100.00 451 25 100.00 452 23 100.00 453 28 100.00 454 35 100.00 455 44 100.00 456 22 100.00 457 42 100.00 458 29 100.00 459 32 100.00 460 41 100.00 461 26 100.00 462 21 100.00 463 34 100.00 464 41 100.00 465 37 100.00 466 37 100.00 467 41 100.00 468 46 100.00 469 40 100.00 470 43 97.60 471 30 87.06 472 35 100.00 473 36 93.77 474 61 100.00 475 38 100.00 476 39 100.00 477 33 99.66 478 32 100.00 479 31 100.00 480 50 100.00 481 48 91.44 482 43 100.00 483 25 87.33 484 28 100.00 485 36 100.00 486 27 89.38 487 25 100.00 488 40 100.00 489 34 95.55 490 50 100.00 491 38 100.00 492 37 95.55 493 43 89.38 494 43 86.30 495 46 95.13 496 42 36.11 497 50 50.18 498 44 100.00 499 27 93.16 500 35 100.00 501 51 95.55 502 41 50.14 503 48 49.06 504 42 54.99 505 49 43.13 506 30 58.22 507 45 51.21 508 48 44.21 509 32 54.45 510 46 53.37 511 48 63.61 512 33 43.13 513 31 48.52 514 20 58.22 515 29 51.75 516 27 57.68 517 24 56.07 518 37 48.52 519 25 44.21 520 41 57.68 521 27 89.89 522 21 58.95 523 22 72.41 524 32 98.63 525 25 52.83 526 42 100.00 527 25 51.75 528 37 100.00 529 26 100.00 530 44 99.55 531 47 100.00 532 43 100.00 533 42 100.00 534 42 100.00 535 29 100.00 536 40 100.00 537 38 100.00 538 38 100.00 539 21 100.00 540 24 100.00 541 36 100.00 542 23 100.00 543 20 100.00 544 32 100.00 545 29 100.00 546 44 100.00 547 44 100.00 548 36 100.00 549 49 56.30 550 34 42.64 551 59 100.00 552 37 100.00 553 36 100.00 554 43 100.00 555 21 100.00 556 32 100.00 557 38 100.00 558 43 100.00 559 42 100.00 560 32 100.00 561 42 100.00 562 31 100.00 563 49 100.00 564 45 100.00 565 49 100.00 566 41 100.00 567 45 100.00 568 36 100.00 569 39 100.00 570 27 100.00 571 25 100.00 572 41 100.00 573 39 99.52 574 28 57.55 575 25 54.57 576 33 100.00 577 34 100.00 578 24 100.00 579 30 100.00 580 42 100.00 581 21 100.00 582 34 100.00 583 29 100.00 584 24 100.00 585 44 100.00 586 21 100.00 587 33 100.00 588 30 100.00 589 26 100.00 590 41 100.00 591 26 100.00 592 32 100.00 593 43 100.00 594 48 100.00 595 44 74.04 596 45 100.00 597 37 100.00 598 39 100.00 599 76 100.00 600 37 100.00 601 38 82.39 602 43 72.38 603 48 79.31 604 26 82.39 605 38 88.55 606 20 63.14 607 22 73.92 608 45 90.86 609 45 85.47 610 20 66.99 611 47 64.68 612 46 73.92 613 23 83.93 614 33 74.69 615 29 90.86 616 44 82.39 617 41 92.40 618 20 91.63 619 37 78.54 620 29 100.00 621 55 65.45 622 22 100.00 623 31 67.76 624 49 79.22 625 61 73.92 626 39 83.93 627 38 100.00 628 31 100.00 629 36 100.00 630 25 100.00 631 48 100.00 632 35 100.00 633 21 100.00 634 47 100.00 635 38 100.00 636 41 100.00 637 24 100.00 638 37 100.00 639 33 100.00 640 49 100.00 641 29 100.00 642 24 100.00 643 47 100.00 644 24 100.00 645 25 100.00 646 30 32.47 647 22 100.00 648 27 64.69 649 34 100.00 650 36 100.00 651 34 43.05 652 48 100.00 653 34 100.00 654 24 100.00 655 46 100.00 656 45 100.00 657 39 100.00 658 43 100.00 659 29 100.00 660 20 100.00 661 46 100.00 662 27 100.00 663 44 100.00 664 43 100.00 665 49 100.00 666 40 100.00 667 30 100.00 668 50 100.00 669 23 100.00 670 26 100.00 671 27 100.00 672 42 100.00 673 47 100.00 674 49 100.00 675 38 100.00 676 20 100.00 677 25 100.00 678 25 88.00 679 41 100.00 680 28 100.00 681 50 67.80 682 32 50.25 683 42 53.88 684 24 62.36 685 27 69.62 686 26 57.51 687 38 61.15 688 42 59.33 689 23 71.44 690 21 62.96 691 28 50.85 692 33 72.65 693 25 62.96 694 28 61.75 695 46 49.04 696 30 61.15 697 38 84.25 698 40 56.91 699 45 100.00 700 27 49.30 701 42 72.65 702 36 63.57 703 29 100.00 704 39 100.00 705 45 100.00 706 47 100.00 707 49 100.00 708 46 100.00 709 48 100.00 710 46 100.00 711 35 100.00 712 43 100.00 713 26 100.00 714 22 98.18 715 34 99.41 716 50 100.00 717 48 100.00 718 41 100.00 719 36 100.00 720 29 100.00 721 33 37.48 722 46 100.00 723 38 100.00 724 20 36.42 725 22 100.00 726 27 100.00 727 56 98.18 728 38 99.41 729 25 100.00 730 33 100.00 731 42 100.00 732 33 100.00 733 38 100.00 734 31 100.00 735 20 100.00 736 44 100.00 737 26 100.00 738 27 100.00 739 46 100.00 740 47 100.00 741 37 100.00 742 31 100.00 743 24 100.00 744 31 100.00 745 50 100.00 746 35 64.69 747 30 100.00 748 29 100.00 749 27 100.00 750 40 100.00 751 31 98.99 752 6 100.00 753 45 100.00 754 22 54.09 755 45 68.67 756 43 65.02 757 46 61.99 758 39 69.28 759 31 71.10 760 41 69.28 761 44 60.16 762 45 70.49 763 37 69.89 764 35 61.38 765 28 59.55 766 30 61.99 767 30 49.22 768 25 69.28 769 29 57.73 770 26 57.73 771 41 53.48 772 34 52.87 773 35 61.21 774 34 61.38 775 50 100.00 776 41 61.99 777 22 96.86 778 35 48.62 779 44 38.50 780 47 61.99 781 19 49.22 782 34 90.39 783 29 71.81 784 49 69.27 785 30 85.32 786 21 70.96 787 50 76.88 788 47 100.00 789 24 76.03 790 27 98.84 791 33 86.17 792 35 90.39 793 31 71.81 794 25 82.79 795 27 82.79 796 31 100.00 797 45 100.00 798 27 100.00 799 27 100.00 800 42 69.27 801 21 74.77 802 34 76.88 803 42 76.03 804 15 98.84 805 29 70.87 806 46 58.15 807 30 61.78 808 30 49.67 809 42 51.48 810 46 61.18 811 25 64.20 812 32 65.42 813 30 64.81 814 40 49.67 815 28 60.57 816 23 55.72 817 29 61.18 818 34 58.75 819 37 63.60 820 20 49.06 821 32 48.46 822 34 52.09 823 42 52.70 824 38 100.00 825 30 62.16 826 23 49.67 827 22 53.30 828 39 100.00 829 55 55.72 830 36 61.18 831 26 100.00 832 31 100.00 833 34 100.00 834 41 100.00 835 23 100.00 836 48 100.00 837 22 100.00 838 21 100.00 839 22 100.00 840 40 100.00 841 50 100.00 842 29 100.00 843 43 100.00 844 24 100.00 845 22 100.00 846 43 100.00 847 20 100.00 848 25 100.00 849 36 100.00 850 24 52.67 851 21 100.00 852 30 100.00 853 32 94.79 854 21 47.18 855 26 78.11 856 35 100.00 857 26 100.00 858 46 100.00 859 37 100.00 860 27 100.00 861 23 100.00 862 39 100.00 863 27 100.00 864 38 100.00 865 27 100.00 866 40 100.00 867 24 100.00 868 44 100.00 869 37 100.00 870 20 100.00 871 39 100.00 872 44 100.00 873 22 100.00 874 43 100.00 875 27 100.00 876 26 64.90 877 25 52.32 878 49 100.00 879 29 100.00 880 41 100.00 881 55 100.00 882 27 83.07 883 23 100.00 884 31 90.17 885 46 100.00 886 47 91.18 887 31 100.00 888 46 100.00 889 37 89.15 890 28 93.21 891 37 90.17 892 49 100.00 893 24 100.00 894 30 100.00 895 50 88.14 896 31 96.24 897 46 100.00 898 47 100.00 899 46 100.00 900 37 100.00 901 33 100.00 902 31 90.17 903 48 100.00 904 41 87.13 905 42 100.00 906 41 100.00 907 32 45.25 908 10 88.14 909 35 57.46 910 28 64.33 911 46 73.70 912 20 71.20 913 30 49.97 914 48 69.96 915 28 53.72 916 39 68.08 917 24 51.84 918 28 67.46 919 31 58.71 920 45 63.71 921 24 58.09 922 49 53.72 923 32 63.08 924 43 68.71 925 37 50.59 926 24 64.96 927 35 53.72 928 41 29.87 929 26 100.00 930 34 64.96 931 49 70.58 932 28 44.21 933 40 68.08 934 37 59.96 935 31 53.72 936 41 83.44 937 21 89.46 938 40 96.34 939 46 74.84 940 44 79.14 941 46 73.12 942 41 81.72 943 32 89.46 944 46 87.74 945 28 100.00 946 49 94.62 947 21 73.98 948 32 84.30 949 34 98.06 950 21 98.06 951 21 96.34 952 31 83.44 953 21 94.62 954 25 45.86 955 28 82.58 956 43 64.97 957 22 86.74 958 37 93.01 959 28 72.26 960 30 74.84 961 44 73.98 962 25 100.00 963 43 100.00 964 30 97.39 965 20 90.06 966 26 100.00 967 40 100.00 968 31 89.01 969 22 100.00 970 23 100.00 971 30 100.00 972 49 100.00 973 31 100.00 974 29 100.00 975 37 84.82 976 38 100.00 977 29 100.00 978 23 100.00 979 26 85.87 980 38 100.00 981 48 47.04 982 40 39.80 983 45 100.00 984 44 100.00 985 21 94.22 986 35 100.00 987 29 86.92 988 21 84.82 989 22 100.00 990 26 100.00 991 41 100.00 992 47 100.00 993 31 100.00 994 43 100.00 995 23 100.00 996 28 100.00 997 49 100.00 998 24 100.00 999 33 100.00 1000 22 100.00 1001 32 100.00 1002 40 100.00 1003 43 100.00 1004 24 100.00 1005 32 100.00 1006 20 100.00 1007 24 69.12 1008 48 100.00 1009 44 100.00 1010 28 100.00 1011 24 61.52 1012 33 100.00 1013 41 100.00 1014 23 100.00 1015 46 100.00 1016 48 100.00 1017 25 100.00 1018 22 100.00 1019 41 100.00 1020 34 100.00 1021 32 100.00 1022 21 100.00 1023 20 100.00 1024 47 100.00 1025 39 100.00 1026 29 100.00 1027 45 100.00 1028 28 100.00 1029 26 100.00 1030 50 100.00 1031 48 100.00 1032 25 100.00 1033 40 100.00 1034 43 100.00 1035 22 100.00 1036 47 100.00 1037 36 100.00 1038 40 100.00 1039 27 100.00 1040 29 100.00 1041 20 100.00 1042 42 100.00 1043 25 100.00 1044 36 100.00 1045 21 100.00 1046 23 100.00 1047 37 100.00 1048 48 100.00 1049 25 100.00 1050 33 100.00 1051 27 100.00 1052 27 100.00 1053 20 100.00 1054 30 100.00 1055 48 100.00 1056 32 93.49 1057 34 100.00 1058 27 56.85 1059 39 100.00 1060 47 100.00 1061 22 100.00 1062 55 100.00 1063 60 100.00 1064 35 100.00 1065 28 100.00 1066 38 100.00 1067 21 95.80 1068 41 100.00 1069 22 97.81 1070 29 88.74 1071 50 100.00 1072 29 100.00 1073 49 80.67 1074 35 100.00 1075 48 100.00 1076 23 80.67 1077 48 95.80 1078 42 100.00 1079 47 100.00 1080 36 100.00 1081 22 100.00 1082 40 91.76 1083 23 100.00 1084 32 100.00 1085 21 100.00 1086 41 93.04 1087 25 84.71 1088 26 100.00 1089 24 89.75 1090 48 100.00 1091 26 68.35 1092 21 73.17 1093 45 78.00 1094 36 86.04 1095 21 81.21 1096 32 70.76 1097 30 82.82 1098 36 94.88 1099 33 86.04 1100 35 78.00 1101 37 95.69 1102 41 73.17 1103 20 76.39 1104 45 86.84 1105 38 69.96 1106 43 70.76 1107 49 78.80 1108 27 80.41 1109 46 73.98 1110 38 59.10 1111 25 66.74 1112 46 60.30 1113 22 100.00 1114 40 100.00 1115 46 100.00 1116 39 100.00 1117 38 82.34 1118 30 100.00 1119 42 94.25 1120 43 100.00 1121 29 95.24 1122 33 86.31 1123 32 79.37 1124 28 87.30 1125 41 100.00 1126 33 100.00 1127 36 84.33 1128 26 89.29 1129 34 100.00 1130 26 96.23 1131 38 100.00 1132 33 100.00 1133 33 91.27 1134 46 100.00 1135 26 100.00 1136 25 100.00 1137 45 73.08 1138 50 100.00 1139 36 100.00 1140 21 89.29 1141 29 100.00 1142 21 100.00 1143 42 100.00 1144 37 100.00 1145 25 100.00 1146 36 100.00 1147 22 100.00 1148 23 100.00 1149 32 100.00 1150 28 100.00 1151 27 100.00 1152 49 100.00 1153 41 100.00 1154 49 100.00 1155 30 100.00 1156 40 100.00 1157 23 100.00 1158 49 100.00 1159 25 100.00 1160 37 100.00 1161 55 100.00 1162 23 100.00 1163 24 100.00 1164 43 96.49 1165 50 100.00 1166 47 100.00 1167 34 100.00 1168 31 100.00 1169 28 100.00 1170 36 100.00 1171 48 100.00 1172 39 100.00 1173 45 100.00 1174 35 100.00 1175 45 100.00 1176 46 100.00 1177 37 100.00 1178 31 100.00 1179 33 100.00 1180 31 100.00 1181 27 100.00 1182 39 100.00 1183 32 100.00 1184 28 100.00 1185 26 67.91 1186 44 84.88 1187 46 100.00 1188 32 70.83 1189 65 100.00 1190 43 100.00 1191 43 67.77 1192 35 49.74 1193 45 50.36 1194 47 67.14 1195 21 64.66 1196 38 68.39 1197 21 50.36 1198 43 72.74 1199 46 54.09 1200 38 58.44 1201 26 52.22 1202 31 52.84 1203 48 54.71 1204 33 50.36 1205 38 57.20 1206 39 55.95 1207 42 67.14 1208 44 59.06 1209 29 69.63 1210 26 55.95 1211 31 53.47 1212 32 89.12 1213 28 100.00 1214 36 100.00 1215 36 52.22 1216 41 100.00 1217 27 99.52 1218 33 100.00 1219 34 100.00 1220 29 100.00 1221 34 100.00 1222 48 100.00 1223 46 100.00 1224 22 100.00 1225 20 100.00 1226 45 85.75 1227 46 100.00 1228 34 100.00 1229 50 85.75 1230 46 100.00 1231 22 84.70 1232 48 86.81 1233 47 86.81 1234 34 100.00 1235 45 100.00 1236 20 100.00 1237 50 60.49 1238 22 57.55 1239 45 100.00 1240 58 100.00 1241 51 100.00 1242 38 100.00 1243 22 100.00 1244 25 100.00 1245 24 100.00 1246 35 100.00 1247 28 100.00 1248 36 100.00 1249 39 100.00 1250 27 100.00 1251 40 100.00 1252 50 100.00 1253 42 100.00 1254 48 100.00 1255 25 100.00 1256 31 100.00 1257 44 100.00 1258 23 100.00 1259 29 100.00 1260 49 100.00 1261 36 100.00 1262 34 100.00 1263 25 100.00 1264 48 100.00 1265 38 100.00 1266 37 100.00 1267 49 100.00 1268 22 86.51 1269 28 89.27 1270 36 85.59 1271 34 100.00 1272 39 100.00 1273 21 75.46 1274 36 100.00 1275 24 97.55 1276 29 85.59 1277 38 94.79 1278 34 100.00 1279 42 90.19 1280 35 100.00 1281 35 80.99 1282 38 89.27 1283 41 81.91 1284 50 100.00 1285 21 100.00 1286 43 62.72 1287 32 100.00 1288 6 90.19 1289 66 92.95 1290 41 82.50 1291 23 97.42 1292 43 92.16 1293 24 70.22 1294 22 83.38 1295 26 73.73 1296 35 74.60 1297 47 77.24 1298 50 100.00 1299 45 87.77 1300 39 89.53 1301 23 89.53 1302 42 75.48 1303 20 89.53 1304 33 71.09 1305 34 100.00 1306 49 100.00 1307 39 90.40 1308 36 100.00 1309 50 86.01 1310 29 100.00 1311 30 100.00 1312 41 86.89 1313 28 58.58 1314 45 100.00 1315 16 75.48 1316 36 100.00 1317 41 100.00 1318 50 100.00 1319 40 100.00 1320 49 100.00 1321 45 100.00 1322 47 100.00 1323 21 100.00 1324 32 100.00 1325 47 100.00 1326 38 100.00 1327 41 100.00 1328 21 100.00 1329 41 100.00 1330 38 100.00 1331 25 99.29 1332 48 100.00 1333 22 99.29 1334 28 100.00 1335 47 100.00 1336 49 100.00 1337 45 100.00 1338 28 100.00 1339 29 57.53 1340 39 100.00 1341 46 100.00 1342 38 100.00 1343 41 47.29 1344 50 49.81 1345 43 53.83 1346 29 43.27 1347 30 42.76 1348 25 53.83 1349 49 44.78 1350 40 49.30 1351 41 44.78 1352 21 53.33 1353 46 45.28 1354 39 40.25 1355 45 59.87 1356 21 59.87 1357 44 58.36 1358 44 59.87 1359 29 51.82 1360 34 49.30 1361 39 56.85 1362 38 100.00 1363 24 79.86 1364 29 100.00 1365 30 100.00 1366 20 100.00 1367 39 100.00 1368 35 59.87 1369 26 59.87 1370 44 100.00 1371 28 100.00 1372 31 100.00 1373 29 100.00 1374 32 100.00 1375 33 100.00 1376 44 100.00 1377 32 100.00 1378 41 100.00 1379 35 100.00 1380 44 100.00 1381 26 100.00 1382 20 100.00 1383 48 100.00 1384 34 100.00 1385 49 100.00 1386 40 100.00 1387 45 100.00 1388 50 100.00 1389 38 100.00 1390 25 100.00 1391 28 58.18 1392 49 67.14 1393 49 100.00 1394 42 61.29 1395 23 57.73 1396 29 81.25 1397 25 80.54 1398 39 71.98 1399 44 69.84 1400 25 76.26 1401 45 76.26 1402 25 83.39 1403 37 57.73 1404 30 66.99 1405 36 75.55 1406 26 60.58 1407 23 73.41 1408 23 72.70 1409 25 66.99 1410 21 100.00 1411 26 63.43 1412 44 85.25 1413 24 100.00 1414 66 66.99 1415 36 57.73 1416 36 85.25 1417 22 77.90 1418 25 60.26 1419 37 72.76 1420 32 75.69 1421 47 74.22 1422 37 69.82 1423 20 62.47 1424 41 82.31 1425 21 60.26 1426 22 76.43 1427 40 80.10 1428 32 74.96 1429 36 66.14 1430 27 72.02 1431 26 87.45 1432 30 70.55 1433 23 56.84 1434 29 59.53 1435 21 60.37 1436 34 100.00 1437 26 76.43 1438 60 64.67 1439 35 55.49 1440 47 69.36 1441 20 60.69 1442 20 54.33 1443 25 65.31 1444 25 69.36 1445 27 68.78 1446 31 60.11 1447 44 66.47 1448 49 46.82 1449 26 56.07 1450 36 54.33 1451 44 52.60 1452 28 46.82 1453 45 64.74 1454 29 46.82 1455 40 53.75 1456 45 61.85 1457 44 53.18 1458 25 69.16 1459 45 100.00 1460 48 47.40 1461 44 60.76 1462 25 97.27 1463 22 91.76 1464 31 50.29 1465 21 52.60 1466 55 46.82 1467 25 100.00 1468 35 98.05 1469 35 93.54 1470 43 95.80 1471 44 100.00 1472 50 100.00 1473 48 100.00 1474 25 100.00 1475 39 100.00 1476 25 90.16 1477 32 91.29 1478 20 100.00 1479 26 100.00 1480 42 100.00 1481 21 100.00 1482 34 100.00 1483 47 100.00 1484 21 100.00 1485 48 100.00 1486 30 87.78 1487 27 84.39 1488 50 96.92 1489 38 100.00 1490 45 100.00 1491 46 100.00 1492 35 100.00 1493 29 59.37 1494 50 59.87 1495 26 49.81 1496 47 56.85 1497 23 53.33 1498 34 42.76 1499 34 53.83 1500 47 53.83 1501 45 49.81 1502 45 53.33 1503 36 43.27 1504 21 40.25 1505 28 48.30 1506 35 45.28 1507 50 52.32 1508 22 51.32 1509 45 49.30 1510 48 42.26 1511 20 87.96 1512 27 36.21 1513 38 38.50 1514 32 100.00 1515 64 40.25 1516 37 60.37 1517 28 88.63 1518 39 100.00 1519 41 94.10 1520 40 87.54 1521 49 100.00 1522 27 98.48 1523 34 100.00 1524 23 96.29 1525 31 88.63 1526 34 97.38 1527 25 95.20 1528 22 100.00 1529 32 100.00 1530 31 100.00 1531 25 100.00 1532 47 87.54 1533 21 50.65 1534 28 71.73 1535 46 94.10 1536 33 41.71 1537 43 100.00 1538 38 96.29 1539 47 88.63 1540 45 31.20 1541 20 35.51 1542 45 37.84 1543 36 33.19 1544 37 27.22 1545 31 31.53 1546 39 36.84 1547 26 29.21 1548 32 37.17 1549 20 34.19 1550 42 29.21 1551 33 29.54 1552 20 28.88 1553 29 38.17 1554 23 30.20 1555 39 29.54 1556 20 100.00 1557 45 81.91 1558 20 35.18 1559 48 100.00 1560 23 36.29 1561 32 70.56 1562 33 100.00 1563 61 29.54 1564 45 26.88 1565 38 83.03 1566 34 83.79 1567 43 83.03 1568 47 83.03 1569 22 67.03 1570 29 75.41 1571 28 68.55 1572 40 91.40 1573 25 73.88 1574 30 61.70 1575 38 69.31 1576 36 87.60 1577 32 87.60 1578 37 62.46 1579 30 79.98 1580 39 70.08 1581 32 65.51 1582 47 63.22 1583 26 86.83 1584 37 94.43 1585 55 79.98 1586 21 100.00 1587 23 100.00 1588 49 81.40 1589 59 87.60 1590 32 87.60 1591 43 100.00 1592 41 100.00 1593 45 100.00 1594 33 100.00 1595 40 100.00 1596 33 100.00 1597 50 100.00 1598 30 100.00 1599 41 100.00 1600 35 100.00 1601 49 100.00 1602 46 100.00 1603 48 100.00 1604 36 100.00 1605 22 100.00 1606 42 100.00 1607 21 100.00 1608 29 100.00 1609 35 100.00 1610 41 100.00 1611 29 71.97 1612 34 50.33 1613 37 100.00 1614 28 80.54 1615 49 100.00 1616 23 100.00 1617 46 53.76 1618 39 44.35 1619 22 45.25 1620 49 49.28 1621 43 36.29 1622 27 41.22 1623 31 36.74 1624 20 50.62 1625 24 38.08 1626 49 47.94 1627 24 48.38 1628 39 45.25 1629 37 45.70 1630 45 47.49 1631 45 48.38 1632 44 39.42 1633 23 37.63 1634 30 100.00 1635 26 85.52 1636 43 53.76 1637 26 31.86 1638 28 30.59 1639 27 68.35 1640 24 100.00 1641 40 45.70 1642 36 100.00 1643 21 100.00 1644 27 100.00 1645 47 100.00 1646 42 100.00 1647 32 100.00 1648 28 100.00 1649 24 100.00 1650 49 100.00 1651 46 100.00 1652 28 100.00 1653 48 100.00 1654 29 100.00 1655 47 100.00 1656 43 100.00 1657 25 100.00 1658 48 100.00 1659 24 100.00 1660 42 100.00 1661 31 100.00 1662 42 100.00 1663 37 100.00 1664 41 100.00 1665 20 100.00 1666 20 100.00 1667 70 100.00 1668 49 100.00 1669 35 58.87 1670 32 76.88 1671 29 61.64 1672 27 60.95 1673 27 80.34 1674 38 74.11 1675 35 72.03 1676 42 76.19 1677 21 63.72 1678 37 80.34 1679 26 79.65 1680 47 65.80 1681 37 65.10 1682 46 75.49 1683 38 59.56 1684 33 66.49 1685 24 56.10 1686 31 81.73 1687 42 81.03 1688 32 100.00 1689 41 70.65 1690 43 61.23 1691 20 100.00 1692 35 65.13 1693 27 79.65 1694 43 78.15 1695 32 72.70 1696 21 73.60 1697 20 100.00 1698 22 74.51 1699 36 73.60 1700 46 83.60 1701 47 96.32 1702 45 88.14 1703 47 88.14 1704 47 94.50 1705 38 87.24 1706 49 79.97 1707 35 80.87 1708 49 100.00 1709 28 93.60 1710 30 72.70 1711 39 86.72 1712 25 100.00 1713 40 100.00 1714 36 37.50 1715 76 94.50 1716 39 100.00 1717 44 39.60 1718 24 30.06 1719 39 38.19 1720 21 42.43 1721 30 40.31 1722 27 31.82 1723 37 31.12 1724 42 31.82 1725 32 28.29 1726 42 29.70 1727 21 40.31 1728 33 32.88 1729 49 36.07 1730 31 33.24 1731 38 41.72 1732 20 40.66 1733 39 30.06 1734 48 31.47 1735 39 37.13 1736 30 100.00 1737 33 37.13 1738 36 37.13 1739 36 82.94 1740 45 100.00 1741 40 100.00 1742 46 38.90 1743 30 36.07 1744 31 33.24 1745 49 74.68 1746 41 59.60 1747 35 67.14 1748 27 60.97 1749 23 72.62 1750 21 69.88 1751 34 80.84 1752 22 69.20 1753 48 67.82 1754 43 82.21 1755 32 81.53 1756 20 67.82 1757 24 67.14 1758 40 65.08 1759 30 73.99 1760 21 71.25 1761 25 75.36 1762 34 63.71 1763 48 58.92 1764 55 100.00 1765 25 74.68 1766 38 70.44 1767 39 55.96 1768 28 57.55 1769 24 61.66 1770 21 67.82 1771 46 100.00 1772 25 93.95 1773 34 100.00 1774 25 100.00 1775 23 100.00 1776 20 100.00 1777 23 100.00 1778 42 100.00 1779 27 100.00 1780 33 100.00 1781 28 98.65 1782 43 100.00 1783 48 100.00 1784 48 100.00 1785 45 100.00 1786 43 100.00 1787 44 42.26 1788 24 87.24 1789 31 100.00 1790 44 36.29 1791 59 98.65 1792 55 96.30 1793 29 32.10 1794 39 30.96 1795 20 35.87 1796 25 42.67 1797 42 37.00 1798 36 35.49 1799 37 42.67 1800 30 30.59 1801 21 37.00 1802 34 43.42 1803 42 36.63 1804 20 44.56 1805 40 42.67 1806 34 40.40 1807 31 38.89 1808 36 39.65 1809 48 34.36 1810 33 41.91 1811 37 33.23 1812 27 42.24 1813 39 40.40 1814 36 38.52 1815 36 100.00 1816 41 100.00 1817 37 100.00 1818 47 44.56 1819 15 42.67 1820 44 72.58 1821 35 87.62 1822 41 94.71 1823 49 98.25 1824 31 91.17 1825 20 79.66 1826 45 72.58 1827 33 74.35 1828 47 83.20 1829 20 89.40 1830 47 70.81 1831 40 94.71 1832 30 100.00 1833 22 91.17 1834 27 100.00 1835 34 92.94 1836 46 84.97 1837 31 84.08 1838 24 86.74 1839 41 85.85 1840 55 100.00 1841 30 100.00 1842 33 57.32 1843 43 97.87 1844 27 83.20 1845 60 100.00 1846 27 73.62 1847 49 83.04 1848 31 73.62 1849 20 77.05 1850 24 81.33 1851 33 94.17 1852 32 72.77 1853 40 79.62 1854 27 79.62 1855 40 79.62 1856 26 81.33 1857 44 96.74 1858 33 71.06 1859 34 68.49 1860 48 74.48 1861 25 83.04 1862 39 84.75 1863 45 34.19 1864 24 100.00 1865 46 79.62 1866 44 79.06 1867 13 81.33 1868 35 96.74 1869 30 63.07 1870 34 50.21 1871 27 66.13 1872 30 68.58 1873 50 69.80 1874 34 50.21 1875 23 65.52 1876 48 60.01 1877 34 64.90 1878 48 48.98 1879 24 50.21 1880 47 62.45 1881 24 52.66 1882 47 62.45 1883 20 61.23 1884 20 67.97 1885 31 58.78 1886 38 56.94 1887 26 61.23 1888 25 100.00 1889 48 62.45 1890 44 62.45 1891 21 100.00 1892 46 41.54 1893 46 52.84 1894 55 52.66 1895 31 52.60 1896 20 72.98 1897 29 59.18 1898 33 77.59 1899 34 55.89 1900 32 63.12 1901 27 73.64 1902 21 69.04 1903 27 71.67 1904 36 77.59 1905 43 70.35 1906 25 69.70 1907 46 70.35 1908 24 72.33 1909 39 71.67 1910 31 53.92 1911 22 71.67 1912 47 76.93 1913 20 72.98 1914 29 99.69 1915 38 68.38 1916 34 100.00 1917 46 66.00 1918 35 63.76 1919 34 71.67 1920 38 57.20 1921 18 69.70 1922 37 100.00 1923 43 100.00 1924 27 100.00 1925 30 100.00 1926 22 98.51 1927 49 100.00 1928 46 100.00 1929 48 91.02 1930 46 87.81 1931 48 92.09 1932 27 86.73 1933 43 100.00 1934 48 100.00 1935 41 100.00 1936 22 96.37 1937 46 92.09 1938 21 89.95 1939 31 37.18 1940 26 95.88 1941 20 99.58 1942 34 100.00 1943 43 86.73 1944 26 100.00 1945 50 79.67 1946 35 90.57 1947 50 77.99 1948 23 80.51 1949 37 67.93 1950 29 83.86 1951 21 72.12 1952 36 85.54 1953 22 86.38 1954 22 89.73 1955 46 80.51 1956 23 76.31 1957 49 87.21 1958 48 83.02 1959 33 72.96 1960 22 77.15 1961 22 91.41 1962 25 92.25 1963 20 92.25 1964 42 59.36 1965 25 60.34 1966 23 100.00 1967 37 85.54 1968 37 90.57 1969 42 72.96 1970 51 76.31 1971 40 100.00 1972 43 100.00 1973 47 100.00 1974 23 100.00 1975 35 100.00 1976 34 100.00 1977 25 100.00 1978 45 100.00 1979 47 100.00 1980 49 100.00 1981 40 100.00 1982 29 100.00 1983 39 100.00 1984 24 100.00 1985 25 100.00 1986 36 100.00 1987 50 100.00 1988 45 100.00 1989 26 100.00 1990 21 100.00 1991 42 100.00 1992 32 100.00 1993 31 94.58 1994 33 53.27 1995 45 100.00 1996 76 100.00 1997 70 100.00 1998 50 64.83 1999 28 70.29 2000 50 81.89 2001 28 66.19 2002 44 77.11 2003 27 73.02 2004 30 72.33 2005 43 66.19 2006 29 69.60 2007 48 56.64 2008 33 60.05 2009 40 75.06 2010 48 61.42 2011 41 81.89 2012 21 55.96 2013 32 71.65 2014 43 76.43 2015 30 77.79 2016 35 76.43 2017 45 96.92 2018 34 59.37 2019 26 100.00 2020 39 73.00 2021 41 73.32 2022 41 68.24 2023 64 60.05 2024 18 75.06 2025 49 34.47 2026 48 34.47 2027 46 33.23 2028 26 38.98 2029 37 38.98 2030 35 33.23 2031 23 42.26 2032 22 41.03 2033 39 33.23 2034 44 34.88 2035 27 43.90 2036 46 36.93 2037 33 41.85 2038 33 40.62 2039 24 40.21 2040 31 35.29 2041 41 77.24 2042 22 97.44 2043 46 37.34 2044 43 95.03 2045 15 36.93 2046 15 43.49 2047 26 100.00 2048 44 100.00 2049 20 96.99 2050 40 94.62 2051 23 100.00 2052 24 99.36 2053 29 100.00 2054 49 100.00 2055 34 100.00 2056 28 100.00 2057 37 100.00 2058 45 100.00 2059 46 100.00 2060 22 100.00 2061 39 100.00 2062 27 100.00 2063 36 100.00 2064 38 100.00 2065 44 100.00 2066 31 100.00 2067 23 100.00 2068 22 100.00 2069 28 50.32 2070 21 93.91 2071 37 100.00 2072 31 100.00 2073 25 100.00 2074 26 86.68 2075 34 100.00 2076 29 100.00 2077 20 90.57 2078 42 91.55 2079 22 100.00 2080 47 100.00 2081 20 100.00 2082 33 97.39 2083 39 90.57 2084 33 100.00 2085 40 86.68 2086 46 78.89 2087 48 97.39 2088 21 78.89 2089 45 100.00 2090 33 100.00 2091 44 100.00 2092 33 100.00 2093 39 100.00 2094 39 50.31 2095 41 100.00 2096 40 86.92 2097 33 100.00 2098 28 78.89 2099 26 63.76 2100 29 85.49 2101 46 77.52 2102 33 84.77 2103 48 78.25 2104 40 71.00 2105 23 74.62 2106 40 81.14 2107 37 74.62 2108 24 75.35 2109 27 62.31 2110 21 71.00 2111 23 72.45 2112 44 83.32 2113 35 83.32 2114 43 60.86 2115 40 84.77 2116 35 89.90 2117 25 62.46 2118 43 100.00 2119 50 63.34 2120 45 78.25 2121 52 81.14 2122 48 74.62 2123 31 68.71 2124 29 71.14 2125 23 87.31 2126 31 64.67 2127 23 67.10 2128 24 94.58 2129 28 71.14 2130 44 66.29 2131 22 92.16 2132 46 70.33 2133 22 93.77 2134 38 87.31 2135 47 83.27 2136 48 75.18 2137 40 88.12 2138 32 80.84 2139 49 97.01 2140 43 85.69 2141 41 100.00 2142 30 100.00 2143 28 95.39 2144 43 100.00 2145 41 100.00 2146 30 82.42 2147 31 100.00 2148 32 100.00 2149 43 96.31 2150 26 100.00 2151 27 100.00 2152 24 100.00 2153 22 100.00 2154 46 100.00 2155 37 97.27 2156 49 80.90 2157 21 100.00 2158 25 100.00 2159 37 100.00 2160 45 86.68 2161 32 85.72 2162 29 82.83 2163 26 83.79 2164 28 100.00 2165 27 87.64 2166 20 98.18 2167 44 100.00 2168 42 100.00 2169 41 100.00 2170 26 100.00 2171 26 100.00 2172 26 100.00 2173 41 86.68 2174 20 92.90 2175 22 100.00 2176 23 100.00 2177 33 93.90 2178 28 100.00 2179 44 98.89 2180 46 79.91 2181 21 100.00 2182 41 100.00 2183 31 100.00 2184 31 79.91 2185 23 81.91 2186 37 98.89 2187 26 100.00 2188 24 79.91 2189 47 100.00 2190 45 63.91 2191 55 100.00 2192 46 81.17 2193 50 100.00 2194 37 100.00 2195 44 94.90 2196 49 100.00 2197 45 100.00 2198 27 43.45 2199 31 44.66 2200 33 40.23 2201 31 35.80 2202 35 35.40 2203 26 39.83 2204 34 45.46 2205 46 32.99 2206 41 42.24 2207 43 39.43 2208 26 40.23 2209 36 48.28 2210 20 32.59 2211 27 36.61 2212 37 41.03 2213 24 42.24 2214 36 43.05 2215 29 38.22 2216 28 100.00 2217 29 100.00 2218 38 39.83 2219 48 48.28 2220 40 82.46 2221 41 44.56 2222 30 40.23 2223 35 47.62 2224 28 55.73 2225 45 51.95 2226 24 45.99 2227 41 63.85 2228 48 45.99 2229 50 63.31 2230 33 62.77 2231 32 43.29 2232 27 60.06 2233 35 55.19 2234 23 54.11 2235 35 48.70 2236 40 43.83 2237 35 47.62 2238 31 55.19 2239 50 46.53 2240 40 57.90 2241 38 45.45 2242 38 100.00 2243 40 60.60 2244 33 46.53 2245 36 100.00 2246 20 66.47 2247 32 53.18 2248 36 62.77 2249 19 48.70 2250 11 43.83 2251 49 65.87 2252 27 63.38 2253 29 70.84 2254 42 74.57 2255 33 50.95 2256 44 53.44 2257 22 64.00 2258 48 50.95 2259 33 54.68 2260 45 56.55 2261 20 52.82 2262 46 60.90 2263 40 49.71 2264 45 64.63 2265 36 59.65 2266 31 67.73 2267 46 50.33 2268 35 66.49 2269 28 100.00 2270 31 84.71 2271 27 100.00 2272 22 100.00 2273 30 99.55 2274 44 36.07 2275 30 60.28 2276 24 49.71 2277 45 75.63 2278 23 68.52 2279 26 62.70 2280 28 60.76 2281 49 58.18 2282 49 54.94 2283 29 74.98 2284 49 64.64 2285 39 54.94 2286 36 58.82 2287 39 62.05 2288 30 73.04 2289 44 69.16 2290 20 61.41 2291 21 63.35 2292 36 77.57 2293 32 71.75 2294 36 73.04 2295 34 56.24 2296 48 100.00 2297 33 73.69 2298 31 100.00 2299 36 100.00 2300 25 100.00 2301 48 100.00 2302 27 69.16 2303 44 61.41 2304 33 72.92 2305 29 72.23 2306 49 57.10 2307 20 81.86 2308 31 73.61 2309 39 59.16 2310 20 66.04 2311 34 77.73 2312 50 61.22 2313 40 79.11 2314 28 63.97 2315 50 81.86 2316 28 79.80 2317 46 66.04 2318 24 59.16 2319 24 81.17 2320 39 59.16 2321 40 44.51 2322 49 72.33 2323 44 82.26 2324 35 100.00 2325 22 67.41 2326 62 77.73 2327 26 61.22 2328 31 100.00 2329 25 86.74 2330 30 89.80 2331 27 100.00 2332 23 100.00 2333 34 100.00 2334 22 100.00 2335 42 85.72 2336 37 100.00 2337 30 100.00 2338 27 100.00 2339 25 100.00 2340 34 97.97 2341 38 100.00 2342 26 100.00 2343 38 100.00 2344 50 84.70 2345 22 100.00 2346 32 100.00 2347 31 71.02 2348 40 100.00 2349 22 100.00 2350 41 100.00 2351 45 48.98 2352 45 100.00 2353 39 40.15 2354 49 50.62 2355 27 50.19 2356 34 36.66 2357 20 41.02 2358 48 51.93 2359 29 38.40 2360 43 41.02 2361 41 46.26 2362 41 35.35 2363 36 51.93 2364 49 37.97 2365 38 45.39 2366 33 51.93 2367 26 48.44 2368 47 43.64 2369 34 47.57 2370 34 51.93 2371 40 50.62 2372 40 82.21 2373 33 82.59 2374 49 65.80 2375 27 100.00 2376 49 36.66 2377 56 35.35 2378 37 51.93 2379 33 100.00 2380 27 100.00 2381 46 100.00 2382 44 100.00 2383 26 100.00 2384 48 94.92 2385 23 100.00 2386 45 100.00 2387 49 100.00 2388 28 94.92 2389 37 100.00 2390 34 100.00 2391 22 100.00 2392 29 100.00 2393 34 98.39 2394 38 100.00 2395 41 100.00 2396 42 100.00 2397 28 100.00 2398 38 100.00 2399 23 100.00 2400 31 71.40 2401 46 100.00 2402 48 56.55 2403 29 100.00 2404 46 100.00 2405 26 100.00 2406 18 100.00 2407 32 53.31 2408 21 49.21 2409 46 69.12 2410 42 49.79 2411 31 57.41 2412 38 66.78 2413 38 64.44 2414 20 48.62 2415 46 62.09 2416 30 65.61 2417 30 68.54 2418 43 52.14 2419 49 63.85 2420 43 56.82 2421 37 66.78 2422 35 55.07 2423 34 60.34 2424 38 61.51 2425 44 100.00 2426 21 100.00 2427 44 100.00 2428 25 64.93 2429 24 58.58 2430 38 60.06 2431 45 100.00 2432 51 63.85 2433 34 82.99 2434 44 74.85 2435 44 96.00 2436 38 81.36 2437 31 71.60 2438 48 80.55 2439 21 93.56 2440 40 66.72 2441 40 80.55 2442 50 77.29 2443 20 68.34 2444 48 72.41 2445 47 89.50 2446 21 70.78 2447 39 78.92 2448 44 80.55 2449 28 88.68 2450 45 77.29 2451 20 100.00 2452 38 100.00 2453 26 100.00 2454 44 100.00 2455 49 100.00 2456 22 100.00 2457 31 68.34 2458 41 70.67 2459 25 76.67 2460 31 60.00 2461 41 64.00 2462 43 64.67 2463 43 75.34 2464 24 76.00 2465 21 54.00 2466 23 64.67 2467 38 74.67 2468 31 62.67 2469 36 70.67 2470 36 71.34 2471 34 62.00 2472 21 65.34 2473 45 78.67 2474 26 75.34 2475 50 54.00 2476 41 62.00 2477 39 60.00 2478 22 100.00 2479 46 76.67 2480 44 100.00 2481 25 77.34 2482 39 66.67 2483 37 71.34 2484 31 100.00 2485 47 82.21 2486 24 77.64 2487 36 100.00 2488 48 100.00 2489 28 98.65 2490 48 83.12 2491 21 78.55 2492 25 100.00 2493 25 100.00 2494 31 91.34 2495 40 84.03 2496 32 89.51 2497 24 83.12 2498 42 100.00 2499 21 100.00 2500 34 82.21 2501 27 100.00 2502 30 88.60 2503 39 100.00 2504 20 60.54 2505 37 81.87 2506 46 100.00 2507 47 87.69 2508 11 100.00 2509 23 91.34 2510 29 70.15 2511 38 79.68 2512 32 97.00 2513 43 84.01 2514 31 87.48 2515 29 100.00 2516 31 88.34 2517 30 94.40 2518 50 94.40 2519 40 80.55 2520 23 97.00 2521 26 88.34 2522 40 100.00 2523 21 100.00 2524 43 86.61 2525 29 71.89 2526 38 91.81 2527 23 76.22 2528 20 100.00 2529 36 70.30 2530 28 100.00 2531 44 100.00 2532 49 100.00 2533 32 80.55 2534 34 100.00 2535 30 100.00 2536 29 94.14 2537 22 85.99 2538 26 100.00 2539 32 91.43 2540 29 100.00 2541 34 96.86 2542 24 99.57 2543 24 90.52 2544 33 88.71 2545 26 100.00 2546 40 95.95 2547 44 94.14 2548 24 90.52 2549 20 94.14 2550 34 100.00 2551 34 97.76 2552 45 93.24 2553 41 100.00 2554 55 71.25 2555 23 100.00 2556 24 45.39 2557 32 84.41 2558 29 85.76 2559 36 100.00 2560 46 87.80 2561 32 95.95 2562 34 100.00 2563 24 100.00 2564 40 100.00 2565 26 82.77 2566 20 100.00 2567 31 100.00 2568 22 87.75 2569 42 100.00 2570 26 99.72 2571 37 87.75 2572 38 88.75 2573 35 100.00 2574 33 90.75 2575 39 100.00 2576 45 100.00 2577 24 100.00 2578 35 88.75 2579 23 100.00 2580 37 100.00 2581 55 87.75 2582 49 100.00 2583 26 100.00 2584 33 100.00 2585 37 83.84 2586 22 86.76 2587 85 88.75 2588 22 100.00 2589 31 65.77 2590 38 65.77 2591 45 85.29 2592 31 85.29 2593 36 64.33 2594 46 70.11 2595 32 76.62 2596 39 57.82 2597 50 78.79 2598 46 74.45 2599 36 80.95 2600 29 82.40 2601 32 75.89 2602 44 68.67 2603 42 62.16 2604 47 65.77 2605 44 58.55 2606 43 75.17 2607 48 74.45 2608 21 96.31 2609 50 74.35 2610 29 75.35 2611 41 70.33 2612 37 100.00 2613 22 66.50 2614 31 75.89 2615 42 100.00 2616 42 100.00 2617 45 100.00 2618 36 100.00 2619 20 100.00 2620 39 81.93 2621 42 85.98 2622 23 86.99 2623 26 89.01 2624 33 100.00 2625 31 88.00 2626 50 100.00 2627 44 100.00 2628 45 80.92 2629 46 88.00 2630 27 85.98 2631 28 100.00 2632 40 100.00 2633 30 99.13 2634 34 100.00 2635 46 100.00 2636 32 82.83 2637 27 100.00 2638 34 100.00 2639 34 54.84 2640 34 100.00 2641 46 80.92 2642 32 100.00 2643 24 100.00 2644 27 99.67 2645 20 100.00 2646 36 100.00 2647 29 100.00 2648 25 100.00 2649 29 100.00 2650 25 96.11 2651 44 100.00 2652 47 100.00 2653 48 100.00 2654 45 100.00 2655 35 100.00 2656 31 100.00 2657 50 100.00 2658 33 100.00 2659 29 100.00 2660 48 68.80 2661 44 72.42 2662 25 66.73 2663 50 100.00 2664 23 100.00 2665 21 96.11 2666 41 100.00 2667 44 74.40 2668 43 76.00 2669 28 96.00 2670 43 86.40 2671 48 96.00 2672 38 82.40 2673 31 86.40 2674 26 67.20 2675 32 92.00 2676 44 67.20 2677 27 76.00 2678 43 73.60 2679 25 69.60 2680 22 80.80 2681 21 87.20 2682 48 75.20 2683 33 64.00 2684 34 100.00 2685 43 81.95 2686 44 100.00 2687 44 100.00 2688 32 94.34 2689 29 65.60 2690 77 92.00 2691 39 67.20 2692 39 81.14 2693 36 100.00 2694 42 91.15 2695 21 100.00 2696 50 88.15 2697 24 100.00 2698 44 92.16 2699 37 100.00 2700 27 92.16 2701 37 100.00 2702 38 100.00 2703 48 96.16 2704 30 100.00 2705 25 88.15 2706 40 86.15 2707 22 88.15 2708 34 100.00 2709 32 90.15 2710 31 86.15 2711 43 80.00 2712 31 89.38 2713 31 77.34 2714 34 96.34 2715 45 92.08 2716 48 100.00 2717 28 100.00 2718 22 100.00 2719 45 83.42 2720 30 85.41 2721 38 85.41 2722 20 100.00 2723 28 100.00 2724 24 100.00 2725 22 79.45 2726 35 93.35 2727 33 85.41 2728 31 95.34 2729 35 82.43 2730 35 90.37 2731 50 81.43 2732 26 100.00 2733 38 89.38 2734 45 100.00 2735 30 100.00 2736 37 91.37 2737 37 86.61 2738 36 71.89 2739 25 100.00 2740 37 100.00 2741 30 95.48 2742 36 100.00 2743 27 90.37 2744 48 61.44 2745 26 59.22 2746 26 85.13 2747 34 85.87 2748 44 85.87 2749 39 82.91 2750 45 76.25 2751 40 63.67 2752 42 70.33 2753 43 74.03 2754 34 72.55 2755 38 62.19 2756 35 71.07 2757 31 72.55 2758 32 64.41 2759 47 86.62 2760 39 68.11 2761 44 62.19 2762 39 85.87 2763 50 57.86 2764 22 75.51 2765 35 100.00 2766 45 55.62 2767 44 86.40 2768 36 87.36 2769 28 72.55 2770 43 62.19 2771 48 52.64 2772 28 48.17 2773 21 41.71 2774 37 50.65 2775 34 49.16 2776 40 41.71 2777 45 51.15 2778 28 52.14 2779 29 41.71 2780 48 44.69 2781 31 45.69 2782 32 57.61 2783 21 57.11 2784 32 58.60 2785 43 57.61 2786 21 45.19 2787 34 53.63 2788 34 43.70 2789 44 86.13 2790 27 76.31 2791 49 52.64 2792 23 95.20 2793 25 64.97 2794 50 87.15 2795 34 40.22 2796 20 56.12 2797 42 57.61 2798 25 56.78 2799 50 43.68 2800 32 64.97 2801 39 44.23 2802 50 60.06 2803 38 48.59 2804 40 50.23 2805 28 64.43 2806 42 50.23 2807 42 63.88 2808 36 63.34 2809 24 49.69 2810 23 65.52 2811 29 50.78 2812 37 45.86 2813 33 51.32 2814 32 60.06 2815 35 59.51 2816 40 55.69 2817 37 86.74 2818 42 97.16 2819 20 100.00 2820 29 100.00 2821 43 100.00 2822 34 62.24 2823 47 65.52 don‚Äôt forget to assign it!\nselect_data \u0026lt;- select(sales_data, QUANTITYORDERED, PRICEEACH) arrange() function order your data  arranged_asc_data \u0026lt;- arrange(select_data, PRICEEACH) arranged_desc_data \u0026lt;- arrange(select_data, -PRICEEACH) filter() function filter out data first argument is the dataset, second argument is the filter conditions  filtered_data \u0026lt;- filter(sales_data, STATE == \u0026#39;NY\u0026#39;) filtered_data \u0026lt;- filter(sales_data, STATE == \u0026#39;NY\u0026#39; \u0026amp; PRODUCTLINE == \u0026#39;Classic Cars\u0026#39;)   If I want to ‚Äòfilter‚Äô and ‚Äòselect‚Äô data, I have to run the command twice filtered_data \u0026lt;- filter(sales_data, STATE == \u0026#39;NY\u0026#39; \u0026amp; PRODUCTLINE == \u0026#39;Classic Cars\u0026#39;) # notice the first argument is filtered_data select_data \u0026lt;- select(filtered_data, QUANTITYORDERED, PRICEEACH) # notice the first argument is select_data arrange_data \u0026lt;- arrange(select_data, PRICEEACH)  It‚Äôs kind of tedious to have to run the command twice, so we will use a concept called piping (%\u0026gt;%) Piping is sending the output of one function into the input of another. The output will be the first argument of the next function The same command above can be written like this:\npiped_data \u0026lt;- sales_data %\u0026gt;% filter(STATE == \u0026#39;NY\u0026#39; \u0026amp; PRODUCTLINE == \u0026#39;Classic Cars\u0026#39;) %\u0026gt;% select(QUANTITYORDERED, PRICEEACH) %\u0026gt;% arrange(PRICEEACH) mutate() function create your own columns using mutate Same as above but using piping  mutated_data \u0026lt;- mutate(sales_data, discounted = 0.95 * SALES) # same as above but using piping mutated_data \u0026lt;- sales_data %\u0026gt;% mutate(discounted = 0.95 * SALES) YOUR TURN!\nsales_data \u0026lt;- read.csv(\u0026#39;sales_data_sample.csv\u0026#39;) # Reread the data in case you made any changes to it # Q1: what is the most common deal size (column name: DEALSIZE)? summary(sales_data$DEALSIZE)  Large Medium Small 157 1384 1282  # Q2: what is the average quantity ordered? (HINT: Can also use mean function) mean(sales_data$QUANTITYORDERED) [1] 35.09281 summary(sales_data$QUANTITYORDERED)  Min. 1st Qu. Median Mean 3rd Qu. Max. 6.00 27.00 35.00 35.09 43.00 97.00  # Q3: create a new dataset called q3_data with # - a new column called MSRP_REV which is equal to the MSRP * QUANTITYORDERED # - filtered to only have \u0026#39;Large\u0026#39; sized deals # - with only the selected columns ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV # - ordered in descending order by SALES q3_data \u0026lt;- sales_data %\u0026gt;% mutate(MSRP_REV = MSRP * QUANTITYORDERED) %\u0026gt;% filter(DEALSIZE == \u0026#39;Large\u0026#39;) %\u0026gt;% select(ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV) %\u0026gt;% arrange(-SALES) # 5. group_by() and summarise() function # group_by() and summarise() will help us solve questions such as, what are the total sales by country? grouped_data \u0026lt;- group_by(sales_data, COUNTRY) summarised_data \u0026lt;- summarise(grouped_data, total_sales = sum(SALES)) # same as above, But using piping summarised_data \u0026lt;- sales_data %\u0026gt;% group_by(COUNTRY) %\u0026gt;% summarise(total_sales = sum(SALES)) # Instead of sum(), can also do max(), min(), mean(), n() for count, and others # Q4: what is the average SALE by PRODUCTLINE? summarised_data \u0026lt;- sales_data %\u0026gt;% group_by(PRODUCTLINE) %\u0026gt;% summarise(average_sales = mean(SALES)) # create a simple dot plot ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) + geom_point() + theme_classic() # change color ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) + geom_point(aes(color = \u0026#39;red\u0026#39;)) + theme_classic() # add labels and remove legend ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) + geom_point(aes(color = \u0026#39;red\u0026#39;)) + labs(title = \u0026#39;Sales and Quantity Ordered\u0026#39;, y = \u0026#39;Unit Price ($)\u0026#39;, x = \u0026#39;Quantity Ordered (Units)\u0026#39;) + theme(legend.position=\u0026quot;none\u0026quot;) + theme_classic() # add regression line ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) + geom_point(aes(color = \u0026#39;red\u0026#39;)) + labs(title = \u0026#39;Sales and Quantity Ordered\u0026#39;, y = \u0026#39;Unit Price ($)\u0026#39;, x = \u0026#39;Quantity Ordered (Units)\u0026#39;) + theme(legend.position = \u0026quot;none\u0026quot;)+ geom_smooth(method = \u0026quot;lm\u0026quot;) + theme_classic() # bar charts # will first create a grouped by and summarised dataset status_data \u0026lt;- sales_data %\u0026gt;% group_by(STATUS) %\u0026gt;% summarise(total_count = n()) %\u0026gt;% select(STATUS, total_count) # now we will create a bar chart ggplot(status_data, aes(x = STATUS, y = total_count)) + geom_bar(stat = \u0026#39;identity\u0026#39;, color = \u0026#39;red\u0026#39;, fill = \u0026#39;blue\u0026#39;) + theme_classic() # Q5: create a bar chart of the total sales by country with the following properties: # - x axis label: Product Line # - y axis label: Total Sales ($) # - title: Sales by Product Line # - Outline of bars: red # - Fill of bars: pink # Will first create a grouped by and summarised dataset status_data \u0026lt;- sales_data %\u0026gt;% group_by(PRODUCTLINE) %\u0026gt;% summarise(total_sales = sum(SALES)) # bar plot ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) + geom_bar(stat = \u0026#39;identity\u0026#39;, color = \u0026#39;red\u0026#39;, fill = \u0026#39;pink\u0026#39;) + labs(title = \u0026#39;Sales by Product Line\u0026#39;) + theme_classic() + coord_flip() # Q6: using the ggplot2 cheat sheet, try constructing your own plot of choice! # bar plot ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) + geom_bar(stat = \u0026#39;identity\u0026#39;, color = \u0026#39;red\u0026#39;, fill = \u0026#39;yellow\u0026#39;) + labs(title = \u0026#39;Sales by Product Line\u0026#39;) + theme_classic() + coord_polar()  ","date":1553731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553731200,"objectID":"b6d3be025674ccf195fb162e00cdfb07","permalink":"/post/sales_analysis/sales-analysis/","publishdate":"2019-03-28T00:00:00Z","relpermalink":"/post/sales_analysis/sales-analysis/","section":"post","summary":"Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.  Originally Written by Mar√≠a Carina Rold√°n, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014.","tags":[],"title":"Sales Analysis","type":"post"},{"authors":null,"categories":["R","Machine Learning"],"content":" Preamble: This document focuses on the time series analysis. The variable ‚ÄòCash‚Äô is provided in hundreds of dollars.\nThis is a time series spanning daily transactions from May 1, 2009 to April 30, 2010 from four ATMs.\n Research question: forecast how much cash is taken out of 4 different ATM machines for May 2010   Structure of analysis: Exploratory Data Analysis Visualizations ACF and PACF Clean The Data Trend Preview Data Decomposition Plot Stationarity Test Model Data Transformation ARIMA Model Evaluation Box-Ljung Test Forecasting Model Accuracy  sourceURL \u0026lt;- \u0026quot;https://raw.githubusercontent.com/jzuniga123\u0026quot; file \u0026lt;- \u0026quot;/SPS/master/DATA%20624/ATM624Data.xlsx\u0026quot; download.file(paste0(sourceURL, file), \u0026quot;temp.xlsx\u0026quot;, mode=\u0026quot;wb\u0026quot;) atm \u0026lt;- xlsx::read.xlsx(\u0026quot;temp.xlsx\u0026quot;, sheetIndex=1, header=T) invisible(file.remove(\u0026quot;temp.xlsx\u0026quot;))  Exploratory Data Analysis # preview first 5 rows head(atm)  DATE ATM Cash 1 2009-05-01 ATM1 96 2 2009-05-01 ATM2 107 3 2009-05-02 ATM1 82 4 2009-05-02 ATM2 89 5 2009-05-03 ATM1 85 6 2009-05-03 ATM2 90 class(atm) [1] \u0026quot;data.frame\u0026quot; str(atm) \u0026#39;data.frame\u0026#39;: 1474 obs. of 3 variables: $ DATE: Date, format: \u0026quot;2009-05-01\u0026quot; \u0026quot;2009-05-01\u0026quot; ... $ ATM : Factor w/ 4 levels \u0026quot;ATM1\u0026quot;,\u0026quot;ATM2\u0026quot;,..: 1 2 1 2 1 2 1 2 1 2 ... $ Cash: num 96 107 82 89 85 90 90 55 99 79 ... # preview descriptive statistics on quantitative and qualitative variables summary(atm)  DATE ATM Cash Min. :2009-05-01 ATM1:365 Min. : 0.0 1st Qu.:2009-08-01 ATM2:365 1st Qu.: 0.5 Median :2009-11-01 ATM3:365 Median : 73.0 Mean :2009-10-31 ATM4:365 Mean : 155.6 3rd Qu.:2010-02-01 NA\u0026#39;s: 14 3rd Qu.: 114.0 Max. :2010-05-14 Max. :10919.8 NA\u0026#39;s :19  Skewed distribution since the mean is higher than the third quartile.\n# preview periods between dates in the time series xts::periodicity(unique(atm$DATE)) Daily periodicity from 2009-05-01 to 2010-05-14  Dataframe spans daily transactions from May 1, 2009 to May 14, 2010.\n# preview observations that have no missing values atm[!complete.cases(atm), ]  DATE ATM Cash 87 2009-06-13 ATM1 NA 93 2009-06-16 ATM1 NA 98 2009-06-18 ATM2 NA 105 2009-06-22 ATM1 NA 110 2009-06-24 ATM2 NA 731 2010-05-01 \u0026lt;NA\u0026gt; NA 732 2010-05-02 \u0026lt;NA\u0026gt; NA 733 2010-05-03 \u0026lt;NA\u0026gt; NA 734 2010-05-04 \u0026lt;NA\u0026gt; NA 735 2010-05-05 \u0026lt;NA\u0026gt; NA 736 2010-05-06 \u0026lt;NA\u0026gt; NA 737 2010-05-07 \u0026lt;NA\u0026gt; NA 738 2010-05-08 \u0026lt;NA\u0026gt; NA 739 2010-05-09 \u0026lt;NA\u0026gt; NA 740 2010-05-10 \u0026lt;NA\u0026gt; NA 741 2010-05-11 \u0026lt;NA\u0026gt; NA 742 2010-05-12 \u0026lt;NA\u0026gt; NA 743 2010-05-13 \u0026lt;NA\u0026gt; NA 744 2010-05-14 \u0026lt;NA\u0026gt; NA ATM transactions have missing values.\nsummary(factor(atm$ATM)[!is.na(atm$Cash) \u0026amp; atm$Cash %% 1 != 0]) ATM1 ATM2 ATM3 ATM4 0 0 0 365  There are non-integer transactions at ATM 4 implying that these data are likely debit card purchase transactions.\n Visualizations # time plot represents a line graph that plots each observed value against the time of the observation, with a single line connecting each observation across the entire period par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) for(i in 1:length(levels(atm$ATM))) { atm_sub \u0026lt;- subset(atm, ATM == paste0(\u0026quot;ATM\u0026quot;, i)) atm_ts \u0026lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE) n \u0026lt;- nrow(atm_ts); l \u0026lt;- rep(1, n); m \u0026lt;- rep(20, n); h \u0026lt;- rep(100, n) print(plot(cbind(atm_ts, l, m,h), main=paste0(\u0026quot;ATM\u0026quot;, i))) # histogram displays the frequency at which values in a vector occur. hist(atm_ts, col=\u0026quot;green\u0026quot;, xlab=\u0026quot;\u0026quot;, main=\u0026quot;\u0026quot;) } Observations:\n Time Plots and Histograms for ATM1 and ATM2 are unremarkable. Time Plot and Histogram of ATM3 shows the data consists mostly of zero values with a handful of transactions occurring at the end of the series. ATM3 will not be modeled due to these degenerative properties. Time Plot and Histogram of ATM4 shows an extreme outlier around the three-quarter mark of the series. The horizontal lines in the Time Plots delineate $1, $20, and $100 in red, green, and blue; respectively.   ACF and PACF ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation The xts::xts()function converts data to a time series object which displays better in visualizations than time series objects created using other packages.\npar(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) for(i in 1:length(levels(atm$ATM))) { atm_sub \u0026lt;- subset(atm, ATM == paste0(\u0026quot;ATM\u0026quot;, i)) atm_ts \u0026lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE) acf(na.omit(atm_ts), ylab=paste0(\u0026quot;ACF ATM\u0026quot;, i), main=\u0026quot;\u0026quot;) pacf(na.omit(atm_ts), ylab=paste0(\u0026quot;PACF ATM\u0026quot;, i), main=\u0026quot;\u0026quot;) } Observations:\n ACF and PACF plots for ATM1, ATM2, and ATM3 show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations. The ACF and PACF plots for ATM3 however, are not reliable due to the death of observations.   Clean The Data Data are cleaned using forecast::tsclean() and then converted to a time series object using the ts() function. The tsclean() function imputes nulls and removes outliers. The ts()function converts data to a time series object which is compatible with the forecast package.\nfor(i in 1:length(levels(atm$ATM))) { atm_num \u0026lt;- paste0(\u0026quot;ATM\u0026quot;, i) atm_sub \u0026lt;- subset(atm, ATM == atm_num, select=-2) atm_sub$Cash \u0026lt;- forecast::tsclean(atm_sub$Cash, replace.missing=T) assign(atm_num, ts(atm_sub$Cash, frequency = 7, start=start(atm_sub$DATE))) }  Trend Examine A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.\npar(mfrow=c(3, 1), mar = c(0, 4, 0, 0), oma = c(0, 0, 0.5, 0.5)) plot(ATM1, col=8, xaxt = \u0026quot;n\u0026quot;, ylab=\u0026quot;ATM1\u0026quot;) lines(forecast::ma(ATM1, order=7), col=2) # weekly lines(forecast::ma(ATM1, order=30), col=4) # monthly plot(ATM2, col=8, xaxt = \u0026quot;n\u0026quot;, ylab=\u0026quot;ATM3\u0026quot;) lines(forecast::ma(ATM2, order=7), col=2) # weekly lines(forecast::ma(ATM2, order=30), col=4) # monthly plot(ATM4, col=8, xaxt = \u0026quot;n\u0026quot;, ylab=\u0026quot;ATM4\u0026quot;) lines(forecast::ma(ATM4, order=7), col=2) # weekly lines(forecast::ma(ATM4, order=30), col=4) # monthly Observations:\n The 7-day (weekly) and 30-day (monthly) moving average smoother line shows that the data for the ATMs have no apparent trend.   Data Decomposition Plot Decomposition Plot decomposes and plots the observed values, the underlying trend, seasonality, and randomness of the time series data.\nplot(decompose(ATM1), col=3) plot(decompose(ATM2), col=3) plot(decompose(ATM4), col=3) Observations:\n Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have no apparent trend, seasonal fluctuations, and fairly random residuals.   Stationarity Test Dickey-Fuller Test An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.\ntseries::adf.test(ATM1) Warning in tseries::adf.test(ATM1): p-value smaller than printed p-value  Augmented Dickey-Fuller Test data: ATM1 Dickey-Fuller = -4.5329, Lag order = 7, p-value = 0.01 alternative hypothesis: stationary tseries::adf.test(ATM2) Warning in tseries::adf.test(ATM2): p-value smaller than printed p-value  Augmented Dickey-Fuller Test data: ATM2 Dickey-Fuller = -6.046, Lag order = 7, p-value = 0.01 alternative hypothesis: stationary tseries::adf.test(ATM4) Warning in tseries::adf.test(ATM4): p-value smaller than printed p-value  Augmented Dickey-Fuller Test data: ATM4 Dickey-Fuller = -5.6304, Lag order = 7, p-value = 0.01 alternative hypothesis: stationary The augmented Dickey-Fuller unit root test p-values are below Œ±=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when œï\u0026lt;1. When œï=1 the AR(1) model is like a first-order difference.\n  Model Data The train and test sets are created by referencing rows by index.\n# train/test split index_train \u0026lt;- 1:(length(ATM1) - 30) ATM1_train \u0026lt;- ts(ATM1[index_train], frequency=7) ATM1_test \u0026lt;- ts(ATM1[-index_train], frequency=7) index_train \u0026lt;- 1:(length(ATM2) - 30) ATM2_train \u0026lt;- ts(ATM2[index_train], frequency=7) ATM2_test \u0026lt;- ts(ATM2[-index_train], frequency=7) index_train \u0026lt;- 1:(length(ATM3) - 30) ATM3_train \u0026lt;- ts(ATM3[index_train], frequency=7) ATM3_test \u0026lt;- ts(ATM3[-index_train], frequency=7) index_train \u0026lt;- 1:(length(ATM4) - 30) ATM4_train \u0026lt;- ts(ATM4[index_train], frequency=7) ATM4_test \u0026lt;- ts(ATM4[-index_train], frequency=7) The indexed rows for the test set are a window at the end of the times series. The window sized for the testing set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.\n Transformation The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted.Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.\n(lambda1 \u0026lt;- forecast::BoxCox.lambda(ATM1_train)) [1] 0.4355901 (lambda2 \u0026lt;- forecast::BoxCox.lambda(ATM2_train)) [1] 0.7156895 (lambda4 \u0026lt;- forecast::BoxCox.lambda(ATM4_train)) [1] 0.3945256 The Box-Cox transformation parameters suggested are around Œª=0.5. This rounded (more interpretable) value is suggestive of a 1/yt‚Äæ‚Äæ‚àö transformation. These Box-Cox transformations stabilize the variance and make each series relatively homoskedastic with equal variance.\n ARIMA Model The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.\n(fit1 \u0026lt;- forecast::auto.arima(ATM1_train, stepwise=F, approximation=F, d=0, lambda=lambda1)) Series: ATM1_train ARIMA(0,0,2)(1,1,1)[7] Box Cox transformation: lambda= 0.4355901 Coefficients: ma1 ma2 sar1 sma1 0.1449 -0.1116 0.1320 -0.7243 s.e. 0.0547 0.0537 0.0893 0.0669 sigma^2 estimated as 6.441: log likelihood=-770.86 AIC=1551.73 AICc=1551.92 BIC=1570.69 (fit2 \u0026lt;- forecast::auto.arima(ATM2_train, stepwise=F, approximation=F, d=0, lambda=lambda2)) Series: ATM2_train ARIMA(2,0,2)(0,1,1)[7] with drift Box Cox transformation: lambda= 0.7156895 Coefficients: ar1 ar2 ma1 ma2 sma1 drift -0.4282 -0.9254 0.4761 0.8044 -0.7672 -0.0246 s.e. 0.0464 0.0413 0.0764 0.0555 0.0483 0.0155 sigma^2 estimated as 66.34: log likelihood=-1152.83 AIC=2319.66 AICc=2320.01 BIC=2346.21 (fit4 \u0026lt;- forecast::auto.arima(ATM4_train, stepwise=F, approximation=F, d=0, lambda=lambda4)) Series: ATM4_train ARIMA(1,0,0)(2,0,0)[7] with non-zero mean Box Cox transformation: lambda= 0.3945256 Coefficients: ar1 sar1 sar2 mean 0.0814 0.2060 0.1911 22.7977 s.e. 0.0548 0.0537 0.0547 0.9477 sigma^2 estimated as 97.25: log likelihood=-1240.53 AIC=2491.06 AICc=2491.24 BIC=2510.13  Evaluate ACF and PACF\nACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation.\npar(mfrow=c(3, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) acf(residuals(fit1), ylab=\u0026quot;ACF ATM1\u0026quot;); pacf(residuals(fit1), ylab=\u0026quot;PACF ATM1\u0026quot;) acf(residuals(fit2), ylab=\u0026quot;ACF ATM2\u0026quot;); pacf(residuals(fit2), ylab=\u0026quot;PACF ATM2\u0026quot;) acf(residuals(fit4), ylab=\u0026quot;ACF ATM4\u0026quot;); pacf(residuals(fit4), ylab=\u0026quot;PACF ATM4\u0026quot;) Observations:\n The residuals of the models appear to display the characteristics of White Noise in the ACF and PACF plots with only one of the twenty residuals (or 0.05%) being significant. At a 95% confidence interval this is within probabilistic expectations.   Box-Ljung Test The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The arma attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,period,d,and D; in that order.\nBox.test(residuals(fit1), lag=7, fitdf=sum(fit1$arma[1:2]), type=\u0026quot;Ljung-Box\u0026quot;)  Box-Ljung test data: residuals(fit1) X-squared = 5.7195, df = 5, p-value = 0.3345 Box.test(residuals(fit2), lag=7, fitdf=sum(fit1$arma[1:2]), type=\u0026quot;Ljung-Box\u0026quot;)  Box-Ljung test data: residuals(fit2) X-squared = 7.9286, df = 5, p-value = 0.1602 Box.test(residuals(fit4), lag=7, fitdf=sum(fit1$arma[1:2]), type=\u0026quot;Ljung-Box\u0026quot;)  Box-Ljung test data: residuals(fit4) X-squared = 4.6833, df = 5, p-value = 0.4557 The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the models are not significantly different from zero at Œ±=0.05. The residuals of the models display the characteristics of White Noise. The models pass the required checks and are therefore suitable for forecasting.\n Forecasting ATM3 was not modeled due to its degenerative properties. To forecast values for ATM3, the model for an ATM with a similar mean will be used.\nc(mean(ATM1), mean(ATM2), mean(ATM3[ATM3!=0]), mean(ATM4)) [1] 84.15479 62.59178 87.66667 444.75681 The mean of ATM1 is very close to the mean of the few values in ATM3. Therefore, the ARIMA(0,0,1)(2,0,0)7 ARIMA model for ATM1 will be used to make predictions for ATM3.\nfit3 \u0026lt;- forecast::Arima(ATM3_train, model=fit1) Forecasts are done using the forecast::forecast() function. Since the data were not seasonally adjusted, they need not be reseasonalized prior to forecast. Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a red line.\nfcast1 \u0026lt;- forecast::forecast(fit1, h=30) fcast2 \u0026lt;- forecast::forecast(fit2, h=30) fcast3 \u0026lt;- forecast::forecast(fit3, h=30) fcast4 \u0026lt;- forecast::forecast(fit4, h=30) par(mfrow=c(4, 1), mar = c(0, 4, 0, 0), oma = c(4, 4, 2, 0.5)) plot(fcast1, ylab=\u0026quot;Cash ATM1\u0026quot;, main=\u0026quot;\u0026quot;, xaxt=\u0026quot;n\u0026quot;); lines(lag(ATM1_test, -length(ATM1_train)), col=\u0026quot;red\u0026quot;) plot(fcast2, ylab=\u0026quot;Cash ATM2\u0026quot;, main=\u0026quot;\u0026quot;, xaxt=\u0026quot;n\u0026quot;); lines(lag(ATM2_test, -length(ATM2_train)), col=\u0026quot;red\u0026quot;) plot(fcast3, ylab=\u0026quot;Cash ATM3\u0026quot;, main=\u0026quot;\u0026quot;, xaxt=\u0026quot;n\u0026quot;) lines(lag(ATM3_test, -length(ATM3_train)), col=\u0026quot;red\u0026quot;) plot(fcast4, ylab=\u0026quot;Cash ATM4\u0026quot;, main=\u0026quot;\u0026quot;, xaxt=\u0026quot;n\u0026quot;) lines(lag(ATM4_test, -length(ATM4_train)), col=\u0026quot;red\u0026quot;) title(\u0026quot;ATM Predictions\u0026quot;, outer=TRUE) Observations:\n The predictions appear to produce a useful forecasts that reflect patterns in the original data.   Model Accuracy The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).\nround(forecast::accuracy(fcast1, length(ATM1_test)), 3)  ME RMSE MAE MPE MAPE MASE ACF1 Training set 2.038 25.007 16.039 -96.186 114.754 0.427 0.011 Test set -53.187 53.187 53.187 -177.290 177.290 1.416 NA round(forecast::accuracy(fcast2, length(ATM2_test)), 3)  ME RMSE MAE MPE MAPE MASE ACF1 Training set 1.456 24.795 17.275 -Inf Inf 0.40 -0.013 Test set -44.485 44.485 44.485 -148.284 148.284 1.03 NA round(forecast::accuracy(fcast4, length(ATM4_test)), 3)  ME RMSE MAE MPE MAPE MASE ACF1 Training set 96.204 360.567 280.446 -342.343 388.847 0.72 0.017 Test set -385.878 385.878 385.878 -1286.261 1286.261 0.99 NA These accuracy for the predications vary. ATM1 and ATM2 predictions are more accurate than ATM4 predictions. The closer the original data are to being White Noise, the less accurate the predictions.\n ","date":1549497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549497600,"objectID":"a2c0e2ed0f1857386c7e9fb0252bcb11","permalink":"/post/atm/bank/","publishdate":"2019-02-07T00:00:00Z","relpermalink":"/post/atm/bank/","section":"post","summary":"Forcast monthly residential energy usage","tags":[],"title":"Bank ATM Cash Machine Forecast w/ Time Series","type":"post"},{"authors":null,"categories":["R","Machine Learning"],"content":" Preamble: This document focuses on the time series analysis. A simple dataset of residential power usage from January 1998 to December 2013.\n Research question: through an analysis, model this data and monthly forecast for 2014   Structure of analysis: Exploratory Data Analysis Visualizations ACF and PACF Clean The Data Trend Preview Data Decomposition Plot Stationarity Test Model Data Transformation ARIMA Model Evaluation Box-Ljung Test Forecasting Model Accuracy  sourceURL \u0026lt;- \u0026quot;https://raw.githubusercontent.com/jzuniga123\u0026quot; file \u0026lt;- \u0026quot;/SPS/master/DATA%20624/ResidentialCustomerForecastLoad-624.xlsx\u0026quot; download.file(paste0(sourceURL, file), \u0026quot;temp.xlsx\u0026quot;, mode=\u0026quot;wb\u0026quot;) energy \u0026lt;- xlsx::read.xlsx(\u0026quot;temp.xlsx\u0026quot;, sheetIndex=1, header=T) # the ‚ÄúYYYY-MMM‚Äù format dates are interpreted as factors. They must be converted to dates energy$YYYY.MMM \u0026lt;- as.Date(paste0(energy$YYYY.MMM,\u0026quot;-01\u0026quot;), format = \u0026quot;%Y-%b-%d\u0026quot;) invisible(file.remove(\u0026quot;temp.xlsx\u0026quot;))  Exploratory Data Analysis head(energy)  CaseSequence YYYY.MMM KWH 1 733 1998-01-01 6862583 2 734 1998-02-01 5838198 3 735 1998-03-01 5420658 4 736 1998-04-01 5010364 5 737 1998-05-01 4665377 6 738 1998-06-01 6467147 # preview the class of the dataset class(energy) [1] \u0026quot;data.frame\u0026quot; str(energy) \u0026#39;data.frame\u0026#39;: 192 obs. of 3 variables: $ CaseSequence: num 733 734 735 736 737 738 739 740 741 742 ... $ YYYY.MMM : Date, format: \u0026quot;1998-01-01\u0026quot; \u0026quot;1998-02-01\u0026quot; ... $ KWH : num 6862583 5838198 5420658 5010364 4665377 ... # preview descriptive statistics on quantitative and qualitative variables summary(energy)  CaseSequence YYYY.MMM KWH Min. :733.0 Min. :1998-01-01 Min. : 770523 1st Qu.:780.8 1st Qu.:2001-12-24 1st Qu.: 5429912 Median :828.5 Median :2005-12-16 Median : 6283324 Mean :828.5 Mean :2005-12-15 Mean : 6502475 3rd Qu.:876.2 3rd Qu.:2009-12-08 3rd Qu.: 7620524 Max. :924.0 Max. :2013-12-01 Max. :10655730 NA\u0026#39;s :1  # preview the periods between dates in dataset xts::periodicity(unique(energy$YYYY.MMM)) Monthly periodicity from 1998-01-01 to 2013-12-01  Dataframe spans monthly from January 1, 1998 to December 1, 2013.\n# preview observations in the dataframe that have no missing values energy[!complete.cases(energy), ]  CaseSequence YYYY.MMM KWH 129 861 2008-09-01 NA Dataframe contains one missing value in kWh usage.\n Visualizations # plots each observed value against the time of the observation, with a single line connecting each observation across the entire period kWh \u0026lt;- xts::xts(energy$KWH, order.by=energy$YYYY.MMM) par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) plot(kWh, main=\u0026quot;kWh\u0026quot;) # display frequency at which values in a vector occur hist(kWh, col=\u0026quot;yellow\u0026quot;, xlab=\u0026quot;\u0026quot;, main=\u0026quot;\u0026quot;) Obervations:\n Line plot and Histogram shows an outlier around the three-quarter mark of the series.   ACF and PACF par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) # ACF autocorrelations between each observation and its immediate predecessor (lagged observation) acf(na.omit(kWh), ylab=\u0026quot;kWh\u0026quot;, main=\u0026quot;\u0026quot;) # PACF autocorrelations between the current observation and each individual lagged observation pacf(na.omit(kWh), ylab=\u0026quot;kWh\u0026quot;, main=\u0026quot;\u0026quot;) Observations:\n ACF and PACF plots show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations.   Clean The Data # data cleaning w/ forecast::tsclean() and converted to a time series object using the ts(). # tsclean() function imputes nulls and removes outliers. # ts() function converts data to a time series object which is compatible with the forecast package. kWh \u0026lt;- ts(forecast::tsclean(energy$KWH, replace.missing=T), frequency = 12, start=start(energy$YYYY.MMM)) # data sampled monthly = 12 kWh[kWh==min(kWh)] \u0026lt;- mean(kWh[kWh!=min(kWh)])  Trend Preview A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.\nplot(kWh, col=8, xaxt = \u0026quot;n\u0026quot;, ylab=\u0026quot;ATM1\u0026quot;) lines(forecast::ma(kWh, order=6), col=6) # pink line biannual period lines(forecast::ma(kWh, order=12), col=4) # blue line annual period Observations:\n The 6-month and 12-month moving average smoother line shows that the data has a slight apparent trend.   Data Decomposition Plot Decomposes and plots the observed values, the underlying trend, seasonality, and randomness of the time series data.\nplot(decompose(kWh), col=5) Obseravations:\n Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have a slight apparent trend, seasonal fluctuations, and fairly random residuals.   Stationarity Test Dickey_Fuller Test An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.\ntseries::adf.test(kWh) Warning in tseries::adf.test(kWh): p-value smaller than printed p-value  Augmented Dickey-Fuller Test data: kWh Dickey-Fuller = -4.5454, Lag order = 5, p-value = 0.01 alternative hypothesis: stationary The augmented Dickey-Fuller unit root test p-value is below Œ±=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when œï\u0026lt;1. When œï=1 the AR(1) model is like a first-order difference.\n  Model Data The train and test sets are created by referencing rows w/ index. The indexed rows for the testing set are a window at the end of the times series. The window sized for the test set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.\nindex_train \u0026lt;- 1:(length(kWh) - 12) kWh_train \u0026lt;- ts(kWh[index_train], frequency=12) kWh_test \u0026lt;- ts(kWh[index_train], frequency=12)  Transformation The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted. Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.\n(lambda \u0026lt;- forecast::BoxCox.lambda(kWh_train)) [1] -0.1733063 The Box-Cox transformation parameter suggested is about Œª=‚àí0.25. This rounded (slightly more interpretable) value is suggestive of an inverse quartic root. This Box-Cox transformation stabilizes the variance and makes the series relatively homoskedastic with equal variance.\n ARIMA Model The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.\n(fit \u0026lt;- forecast::auto.arima(kWh_train, stepwise=F, approximation=F, d=0, lambda=lambda)) Series: kWh_train ARIMA(0,0,3)(2,1,0)[12] with drift Box Cox transformation: lambda= -0.1733063 Coefficients: ma1 ma2 ma3 sar1 sar2 drift 0.2807 0.0855 0.2232 -0.7724 -0.4408 1e-04 s.e. 0.0757 0.0823 0.0687 0.0742 0.0812 1e-04 sigma^2 estimated as 3.707e-05: log likelihood=621.98 AIC=-1229.95 AICc=-1229.25 BIC=-1208.08 The auto.arima() function suggests an ARIMA(0,0,3)(2,1,0)12 model.\n Evaluation par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5)) acf(residuals(fit), ylab=\u0026quot;ACF kWh\u0026quot;); pacf(residuals(fit), ylab=\u0026quot;PACF kWh\u0026quot;) Observations:\n The residuals of the model appear to display the characteristics of White Noise in both the ACF and PACF plots. None of the residuals are significant. At a 95% confidence interval this is well within probabilistic expectations.   Box-Ljung Test The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The ARIMA attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,periods,d and D; in that order.\nBox.test(residuals(fit), lag=7, fitdf=sum(fit$arma[1:2]), type=\u0026quot;Ljung-Box\u0026quot;)  Box-Ljung test data: residuals(fit) X-squared = 7.2523, df = 4, p-value = 0.1231 The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the model are not significantly different from zero at Œ±=0.05. The residuals of the model displays the characteristics of White Noise. The model passes the required checks and is therefore suitable for forecasting.\n Forecasting Forecasts are done using the forecast::forecast() function. Since the data was not seasonally adjusted, they need not be reseasonalized prior to forecast.\nfcast \u0026lt;- forecast::forecast(fit, h=15) plot(fcast, ylab=\u0026quot;kWh\u0026quot;, main=\u0026quot;kWh Predictions\u0026quot;, xaxt=\u0026quot;n\u0026quot;) lines(lag(kWh_test, -length(kWh_train)), col=6) Observations:\n The prediction appears to produce a useful forecasts that reflect patterns in the original data. Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a pink line.   Model Accuracy The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).\nround(forecast::accuracy(fcast, length(kWh_test)), 3)  ME RMSE MAE MPE MAPE Training set 39449.18 581186.1 456353.6 0.056 7.067 Test set -9046871.23 9046871.2 9046871.2 -5026039.573 5026039.573 MASE ACF1 Training set 0.413 0.115 Test set 8.185 NA These accuracy for the predications is fair. The large metrics are representative of the large values found in the data.\n ","date":1549497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549497600,"objectID":"a5a8d9f54988c0f7a79f7e1927bd9ad2","permalink":"/post/residential_energy/residential-energy-usage/","publishdate":"2019-02-07T00:00:00Z","relpermalink":"/post/residential_energy/residential-energy-usage/","section":"post","summary":"Forcast monthly residential energy usage","tags":[],"title":"Energy Forecasting w/ Time Series Analysis","type":"post"},{"authors":null,"categories":["R"],"content":" Preamble: The dataset is available at [https://archive.ics.uci.edu/ml/datasets/Energy+efficiency].\n Reseach questions: to explore three data points, and visualize how they influence the energy load.  The following variables Wall.Area, Roof.Area, Glazing.Area are identified as key indicators that can influence the energy load efficiency for both (Heating and Cooling spaces).\n Structure of analysis: A time series forecast using the arima model as follows:\nExploratory Data Analysis Plot Load Distribution Using Scatter Plot Plot Heating Load Efficiency Plot Cooling Load Efficiency Plot Energy Efficiency  rm(list = ls()) sourceURL \u0026lt;- \u0026quot;https://raw.githubusercontent.com/StephenElston/DataScience350/master/Lecture1/EnergyEfficiencyData.csv\u0026quot; df \u0026lt;- read.csv( sourceURL, header = TRUE)  Exploratory Data Analysis head(df)  Relative.Compactness Surface.Area Wall.Area Roof.Area Overall.Height 1 0.98 514.5 294.0 110.25 7 2 0.98 514.5 294.0 110.25 7 3 0.98 514.5 294.0 110.25 7 4 0.98 514.5 294.0 110.25 7 5 0.90 563.5 318.5 122.50 7 6 0.90 563.5 318.5 122.50 7 Orientation Glazing.Area Glazing.Area.Distribution Heating.Load 1 2 0 0 15.55 2 3 0 0 15.55 3 4 0 0 15.55 4 5 0 0 15.55 5 2 0 0 20.84 6 3 0 0 21.46 Cooling.Load 1 21.33 2 21.33 3 21.33 4 21.33 5 28.28 6 25.38 str(df) \u0026#39;data.frame\u0026#39;: 768 obs. of 10 variables: $ Relative.Compactness : num 0.98 0.98 0.98 0.98 0.9 0.9 0.9 0.9 0.86 0.86 ... $ Surface.Area : num 514 514 514 514 564 ... $ Wall.Area : num 294 294 294 294 318 ... $ Roof.Area : num 110 110 110 110 122 ... $ Overall.Height : num 7 7 7 7 7 7 7 7 7 7 ... $ Orientation : int 2 3 4 5 2 3 4 5 2 3 ... $ Glazing.Area : num 0 0 0 0 0 0 0 0 0 0 ... $ Glazing.Area.Distribution: int 0 0 0 0 0 0 0 0 0 0 ... $ Heating.Load : num 15.6 15.6 15.6 15.6 20.8 ... $ Cooling.Load : num 21.3 21.3 21.3 21.3 28.3 ... Categorize useful variables and convert them to a categorical variables, namely Orientation, Glazing.Area.Distribution, and Glazing.Area (variance) variables.\n# change vector values to factor values df$Orientation \u0026lt;- as.factor(df$Orientation) # attributes of variable levels(df$Orientation) \u0026lt;- c(\u0026quot;North\u0026quot;, \u0026quot;East\u0026quot;, \u0026quot;South\u0026quot;, \u0026quot;West\u0026quot;) # change vector values to factor values df$Glazing.Area.Distribution \u0026lt;- as.factor(df$Glazing.Area.Distribution) # attributes of variable levels(df$Glazing.Area.Distribution) \u0026lt;- c(\u0026quot;UnKnown\u0026quot;, \u0026quot;Uniform\u0026quot;, \u0026quot;North\u0026quot;, \u0026quot;East\u0026quot;, \u0026quot;South\u0026quot;, \u0026quot;West\u0026quot;) # change vector values to factor values df$Glazing.Area \u0026lt;- as.factor(df$Glazing.Area) # attributes of variable levels(df$Glazing.Area) \u0026lt;- c(\u0026quot;0%\u0026quot;, \u0026quot;10%\u0026quot;, \u0026quot;25%\u0026quot;, \u0026quot;40%\u0026quot;) summary(df)  Relative.Compactness Surface.Area Wall.Area Roof.Area Min. :0.6200 Min. :514.5 Min. :245.0 Min. :110.2 1st Qu.:0.6825 1st Qu.:606.4 1st Qu.:294.0 1st Qu.:140.9 Median :0.7500 Median :673.8 Median :318.5 Median :183.8 Mean :0.7642 Mean :671.7 Mean :318.5 Mean :176.6 3rd Qu.:0.8300 3rd Qu.:741.1 3rd Qu.:343.0 3rd Qu.:220.5 Max. :0.9800 Max. :808.5 Max. :416.5 Max. :220.5 Overall.Height Orientation Glazing.Area Glazing.Area.Distribution Min. :3.50 North:192 0% : 48 UnKnown: 48 1st Qu.:3.50 East :192 10%:240 Uniform:144 Median :5.25 South:192 25%:240 North :144 Mean :5.25 West :192 40%:240 East :144 3rd Qu.:7.00 South :144 Max. :7.00 West :144 Heating.Load Cooling.Load Min. : 6.01 Min. :10.90 1st Qu.:12.99 1st Qu.:15.62 Median :18.95 Median :22.08 Mean :22.31 Mean :24.59 3rd Qu.:31.67 3rd Qu.:33.13 Max. :43.10 Max. :48.03   Plot Load Distribution Using Scatter Plot # visualize if there is any relation between \u0026#39;Roof.Area\u0026#39;, \u0026#39;Surface.Area\u0026#39; and \u0026#39;Glazing.Area\u0026#39; and how load is distributed using scatter plot. ggplot(df, aes(x = Cooling.Load, y = Heating.Load), alpha = 0.5)+ geom_point(aes(colour = Roof.Area ))+ facet_grid(Overall.Height + Glazing.Area ~ Surface.Area, space = \u0026quot;free\u0026quot;) + ggtitle(\u0026quot;Load distribuiton of energy by Roof Area and Surface Area \\n by Glazing Area and Overall Height\u0026quot;) Observations:\n Roof area and Surface area range is high for minimum/ lowest (3.5) over-all height and Roof area and Surface area range is low for maximum/ highest (7.0) over-all height. There are no data points when the overall height is 7 and highest surface area range and also for low overall height 3.5, we have no data points with the low surface area range.   Plot Heating Load Efficiency # plot how \u0026#39;Wall.Area\u0026#39; influence heating load using raster plot. ggplot(df, aes( Surface.Area, Roof.Area)) + geom_raster(aes(fill = Heating.Load), interpolate = TRUE) + scale_fill_gradient(low = \u0026quot;steelblue\u0026quot;, high = \u0026quot;red\u0026quot;)+ facet_wrap(~Wall.Area, scales = \u0026quot;free\u0026quot; )+ ggtitle(\u0026#39;Measuring Heating Load distribution \\n by Wall Area, Surface Area and Roof Area\u0026#39;) + xlab(\u0026#39;Surface Area\u0026#39;) + ylab(\u0026#39;Roof Area\u0026#39;) + theme_classic() Observations:\n By looking at the figures, we can conclude that Wall Area plays a significant role in heating, irrespective of Surface Area and Roof Area. (Higher the wall area, higher the heating load).   Plot Cooling Load Efficiency # plot how \u0026#39;Wall.Area\u0026#39; influence cooling load using raster plot. ggplot(df, aes(Surface.Area, Roof.Area)) + geom_raster(aes(fill = Cooling.Load), interpolate = TRUE) + scale_fill_gradient(low = \u0026quot;grey\u0026quot;, high = \u0026quot;steelblue\u0026quot;)+ facet_wrap(~Wall.Area, scales = \u0026quot;free\u0026quot; )+ ggtitle(\u0026#39;Measuring Cooling Load distribution \\n by Wall Area, Surface Area and Roof Area\u0026#39;) + xlab(\u0026#39;Surface Area\u0026#39;) + ylab(\u0026#39;Roof Area\u0026#39;) + theme_classic() Observations:\n So, Wall Area plays a significant role in both Heating and Cooling Load efficiency.   Plot Energy Efficiency # we have seen more variation in load data when the overall height is (7.0). So lets create a subset named(energy.eff.sub7.0) which contains the filtered data with overall height = 7.0. Lets visualize, if the \u0026#39;Roof.Area\u0026#39;, \u0026#39;Wall.Area\u0026#39;, \u0026#39;Surface.Area\u0026#39; and \u0026#39;Glazing.Area\u0026#39; are influencing the load efficiency. energy.eff.sub7.0 \u0026lt;- df[ df$Overall.Height ==7.0,] ggplot(energy.eff.sub7.0, aes(x = Cooling.Load, y = Heating.Load, group = factor(round(Wall.Area)), size = Glazing.Area, shape = factor(round(Wall.Area))))+ geom_point(aes(colour= factor(round(Surface.Area))), alpha = 0.3)+ geom_smooth(method = \u0026quot;lm\u0026quot;,se = TRUE )+ facet_wrap(~ Roof.Area) + ggtitle(\u0026#39;Load efficiency by Roof Area, by Wall Area by Surface Area and by Glazing Area\u0026#39;) + theme_classic() Observations:\n It is clearly evident that the Load efficiency is influenced by the Roof Area, Wall Area, Surface Area, and Glazing Area. When the Glazing Area is high, Roof Area is high and Wall Area is high, Load will be high and viceversa.   ","date":1549152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549152000,"objectID":"0ff3837cbb34de66e054c571c202a4fe","permalink":"/post/energye/energye/","publishdate":"2019-02-03T00:00:00Z","relpermalink":"/post/energye/energye/","section":"post","summary":"Energy Efficiency On Buildings","tags":[],"title":"Energy Efficiency on Buildings","type":"post"},{"authors":null,"categories":["Machine Learning","R"],"content":" References:  R. H. Shumway, D. S. Stoffer. Time Series Analysis and Its Applications. 2010. R. J. Hyndman. Forecasting: principles and practice. 2013. P. S. P. Cowpertwait, A. V. Metcalfe. Introductory Time Series with R. 2009.  Preamble: This document focuses on an analysis of the energy demands of a European country.\nThe dataset of the daily energy needs (in GWh) between 2004 and 2010.\n Reseach questions: build a model for energy demand forecasting using time series analysis.   Structure of analysis: A time series forecast using the arima model as follows:\nExploratory data analysis Data decomposition seasonal ARIMA model Forecast model  sourceURL \u0026lt;- \u0026#39;https://gist.githubusercontent.com/Peque/715e91350f0e68e3342f/raw/d28312ac0e49888a5079fcea188770acaf3aa4a2/mme.csv\u0026#39; # download and load data into memory tmp \u0026lt;- tempfile() download.file(sourceURL, tmp, method = \u0026#39;curl\u0026#39;) df \u0026lt;- read.csv(tmp) unlink(tmp)  Exploratory data analysis head(df)  date demand 1 01-01-04 488.07 2 02-01-04 582.02 3 03-01-04 575.58 4 04-01-04 542.39 5 05-01-04 600.26 6 06-01-04 544.76 # convert date strings to POSIX dates df$date \u0026lt;- strptime(df$date, format = \u0026#39;%d-%m-%y\u0026#39;) # day of week df$day \u0026lt;- as.factor(strftime(df$date, format = \u0026#39;%A\u0026#39;)) # day of year df$yearday \u0026lt;- as.factor(strftime(df$date, format = \u0026#39;%m%d\u0026#39;)) # structure for analysis str(df) \u0026#39;data.frame\u0026#39;: 3288 obs. of 4 variables: $ date : POSIXlt, format: \u0026quot;2004-01-01\u0026quot; \u0026quot;2004-01-02\u0026quot; ... $ demand : num 488 582 576 542 600 ... $ day : Factor w/ 7 levels \u0026quot;Friday\u0026quot;,\u0026quot;Monday\u0026quot;,..: 5 1 3 4 2 6 7 5 1 3 ... $ yearday: Factor w/ 366 levels \u0026quot;0101\u0026quot;,\u0026quot;0102\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... # df split to create test set df_test \u0026lt;- subset(df, date \u0026gt;= strptime(\u0026#39;01-01-2011\u0026#39;, format = \u0026#39;%d-%m-%Y\u0026#39;)) df \u0026lt;- subset(df, date \u0026lt; strptime(\u0026#39;01-01-2011\u0026#39;, format = \u0026#39;%d-%m-%Y\u0026#39;)) ts \u0026lt;- ts(df$demand, frequency = 1) # df and time series objects demandts \u0026lt;- xts(df$demand, df$date) plot(demandts, main = \u0026#39;Energy Demand Preview\u0026#39;, xlab = \u0026#39;Time\u0026#39;, ylab = \u0026#39;Demand (GWh)\u0026#39;) Observations:\n A seasonal dependency of demand can be easily spotted in the graphics, although there are other factors that may affect the results, such as the temperature, holidays, weekends, etc..  # demand by day of the week ggplot(df, aes(day, demand)) + geom_boxplot(fill=\u0026#39;slateblue\u0026#39;, alpha=0.2) + xlab(\u0026#39;Time\u0026#39;) + ylab(\u0026#39;Demand (GWh)\u0026#39;) + ggtitle(\u0026#39;Demand per day of the week\u0026#39;) + theme_classic() Observations:\n During weekends, the demand decreases considerably compared to the rest of the week days.  # aggregating demand by day of the year (average) avg_demand_per_yearday \u0026lt;- aggregate(demand ~ yearday, df, \u0026#39;mean\u0026#39;) # computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity smooth_yearday \u0026lt;- rbind(avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday) smooth_yearday \u0026lt;- lowess(smooth_yearday$demand, f = 1 / 45) l \u0026lt;- length(avg_demand_per_yearday$demand) l0 \u0026lt;- 2 * l + 1 l1 \u0026lt;- 3 * l smooth_yearday \u0026lt;- smooth_yearday$y[l0:l1] # plotting results par(mfrow = c(1, 1)) # setting year to 2000 to allow existence of 29th February dates \u0026lt;- as.Date(paste(levels(df$yearday), \u0026#39;2000\u0026#39;), format = \u0026#39;%m%d%Y\u0026#39;) plot(dates, avg_demand_per_yearday$demand, type = \u0026#39;l\u0026#39;, main = \u0026#39;Average Daily Demand\u0026#39;, xlab = \u0026#39;Time\u0026#39;, ylab = \u0026#39;Demand (GWh)\u0026#39;) lines(dates, smooth_yearday, col = \u0026#39;yellow\u0026#39;, lwd = 2) Observations:\n During the winter \u0026amp; summer seasons the demand is clearly higher exept for, vacation periods. Holydays are also easily spotted in the graphics, being the lowest peaks of demand.  par(mfrow = c(1, 2)) diff \u0026lt;- avg_demand_per_yearday$demand - smooth_yearday abs_diff \u0026lt;- abs(diff) barplot(diff[order(-abs_diff)], main = \u0026#39;Smoothing error\u0026#39;, ylab = \u0026#39;Error\u0026#39;) boxplot(diff, main = \u0026#39;Smoothing error\u0026#39;, ylab = \u0026#39;Error\u0026#39;) Observations:\n The graphics show the errors. Notice how the biggest errors are all negative.  head(strftime(dates[order(-abs_diff)], format = \u0026#39;%B %d\u0026#39;), 10)  [1] \u0026quot;January 01\u0026quot; \u0026quot;December 25\u0026quot; \u0026quot;May 01\u0026quot; \u0026quot;January 06\u0026quot; \u0026quot;August 15\u0026quot; [6] \u0026quot;December 08\u0026quot; \u0026quot;December 31\u0026quot; \u0026quot;October 12\u0026quot; \u0026quot;November 01\u0026quot; \u0026quot;December 26\u0026quot; The exact dates which are generating these errors are indeed, holidays or the day just before holidays (as is the case for the 25th November and 31th Devember).\npar(mfrow = c(2, 2)) acf(df$demand, 100, main = \u0026#39;Autocorrelation\u0026#39;) acf(df$demand, 1500, main = \u0026#39;Autocorrelation\u0026#39;) pacf(df$demand, 100, main = \u0026#39;Partial autocorrelation\u0026#39;) pacf(df$demand, 1500, main = \u0026#39;Partial autocorrelation\u0026#39;) Observations:\n The autocorrelation function shows a highly autocorrelated seasonal non-stationary process with, as expected, yearly and weekly cicles. The ACF alone, however, tells us little about the orders of dependence for ARMIA or AR processes. The PACF is better for AR models, and also shows the weekly and yearly seasons, although the correlation is lost faster with the lag.   Data decomposition I‚Äôll decompose the time series for estimates of trend, seasonal, and random components using moving average method.\nThe model is:\nY[t]=T[t]‚àóS[t]‚àóe[t]\nwhere\nY(t) is the number of weeks at time t, T(t) is the trend component at time t, S(t) is the seasonal component at time t, e(t) is the random error component at time t.\n# decomposition of weekly seasonal time series wts \u0026lt;- ts(ts, frequency = 7) dec_wts \u0026lt;- decompose(wts) plot(dec_wts) # demand minus week seasonal df$demand_mws \u0026lt;- df$demand - as.numeric(dec_wts$season) # decomposition of yearly time series yts \u0026lt;- ts(subset(df, yearday != \u0026#39;0229\u0026#39;)$demand_mws, frequency = 365) dec_yts \u0026lt;- decompose(yts) plot(dec_yts) Observations:\n Decomposition of the yearly seasonal time series. 29th February days are excluded for frequency matching. The time series is formed out of the original observation minus the weekly seasonal data.  days365 \u0026lt;- which(df$yearday != \u0026#39;0229\u0026#39;) february29ths \u0026lt;- which(df$yearday == \u0026#39;0229\u0026#39;) df$demand_mwys[days365] \u0026lt;- df$demand_mws[days365] - as.numeric(dec_yts$season) # Fill values on February 29th df$demand_mwys[february29ths] \u0026lt;- df$demand_mws[february29ths] # form new ts from original observations less the weekly and yearly seasonal data par(mfrow = c(1, 1)) ts_mwys \u0026lt;- ts(df$demand_mwys, frequency = 1) demandts_mwys \u0026lt;- xts(df$demand_mwys, df$date) plot(demandts_mwys, main = \u0026#39;Energy Demand Less Seasonal Data\u0026#39;, xlab = \u0026#39;Time\u0026#39;, ylab = \u0026#39;Demand (GWh)\u0026#39;) # aggregating demand by day of the year (average) avg_demand_mwys_per_yearday \u0026lt;- aggregate(demand_mwys ~ yearday, df, \u0026#39;mean\u0026#39;) # computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity smooth_yearday \u0026lt;- rbind(avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday) smooth_yearday \u0026lt;- lowess(smooth_yearday$demand_mwys, f = 1 / 45) l \u0026lt;- length(avg_demand_mwys_per_yearday$demand_mwys) l0 \u0026lt;- 2 * l + 1 l1 \u0026lt;- 3 * l smooth_yearday \u0026lt;- smooth_yearday$y[l0:l1] # plotting the result par(mfrow = c(1, 1)) # setting year to 2000 to allow existence of 29th February dates \u0026lt;- as.Date(paste(levels(df$yearday), \u0026#39;2000\u0026#39;), format = \u0026#39;%m%d%Y\u0026#39;) plot(dates, avg_demand_mwys_per_yearday$demand_mwys, type = \u0026#39;l\u0026#39;, main = \u0026#39;Mean Daily Demand\u0026#39;, xlab = \u0026#39;Time\u0026#39;, ylab = \u0026#39;Demand (GWh)\u0026#39;) lines(dates, smooth_yearday, col = \u0026#39;yellow\u0026#39;, lwd = 2) par(mfrow = c(1, 2)) diff \u0026lt;- avg_demand_mwys_per_yearday$demand_mwys - smooth_yearday abs_diff \u0026lt;- abs(diff) barplot(diff[order(-abs_diff)], main = \u0026#39;Smoothing error\u0026#39;, ylab = \u0026#39;Error\u0026#39;) boxplot(diff, main = \u0026#39;Smoothing error\u0026#39;, ylab = \u0026#39;Error\u0026#39;) Observations:\n Plotting the average daily demand of the demand less the seasonal data shows a new error rate much lower than the one seen before.  # new acf and pacf created par(mfrow = c(1, 2)) acf(df$demand_mwys, 100, main = \u0026#39;Autocorrelation\u0026#39;) pacf(df$demand_mwys, 100, main = \u0026#39;Partial autocorrelation\u0026#39;)  seasonal ARIMA model The initial ARIMA parameters have been found using the R \\(auto.arima()\\) function. The differencing parameter \\(d\\) is selected using the KPSS test. If the null hypothesis of stationarity is accepted when the KPSS is applied to the original time series, then \\(d = 0\\). Otherwise, the series is differenced until the KPSS accepts the null hypothesis. After that, \\(p\\) and \\(q\\) are selected using either AIC or BIC. The SARIMA model has been created using those ARIMA parameters.\nmodel \u0026lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7)) # forecast the error w/ test dataframe auxts \u0026lt;- ts auxmodel \u0026lt;- model errs \u0026lt;- c() pred \u0026lt;- c() perc \u0026lt;- c() for (i in 1:nrow(df_test)) { p \u0026lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred) pred \u0026lt;- c(pred, p) errs \u0026lt;- c(errs, p - df_test$demand[i]) perc \u0026lt;- c(perc, (p - df_test$demand[i]) / df_test$demand[i]) auxts \u0026lt;- ts(c(auxts, df_test$demand[i]), frequency = 7) auxmodel \u0026lt;- Arima(auxts, model = auxmodel) } par(mfrow = c(1, 1)) plot(errs, type = \u0026#39;l\u0026#39;, main = \u0026#39;Error in the forecast\u0026#39;) plot(pred, type = \u0026#39;l\u0026#39;, main = \u0026#39;Real vs. Forecast\u0026#39;, col = \u0026#39;green\u0026#39;) lines(df_test$demand) legend(\u0026#39;topright\u0026#39;, c(\u0026#39;Real\u0026#39;, \u0026#39;Forecast\u0026#39;), lty = 1, col = c(\u0026#39;black\u0026#39;, \u0026#39;green\u0026#39;)) abserr \u0026lt;- mean(abs(errs)) percerr \u0026lt;- mean(abs(perc)) * 100 percerr [1] 2.299037 Mean error across test datadrame (2.3%).\n# special days present less demand than others. Those days may be taken into account in order to reduce the error specialday \u0026lt;- function(day) { correction = 0 if (format(day, \u0026#39;%m%d\u0026#39;) %in% c(\u0026#39;0101\u0026#39;, \u0026#39;0501\u0026#39;, \u0026#39;0106\u0026#39;, \u0026#39;0815\u0026#39;, \u0026#39;1012\u0026#39;, \u0026#39;1101\u0026#39;, \u0026#39;1206\u0026#39;, \u0026#39;1208\u0026#39;, \u0026#39;1224\u0026#39;, \u0026#39;1225\u0026#39;, \u0026#39;1226\u0026#39;, \u0026#39;1231\u0026#39;)) correction = -100 else if (format(day, \u0026#39;%m%d\u0026#39;) %in% c(\u0026#39;0319\u0026#39;)) correction = -50 # on Sunday, do not apply correction if (as.factor(strftime(day, format = \u0026#39;%A\u0026#39;)) == \u0026#39;Sunday\u0026#39;) return(0) return(correction) } model \u0026lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7)) auxts \u0026lt;- ts auxmodel \u0026lt;- model errs \u0026lt;- c() pred \u0026lt;- c() perc \u0026lt;- c() for (i in 1:nrow(df_test)) { p \u0026lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred) correction = specialday(df_test$date[i]) pred \u0026lt;- c(pred, p + correction) errs \u0026lt;- c(errs, p + correction - df_test$demand[i]) perc \u0026lt;- c(perc, (p + correction - df_test$demand[i]) / df_test$demand[i]) if (!correction) auxts \u0026lt;- ts(c(auxts, df_test$demand[i]), frequency = 7) else auxts \u0026lt;- ts(c(auxts, p), frequency = 7) auxmodel \u0026lt;- Arima(auxts, model = auxmodel) } par(mfrow = c(1, 1)) plot(errs, type = \u0026#39;l\u0026#39;, main = \u0026#39;Error in the forecast\u0026#39;) plot(pred, type = \u0026#39;l\u0026#39;, main = \u0026#39;Real vs. Forecast\u0026#39;, col = \u0026#39;green\u0026#39;) lines(df_test$demand) legend(\u0026#39;topright\u0026#39;, c(\u0026#39;Real\u0026#39;, \u0026#39;Forecast\u0026#39;), lty = 1, col = c(\u0026#39;black\u0026#39;, \u0026#39;green\u0026#39;)) abserr \u0026lt;- mean(abs(errs)) percerr \u0026lt;- mean(abs(perc)) * 100 percerr [1] 1.956568 Mean error across test dataframe (1,96%).\n Forecast Model autoplot(forecast(Arima(tail(ts, 200), model = model))) + labs(x=\u0026quot;Time\u0026quot;, y=\u0026quot;Energy Demand (GWh)\u0026quot;) + theme_classic()   ","date":1549065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549065600,"objectID":"5f710491d3d7c56f8e0b5aba7efa412b","permalink":"/post/energyd/energy-demand-analysis-w-time-series-forecasting/","publishdate":"2019-02-02T00:00:00Z","relpermalink":"/post/energyd/energy-demand-analysis-w-time-series-forecasting/","section":"post","summary":"Energy Demand Forecasting","tags":[],"title":"Energy Demand Analysis w/ Time Series Forecasting","type":"post"},{"authors":null,"categories":["Machine Learning","R"],"content":" Preamble: This document focuses on the analysis of the airpassengers dataframe.\nThe AirPassenger dataset in R provides monthly totals of US airline passengers, from 1949 to 1960.\nDescription of dataframe airpassengers can be found at https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html\n Research question: through analysis and modelling, preview a time series forecast   Structure of analysis: I will asssess whether a linear regression or arima model is a best fit for the time series forecast as follows:\nExploratory data analysis Data decomposition Stationarity test Fit a model using an algorithm Forecasting  data(AirPassengers) AP \u0026lt;- AirPassengers # Take a look at the class of the dataset AirPassengers class(AP) ## [1] \u0026quot;ts\u0026quot; The dataset is already of a time series class.\nExploratory data analysis # preview of data AP ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 112 118 132 129 121 135 148 148 136 119 104 118 ## 1950 115 126 141 135 125 149 170 170 158 133 114 140 ## 1951 145 150 178 163 172 178 199 199 184 162 146 166 ## 1952 171 180 193 181 183 218 230 242 209 191 172 194 ## 1953 196 196 236 235 229 243 264 272 237 211 180 201 ## 1954 204 188 235 227 234 264 302 293 259 229 203 229 ## 1955 242 233 267 269 270 315 364 347 312 274 237 278 ## 1956 284 277 317 313 318 374 413 405 355 306 271 306 ## 1957 315 301 356 348 355 422 465 467 404 347 305 336 ## 1958 340 318 362 348 363 435 491 505 404 359 310 337 ## 1959 360 342 406 396 420 472 548 559 463 407 362 405 ## 1960 417 391 419 461 472 535 622 606 508 461 390 432 Passenger numbers in (‚Äô000) per month for the relevant years.\n# test for missing values sum(is.na(AP)) ## [1] 0 Zero missing values GREAT!\n# test frequency frequency(AP) ## [1] 12 12 calendar months.\n# test cycle cycle(AP) ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1949 1 2 3 4 5 6 7 8 9 10 11 12 ## 1950 1 2 3 4 5 6 7 8 9 10 11 12 ## 1951 1 2 3 4 5 6 7 8 9 10 11 12 ## 1952 1 2 3 4 5 6 7 8 9 10 11 12 ## 1953 1 2 3 4 5 6 7 8 9 10 11 12 ## 1954 1 2 3 4 5 6 7 8 9 10 11 12 ## 1955 1 2 3 4 5 6 7 8 9 10 11 12 ## 1956 1 2 3 4 5 6 7 8 9 10 11 12 ## 1957 1 2 3 4 5 6 7 8 9 10 11 12 ## 1958 1 2 3 4 5 6 7 8 9 10 11 12 ## 1959 1 2 3 4 5 6 7 8 9 10 11 12 ## 1960 1 2 3 4 5 6 7 8 9 10 11 12 # dataset summary summary(AP) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 104.0 180.0 265.5 280.3 360.5 622.0 Statistical values.\n# plot the raw data using the base plot function autoplot(AP) + labs(x=\u0026quot;Time\u0026quot;, y =\u0026quot;Passenger numbers (\u0026#39;000)\u0026quot;, title=\u0026quot;Air Passengers from 1949 to 1961\u0026quot;) + theme_classic() boxplot(AP~cycle(AP), xlab=\u0026quot;Passenger Numbers (\u0026#39;000)\u0026quot;, ylab=\u0026quot;Months\u0026quot;, col=rgb(0.1,0.9,0.3,0.4), main=\u0026quot;Monthly Air Passengers Boxplot from 1949 to 1961\u0026quot;, horizontal=TRUE, notch=FALSE) Observations:\n The passenger numbers increase over time with each year which may be indicative of an increasing linear trend. Possible due to an increase in demand for flights and commercialisation of airlines in that time period. The boxplot shows more passengers travelling in months 6 to 9 with higher averages and higher variances than the other months, indicating seasonality within an apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US. The dataset appears to be a multiplicative time series, since passenger numbers increase, with a pattern of seasonality. There do not appear to be any outliers and there are no missing values.   Data decomposition I‚Äôll decompose the time series for estimates of trend, seasonal, and random components using moving average method.\nThe multiplicative model is:\nY[t]=T[t]‚àóS[t]‚àóe[t]\nwhere\nY(t) is the number of passengers at time t, T(t) is the trend component at time t, S(t) is the seasonal component at time t, e(t) is the random error component at time t.\ndecomposeAP \u0026lt;- decompose(AP,\u0026quot;multiplicative\u0026quot;) autoplot(decomposeAP) + theme_classic() Observations:\n In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the ‚Äúremainder‚Äù.   Stationarity test A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. I‚Äôll use two methods to test the stationarity.\nTest stationarity of the time series (ADF)  In order to test the stationarity of the time series, let‚Äôs run the Augmented Dickey-Fuller (ADF) Test. using the adf.test function from the tseries R package.\nFirst set the hypothesis test:\nThe null hypothesis: that the time series is non stationary The alternative hypothesis: that the time series is stationary\nadf.test(AP) ## Warning in adf.test(AP): p-value smaller than printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: AP ## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary As a rule of thumb, where the p-value is less than 5%, we reject the null hypothesis. As the p-value is 0.01 which is less than 0.05 we reject the null in favour of the alternative hypothesis that the time series is stationary.\nTest stationarity of the time series (Autocorrelation)  Another way to test for stationarity is to use autocorrelation. I‚Äôll use autocorrelation function (acf). This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.\nautoplot(acf(AP, plot=FALSE)) + labs(title=\u0026quot;Correlogram of Air Passengers from 1949 to 1961\u0026quot;) + theme_classic() Observations:\n The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.  Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.\n# review random time series for any missing values decomposeAP$random  ## Jan Feb Mar Apr May Jun Jul ## 1949 NA NA NA NA NA NA 0.9516643 ## 1950 0.9626030 1.0714668 1.0374474 1.0140476 0.9269030 0.9650406 0.9835566 ## 1951 1.0138446 1.0640180 1.0918541 1.0176651 1.0515825 0.9460444 0.9474041 ## 1952 1.0258814 1.0939696 1.0134734 0.9695596 0.9632673 1.0003735 0.9468562 ## 1953 0.9976684 1.0151646 1.0604644 1.0802327 1.0413329 0.9718056 0.9551933 ## 1954 0.9829785 0.9232032 1.0044417 0.9943899 1.0119479 0.9978740 1.0237753 ## 1955 1.0154046 0.9888241 0.9775844 1.0015732 0.9878755 1.0039635 1.0385512 ## 1956 1.0066157 0.9970250 0.9876248 0.9968224 0.9985644 1.0275560 1.0217685 ## 1957 0.9937293 0.9649918 0.9881769 0.9867637 0.9924177 1.0328601 1.0261250 ## 1958 0.9954212 0.9522762 0.9469115 0.9383993 0.9715785 1.0261340 1.0483841 ## 1959 0.9825176 0.9505736 0.9785278 0.9746440 1.0177637 0.9968613 1.0373136 ## 1960 1.0039279 0.9590794 0.8940857 1.0064948 1.0173588 1.0120790 NA ## Aug Sep Oct Nov Dec ## 1949 0.9534014 1.0022198 1.0040278 1.0062701 1.0118119 ## 1950 0.9733720 1.0225047 0.9721928 0.9389527 1.0067914 ## 1951 0.9397599 0.9888637 0.9938809 1.0235337 1.0250824 ## 1952 0.9931171 0.9746302 1.0046687 1.0202797 1.0115407 ## 1953 0.9894989 0.9934337 1.0192680 1.0009392 0.9915039 ## 1954 0.9845184 0.9881036 0.9927613 0.9995143 0.9908692 ## 1955 0.9831117 1.0032501 1.0003084 0.9827720 1.0125535 ## 1956 1.0004765 1.0008730 0.9835071 0.9932761 0.9894251 ## 1957 1.0312668 1.0236147 1.0108432 1.0212995 1.0005263 ## 1958 1.0789695 0.9856540 0.9977971 0.9802940 0.9405687 ## 1959 1.0531001 0.9974447 1.0013371 1.0134608 0.9999192 ## 1960 NA NA NA NA NA # autoplot the random time series from 7:138 which exclude the NA values autoplot(acf(decomposeAP$random[7:138], plot=FALSE)) + labs(title=\u0026quot;Correlogram of Air Passengers Random Component from 1949 to 1961\u0026quot;) + theme_classic() Observations:\n acf of the residuals are centered around zero.   Fit a model using an algorithm 1. Linear regression Model\nGiven there is an upwards trend we‚Äôll look at a linear model first for comparison.\nautoplot(AP) + geom_smooth(method=\u0026quot;lm\u0026quot;) + labs(x=\u0026quot;Time\u0026quot;, y=\u0026quot;Passenger numbers (\u0026#39;000)\u0026quot;, title=\u0026quot;Air Passengers from 1949 to 1961\u0026quot;) + theme_classic() Observations:\n This may not be the best model to fit as it doesn‚Äôt capture the seasonality and multiplicative effects over time.  2. ARIMA Model\nUsing the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE.\narimaAP \u0026lt;- auto.arima(AP) arimaAP ## Series: AP ## ARIMA(2,1,1)(0,1,0)[12] ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2143 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 estimated as 132.3: log likelihood=-504.92 ## AIC=1017.85 AICc=1018.17 BIC=1029.35 The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d), an autoregressive term of second lag (p) and a moving average model of order 1 (q). Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units, in this case months.\nggtsdiag(arimaAP) + theme_classic() Observations:\n The residual plots appear to be centered around 0 as noise, with no pattern. The arima model is a fairly good fit.   Forcasting Plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.\nforecastAP \u0026lt;- forecast(arimaAP, level = c(95), h = 36) autoplot(forecastAP) + labs(x=\u0026quot;Time\u0026quot;, y=\u0026quot;Passenger numbers (\u0026#39;000)\u0026quot;) + theme_classic()   ","date":1548892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548892800,"objectID":"14ecb2239aaeb0bcabace13e0b768fd6","permalink":"/post/time_series_ap/time-series-analysis/","publishdate":"2019-01-31T00:00:00Z","relpermalink":"/post/time_series_ap/time-series-analysis/","section":"post","summary":"Time Series Forecast of airline passengers","tags":[],"title":"Time Series Analysis","type":"post"},{"authors":null,"categories":["R"],"content":" Preamble: This document focuses on the analysis of the mtcars dataframe.\nDescription of dataframe mtcars can be found at the link\n Research questions: is a vehicle with auto or manual transmission better in terms of miles p/gallons(mpg)?\n quantify the (mpg) difference between auto \u0026amp; manual transmission.\n   Structure of analysis: I will asssess both queries from different perspectives employing a set of methodologies that can be broadly grouped as follows:\n Univariate Analysis on target varibale (mpg). Bivariate Analysis on target varibale \u0026amp; relevant covariates. Multivariate Analysis by estimating a set of regresssion models for the conditional mean of mpg. For model selection, I compare the best fit and forward stepwise selection process.   Univariate Analysis Analysing the target variable alone by splitting the observations into two groups, i.e.¬†vehicles with auto or manual transmission. I shall deploy 3 analysis:\n Compute sample means by group ie auto VS manual. Validate if the difference of the group means are statistically significant by computing a 95% confidence interval for means‚Äô difference. Verify the robustness of this result by executing a permutation test with Monte Carlo trials that shuffle the allocation group \u0026gt; mpg.   Get to know the data str(mtcars) \u0026#39;data.frame\u0026#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... We notice that the set is relatively small! We‚Äôll look at the desriptive statistics for each field - (min, 1st Q, Median, Mean, 3rd Q, max)\nsummary(mtcars) mpg cyl disp hp Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 Median :19.20 Median :6.000 Median :196.3 Median :123.0 Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 drat wt qsec vs Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 Median :3.695 Median :3.325 Median :17.71 Median :0.0000 Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 am gear carb Min. :0.0000 Min. :3.000 Min. :1.000 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 Median :0.0000 Median :4.000 Median :2.000 Mean :0.4062 Mean :3.688 Mean :2.812 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 Max. :1.0000 Max. :5.000 Max. :8.000   Sample means by group #### generate subset: automatic and manual cars #### cars_auto = subset(mtcars, am == 0) cars_manu = subset(mtcars, am == 1) # dimensions dim(mtcars) [1] 32 11 dim(cars_auto); dim(cars_manu) [1] 19 11 [1] 13 11 # sample means mpg by group mean(cars_auto$mpg); mean(cars_manu$mpg) [1] 17.14737 [1] 24.39231 sd(cars_auto$mpg); sd(cars_manu$mpg) [1] 3.833966 [1] 6.166504 # % increase in mpg based on the sample mean (mean(cars_manu$mpg) - mean(cars_auto$mpg))/mean(cars_auto$mpg) [1] 0.4225103  Including plots To get a feel for the distribution of some of the data to be analyzed, we plot some histograms, the first against mpg - auto transmission, the second against mpg - manual transission:\nboxplot(mpg ~ am, data = mtcars, col=rgb(0.3,0.2,0.5,0.6), ylab = \u0026quot;mpg\u0026quot;, xlab = \u0026quot;am\u0026quot;) Conclusions:\n mpg empirical mean of vehicles with manual transmission is greater than cars with auto transmission, however this also has a higher variance.   95% confidence interval for the difference of the group means The analysis on sample means concludes that sample mean of mpg for vehicles with manual trasmission is greater than automatic:\nNow I test if this difference (i.e.¬†in the sample means) is statistically significant (from zero).\nI execute a t.test for unpaired samples: I assume inequality in variances for the two groups for the computation of the pooled variance.\n#### 95% confidence interval for mean difference #### # Question: is the sample mean difference significant? t.test(cars_manu$mpg, cars_auto$mpg, paired = F, var.equal = F) Welch Two Sample t-test data: cars_manu$mpg and cars_auto$mpg t = 3.7671, df = 18.332, p-value = 0.001374 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 3.209684 11.280194 sample estimates: mean of x mean of y 24.39231 17.14737  Conclusions:\n 95% interval does not contain 0 sample mean difference is significant at 95% (p-value 0.1%)   Permutation test on groups association I test the robustness of results obtained in the previous step.\nI execute a permutation test by shuffling the allocation mean \u0026gt; groups with 100,000 trials of Montecarlo simulation.\n#### Permutation test #### # what if I shuffle the am groups and calculate the mean? # get target variable and group vectors y = mtcars$mpg group = mtcars$am y; group [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 [29] 15.8 19.7 15.0 21.4 [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 # baseline group means and difference baselineMeans = tapply(mtcars$mpg, mtcars$am, mean) baselineMeansDiff = baselineMeans[2] - baselineMeans[1] tStat = function(w, g) mean(w[g == 1]) - mean(w[g == 0]) observedDiff = tStat(y, group) # check if function works - should be 0: baselineMeansDiff - observedDiff 1 0 # execute shuffle: permutations = sapply(1:100000, function(i) tStat(y, sample(group)))  Plot the analysis: # shuffle experiment results plots: par(mfrow = c(2, 1), mar = c(4, 4, 2, 2)) hist(permutations, main = \u0026quot;Distribution of shuffled group mean differences\u0026quot;) # distribution of difference of averages of permuted groups plot(permutations, type = \u0026quot;b\u0026quot;, main = \u0026quot;Shuffled group mean trials\u0026quot;, xlab = \u0026quot;trial\u0026quot;, ylab = \u0026quot;shuffled group mean differences\u0026quot;, ylim = c(-14, 14)) abline(h = observedDiff, col = \u0026quot;red\u0026quot;, lwd = 3) # there is not even 1 case where by chance I get a difference greater than the observed! mean(permutations \u0026gt; observedDiff) [1] 0.00019 Conclusions:\n out of 100,000 trails only 0.002% has breached the observed value for the diffs in the group empirical means. concluding that empirical means diffs of groups is robust with regards to random reshuffling and is not likely to be generated by pure chance. is this correct?   Bivariate Analysis Analyse the behaviour of target variable (mpg) conditional upon a set of explanatory variables.\n#### generate subset: automatic and manual cars #### cars_auto = subset(mtcars, am == 0) cars_manu = subset(mtcars, am == 1) #### Visual inspection of all covariates #### pairs(mtcars) #### 4 bivariate analysis: hp / wt / drat / disp #### par(mfrow = c(2, 2), mar = c(2, 3, 2, 3)) # plot1 with(mtcars, plot(hp, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs hp - by transmission type\u0026quot;)) # no data with(cars_auto, points(hp, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(hp, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend model1_auto = lm(mpg ~ hp, data = cars_auto) model1_manu = lm(mpg ~ hp, data = cars_manu) abline(model1_auto, col = \u0026quot;red\u0026quot;, lwd = 2) abline(model1_manu, col = \u0026quot;blue\u0026quot;, lwd = 2) abline(v = 175, lty = 2) # plot2 with(mtcars, plot(wt, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs weight - by transmission type\u0026quot;)) # no data with(cars_auto, points(wt, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(wt, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend abline(v = 3.2, lty = 2) # plot 3 with(mtcars, plot(drat, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs drat - by transmission type\u0026quot;)) # no data with(cars_auto, points(drat, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(drat, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend model2_auto = lm(mpg ~ drat, data = cars_auto) model2_manu = lm(mpg ~ drat, data = cars_manu) abline(model2_auto, col = \u0026quot;red\u0026quot;, lwd = 2) abline(model2_manu, col = \u0026quot;blue\u0026quot;, lwd = 2) abline(v = 175, lty = 2) # plot 4 with(mtcars, plot(disp, mpg, type = \u0026quot;n\u0026quot;, main = \u0026quot;mpg vs disp - by transmission type\u0026quot;)) # no data with(cars_auto, points(disp, mpg, col = \u0026quot;red\u0026quot;, pch = 20)) with(cars_manu, points(disp, mpg, col = \u0026quot;blue\u0026quot;, pch = 20)) legend(\u0026quot;topright\u0026quot;, pch = 20, col = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), legend = c(\u0026quot;auto\u0026quot;, \u0026quot;manu\u0026quot;)) # add legend labels = with(mtcars, paste(as.character(disp), as.character(mpg), sep = \u0026quot;,\u0026quot;)) # generate point labels with(mtcars, text(disp, mpg, labels = labels, cex = 0.7, pos = 2)) abline(v = 167.6, lty = 2) Conclusions:\n mpg vs hp: linear negative relation: as horse power of the engine (hp) increases, the mileage (mpg) reduces. Vehicles with manual transmission seems however to be more efficient: the group restricted regression (blue) has a higher intercept. It has to be highlighted however, that the parameters of blue regression might be influenced by two extreme values with high hp - the regression should be re-estimated by removing the two datapoints. mpg vs weight: negative relation, the functional form might be non-linear (hyperbolic ?), as weight of the vehicle increases, the mileage decreases. The weight variable seems to provide perfect separation between manual and auto transmission vehilces, i.e.¬†all vehicles that are heavier than 3.2 ton (circa) are auto and vice-versa. mpg vs drat: the functional form is not clear: it appears also to be an increase in the variance as the rear axel ratio (drat) increases. To verify this a regression model using all observations has to be estimated and analyse the residuals for verifying if the model is heteroskedastic. mpg vs disp: seems to have a negative (hyperbolic ?) relation: as the displacement (disp) of the engine increases, the mileage decreases. Also, in this case it seems that disp accounts for perfect separation in the transmission type: almost all vehilces with disp \u0026gt; 180 are auto.   Multivariate analysis Run a set of regression models for estimating the impact of some predictions on mpg.\nFor model selection, I employ the following techniques:\n Manual selection of regressors: I hand pick regressors for: Best fit procedure Forward stepwise procedure   Manual selection Analysis of covariance matrix:\n### analyse covariance matrix for regressor selection: z \u0026lt;- cor(mtcars) require(lattice) Loading required package: lattice levelplot(z) A model with only transmission:\n# only am data = mtcars data$am = as.factor(data$am) model2 = lm(mpg ~ am, data = data) # get results summary(model2) Call: lm(formula = mpg ~ am, data = data) Residuals: Min 1Q Median 3Q Max -9.3923 -3.0923 -0.2974 3.2439 9.5077 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 17.147 1.125 15.247 1.13e-15 *** am1 7.245 1.764 4.106 0.000285 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 4.902 on 30 degrees of freedom Multiple R-squared: 0.3598, Adjusted R-squared: 0.3385 F-statistic: 16.86 on 1 and 30 DF, p-value: 0.000285 Observations:\n the intercept is 17.15: exactly the same mean of mpg for vehicles with auto transmission. the coefficient of am is 7.24: exactly the difference of mpg means for vehicles with manual / auto transmission. the sum of intercept and am coefficient gives the mpg unconditional mean for vehicles with manual transmission.   Best Fit Procedure Run the best fit procedure for identifying the optimal number of regressors that minimises the cp, which is (‚Ä¶)\n#### model selection using leaps #### data = mtcars data$log_mpg = log(data$mpg) # add log of y #### method 1. best fit #### regfit.full = regsubsets(log_mpg ~. , data = data, nvmax = 10) reg.summary = summary(regfit.full) reg.summary Subset selection object Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10) 11 Variables (and intercept) Forced in Forced out mpg FALSE FALSE cyl FALSE FALSE disp FALSE FALSE hp FALSE FALSE drat FALSE FALSE wt FALSE FALSE qsec FALSE FALSE vs FALSE FALSE am FALSE FALSE gear FALSE FALSE carb FALSE FALSE 1 subsets of each size up to 10 Selection Algorithm: exhaustive mpg cyl disp hp drat wt qsec vs am gear carb 1 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 2 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 3 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot;   Plot the analysis # how I selected the optimal number of variables? plot(reg.summary$cp, xlab = \u0026quot;Number of variables\u0026quot;, ylab = \u0026quot;cp\u0026quot;, type = \u0026quot;b\u0026quot;)  Forward Stepwise Procedure regfit.fwd = regsubsets(log_mpg ~ ., data = data, nvmax = 10, method = \u0026quot;forward\u0026quot;) summary(regfit.fwd) Subset selection object Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10, method = \u0026quot;forward\u0026quot;) 11 Variables (and intercept) Forced in Forced out mpg FALSE FALSE cyl FALSE FALSE disp FALSE FALSE hp FALSE FALSE drat FALSE FALSE wt FALSE FALSE qsec FALSE FALSE vs FALSE FALSE am FALSE FALSE gear FALSE FALSE carb FALSE FALSE 1 subsets of each size up to 10 Selection Algorithm: forward mpg cyl disp hp drat wt qsec vs am gear carb 1 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 2 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 3 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 4 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 5 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; 6 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; 7 ( 1 ) \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 8 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 9 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; 10 ( 1 ) \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot; \u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot; \u0026quot;*\u0026quot;   Plot the analysis plot(regfit.fwd, scale = \u0026quot;Cp\u0026quot;) Appendix\nA model including all regressors.\n#### lm with all variables / no split #### # prepare data data = mtcars data$am = as.factor(data$am) model1 = lm(mpg ~ ., data = data) # get results summary(model1) Call: lm(formula = mpg ~ ., data = data) Residuals: Min 1Q Median 3Q Max -3.4506 -1.6044 -0.1196 1.2193 4.6271 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 12.30337 18.71788 0.657 0.5181 cyl -0.11144 1.04502 -0.107 0.9161 disp 0.01334 0.01786 0.747 0.4635 hp -0.02148 0.02177 -0.987 0.3350 drat 0.78711 1.63537 0.481 0.6353 wt -3.71530 1.89441 -1.961 0.0633 . qsec 0.82104 0.73084 1.123 0.2739 vs 0.31776 2.10451 0.151 0.8814 am1 2.52023 2.05665 1.225 0.2340 gear 0.65541 1.49326 0.439 0.6652 carb -0.19942 0.82875 -0.241 0.8122 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2.65 on 21 degrees of freedom Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07  Plot the analysis # plot residual analysis par(mfrow = c(2, 2)) plot(model1) # plot hist par(mfrow = c(1, 1)) hist(model1$residuals) # normality test on residuals shapiro.test(model1$residuals) Shapiro-Wilk normality test data: model1$residuals W = 0.95694, p-value = 0.2261  ","date":1546473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546473600,"objectID":"e7334ef87192c7900880836c0b9223cc","permalink":"/post/mtcars/2019-01-03-r-rmarkdown/","publishdate":"2019-01-03T00:00:00Z","relpermalink":"/post/mtcars/2019-01-03-r-rmarkdown/","section":"post","summary":"Which type of vehicle transmission is more fuel efficient?","tags":["plot","regression"],"title":"mtcars Data Analysis","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]