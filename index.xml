<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rihad Variawa on Rihad Variawa</title>
    <link>/</link>
    <description>Recent content in Rihad Variawa on Rihad Variawa</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Bank ATM Cash Machine Forecast w/ Time Series</title>
      <link>/post/atm/bank/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/atm/bank/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. The variable ‘Cash’ is provided in hundreds of dollars.&lt;/p&gt;
&lt;p&gt;This is a time series spanning daily transactions from May 1, 2009 to April 30, 2010 from four ATMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;forecast how much cash is taken out of 4 different ATM machines for May 2010&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ATM624Data.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
atm &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview first 5 rows
head(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         DATE  ATM Cash
## 1 2009-05-01 ATM1   96
## 2 2009-05-01 ATM2  107
## 3 2009-05-02 ATM1   82
## 4 2009-05-02 ATM2   89
## 5 2009-05-03 ATM1   85
## 6 2009-05-03 ATM2   90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1474 obs. of  3 variables:
##  $ DATE: Date, format: &amp;quot;2009-05-01&amp;quot; &amp;quot;2009-05-01&amp;quot; ...
##  $ ATM : Factor w/ 4 levels &amp;quot;ATM1&amp;quot;,&amp;quot;ATM2&amp;quot;,..: 1 2 1 2 1 2 1 2 1 2 ...
##  $ Cash: num  96 107 82 89 85 90 90 55 99 79 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       DATE              ATM           Cash        
##  Min.   :2009-05-01   ATM1:365   Min.   :    0.0  
##  1st Qu.:2009-08-01   ATM2:365   1st Qu.:    0.5  
##  Median :2009-11-01   ATM3:365   Median :   73.0  
##  Mean   :2009-10-31   ATM4:365   Mean   :  155.6  
##  3rd Qu.:2010-02-01   NA&amp;#39;s: 14   3rd Qu.:  114.0  
##  Max.   :2010-05-14              Max.   :10919.8  
##                                  NA&amp;#39;s   :19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Skewed distribution since the mean is higher than the third quartile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview periods between dates in the time series
xts::periodicity(unique(atm$DATE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Daily periodicity from 2009-05-01 to 2010-05-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans daily transactions from May 1, 2009 to May 14, 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations that have no missing values
atm[!complete.cases(atm), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           DATE  ATM Cash
## 87  2009-06-13 ATM1   NA
## 93  2009-06-16 ATM1   NA
## 98  2009-06-18 ATM2   NA
## 105 2009-06-22 ATM1   NA
## 110 2009-06-24 ATM2   NA
## 731 2010-05-01 &amp;lt;NA&amp;gt;   NA
## 732 2010-05-02 &amp;lt;NA&amp;gt;   NA
## 733 2010-05-03 &amp;lt;NA&amp;gt;   NA
## 734 2010-05-04 &amp;lt;NA&amp;gt;   NA
## 735 2010-05-05 &amp;lt;NA&amp;gt;   NA
## 736 2010-05-06 &amp;lt;NA&amp;gt;   NA
## 737 2010-05-07 &amp;lt;NA&amp;gt;   NA
## 738 2010-05-08 &amp;lt;NA&amp;gt;   NA
## 739 2010-05-09 &amp;lt;NA&amp;gt;   NA
## 740 2010-05-10 &amp;lt;NA&amp;gt;   NA
## 741 2010-05-11 &amp;lt;NA&amp;gt;   NA
## 742 2010-05-12 &amp;lt;NA&amp;gt;   NA
## 743 2010-05-13 &amp;lt;NA&amp;gt;   NA
## 744 2010-05-14 &amp;lt;NA&amp;gt;   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ATM transactions have missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(factor(atm$ATM)[!is.na(atm$Cash) &amp;amp; atm$Cash %% 1 != 0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ATM1 ATM2 ATM3 ATM4 
##    0    0    0  365&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are non-integer transactions at ATM 4 implying that these data are likely debit card purchase transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# time plot represents a line graph that plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  n &amp;lt;- nrow(atm_ts); l &amp;lt;- rep(1, n); m &amp;lt;- rep(20, n); h &amp;lt;- rep(100, n)
  print(plot(cbind(atm_ts, l, m,h), main=paste0(&amp;quot;ATM&amp;quot;, i)))
  
# histogram displays the frequency at which values in a vector occur.
  hist(atm_ts, col=&amp;quot;green&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time Plots and Histograms for ATM1 and ATM2 are unremarkable.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM3 shows the data consists mostly of zero values with a handful of transactions occurring at the end of the series.&lt;/li&gt;
&lt;li&gt;ATM3 will not be modeled due to these degenerative properties.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM4 shows an extreme outlier around the three-quarter mark of the series. The horizontal lines in the Time Plots delineate $1, $20, and $100 in red, green, and blue; respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation The xts::xts()function converts data to a time series object which displays better in visualizations than time series objects created using other packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  acf(na.omit(atm_ts), ylab=paste0(&amp;quot;ACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;) 
  pacf(na.omit(atm_ts), ylab=paste0(&amp;quot;PACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots for ATM1, ATM2, and ATM3 show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations. The ACF and PACF plots for ATM3 however, are not reliable due to the death of observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;p&gt;Data are cleaned using forecast::tsclean() and then converted to a time series object using the ts() function. The tsclean() function imputes nulls and removes outliers. The ts()function converts data to a time series object which is compatible with the forecast package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:length(levels(atm$ATM))) {
  atm_num &amp;lt;- paste0(&amp;quot;ATM&amp;quot;, i)
  atm_sub &amp;lt;- subset(atm, ATM == atm_num, select=-2)
  atm_sub$Cash &amp;lt;- forecast::tsclean(atm_sub$Cash, replace.missing=T)
  assign(atm_num, ts(atm_sub$Cash, frequency = 7, start=start(atm_sub$DATE)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-examine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Examine&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 1), mar = c(0, 4, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(ATM1, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(ATM1, order=7), col=2)  # weekly
lines(forecast::ma(ATM1, order=30), col=4) # monthly
plot(ATM2, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM3&amp;quot;)
lines(forecast::ma(ATM2, order=7), col=2)  # weekly
lines(forecast::ma(ATM2, order=30), col=4) # monthly
plot(ATM4, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM4&amp;quot;)
lines(forecast::ma(ATM4, order=7), col=2)  # weekly
lines(forecast::ma(ATM4, order=30), col=4) # monthly&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 7-day (weekly) and 30-day (monthly) moving average smoother line shows that the data for the ATMs have no apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposition Plot decomposes and plots the observed values, the underlying trend, seasonality, and randomness of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM1), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM2), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM4), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have no apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey-fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey-Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM1): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM1
## Dickey-Fuller = -4.5329, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM2): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM2
## Dickey-Fuller = -6.046, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM4): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM4
## Dickey-Fuller = -5.6304, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-values are below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows by index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train/test split
index_train &amp;lt;- 1:(length(ATM1) - 30)
ATM1_train &amp;lt;- ts(ATM1[index_train], frequency=7)
ATM1_test &amp;lt;- ts(ATM1[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM2) - 30)
ATM2_train &amp;lt;- ts(ATM2[index_train], frequency=7)
ATM2_test &amp;lt;- ts(ATM2[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM3) - 30)
ATM3_train &amp;lt;- ts(ATM3[index_train], frequency=7)
ATM3_test &amp;lt;- ts(ATM3[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM4) - 30)
ATM4_train &amp;lt;- ts(ATM4[index_train], frequency=7)
ATM4_test &amp;lt;- ts(ATM4[-index_train], frequency=7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The indexed rows for the test set are a window at the end of the times series. The window sized for the testing set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted.Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda1 &amp;lt;- forecast::BoxCox.lambda(ATM1_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4355901&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda2 &amp;lt;- forecast::BoxCox.lambda(ATM2_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7156895&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda4 &amp;lt;- forecast::BoxCox.lambda(ATM4_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3945256&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameters suggested are around λ=0.5. This rounded (more interpretable) value is suggestive of a 1/yt‾‾√ transformation. These Box-Cox transformations stabilize the variance and make each series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit1 &amp;lt;- forecast::auto.arima(ATM1_train, stepwise=F, approximation=F, d=0, lambda=lambda1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM1_train 
## ARIMA(0,0,2)(1,1,1)[7] 
## Box Cox transformation: lambda= 0.4355901 
## 
## Coefficients:
##          ma1      ma2    sar1     sma1
##       0.1449  -0.1116  0.1320  -0.7243
## s.e.  0.0547   0.0537  0.0893   0.0669
## 
## sigma^2 estimated as 6.441:  log likelihood=-770.86
## AIC=1551.73   AICc=1551.92   BIC=1570.69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit2 &amp;lt;- forecast::auto.arima(ATM2_train, stepwise=F, approximation=F, d=0, lambda=lambda2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM2_train 
## ARIMA(2,0,2)(0,1,1)[7] with drift 
## Box Cox transformation: lambda= 0.7156895 
## 
## Coefficients:
##           ar1      ar2     ma1     ma2     sma1    drift
##       -0.4282  -0.9254  0.4761  0.8044  -0.7672  -0.0246
## s.e.   0.0464   0.0413  0.0764  0.0555   0.0483   0.0155
## 
## sigma^2 estimated as 66.34:  log likelihood=-1152.83
## AIC=2319.66   AICc=2320.01   BIC=2346.21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit4 &amp;lt;- forecast::auto.arima(ATM4_train, stepwise=F, approximation=F, d=0, lambda=lambda4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM4_train 
## ARIMA(1,0,0)(2,0,0)[7] with non-zero mean 
## Box Cox transformation: lambda= 0.3945256 
## 
## Coefficients:
##          ar1    sar1    sar2     mean
##       0.0814  0.2060  0.1911  22.7977
## s.e.  0.0548  0.0537  0.0547   0.9477
## 
## sigma^2 estimated as 97.25:  log likelihood=-1240.53
## AIC=2491.06   AICc=2491.24   BIC=2510.13&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluate&lt;/h2&gt;
&lt;p&gt;ACF and PACF&lt;/p&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit1), ylab=&amp;quot;ACF ATM1&amp;quot;); pacf(residuals(fit1), ylab=&amp;quot;PACF ATM1&amp;quot;)
acf(residuals(fit2), ylab=&amp;quot;ACF ATM2&amp;quot;); pacf(residuals(fit2), ylab=&amp;quot;PACF ATM2&amp;quot;)
acf(residuals(fit4), ylab=&amp;quot;ACF ATM4&amp;quot;); pacf(residuals(fit4), ylab=&amp;quot;PACF ATM4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the models appear to display the characteristics of White Noise in the ACF and PACF plots with only one of the twenty residuals (or 0.05%) being significant. At a 95% confidence interval this is within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The arma attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,period,d,and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit1), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit1)
## X-squared = 5.7195, df = 5, p-value = 0.3345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit2), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit2)
## X-squared = 7.9286, df = 5, p-value = 0.1602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit4), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit4)
## X-squared = 4.6833, df = 5, p-value = 0.4557&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the models are not significantly different from zero at α=0.05. The residuals of the models display the characteristics of White Noise. The models pass the required checks and are therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;ATM3 was not modeled due to its degenerative properties. To forecast values for ATM3, the model for an ATM with a similar mean will be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(mean(ATM1), mean(ATM2), mean(ATM3[ATM3!=0]), mean(ATM4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  84.15479  62.59178  87.66667 444.75681&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of ATM1 is very close to the mean of the few values in ATM3. Therefore, the ARIMA(0,0,1)(2,0,0)7 ARIMA model for ATM1 will be used to make predictions for ATM3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- forecast::Arima(ATM3_train, model=fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data were not seasonally adjusted, they need not be reseasonalized prior to forecast. Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a red line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast1 &amp;lt;- forecast::forecast(fit1, h=30)
fcast2 &amp;lt;- forecast::forecast(fit2, h=30)
fcast3 &amp;lt;- forecast::forecast(fit3, h=30)
fcast4 &amp;lt;- forecast::forecast(fit4, h=30)
par(mfrow=c(4, 1), mar = c(0, 4, 0, 0), oma = c(4, 4, 2, 0.5))
plot(fcast1, ylab=&amp;quot;Cash ATM1&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM1_test, -length(ATM1_train)), col=&amp;quot;red&amp;quot;)
plot(fcast2, ylab=&amp;quot;Cash ATM2&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM2_test, -length(ATM2_train)), col=&amp;quot;red&amp;quot;)
plot(fcast3, ylab=&amp;quot;Cash ATM3&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM3_test, -length(ATM3_train)), col=&amp;quot;red&amp;quot;)
plot(fcast4, ylab=&amp;quot;Cash ATM4&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM4_test, -length(ATM4_train)), col=&amp;quot;red&amp;quot;)
title(&amp;quot;ATM Predictions&amp;quot;, outer=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The predictions appear to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast1, length(ATM1_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ME   RMSE    MAE      MPE    MAPE  MASE  ACF1
## Training set   2.038 25.007 16.039  -96.186 114.754 0.427 0.011
## Test set     -53.187 53.187 53.187 -177.290 177.290 1.416    NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast2, length(ATM2_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ME   RMSE    MAE      MPE    MAPE MASE   ACF1
## Training set   1.456 24.795 17.275     -Inf     Inf 0.40 -0.013
## Test set     -44.485 44.485 44.485 -148.284 148.284 1.03     NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast4, length(ATM4_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    ME    RMSE     MAE       MPE     MAPE MASE  ACF1
## Training set   96.204 360.567 280.446  -342.343  388.847 0.72 0.017
## Test set     -385.878 385.878 385.878 -1286.261 1286.261 0.99    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications vary. ATM1 and ATM2 predictions are more accurate than ATM4 predictions. The closer the original data are to being White Noise, the less accurate the predictions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Forecasting w/ Time Series Analysis</title>
      <link>/post/residential_energy/residential-energy-usage/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/residential_energy/residential-energy-usage/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. A simple dataset of residential power usage from January 1998 to December 2013.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through an analysis, model this data and monthly forecast for 2014&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ResidentialCustomerForecastLoad-624.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
energy &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)

# the “YYYY-MMM” format dates are interpreted as factors. They must be converted to dates
energy$YYYY.MMM &amp;lt;- as.Date(paste0(energy$YYYY.MMM,&amp;quot;-01&amp;quot;), format = &amp;quot;%Y-%b-%d&amp;quot;)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CaseSequence   YYYY.MMM     KWH
## 1          733 1998-01-01 6862583
## 2          734 1998-02-01 5838198
## 3          735 1998-03-01 5420658
## 4          736 1998-04-01 5010364
## 5          737 1998-05-01 4665377
## 6          738 1998-06-01 6467147&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the class of the dataset
class(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    192 obs. of  3 variables:
##  $ CaseSequence: num  733 734 735 736 737 738 739 740 741 742 ...
##  $ YYYY.MMM    : Date, format: &amp;quot;1998-01-01&amp;quot; &amp;quot;1998-02-01&amp;quot; ...
##  $ KWH         : num  6862583 5838198 5420658 5010364 4665377 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CaseSequence      YYYY.MMM               KWH          
##  Min.   :733.0   Min.   :1998-01-01   Min.   :  770523  
##  1st Qu.:780.8   1st Qu.:2001-12-24   1st Qu.: 5429912  
##  Median :828.5   Median :2005-12-16   Median : 6283324  
##  Mean   :828.5   Mean   :2005-12-15   Mean   : 6502475  
##  3rd Qu.:876.2   3rd Qu.:2009-12-08   3rd Qu.: 7620524  
##  Max.   :924.0   Max.   :2013-12-01   Max.   :10655730  
##                                       NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the periods between dates in dataset
xts::periodicity(unique(energy$YYYY.MMM))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Monthly periodicity from 1998-01-01 to 2013-12-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans monthly from January 1, 1998 to December 1, 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations in the dataframe that have no missing values
energy[!complete.cases(energy), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     CaseSequence   YYYY.MMM KWH
## 129          861 2008-09-01  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe contains one missing value in kWh usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
kWh &amp;lt;- xts::xts(energy$KWH, order.by=energy$YYYY.MMM)
par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(kWh, main=&amp;quot;kWh&amp;quot;)

# display frequency at which values in a vector occur
hist(kWh, col=&amp;quot;yellow&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obervations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Line plot and Histogram shows an outlier around the three-quarter mark of the series.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
# ACF autocorrelations between each observation and its immediate predecessor (lagged observation)
acf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;) 

# PACF autocorrelations between the current observation and each individual lagged observation
pacf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data cleaning w/ forecast::tsclean() and converted to a time series object using the ts(). 
# tsclean() function imputes nulls and removes outliers.
# ts() function converts data to a time series object which is compatible with the forecast package.
kWh &amp;lt;- ts(forecast::tsclean(energy$KWH, replace.missing=T), 
          frequency = 12, start=start(energy$YYYY.MMM)) # data sampled monthly = 12
kWh[kWh==min(kWh)] &amp;lt;- mean(kWh[kWh!=min(kWh)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-preview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Preview&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(kWh, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(kWh, order=6), col=6)  # pink line biannual period
lines(forecast::ma(kWh, order=12), col=4) # blue line annual period&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 6-month and 12-month moving average smoother line shows that the data has a slight apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposes and plots the &lt;strong&gt;observed&lt;/strong&gt; values, the underlying &lt;strong&gt;trend,&lt;/strong&gt; &lt;strong&gt;seasonality,&lt;/strong&gt; and &lt;strong&gt;randomness&lt;/strong&gt; of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(kWh), col=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obseravations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have a slight apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey_fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey_Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(kWh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(kWh): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  kWh
## Dickey-Fuller = -4.5454, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-value is below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows w/ index. The indexed rows for the testing set are a window at the end of the times series. The window sized for the test set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index_train &amp;lt;- 1:(length(kWh) - 12)
kWh_train &amp;lt;- ts(kWh[index_train], frequency=12)
kWh_test &amp;lt;- ts(kWh[index_train], frequency=12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted. Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda &amp;lt;- forecast::BoxCox.lambda(kWh_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.1733063&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameter suggested is about λ=−0.25. This rounded (slightly more interpretable) value is suggestive of an inverse quartic root. This Box-Cox transformation stabilizes the variance and makes the series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- forecast::auto.arima(kWh_train, stepwise=F, approximation=F, d=0, lambda=lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: kWh_train 
## ARIMA(0,0,3)(2,1,0)[12] with drift 
## Box Cox transformation: lambda= -0.1733063 
## 
## Coefficients:
##          ma1     ma2     ma3     sar1     sar2  drift
##       0.2807  0.0855  0.2232  -0.7724  -0.4408  1e-04
## s.e.  0.0757  0.0823  0.0687   0.0742   0.0812  1e-04
## 
## sigma^2 estimated as 3.707e-05:  log likelihood=621.98
## AIC=-1229.95   AICc=-1229.25   BIC=-1208.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The auto.arima() function suggests an ARIMA(0,0,3)(2,1,0)12 model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit), ylab=&amp;quot;ACF kWh&amp;quot;); pacf(residuals(fit), ylab=&amp;quot;PACF kWh&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the model appear to display the characteristics of White Noise in both the ACF and PACF plots. None of the residuals are significant. At a 95% confidence interval this is well within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The ARIMA attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,periods,d and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit), lag=7, fitdf=sum(fit$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit)
## X-squared = 7.2523, df = 4, p-value = 0.1231&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the model are not significantly different from zero at α=0.05. The residuals of the model displays the characteristics of White Noise. The model passes the required checks and is therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data was not seasonally adjusted, they need not be reseasonalized prior to forecast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast &amp;lt;- forecast::forecast(fit, h=15)
plot(fcast, ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;kWh Predictions&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(kWh_test, -length(kWh_train)), col=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The prediction appears to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;li&gt;Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a pink line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast, length(kWh_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       ME      RMSE       MAE          MPE        MAPE
## Training set    39449.18  581186.1  456353.6        0.056       7.067
## Test set     -9046871.23 9046871.2 9046871.2 -5026039.573 5026039.573
##               MASE  ACF1
## Training set 0.413 0.115
## Test set     8.185    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications is fair. The large metrics are representative of the large values found in the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Efficiency on Buildings</title>
      <link>/post/energye/energye/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energye/energye/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;The dataset is available at [&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;to explore three data points, and visualize how they influence the energy load.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following variables &lt;strong&gt;Wall.Area&lt;/strong&gt;, &lt;strong&gt;Roof.Area&lt;/strong&gt;, &lt;strong&gt;Glazing.Area&lt;/strong&gt; are identified as key indicators that can influence the energy load efficiency for both (Heating and Cooling spaces).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Plot Load Distribution Using Scatter Plot&lt;/li&gt;
&lt;li&gt;Plot Heating Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Cooling Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Energy Efficiency&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())

sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/StephenElston/DataScience350/master/Lecture1/EnergyEfficiencyData.csv&amp;quot;

df &amp;lt;- read.csv( sourceURL, header = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Relative.Compactness Surface.Area Wall.Area Roof.Area Overall.Height
## 1                 0.98        514.5     294.0    110.25              7
## 2                 0.98        514.5     294.0    110.25              7
## 3                 0.98        514.5     294.0    110.25              7
## 4                 0.98        514.5     294.0    110.25              7
## 5                 0.90        563.5     318.5    122.50              7
## 6                 0.90        563.5     318.5    122.50              7
##   Orientation Glazing.Area Glazing.Area.Distribution Heating.Load
## 1           2            0                         0        15.55
## 2           3            0                         0        15.55
## 3           4            0                         0        15.55
## 4           5            0                         0        15.55
## 5           2            0                         0        20.84
## 6           3            0                         0        21.46
##   Cooling.Load
## 1        21.33
## 2        21.33
## 3        21.33
## 4        21.33
## 5        28.28
## 6        25.38&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    768 obs. of  10 variables:
##  $ Relative.Compactness     : num  0.98 0.98 0.98 0.98 0.9 0.9 0.9 0.9 0.86 0.86 ...
##  $ Surface.Area             : num  514 514 514 514 564 ...
##  $ Wall.Area                : num  294 294 294 294 318 ...
##  $ Roof.Area                : num  110 110 110 110 122 ...
##  $ Overall.Height           : num  7 7 7 7 7 7 7 7 7 7 ...
##  $ Orientation              : int  2 3 4 5 2 3 4 5 2 3 ...
##  $ Glazing.Area             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Glazing.Area.Distribution: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Heating.Load             : num  15.6 15.6 15.6 15.6 20.8 ...
##  $ Cooling.Load             : num  21.3 21.3 21.3 21.3 28.3 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Categorize useful variables and convert them to a categorical variables, namely &lt;strong&gt;Orientation&lt;/strong&gt;, &lt;strong&gt;Glazing.Area.Distribution&lt;/strong&gt;, and &lt;strong&gt;Glazing.Area&lt;/strong&gt; (variance) variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## change vector values to factor values
df$Orientation &amp;lt;- as.factor(df$Orientation) 

## attributes of variable 
levels(df$Orientation) &amp;lt;- c(&amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

## change vector values to factor values
df$Glazing.Area.Distribution &amp;lt;- as.factor(df$Glazing.Area.Distribution)

## attributes of variable
levels(df$Glazing.Area.Distribution) &amp;lt;- c(&amp;quot;UnKnown&amp;quot;, &amp;quot;Uniform&amp;quot;, &amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

## change vector values to factor values
df$Glazing.Area &amp;lt;- as.factor(df$Glazing.Area) 

## attributes of variable
levels(df$Glazing.Area) &amp;lt;- c(&amp;quot;0%&amp;quot;, &amp;quot;10%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;40%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Relative.Compactness  Surface.Area     Wall.Area       Roof.Area    
##  Min.   :0.6200       Min.   :514.5   Min.   :245.0   Min.   :110.2  
##  1st Qu.:0.6825       1st Qu.:606.4   1st Qu.:294.0   1st Qu.:140.9  
##  Median :0.7500       Median :673.8   Median :318.5   Median :183.8  
##  Mean   :0.7642       Mean   :671.7   Mean   :318.5   Mean   :176.6  
##  3rd Qu.:0.8300       3rd Qu.:741.1   3rd Qu.:343.0   3rd Qu.:220.5  
##  Max.   :0.9800       Max.   :808.5   Max.   :416.5   Max.   :220.5  
##  Overall.Height Orientation Glazing.Area Glazing.Area.Distribution
##  Min.   :3.50   North:192   0% : 48      UnKnown: 48              
##  1st Qu.:3.50   East :192   10%:240      Uniform:144              
##  Median :5.25   South:192   25%:240      North  :144              
##  Mean   :5.25   West :192   40%:240      East   :144              
##  3rd Qu.:7.00                            South  :144              
##  Max.   :7.00                            West   :144              
##   Heating.Load    Cooling.Load  
##  Min.   : 6.01   Min.   :10.90  
##  1st Qu.:12.99   1st Qu.:15.62  
##  Median :18.95   Median :22.08  
##  Mean   :22.31   Mean   :24.59  
##  3rd Qu.:31.67   3rd Qu.:33.13  
##  Max.   :43.10   Max.   :48.03&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-load-distribution-using-scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Load Distribution Using Scatter Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## visualize if there is any relation between &amp;#39;Roof.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; and how load is distributed using scatter plot.
ggplot(df, aes(x = Cooling.Load, y = Heating.Load), alpha = 0.5)+
  geom_point(aes(colour = Roof.Area ))+
  facet_grid(Overall.Height + Glazing.Area ~ Surface.Area,  space = &amp;quot;free&amp;quot;) +
  ggtitle(&amp;quot;Load distribuiton of energy by Roof Area and Surface Area \n by Glazing Area and Overall Height&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Roof area and Surface area range is high for minimum/ lowest (3.5) over-all height and&lt;/li&gt;
&lt;li&gt;Roof area and Surface area range is low for maximum/ highest (7.0) over-all height.&lt;/li&gt;
&lt;li&gt;There are no data points when the overall height is 7 and highest surface area range and also for low overall height 3.5, we have no data points with the low surface area range.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-heating-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Heating Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## plot how &amp;#39;Wall.Area&amp;#39; influence heating load using raster plot.
ggplot(df, aes( Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Heating.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;steelblue&amp;quot;, high = &amp;quot;red&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Heating Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By looking at the figures, we can conclude that &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in heating, irrespective of Surface Area and Roof Area. (Higher the wall area, higher the heating load).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-cooling-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Cooling Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## plot how &amp;#39;Wall.Area&amp;#39; influence cooling load using raster plot.
ggplot(df, aes(Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Cooling.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;grey&amp;quot;, high = &amp;quot;steelblue&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Cooling Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So, &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in both Heating and Cooling Load efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-energy-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Energy Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## we have seen more variation in load data when the overall height is (7.0). So lets create a subset named(energy.eff.sub7.0) which contains the filtered data with overall height = 7.0. Lets visualize, if the &amp;#39;Roof.Area&amp;#39;, &amp;#39;Wall.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; are influencing the load efficiency.
energy.eff.sub7.0 &amp;lt;- df[ df$Overall.Height ==7.0,]
ggplot(energy.eff.sub7.0,
       aes(x = Cooling.Load, y = Heating.Load, group = factor(round(Wall.Area)), 
           size = Glazing.Area,
           shape = factor(round(Wall.Area))))+
  geom_point(aes(colour= factor(round(Surface.Area))), alpha = 0.3)+
  geom_smooth(method = &amp;quot;lm&amp;quot;,se = TRUE )+
  facet_wrap(~ Roof.Area) +
  ggtitle(&amp;#39;Load efficiency by Roof Area, by Wall Area by Surface Area and by Glazing Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is clearly evident that the Load efficiency is influenced by the Roof Area, Wall Area, Surface Area, and Glazing Area.&lt;/li&gt;
&lt;li&gt;When the Glazing Area is high, Roof Area is high and Wall Area is high, Load will be high and viceversa.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Demand Analysis w/ Time Series Forecasting</title>
      <link>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</guid>
      <description>


&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;R. H. Shumway, D. S. Stoffer. &lt;em&gt;Time Series Analysis and Its Applications&lt;/em&gt;. 2010.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman. &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;. 2013.&lt;/li&gt;
&lt;li&gt;P. S. P. Cowpertwait, A. V. Metcalfe. &lt;em&gt;Introductory Time Series with R&lt;/em&gt;. 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on an analysis of the energy demands of a European country.&lt;/p&gt;
&lt;p&gt;The dataset of the daily energy needs (in GWh) between 2004 and 2010.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;build a model for energy demand forecasting using time series analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;seasonal ARIMA model&lt;/li&gt;
&lt;li&gt;Forecast model&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;#39;https://gist.githubusercontent.com/Peque/715e91350f0e68e3342f/raw/d28312ac0e49888a5079fcea188770acaf3aa4a2/mme.csv&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download and load data into memory
tmp &amp;lt;- tempfile()
download.file(sourceURL, tmp, method = &amp;#39;curl&amp;#39;)
df &amp;lt;- read.csv(tmp)
unlink(tmp)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       date demand
## 1 01-01-04 488.07
## 2 02-01-04 582.02
## 3 03-01-04 575.58
## 4 04-01-04 542.39
## 5 05-01-04 600.26
## 6 06-01-04 544.76&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert date strings to POSIX dates
df$date &amp;lt;- strptime(df$date, format = &amp;#39;%d-%m-%y&amp;#39;)
# day of week
df$day &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%A&amp;#39;))
# day of year
df$yearday &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%m%d&amp;#39;))
# structure for analysis
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3288 obs. of  4 variables:
##  $ date   : POSIXlt, format: &amp;quot;2004-01-01&amp;quot; &amp;quot;2004-01-02&amp;quot; ...
##  $ demand : num  488 582 576 542 600 ...
##  $ day    : Factor w/ 7 levels &amp;quot;Friday&amp;quot;,&amp;quot;Monday&amp;quot;,..: 5 1 3 4 2 6 7 5 1 3 ...
##  $ yearday: Factor w/ 366 levels &amp;quot;0101&amp;quot;,&amp;quot;0102&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df split to create test set
df_test &amp;lt;- subset(df, date &amp;gt;= strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
df &amp;lt;- subset(df, date &amp;lt; strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
ts &amp;lt;- ts(df$demand, frequency = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df and time series objects
demandts &amp;lt;- xts(df$demand, df$date)
plot(demandts, main = &amp;#39;Energy Demand Preview&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A seasonal dependency of demand can be easily spotted in the graphics, although there are other factors that may affect the results, such as the temperature, holidays, weekends, etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand by day of the week
ggplot(df, aes(day, demand)) +
  geom_boxplot(fill=&amp;#39;slateblue&amp;#39;, alpha=0.2) + xlab(&amp;#39;Time&amp;#39;) + ylab(&amp;#39;Demand (GWh)&amp;#39;) + ggtitle(&amp;#39;Demand per day of the week&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During weekends, the demand decreases considerably compared to the rest of the week days.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_per_yearday &amp;lt;- aggregate(demand ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand, f = 1 / 45)
l &amp;lt;- length(avg_demand_per_yearday$demand)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting results
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_per_yearday$demand, type = &amp;#39;l&amp;#39;, main = &amp;#39;Average Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During the winter &amp;amp; summer seasons the demand is clearly higher exept for, vacation periods. Holydays are also easily spotted in the graphics, being the lowest peaks of demand.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_per_yearday$demand - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The graphics show the errors. Notice how the biggest errors are all negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(strftime(dates[order(-abs_diff)], format = &amp;#39;%B %d&amp;#39;), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;January 01&amp;quot;  &amp;quot;December 25&amp;quot; &amp;quot;May 01&amp;quot;      &amp;quot;January 06&amp;quot;  &amp;quot;August 15&amp;quot;  
##  [6] &amp;quot;December 08&amp;quot; &amp;quot;December 31&amp;quot; &amp;quot;October 12&amp;quot;  &amp;quot;November 01&amp;quot; &amp;quot;December 26&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact dates which are generating these errors are indeed, holidays or the day just before holidays (as is the case for the 25th November and 31th Devember).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))
acf(df$demand, 100, main = &amp;#39;Autocorrelation&amp;#39;)
acf(df$demand, 1500, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)
pacf(df$demand, 1500, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The autocorrelation function shows a highly autocorrelated seasonal non-stationary process with, as expected, yearly and weekly cicles. The ACF alone, however, tells us little about the orders of dependence for ARMIA or AR processes. The PACF is better for AR models, and also shows the weekly and yearly seasons, although the correlation is lost faster with the lag.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data decomposition&lt;/h2&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of weeks at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of weekly seasonal time series
wts &amp;lt;- ts(ts, frequency = 7)
dec_wts &amp;lt;- decompose(wts)
plot(dec_wts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand minus week seasonal
df$demand_mws &amp;lt;- df$demand - as.numeric(dec_wts$season)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of yearly time series
yts &amp;lt;- ts(subset(df, yearday != &amp;#39;0229&amp;#39;)$demand_mws, frequency = 365)
dec_yts &amp;lt;- decompose(yts)
plot(dec_yts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decomposition of the yearly seasonal time series. 29th February days are excluded for frequency matching. The time series is formed out of the original observation minus the weekly seasonal data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;days365 &amp;lt;- which(df$yearday != &amp;#39;0229&amp;#39;)
february29ths &amp;lt;- which(df$yearday == &amp;#39;0229&amp;#39;)
df$demand_mwys[days365] &amp;lt;- df$demand_mws[days365] - as.numeric(dec_yts$season)
# Fill values on February 29th
df$demand_mwys[february29ths] &amp;lt;- df$demand_mws[february29ths]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# form new ts from original observations less the weekly and yearly seasonal data
par(mfrow = c(1, 1))
ts_mwys &amp;lt;- ts(df$demand_mwys, frequency = 1)
demandts_mwys &amp;lt;- xts(df$demand_mwys, df$date)
plot(demandts_mwys, main = &amp;#39;Energy Demand Less Seasonal Data&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_mwys_per_yearday &amp;lt;- aggregate(demand_mwys ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand_mwys, f = 1 / 45)
l &amp;lt;- length(avg_demand_mwys_per_yearday$demand_mwys)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting the result
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_mwys_per_yearday$demand_mwys, type = &amp;#39;l&amp;#39;, main = &amp;#39;Mean Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_mwys_per_yearday$demand_mwys - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the average daily demand of the demand less the seasonal data shows a new error rate much lower than the one seen before.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new acf and pacf created
par(mfrow = c(1, 2))
acf(df$demand_mwys, 100, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand_mwys, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonal-arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;seasonal ARIMA model&lt;/h2&gt;
&lt;p&gt;The initial ARIMA parameters have been found using the R &lt;span class=&#34;math inline&#34;&gt;\(auto.arima()\)&lt;/span&gt; function. The differencing parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is selected using the KPSS test. If the null hypothesis of stationarity is accepted when the KPSS is applied to the original time series, then &lt;span class=&#34;math inline&#34;&gt;\(d = 0\)&lt;/span&gt;. Otherwise, the series is differenced until the KPSS accepts the null hypothesis. After that, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; are selected using either AIC or BIC. The SARIMA model has been created using those ARIMA parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# forecast the error w/ test dataframe
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  pred &amp;lt;- c(pred, p)
  errs &amp;lt;- c(errs, p - df_test$demand[i])
  perc &amp;lt;- c(perc, (p - df_test$demand[i]) / df_test$demand[i])
  auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.299037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test datadrame &lt;strong&gt;(2.3%)&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# special days present less demand than others. Those days may be taken into account in order to reduce the error
specialday &amp;lt;- function(day) {
  correction = 0
  if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0101&amp;#39;, &amp;#39;0501&amp;#39;, &amp;#39;0106&amp;#39;, &amp;#39;0815&amp;#39;, &amp;#39;1012&amp;#39;, &amp;#39;1101&amp;#39;, &amp;#39;1206&amp;#39;, &amp;#39;1208&amp;#39;, &amp;#39;1224&amp;#39;, &amp;#39;1225&amp;#39;, &amp;#39;1226&amp;#39;, &amp;#39;1231&amp;#39;))
      correction = -100
  else if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0319&amp;#39;))
    correction = -50

# on Sunday, do not apply correction
  if (as.factor(strftime(day, format = &amp;#39;%A&amp;#39;)) == &amp;#39;Sunday&amp;#39;)
    return(0)
  return(correction)
}

model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  correction = specialday(df_test$date[i])
  pred &amp;lt;- c(pred, p + correction)
  errs &amp;lt;- c(errs, p + correction - df_test$demand[i])
  perc &amp;lt;- c(perc, (p + correction - df_test$demand[i]) / df_test$demand[i])
  if (!correction)
    auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  else
    auxts &amp;lt;- ts(c(auxts, p), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.956568&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test dataframe &lt;strong&gt;(1,96%)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecast-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecast Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(forecast(Arima(tail(ts, 200), model = model))) +
  labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Energy Demand (GWh)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Analysis</title>
      <link>/post/time_series_ap/time-series-analysis/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time_series_ap/time-series-analysis/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the airpassengers dataframe.&lt;/p&gt;
&lt;p&gt;The AirPassenger dataset in R provides monthly totals of US airline passengers, from 1949 to 1960.&lt;/p&gt;
&lt;p&gt;Description of dataframe airpassengers can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through analysis and modelling, preview a time series forecast&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess whether a linear regression or arima model is a best fit for the time series forecast as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;Stationarity test&lt;/li&gt;
&lt;li&gt;Fit a model using an algorithm&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(AirPassengers)
AP &amp;lt;- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is already of a time series class.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview of data
AP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Passenger numbers in (’000) per month for the relevant years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for missing values
sum(is.na(AP))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zero missing values GREAT!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test frequency
frequency(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;12 calendar months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test cycle
cycle(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949   1   2   3   4   5   6   7   8   9  10  11  12
## 1950   1   2   3   4   5   6   7   8   9  10  11  12
## 1951   1   2   3   4   5   6   7   8   9  10  11  12
## 1952   1   2   3   4   5   6   7   8   9  10  11  12
## 1953   1   2   3   4   5   6   7   8   9  10  11  12
## 1954   1   2   3   4   5   6   7   8   9  10  11  12
## 1955   1   2   3   4   5   6   7   8   9  10  11  12
## 1956   1   2   3   4   5   6   7   8   9  10  11  12
## 1957   1   2   3   4   5   6   7   8   9  10  11  12
## 1958   1   2   3   4   5   6   7   8   9  10  11  12
## 1959   1   2   3   4   5   6   7   8   9  10  11  12
## 1960   1   2   3   4   5   6   7   8   9  10  11  12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dataset summary
summary(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   104.0   180.0   265.5   280.3   360.5   622.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statistical values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the raw data using the base plot function
autoplot(AP) + labs(x=&amp;quot;Time&amp;quot;, y =&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(AP~cycle(AP), xlab=&amp;quot;Passenger Numbers (&amp;#39;000)&amp;quot;, ylab=&amp;quot;Months&amp;quot;, col=rgb(0.1,0.9,0.3,0.4), main=&amp;quot;Monthly Air Passengers Boxplot from 1949 to 1961&amp;quot;, horizontal=TRUE, notch=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The passenger numbers increase over time with each year which may be indicative of an increasing linear trend. Possible due to an increase in demand for flights and commercialisation of airlines in that time period.&lt;/li&gt;
&lt;li&gt;The boxplot shows more passengers travelling in months 6 to 9 with higher averages and higher variances than the other months, indicating seasonality within an apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US.&lt;/li&gt;
&lt;li&gt;The dataset appears to be a multiplicative time series, since passenger numbers increase, with a pattern of seasonality.&lt;/li&gt;
&lt;li&gt;There do not appear to be any outliers and there are no missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data decomposition&lt;/h3&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The multiplicative model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of passengers at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;decomposeAP &amp;lt;- decompose(AP,&amp;quot;multiplicative&amp;quot;)
autoplot(decomposeAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the “remainder”.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stationarity test&lt;/h3&gt;
&lt;p&gt;A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. I’ll use two methods to test the stationarity.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (ADF)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to test the stationarity of the time series, let’s run the Augmented Dickey-Fuller (ADF) Test. using the adf.test function from the tseries R package.&lt;/p&gt;
&lt;p&gt;First set the hypothesis test:&lt;/p&gt;
&lt;p&gt;The null hypothesis: that the time series is non stationary
The alternative hypothesis: that the time series is stationary&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in adf.test(AP): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  AP
## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a rule of thumb, where the p-value is less than 5%, we reject the null hypothesis. As the p-value is 0.01 which is less than 0.05 we reject the null in favour of the alternative hypothesis that the time series is stationary.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (Autocorrelation)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way to test for stationarity is to use autocorrelation. I’ll use autocorrelation function (acf). This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(acf(AP, plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# review random time series for any missing values
decomposeAP$random &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Jan       Feb       Mar       Apr       May       Jun       Jul
## 1949        NA        NA        NA        NA        NA        NA 0.9516643
## 1950 0.9626030 1.0714668 1.0374474 1.0140476 0.9269030 0.9650406 0.9835566
## 1951 1.0138446 1.0640180 1.0918541 1.0176651 1.0515825 0.9460444 0.9474041
## 1952 1.0258814 1.0939696 1.0134734 0.9695596 0.9632673 1.0003735 0.9468562
## 1953 0.9976684 1.0151646 1.0604644 1.0802327 1.0413329 0.9718056 0.9551933
## 1954 0.9829785 0.9232032 1.0044417 0.9943899 1.0119479 0.9978740 1.0237753
## 1955 1.0154046 0.9888241 0.9775844 1.0015732 0.9878755 1.0039635 1.0385512
## 1956 1.0066157 0.9970250 0.9876248 0.9968224 0.9985644 1.0275560 1.0217685
## 1957 0.9937293 0.9649918 0.9881769 0.9867637 0.9924177 1.0328601 1.0261250
## 1958 0.9954212 0.9522762 0.9469115 0.9383993 0.9715785 1.0261340 1.0483841
## 1959 0.9825176 0.9505736 0.9785278 0.9746440 1.0177637 0.9968613 1.0373136
## 1960 1.0039279 0.9590794 0.8940857 1.0064948 1.0173588 1.0120790        NA
##            Aug       Sep       Oct       Nov       Dec
## 1949 0.9534014 1.0022198 1.0040278 1.0062701 1.0118119
## 1950 0.9733720 1.0225047 0.9721928 0.9389527 1.0067914
## 1951 0.9397599 0.9888637 0.9938809 1.0235337 1.0250824
## 1952 0.9931171 0.9746302 1.0046687 1.0202797 1.0115407
## 1953 0.9894989 0.9934337 1.0192680 1.0009392 0.9915039
## 1954 0.9845184 0.9881036 0.9927613 0.9995143 0.9908692
## 1955 0.9831117 1.0032501 1.0003084 0.9827720 1.0125535
## 1956 1.0004765 1.0008730 0.9835071 0.9932761 0.9894251
## 1957 1.0312668 1.0236147 1.0108432 1.0212995 1.0005263
## 1958 1.0789695 0.9856540 0.9977971 0.9802940 0.9405687
## 1959 1.0531001 0.9974447 1.0013371 1.0134608 0.9999192
## 1960        NA        NA        NA        NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138], plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers Random Component from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acf of the residuals are centered around zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-model-using-an-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit a model using an algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Linear regression Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given there is an upwards trend we’ll look at a linear model first for comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(AP) + geom_smooth(method=&amp;quot;lm&amp;quot;) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This may not be the best model to fit as it doesn’t capture the seasonality and multiplicative effects over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. ARIMA Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arimaAP &amp;lt;- auto.arima(AP)
arimaAP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: AP 
## ARIMA(2,1,1)(0,1,0)[12] 
## 
## Coefficients:
##          ar1     ar2      ma1
##       0.5960  0.2143  -0.9819
## s.e.  0.0888  0.0880   0.0292
## 
## sigma^2 estimated as 132.3:  log likelihood=-504.92
## AIC=1017.85   AICc=1018.17   BIC=1029.35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d), an autoregressive term of second lag (p) and a moving average model of order 1 (q). Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units, in this case months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggtsdiag(arimaAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residual plots appear to be centered around 0 as noise, with no pattern. The arima model is a fairly good fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;forcasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forcasting&lt;/h3&gt;
&lt;p&gt;Plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecastAP &amp;lt;- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict the diamond price based on the 4 c&#39;s</title>
      <link>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the diamonds data frame.&lt;/p&gt;
&lt;p&gt;Descriotion of data frame diamonds can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;am i getting a fair deal when I purchase a diamond?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to build a predictive model for diamonds, that is going to help figure out whether a given diamond is a &lt;strong&gt;good deal&lt;/strong&gt; or a &lt;strong&gt;rip-off!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will use Linear Regression to predict the diamond price using other varaibles in the diamonds dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get to know the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    53940 obs. of  10 variables:
##  $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...
##  $ cut    : Ord.factor w/ 5 levels &amp;quot;Fair&amp;quot;&amp;lt;&amp;quot;Good&amp;quot;&amp;lt;..: 5 4 2 4 2 3 3 3 1 3 ...
##  $ color  : Ord.factor w/ 7 levels &amp;quot;D&amp;quot;&amp;lt;&amp;quot;E&amp;quot;&amp;lt;&amp;quot;F&amp;quot;&amp;lt;&amp;quot;G&amp;quot;&amp;lt;..: 2 2 2 6 7 7 6 5 2 5 ...
##  $ clarity: Ord.factor w/ 8 levels &amp;quot;I1&amp;quot;&amp;lt;&amp;quot;SI2&amp;quot;&amp;lt;&amp;quot;SI1&amp;quot;&amp;lt;..: 2 3 5 4 2 6 7 3 4 5 ...
##  $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...
##  $ table  : num  55 61 65 58 58 57 57 55 61 61 ...
##  $ price  : int  326 326 327 334 335 336 336 337 337 338 ...
##  $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...
##  $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...
##  $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      carat               cut        color        clarity     
##  Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065  
##  1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258  
##  Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194  
##  Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171  
##  3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066  
##  Max.   :5.0100                     I: 5422   VVS1   : 3655  
##                                     J: 2808   (Other): 2531  
##      depth           table           price             x         
##  Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 0.000  
##  1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710  
##  Median :61.80   Median :57.00   Median : 2401   Median : 5.700  
##  Mean   :61.75   Mean   :57.46   Mean   : 3933   Mean   : 5.731  
##  3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540  
##  Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  
##                                                                  
##        y                z         
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 4.720   1st Qu.: 2.910  
##  Median : 5.710   Median : 3.530  
##  Mean   : 5.735   Mean   : 3.539  
##  3rd Qu.: 6.540   3rd Qu.: 4.040  
##  Max.   :58.900   Max.   :31.800  
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scatterplot&lt;/h1&gt;
&lt;p&gt;We’ll start by examining two variables in the set. A scatterplot is a powerful tool to help you understand the relationship between two continuous variables.&lt;/p&gt;
&lt;p&gt;We can quickly see if the relationship is linear or not. In this case, we can use a variety of diamond characteristics to help us figure out whether the price advertised for any given diamond is reasonable or a rip-off.&lt;/p&gt;
&lt;p&gt;Consider the price of a diamond and it’s carat weight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat 
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  xlim(0, quantile(diamonds$carat,0.99)) +
  ylim(0, quantile(diamonds$price,0.99)) +
  ggtitle(&amp;#39;Price vs. Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The larger the diamond is (or the more carats it has), the more expensive the diamond is (price), which is probably what we would have expected.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat with linear trend
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99)) ) +
  scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99)) ) +
  ggtitle(&amp;quot;Price vs. Carat&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The linear trend line doesn’t go through the center of the data at some key places. It should curve in certain parts of the graph, i.e slope up more towards the end. If we tried to use this for predictions, we might be off some key places inside and outside of the existing data that we have displayed.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## sample 10,000 diamonds from the set to get a snapshop of the large dataframe
set.seed(20022012)
diamond_samp &amp;lt;- diamonds[sample(1:length(diamonds$price), 10000), ]
ggpairs(diamond_samp, 
        lower = list(continuous = wrap(&amp;quot;points&amp;quot;, shape = I(&amp;#39;.&amp;#39;))), 
        upper = list(combo = wrap(&amp;quot;box&amp;quot;, outlier.shape = I(&amp;#39;.&amp;#39;))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price is almost linearly correlated with carat: These are the critical factors driving price.&lt;/li&gt;
&lt;li&gt;Price appears related to &lt;strong&gt;cut/color/clarity&lt;/strong&gt; but, is not very clear from this plot.&lt;/li&gt;
&lt;li&gt;Price appears not to be directly related to depth and table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create hist of price and price(log10)
plot1 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;blue&amp;#39;, fill = &amp;#39;blue&amp;#39;, binwidth = 200) +
  scale_x_continuous(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price&amp;#39;) +
  theme_classic()

plot2 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;red&amp;#39;, fill = &amp;#39;red&amp;#39;, binwidth = 0.01) +
  scale_x_log10(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price(log10)&amp;#39;) +
  theme_classic()

grid.arrange(plot1, plot2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price histogram is skewed to the right, while the log10(price) tends to be a bell curve distributed. Also, the two peaks in the log10(price) plot coincides with the 1st and 3rd quantile of price.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create scatterplot of price and price(log10)
p1 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  ggtitle(&amp;quot;Price by Carat&amp;quot;) +
  theme_classic()

p2 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  scale_y_continuous(trans = log10_trans()) +
  ggtitle(&amp;quot;Price(log10) by Carat&amp;quot;) +
  theme_classic()
grid.arrange(p1, p2, ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the log scale, the prices look less dispersed at the high end of carat size and price, however, we can do better. Let’s try using the cube root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. Remember, volume is on a cubic scale!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### create a new function to transform the carat variable
cuberoot_trans = function() trans_new(&amp;#39;cuberoot&amp;#39;,
                                      transform = function(x) x^(1/3),
                                      inverse = function(x) x^3)

### use the cuberoot_trans function
ggplot(diamonds, aes(carat, price, color=clarity)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;quot;jitter&amp;quot;) + 
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The price(log10) is almost linear with cuberoot of carat. We can now move ahead and see how to model our data using just a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;price-vs.carat-and-clarity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Clarity&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=clarity)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Clarity&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Clarity&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clarity factors into the price of a diamond. Hence, a better clarity results in a higher price than lower end clarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-cut&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Cut&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=cut)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Cut&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Cut&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whilst cut does not show as obvious pattern as clarity, it’s still clear that with the same carat the diamonds with the best cut are priced higher. Hence, I think cut should be also included in the price prediction algorithm.
Note, clarity explains a lot of the variance found in price!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-color&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Color&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=color)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Color&amp;#39;, reverse = F,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Color&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds) +
  geom_bar(mapping = aes(clarity, fill=cut), position = &amp;quot;fill&amp;quot; ) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;dodgerblue&amp;quot;, &amp;quot;purple4&amp;quot;)) +
  labs(title = &amp;quot;Clearer diamonds tend to be of higher quality cut&amp;quot;,
       subtitle = &amp;quot;The majority of IF diamonds are an \&amp;quot;Ideal\&amp;quot; cut&amp;quot;) +
  ylab(&amp;quot;proportion&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This looks similar with previous clarity plot. Color should be also considered as an factor for price.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the Linear Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lm(I(log10(price)) ~ I(carat^(1/3)), diamonds)
m2 &amp;lt;- update(m1,~ . +carat)
m3 &amp;lt;- update(m2,~ . +cut)
m4 &amp;lt;- update(m3,~ . +color)
m5 &amp;lt;- update(m4,~ . +clarity)
mtable(m1, m2, m3, m4, m5, sdigits = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Calls:
## m1: lm(formula = I(log10(price)) ~ I(carat^(1/3)), data = diamonds)
## m2: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat, data = diamonds)
## m3: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut, 
##     data = diamonds)
## m4: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color, data = diamonds)
## m5: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color + clarity, data = diamonds)
## 
## ==============================================================================================
##                        m1             m2             m3             m4              m5        
## ----------------------------------------------------------------------------------------------
##   (Intercept)          1.225***       0.451***       0.380***       0.405***        0.180***  
##                       (0.003)        (0.008)        (0.008)        (0.007)         (0.004)    
##   I(carat^(1/3))       2.414***       3.721***       3.780***       3.665***        3.971***  
##                       (0.003)        (0.014)        (0.013)        (0.012)         (0.007)    
##   carat                              -0.494***      -0.505***      -0.431***       -0.474***  
##                                      (0.005)        (0.005)        (0.004)         (0.003)    
##   cut: .L                                            0.097***       0.097***        0.052***  
##                                                     (0.002)        (0.002)         (0.001)    
##   cut: .Q                                           -0.027***      -0.027***       -0.013***  
##                                                     (0.002)        (0.001)         (0.001)    
##   cut: .C                                            0.022***       0.022***        0.006***  
##                                                     (0.001)        (0.001)         (0.001)    
##   cut: ^4                                            0.008***       0.008***       -0.001     
##                                                     (0.001)        (0.001)         (0.001)    
##   color: .L                                                        -0.162***       -0.191***  
##                                                                    (0.001)         (0.001)    
##   color: .Q                                                        -0.056***       -0.040***  
##                                                                    (0.001)         (0.001)    
##   color: .C                                                         0.001          -0.006***  
##                                                                    (0.001)         (0.001)    
##   color: ^4                                                         0.012***        0.005***  
##                                                                    (0.001)         (0.001)    
##   color: ^5                                                        -0.007***       -0.001*    
##                                                                    (0.001)         (0.001)    
##   color: ^6                                                        -0.010***        0.001     
##                                                                    (0.001)         (0.001)    
##   clarity: .L                                                                       0.394***  
##                                                                                    (0.001)    
##   clarity: .Q                                                                      -0.104***  
##                                                                                    (0.001)    
##   clarity: .C                                                                       0.057***  
##                                                                                    (0.001)    
##   clarity: ^4                                                                      -0.027***  
##                                                                                    (0.001)    
##   clarity: ^5                                                                       0.011***  
##                                                                                    (0.001)    
##   clarity: ^6                                                                      -0.001     
##                                                                                    (0.001)    
##   clarity: ^7                                                                       0.014***  
##                                                                                    (0.001)    
## ----------------------------------------------------------------------------------------------
##   R-squared            0.9236         0.9349         0.9391         0.9514          0.9839    
##   adj. R-squared       0.9236         0.9349         0.9391         0.9514          0.9839    
##   sigma                0.1218         0.1124         0.1087         0.0972          0.0559    
##   F               652012.0628    387489.3661    138654.5235     87959.4667     173791.0840    
##   p                    0.0000         0.0000         0.0000         0.0000          0.0000    
##   Log-likelihood   37025.2108     41356.3916     43150.2943     49222.9505      79078.9821    
##   Deviance           800.2475       681.5220       637.6655       509.1030        168.2821    
##   AIC             -74044.4217    -82704.7832    -86284.5886    -98417.9011    -158115.9642    
##   BIC             -74017.7348    -82669.2007    -86213.4236    -98293.3623    -157929.1560    
##   N                53940          53940          53940          53940           53940         
## ==============================================================================================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We get some very nice R square values. We are accounting for almost all of the variance in price using carat, cut, color and clarity. If we want to know whether the price of a diamond is reasonable, we could use this model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thisDiamond &amp;lt;- data.frame(carat = 1, cut = &amp;#39;Very Good&amp;#39;,
                          color = &amp;#39;G&amp;#39;, clarity = &amp;#39;VS2&amp;#39;)
modelEstimate &amp;lt;- predict(m5, newdata = thisDiamond,
                         interval = &amp;quot;prediction&amp;quot;, level = .95)
10^modelEstimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 5232.111 4065.993 6732.668&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(modelEstimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 41.20984 36.93526 45.97911&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Tidying Project</title>
      <link>/project/data_tidying/data-tidying-project/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/data_tidying/data-tidying-project/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The data hosted at [data.world] and contains information about Sales in the US.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get Data&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://data.world/retail/department-store-sales&#34;&gt;Data Set 1&lt;/a&gt;: Sales from the Retail Trade and Food Services Report from the US Census. This dataset only covers Department Stores, though the report covers a wide range of retail types. [1992-2016]&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://data.world/garyhoov/retail-sales-growth&#34;&gt;Data Set 2&lt;/a&gt; US Retail Sales by Store Type with Growth Rate [2009-2014]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1992-2016
#https://data.world/retail/department-store-sales

GET(&amp;quot;https://query.data.world/s/gdk7iwtlisq6vkktmybqqr7hjjty5s&amp;quot;, write_disk(tf &amp;lt;- tempfile(fileext = &amp;quot;.xls&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://download.data.world/file_download/retail/department-store-sales/retail-trade-report-department-stores.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNDIzLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI0YWU5NmEzZjc4Y2EyOGE5MWM1ZDZlMTgxYzg5YjI0NjIzZDY0ZThlIn0.jBFlyy1aloE-EpeYyosD3iRaDPoY75DBqdh7suLoZxKcrsG8N5GtOiFb6sNMjTqqclsHX7P8RUw7T5sAArPbcw]
##   Date: 2019-01-26 21:57
##   Status: 200
##   Content-Type: application/vnd.ms-excel
##   Size: 62.5 kB
## &amp;lt;ON DISK&amp;gt;  /tmp/RtmpbDizpx/file425bddb3fff.xls&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- read_excel(tf)

#2009-2014
# https://data.world/garyhoov/retail-sales-growth
GET(&amp;quot;https://query.data.world/s/py7kinxvyuxjpzwdjs2ti4wdmui6bi&amp;quot;, write_disk(tf &amp;lt;- tempfile(fileext = &amp;quot;.xls&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://download.data.world/file_download/garyhoov/retail-sales-growth/US%20Retail%20Sales%20by%20Store%20Type%202009-2014.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNTAwLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI5OWRhMDIwMzRlY2Q1YmZmZTRmODFjYzJlMTg4ZmUxOGQyZmEyNDdlIn0.NLTr571lKSZMKhmvIFFQGuoVeFVFr9DrQ7nxBO3LOcLTJUrKivBxWpUrcJcY8dxnkL4FlGba3wsL65c3wLzzxA]
##   Date: 2019-01-26 21:57
##   Status: 200
##   Content-Type: application/vnd.ms-excel
##   Size: 169 kB
## &amp;lt;ON DISK&amp;gt;  /tmp/RtmpbDizpx/file425b7afcc0a6.xls&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- read_excel(tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * `` -&amp;gt; `..2`
## * `` -&amp;gt; `..3`
## * `` -&amp;gt; `..4`
## * `` -&amp;gt; `..5`
## * `` -&amp;gt; `..6`
## * … and 24 more&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## the the first row and make that the column names of the data frame
colnames(df2) &amp;lt;- df2[1,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;save-raw-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Save Raw Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## use saveRDS() to save each object as a .rds file 
saveRDS(df1, file = &amp;#39;df_department.rds&amp;#39;)
saveRDS(df2, file = &amp;#39;df_retail.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;wrangle-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrangle Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## work with df2
df_retail &amp;lt;- df2 %&amp;gt;%
  ## remove the r from the column names of df2
  magrittr::set_colnames(gsub(&amp;quot;r&amp;quot;,&amp;quot;&amp;quot;,df2[1,])) %&amp;gt;% 
  ## add a new column called &amp;quot;business&amp;quot;
  mutate(business = gsub(&amp;quot;[…]|[.]&amp;quot;,&amp;quot;&amp;quot;,`Kind of business`)) %&amp;gt;%
  ## filter to include Retail sales or Department stores sales
  filter(grepl(&amp;#39;Retail sales, total |Department stores&amp;#39;, business)) %&amp;gt;%
  ## only look at columns with year information in them
  select(.,c(matches(&amp;#39;19|20&amp;#39;),business)) %&amp;gt;%
  ## take year column and collapse them into a single column
  gather(., &amp;quot;year&amp;quot;, &amp;quot;n&amp;quot;, 1:(ncol(.)-1)) %&amp;gt;%
  ## make sure the count column `n` is numeric
  mutate(n=as.numeric(n)) %&amp;gt;%
  ## filter to only include the businesses we&amp;#39;re interested in
  filter(business == &amp;quot;Retail sales, total &amp;quot;| business==&amp;quot;Department stores &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## work with df1
df_department &amp;lt;- df1 %&amp;gt;% 
  ## split Period column into one column called &amp;quot;month&amp;quot; and one called &amp;quot;year&amp;quot;
  separate(Period, into = c(&amp;#39;month&amp;#39;, &amp;#39;year&amp;#39;), extra = &amp;#39;drop&amp;#39;, remove = FALSE) %&amp;gt;%
  ## add a column `value` which contains the 
  ## information from the `Value (in millions)` 
  mutate(value = `Value (in millions)`) %&amp;gt;%
  ## group the data frame by the `year` column
  group_by(year) %&amp;gt;%
  ## Summarize the data by creating a new column
  ## call this column `n` 
  ## have it contain the sum of the `value` column
  summarize(n = sum(value)) %&amp;gt;% 
  ### create a new column called `business`
  ## set the value of this column to  be &amp;quot;department stores&amp;quot; 
  ## for the entire data set 
  mutate(business = &amp;#39;department stores&amp;#39;) %&amp;gt;%
  ## reorder column names to be : business, year, n
  select(business, year, n)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;merging-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Merging Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Now, combine the two data frames
df_total &amp;lt;- left_join(df_retail, df_department, by = c(&amp;#39;business&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;n&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Retail Sales data
ggplot(df_retail, aes(x=year,y=n,colour=business)) +
  geom_point() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Department Sales data
ggplot(df_department, aes(x=year,y=n)) +
  geom_point() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-2.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Combined Data
ggplot(df_total, aes(x=year,y=as.numeric(n), colour=business)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-3.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build A Simple Blockchain</title>
      <link>/project/simple_blockchain-project/build-a-simple-blockchain/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/simple_blockchain-project/build-a-simple-blockchain/</guid>
      <description>


&lt;div id=&#34;intro-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro &amp;amp; Background&lt;/h1&gt;
&lt;p&gt;Blockchain is a data structure that was first introduced by Satoshi Nakamoto in the Bitcoin protocol white paper a decade ago. Bitcoin’s blockchain stores transaction data, but we can store any type of data in a blockchain.&lt;/p&gt;
&lt;p&gt;Ethereum, for example, enables users to store code snippets called ‘smart contracts’ in their blockchain. In this project, I build a simple blockchain in Python that uses proof-of-work consensus, just like the Bitcoin protocol does. There’s a lot of misunderstanding around what the blockchain is and what it can do, so I hope this project demonstrates how simple it really is. Enjoy!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis Approach&lt;/h1&gt;
&lt;p&gt;You can click on the link below to see the working code in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/2series/100_Days_of_ML_Code/blob/master/Simple_Blockchain.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/2series/100_Days_of_ML_Code/blob/master/Simple_Blockchain.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Password Detection Strength</title>
      <link>/project/password_detection_strength/password-detection-strength/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/password_detection_strength/password-detection-strength/</guid>
      <description>


&lt;div id=&#34;intro-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro &amp;amp; Background&lt;/h1&gt;
&lt;p&gt;A function written that uses regular expressions to make sure the password string it is passed is strong. A strong password is defined as one that is at least eight characters long, contains both uppercase and lowercase characters, and has at least one digit. You may need to test the string against multiple regex patterns to validate its strength.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis Approach&lt;/h1&gt;
&lt;p&gt;You can click on the link below to see the working code in Python and reproduce it to test the strength of your own password.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/2series/100_Days_of_ML_Code/blob/master/Password_Detection_Strength.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/2series/100_Days_of_ML_Code/blob/master/Password_Detection_Strength.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>mtcars Data Analysis</title>
      <link>/post/mtcars/2019-01-03-r-rmarkdown/</link>
      <pubDate>Thu, 03 Jan 2019 21:13:14 -0500</pubDate>
      
      <guid>/post/mtcars/2019-01-03-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the mtcars dataframe.&lt;/p&gt;
&lt;p&gt;Description of dataframe mtcars can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;is a vehicle with auto or manual transmission better in terms of miles p/gallons(mpg)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;quantify the (mpg) difference between auto &amp;amp; manual transmission.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess both queries from different perspectives employing a set of methodologies that can be broadly grouped as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Univariate Analysis on target varibale (mpg).&lt;/li&gt;
&lt;li&gt;Bivariate Analysis on target varibale &amp;amp; relevant covariates.&lt;/li&gt;
&lt;li&gt;Multivariate Analysis by estimating a set of regresssion models for the conditional mean of mpg. For model selection, I compare the best fit and forward stepwise selection process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Univariate Analysis&lt;/h1&gt;
&lt;p&gt;Analysing the target variable alone by splitting the observations into two groups, i.e. vehicles with auto or manual transmission. I shall deploy 3 analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute sample means by group ie auto VS manual.&lt;/li&gt;
&lt;li&gt;Validate if the difference of the group means are statistically significant by computing a 95% confidence interval for means’ difference.&lt;/li&gt;
&lt;li&gt;Verify the robustness of this result by executing a permutation test with Monte Carlo trials that shuffle the allocation group &amp;gt; mpg.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get to know the data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)
## &amp;#39;data.frame&amp;#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We notice that the set is relatively small! We’ll look at the desriptive statistics for each field - (min, 1st Q, Median, Mean, 3rd Q, max)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mtcars)
##       mpg             cyl             disp             hp       
##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  
##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  
##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  
##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  
##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  
##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  
##       drat             wt             qsec             vs        
##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  
##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  
##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  
##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  
##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  
##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  
##        am              gear            carb      
##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  
##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  
##  Median :0.0000   Median :4.000   Median :2.000  
##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  
##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  
##  Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-means-by-group&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample means by group&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

# dimensions
dim(mtcars)
## [1] 32 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(cars_auto); dim(cars_manu)
## [1] 19 11
## [1] 13 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample means mpg by group
mean(cars_auto$mpg); mean(cars_manu$mpg)
## [1] 17.14737
## [1] 24.39231&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(cars_auto$mpg); sd(cars_manu$mpg)
## [1] 3.833966
## [1] 6.166504&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# % increase in mpg based on the sample mean
(mean(cars_manu$mpg) - mean(cars_auto$mpg))/mean(cars_auto$mpg)
## [1] 0.4225103&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including plots&lt;/h1&gt;
&lt;p&gt;To get a feel for the distribution of some of the data to be analyzed, we plot some histograms, the first against mpg - auto transmission, the second against mpg - manual transission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(mpg ~ am, data = mtcars, col=rgb(0.3,0.2,0.5,0.6), ylab = &amp;quot;mpg&amp;quot;, xlab = &amp;quot;am&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg empirical mean of vehicles with manual transmission is greater than cars with auto transmission, however this also has a higher variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;confidence-interval-for-the-difference-of-the-group-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;95% confidence interval for the difference of the group means&lt;/h2&gt;
&lt;p&gt;The analysis on sample means concludes that sample mean of mpg for vehicles with manual trasmission is greater than automatic:&lt;/p&gt;
&lt;p&gt;Now I test if this difference (i.e. in the sample means) is statistically significant (from zero).&lt;/p&gt;
&lt;p&gt;I execute a t.test for unpaired samples: I assume inequality in variances for the two groups for the computation of the pooled variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 95% confidence interval for mean difference ####

# Question: is the sample mean difference significant?
t.test(cars_manu$mpg, cars_auto$mpg, paired = F, var.equal = F)
## 
##  Welch Two Sample t-test
## 
## data:  cars_manu$mpg and cars_auto$mpg
## t = 3.7671, df = 18.332, p-value = 0.001374
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   3.209684 11.280194
## sample estimates:
## mean of x mean of y 
##  24.39231  17.14737&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% interval does not contain 0&lt;/li&gt;
&lt;li&gt;sample mean difference is significant at 95% (p-value 0.1%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-test-on-groups-association&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Permutation test on groups association&lt;/h2&gt;
&lt;p&gt;I test the robustness of results obtained in the previous step.&lt;/p&gt;
&lt;p&gt;I execute a permutation test by shuffling the allocation mean &amp;gt; groups with 100,000 trials of Montecarlo simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### Permutation test ####
# what if I shuffle the am groups and calculate the mean?

# get target variable and group vectors
y = mtcars$mpg
group = mtcars$am
y; group
##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2
## [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4
## [29] 15.8 19.7 15.0 21.4
##  [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1

# baseline group means and difference
baselineMeans = tapply(mtcars$mpg, mtcars$am, mean)
baselineMeansDiff = baselineMeans[2] - baselineMeans[1]

tStat = function(w, g) mean(w[g == 1]) - mean(w[g == 0])
observedDiff = tStat(y, group)

# check if function works - should be 0:
baselineMeansDiff - observedDiff
## 1 
## 0

# execute shuffle:
permutations = sapply(1:100000, function(i) tStat(y, sample(group)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis:&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# shuffle experiment results plots:
par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))
hist(permutations, main = &amp;quot;Distribution of shuffled group mean differences&amp;quot;) # distribution of difference of averages of permuted groups
plot(permutations, type = &amp;quot;b&amp;quot;, main = &amp;quot;Shuffled group mean trials&amp;quot;, xlab = &amp;quot;trial&amp;quot;, ylab = &amp;quot;shuffled group mean differences&amp;quot;, ylim = c(-14, 14))
abline(h = observedDiff, col = &amp;quot;red&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there is not even 1 case where by chance I get a difference greater than the observed!
mean(permutations &amp;gt; observedDiff)
## [1] 1e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out of 100,000 trails only 0.002% has breached the observed value for the diffs in the group empirical means.&lt;/li&gt;
&lt;li&gt;concluding that empirical means diffs of groups is robust with regards to random reshuffling and is not likely to be generated by pure chance. &lt;em&gt;is this correct?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Analysis&lt;/h2&gt;
&lt;p&gt;Analyse the behaviour of target variable (mpg) conditional upon a set of explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

#### Visual inspection of all covariates ####
pairs(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 4 bivariate analysis: hp / wt / drat / disp ####
par(mfrow = c(2, 2), mar = c(2, 3, 2, 3))

# plot1
with(mtcars, plot(hp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs hp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(hp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(hp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model1_auto = lm(mpg ~ hp, data = cars_auto)
model1_manu = lm(mpg ~ hp, data = cars_manu)
abline(model1_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model1_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot2
with(mtcars, plot(wt, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs weight - by transmission type&amp;quot;)) # no data
with(cars_auto, points(wt, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(wt, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
abline(v = 3.2, lty = 2)

# plot 3
with(mtcars, plot(drat, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs drat - by transmission type&amp;quot;)) # no data
with(cars_auto, points(drat, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(drat, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model2_auto = lm(mpg ~ drat, data = cars_auto)
model2_manu = lm(mpg ~ drat, data = cars_manu)
abline(model2_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model2_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot 4
with(mtcars, plot(disp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs disp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(disp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(disp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
labels = with(mtcars, paste(as.character(disp), as.character(mpg), sep = &amp;quot;,&amp;quot;)) # generate point labels
with(mtcars, text(disp, mpg, labels = labels, cex = 0.7, pos = 2))
abline(v = 167.6, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg vs hp: linear negative relation: as horse power of the engine (hp) increases, the mileage (mpg) reduces. Vehicles with manual transmission seems however to be more efficient: the group restricted regression (blue) has a higher intercept. It has to be highlighted however, that the parameters of blue regression might be influenced by two extreme values with high hp - the regression should be re-estimated by removing the two datapoints.&lt;/li&gt;
&lt;li&gt;mpg vs weight: negative relation, the functional form might be non-linear (hyperbolic ?), as weight of the vehicle increases, the mileage decreases. The weight variable seems to provide perfect separation between manual and auto transmission vehilces, i.e. all vehicles that are heavier than 3.2 ton (circa) are auto and vice-versa.&lt;/li&gt;
&lt;li&gt;mpg vs drat: the functional form is not clear: it appears also to be an increase in the variance as the rear axel ratio (drat) increases. To verify this a regression model using all observations has to be estimated and analyse the residuals for verifying if the model is heteroskedastic.&lt;/li&gt;
&lt;li&gt;mpg vs disp: seems to have a negative (hyperbolic ?) relation: as the displacement (disp) of the engine increases, the mileage decreases. Also, in this case it seems that disp accounts for perfect separation in the transmission type: almost all vehilces with disp &amp;gt; 180 are auto.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate analysis&lt;/h2&gt;
&lt;p&gt;Run a set of regression models for estimating the impact of some predictions on mpg.&lt;/p&gt;
&lt;p&gt;For model selection, I employ the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manual selection of regressors: I hand pick regressors for:&lt;/li&gt;
&lt;li&gt;Best fit procedure&lt;/li&gt;
&lt;li&gt;Forward stepwise procedure&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual selection&lt;/h2&gt;
&lt;p&gt;Analysis of covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### analyse covariance matrix for regressor selection:
z &amp;lt;- cor(mtcars)
require(lattice)
## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levelplot(z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A model with only transmission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# only am
data = mtcars
data$am = as.factor(data$am)
model2 = lm(mpg ~ am, data = data)

# get results
summary(model2)
## 
## Call:
## lm(formula = mpg ~ am, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3923 -3.0923 -0.2974  3.2439  9.5077 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   17.147      1.125  15.247 1.13e-15 ***
## am1            7.245      1.764   4.106 0.000285 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.902 on 30 degrees of freedom
## Multiple R-squared:  0.3598, Adjusted R-squared:  0.3385 
## F-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is 17.15: exactly the same mean of mpg for vehicles with auto transmission.&lt;/li&gt;
&lt;li&gt;the coefficient of am is 7.24: exactly the difference of mpg means for vehicles with manual / auto transmission.&lt;/li&gt;
&lt;li&gt;the sum of intercept and am coefficient gives the mpg unconditional mean for vehicles with manual transmission.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;best-fit-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Best Fit Procedure&lt;/h2&gt;
&lt;p&gt;Run the best fit procedure for identifying the optimal number of regressors that minimises the cp, which is (…)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### model selection using leaps ####
data = mtcars
data$log_mpg = log(data$mpg) # add log of y

#### method 1. best fit ####
regfit.full = regsubsets(log_mpg ~. , data = data, nvmax = 10)
reg.summary = summary(regfit.full)
reg.summary
## Subset selection object
## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10)
## 11 Variables  (and intercept)
##      Forced in Forced out
## mpg      FALSE      FALSE
## cyl      FALSE      FALSE
## disp     FALSE      FALSE
## hp       FALSE      FALSE
## drat     FALSE      FALSE
## wt       FALSE      FALSE
## qsec     FALSE      FALSE
## vs       FALSE      FALSE
## am       FALSE      FALSE
## gear     FALSE      FALSE
## carb     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           mpg cyl disp hp  drat wt  qsec vs  am  gear carb
## 1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how I selected the optimal number of variables?
plot(reg.summary$cp, xlab = &amp;quot;Number of variables&amp;quot;, ylab = &amp;quot;cp&amp;quot;, type = &amp;quot;b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forward-stepwise-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forward Stepwise Procedure&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regfit.fwd = regsubsets(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
summary(regfit.fwd)
## Subset selection object
## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
## 11 Variables  (and intercept)
##      Forced in Forced out
## mpg      FALSE      FALSE
## cyl      FALSE      FALSE
## disp     FALSE      FALSE
## hp       FALSE      FALSE
## drat     FALSE      FALSE
## wt       FALSE      FALSE
## qsec     FALSE      FALSE
## vs       FALSE      FALSE
## am       FALSE      FALSE
## gear     FALSE      FALSE
## carb     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: forward
##           mpg cyl disp hp  drat wt  qsec vs  am  gear carb
## 1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; 
## 7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regfit.fwd, scale = &amp;quot;Cp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Appendix&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A model including all regressors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### lm with all variables / no split ####
# prepare data
data = mtcars
data$am = as.factor(data$am)

model1 = lm(mpg ~ ., data = data)

# get results
summary(model1)
## 
## Call:
## lm(formula = mpg ~ ., data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4506 -1.6044 -0.1196  1.2193  4.6271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept) 12.30337   18.71788   0.657   0.5181  
## cyl         -0.11144    1.04502  -0.107   0.9161  
## disp         0.01334    0.01786   0.747   0.4635  
## hp          -0.02148    0.02177  -0.987   0.3350  
## drat         0.78711    1.63537   0.481   0.6353  
## wt          -3.71530    1.89441  -1.961   0.0633 .
## qsec         0.82104    0.73084   1.123   0.2739  
## vs           0.31776    2.10451   0.151   0.8814  
## am1          2.52023    2.05665   1.225   0.2340  
## gear         0.65541    1.49326   0.439   0.6652  
## carb        -0.19942    0.82875  -0.241   0.8122  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.65 on 21 degrees of freedom
## Multiple R-squared:  0.869,  Adjusted R-squared:  0.8066 
## F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot residual analysis
par(mfrow = c(2, 2))
plot(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot hist
par(mfrow = c(1, 1))
hist(model1$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# normality test on residuals
shapiro.test(model1$residuals)
## 
##  Shapiro-Wilk normality test
## 
## data:  model1$residuals
## W = 0.95694, p-value = 0.2261&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Renewable Energy: The New Sharing Economy</title>
      <link>/publication/whitepaper/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/whitepaper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/example-slides/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
