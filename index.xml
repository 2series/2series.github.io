<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rihad Variawa</title>
    <link>/</link>
    <description>Recent content on Rihad Variawa</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Forecasting U.S. Federal Reserve Interest Rate Hikes</title>
      <link>/project/interest_forecast/interest_forecast/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/interest_forecast/interest_forecast/</guid>
      <description>


&lt;p&gt;The federal funds rate is the key interest rate that the U.S. Federal Reserve uses to influence economic growth. The Federal Open Market Committee meets regularly to decide whether to increase, decrease, or maintain the target interest rate. Their choice has important ramifications that cascade through the economy, so the announcement of the interest rates is eagerly awaited each month.&lt;/p&gt;
&lt;p&gt;In this analysis, I’ll use analytics to try to predict when the Fed will raise interest rates. I’ll look at monthly economic and political data dating back to the mid-1960’s. In this analysis, the dependent variable will be the binary outcome variable RaisedFedFunds, which takes value 1 if the federal funds rate was increased that month and 0 if it was lowered or stayed the same. For each month, the file federalFundsRate.csv.xz contains the following independent variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date: The date the change was announced.&lt;/li&gt;
&lt;li&gt;Chairman: The name of the Federal Reserve Chairman at the time the change was announced.&lt;/li&gt;
&lt;li&gt;PreviousRate: The federal funds rate in the prior month.&lt;/li&gt;
&lt;li&gt;Streak: The current streak of raising or not raising the rate, e.g. +8 indicates the rate has been increased 8 months in a row, whereas -3 indicates the rate has been lowered or stayed the same for 3 months in a row.&lt;/li&gt;
&lt;li&gt;GDP: The U.S. Gross Domestic Product, in Billions of Chained 2009 US Dollars.&lt;/li&gt;
&lt;li&gt;Unemployment: The unemployment rate in the U.S.&lt;/li&gt;
&lt;li&gt;CPI: The Consumer Price Index, an indicator of inflation, in the U.S.&lt;/li&gt;
&lt;li&gt;HomeownershipRate: The rate of homeownership in the U.S.&lt;/li&gt;
&lt;li&gt;DebtAsPctGDP: The U.S. national debt as a percentage of GDP&lt;/li&gt;
&lt;li&gt;DemocraticPres: Whether the sitting U.S. President is a Democrat (DemocraticPres=1) or a Republican (DemocraticPres=0)&lt;/li&gt;
&lt;li&gt;MonthsUntilElection: The number of remaining months until the next U.S. presidential election.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1---loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1 - Loading the Data&lt;/h3&gt;
&lt;p&gt;Use the read.csv function to load the contents of federalFundsRate.csv.xz file into a dataframe called fedFunds, using stringsAsFactors=FALSE.&lt;/p&gt;
&lt;p&gt;What proportion of months did the Fed raise the interest rate?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fedFunds &amp;lt;- read.csv(&amp;quot;federalFundsRate.csv.xz&amp;quot;)
str(fedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   585 obs. of  12 variables:
 $ Date               : Factor w/ 585 levels &amp;quot;1966-02-01&amp;quot;,&amp;quot;1966-03-01&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Chairman           : Factor w/ 8 levels &amp;quot;Bernanke, Ben&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
 $ PreviousRate       : num  4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ...
 $ Streak             : int  4 5 6 7 8 9 10 11 -1 1 ...
 $ GDP                : num  4202 4202 4202 4219 4219 ...
 $ Unemployment       : num  4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ...
 $ CPI                : num  31.9 32.1 32.2 32.3 32.4 ...
 $ HomeownershipRate  : num  63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ...
 $ DebtAsPctGDP       : num  40.3 4201.9 4201.9 39.2 4219.1 ...
 $ DemocraticPres     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ MonthsUntilElection: int  33 32 31 30 29 28 27 26 25 24 ...
 $ RaisedFedFunds     : int  1 1 1 1 1 1 1 0 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         Date                   Chairman    PreviousRate   
 1966-02-01:  1   Greenspan, Alan   :221   Min.   : 0.070  
 1966-03-01:  1   Bernanke, Ben     : 96   1st Qu.: 3.290  
 1966-04-01:  1   Burns, Arthur     : 96   Median : 5.390  
 1966-05-01:  1   Volcker, Paul     : 96   Mean   : 5.651  
 1966-06-01:  1   Martin, William M.: 48   3rd Qu.: 7.880  
 1966-07-01:  1   Miller, G. William: 17   Max.   :19.100  
 (Other)   :579   (Other)           : 11                   
     Streak             GDP         Unemployment         CPI        
 Min.   :-16.000   Min.   : 4202   Min.   : 3.400   Min.   : 31.88  
 1st Qu.: -2.000   1st Qu.: 6039   1st Qu.: 5.000   1st Qu.: 63.40  
 Median :  1.000   Median : 8907   Median : 5.900   Median :129.10  
 Mean   :  1.094   Mean   : 9450   Mean   : 6.181   Mean   :127.71  
 3rd Qu.:  3.000   3rd Qu.:12956   3rd Qu.: 7.300   3rd Qu.:180.00  
 Max.   : 27.000   Max.   :16206   Max.   :10.800   Max.   :237.63  
                                                                    
 HomeownershipRate  DebtAsPctGDP      DemocraticPres   MonthsUntilElection
 Min.   :63.20     Min.   :   30.60   Min.   :0.0000   Min.   : 0.00      
 1st Qu.:64.20     1st Qu.:   62.35   1st Qu.:0.0000   1st Qu.:12.00      
 Median :64.80     Median : 6039.16   Median :0.0000   Median :24.00      
 Mean   :65.41     Mean   : 6317.32   Mean   :0.4256   Mean   :23.58      
 3rd Qu.:66.50     3rd Qu.:10529.38   3rd Qu.:1.0000   3rd Qu.:35.00      
 Max.   :69.20     Max.   :16205.59   Max.   :1.0000   Max.   :47.00      
                                                                          
 RaisedFedFunds  
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :1.0000  
 Mean   :0.5026  
 3rd Qu.:1.0000  
 Max.   :1.0000  
                 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(fedFunds$RaisedFedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
291 294 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;294 / (291 + 294)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5025641&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2---the-longest-serving-fed-chair&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2 - The Longest-Serving Fed Chair&lt;/h3&gt;
&lt;p&gt;Which Fed Reserve Chair has presided over the most interest rate decisions?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(fedFunds$Chairman)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     Bernanke, Ben      Burns, Arthur    Greenspan, Alan 
                96                 96                221 
Martin, William M. Miller, G. William                N/A 
                48                 17                  2 
     Volcker, Paul      Yellen, Janet 
                96                  9 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;greenspan-alan&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Greenspan, Alan&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3---converting-variables-to-factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3 - Converting Variables to Factors&lt;/h3&gt;
&lt;p&gt;Convert the following variables to factors using the as.factor function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chairman&lt;/li&gt;
&lt;li&gt;DemocraticPres&lt;/li&gt;
&lt;li&gt;RaisedFedFunds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which of the following methods requires the dependent variables be stored as a factor variable when training a model for classification?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(fedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   585 obs. of  12 variables:
 $ Date               : Factor w/ 585 levels &amp;quot;1966-02-01&amp;quot;,&amp;quot;1966-03-01&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Chairman           : Factor w/ 8 levels &amp;quot;Bernanke, Ben&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
 $ PreviousRate       : num  4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ...
 $ Streak             : int  4 5 6 7 8 9 10 11 -1 1 ...
 $ GDP                : num  4202 4202 4202 4219 4219 ...
 $ Unemployment       : num  4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ...
 $ CPI                : num  31.9 32.1 32.2 32.3 32.4 ...
 $ HomeownershipRate  : num  63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ...
 $ DebtAsPctGDP       : num  40.3 4201.9 4201.9 39.2 4219.1 ...
 $ DemocraticPres     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ MonthsUntilElection: int  33 32 31 30 29 28 27 26 25 24 ...
 $ RaisedFedFunds     : int  1 1 1 1 1 1 1 0 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fedFunds$Chairman &amp;lt;- as.factor(fedFunds$Chairman)
fedFunds$DemocraticPres &amp;lt;- as.factor(fedFunds$DemocraticPres)
fedFunds$RaisedFedFunds &amp;lt;- as.factor(fedFunds$RaisedFedFunds)
str(fedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   585 obs. of  12 variables:
 $ Date               : Factor w/ 585 levels &amp;quot;1966-02-01&amp;quot;,&amp;quot;1966-03-01&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Chairman           : Factor w/ 8 levels &amp;quot;Bernanke, Ben&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
 $ PreviousRate       : num  4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ...
 $ Streak             : int  4 5 6 7 8 9 10 11 -1 1 ...
 $ GDP                : num  4202 4202 4202 4219 4219 ...
 $ Unemployment       : num  4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ...
 $ CPI                : num  31.9 32.1 32.2 32.3 32.4 ...
 $ HomeownershipRate  : num  63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ...
 $ DebtAsPctGDP       : num  40.3 4201.9 4201.9 39.2 4219.1 ...
 $ DemocraticPres     : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 2 2 2 2 2 2 2 2 2 2 ...
 $ MonthsUntilElection: int  33 32 31 30 29 28 27 26 25 24 ...
 $ RaisedFedFunds     : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 2 2 2 2 2 2 2 1 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;random-forest-randomforest&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Random forest (randomForest)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4---splitting-the-dataframe-into-a-training-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4 - Splitting the dataframe into a Training &amp;amp; Testing Set&lt;/h3&gt;
&lt;p&gt;Obtain a random training/testing set split with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(201)
library(caTools)
spl &amp;lt;- sample.split(fedFunds$RaisedFedFunds, 0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Split months into a training dataframe called “training” using the observations for which spl is TRUE and a testing dataframe called “testing” using the observations for which spl is FALSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training &amp;lt;- subset(fedFunds, spl == TRUE)
testing &amp;lt;- subset(fedFunds, spl == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do we use the sample.split() function to split into a training and testing set?
#### It balances the dependent variable between the training and testing sets&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5---training-a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5 - Training a Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Train a logistic regression model using independent variables “PreviousRate”, “Streak”, “Unemployment”, “HomeownershipRate”, “DemocraticPres”, and “MonthsUntilElection”, using the training set to obtain the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LogIntRate &amp;lt;- glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment +
                      HomeownershipRate + DemocraticPres + MonthsUntilElection, 
                  data = training, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which of the following characteristics is the most statistically significant associated with an increased chance of the fed funds rate being raised?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LogIntRate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + 
    HomeownershipRate + DemocraticPres + MonthsUntilElection, 
    family = binomial, data = training)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.8177  -1.0121   0.2301   1.0491   2.5297  

Coefficients:
                     Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)          9.121012   5.155774   1.769   0.0769 .  
PreviousRate        -0.003427   0.032350  -0.106   0.9156    
Streak               0.157658   0.025147   6.270 3.62e-10 ***
Unemployment        -0.047449   0.065438  -0.725   0.4684    
HomeownershipRate   -0.136451   0.076872  -1.775   0.0759 .  
DemocraticPres1      0.347829   0.233200   1.492   0.1358    
MonthsUntilElection -0.006931   0.007678  -0.903   0.3666    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 568.37  on 409  degrees of freedom
Residual deviance: 492.69  on 403  degrees of freedom
AIC: 506.69

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a-longer-consecutive-streak-of-months-in-which-the-fed-funds-rate-was-raised&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A longer consecutive STREAK of months in which the fed funds rate was raised&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6---predicting-using-a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6 - Predicting Using a Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Imagine you are an analyst at a bank and your manager has asked you to predict whether the fed funds rate will be raised next month.&lt;/p&gt;
&lt;p&gt;You know that the rate has been lowered for 3 straight months (Streak = -3) and that the previous month’s rate was 1.7%.&lt;/p&gt;
&lt;p&gt;The unemployment rate is 5.1% and the homeownership rate is 65.3%.&lt;/p&gt;
&lt;p&gt;The current U.S. president is a Republican and the next election will be held in 18 months. According to the logistic regression model you built in Problem 5.&lt;/p&gt;
&lt;p&gt;What is the predicted probability that the interest rate will be raised?&lt;/p&gt;
&lt;div id=&#34;previousrate-0.003427-streak0.157658-unemployment-0.047449-homeownershiprate-0.136451-democraticpres10.347829-monthsuntilelection-0.006931&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;9.121012 + PreviousRate&lt;em&gt;(-0.003427) + Streak&lt;/em&gt;0.157658 + Unemployment&lt;em&gt;(-0.047449) + HomeownershipRate&lt;/em&gt;(-0.136451) + DemocraticPres1&lt;em&gt;0.347829 + MonthsUntilElection&lt;/em&gt;(-0.006931)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;9.121012 + 1.7*(-0.003427) - 3*0.157658 + 
    5.1*(-0.047449) + 65.3*(-0.136451) + 
    0*0.347829 + 18*(-0.006931)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -0.6347861&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;need-to-plug-it-into-the-logistic-response-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;-0.6347861 ==&amp;gt; Need to plug it into the logistic response function&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;problem6 &amp;lt;- training[1, ]
problem6$PreviousRate &amp;lt;- 1.7
problem6$Streak &amp;lt;- -3
problem6$Unemployment &amp;lt;- 5.1
problem6$HomeownershipRate &amp;lt;- 65.3
problem6$DemocraticPres &amp;lt;- as.factor(0)
problem6$MonthsUntilElection &amp;lt;- 18
problem6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        Date           Chairman PreviousRate Streak      GDP Unemployment
1 1966-02-01 Martin, William M.          1.7     -3 4201.891          5.1
    CPI HomeownershipRate DebtAsPctGDP DemocraticPres MonthsUntilElection
1 31.88              65.3     40.26076              0                  18
  RaisedFedFunds
1              1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(problem6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1 obs. of  12 variables:
 $ Date               : Factor w/ 585 levels &amp;quot;1966-02-01&amp;quot;,&amp;quot;1966-03-01&amp;quot;,..: 1
 $ Chairman           : Factor w/ 8 levels &amp;quot;Bernanke, Ben&amp;quot;,..: 4
 $ PreviousRate       : num 1.7
 $ Streak             : num -3
 $ GDP                : num 4202
 $ Unemployment       : num 5.1
 $ CPI                : num 31.9
 $ HomeownershipRate  : num 65.3
 $ DebtAsPctGDP       : num 40.3
 $ DemocraticPres     : Factor w/ 1 level &amp;quot;0&amp;quot;: 1
 $ MonthsUntilElection: num 18
 $ RaisedFedFunds     : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;problem6PredProb &amp;lt;- predict(LogIntRate, newdata = problem6, type = &amp;quot;response&amp;quot;)
problem6PredProb&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1 
0.3464297 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-7---interpreting-model-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 7 - Interpreting Model Coefficients&lt;/h3&gt;
&lt;p&gt;What is the meaning of the coefficient labeled “DemocraticPres1” in the logistic regression summary output?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LogIntRate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + 
    HomeownershipRate + DemocraticPres + MonthsUntilElection, 
    family = binomial, data = training)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.8177  -1.0121   0.2301   1.0491   2.5297  

Coefficients:
                     Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)          9.121012   5.155774   1.769   0.0769 .  
PreviousRate        -0.003427   0.032350  -0.106   0.9156    
Streak               0.157658   0.025147   6.270 3.62e-10 ***
Unemployment        -0.047449   0.065438  -0.725   0.4684    
HomeownershipRate   -0.136451   0.076872  -1.775   0.0759 .  
DemocraticPres1      0.347829   0.233200   1.492   0.1358    
MonthsUntilElection -0.006931   0.007678  -0.903   0.3666    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 568.37  on 409  degrees of freedom
Residual deviance: 492.69  on 403  degrees of freedom
AIC: 506.69

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;when-the-president-is-democratic-the-odds-of-the-fed-funds-rate-increasing-are-41.6-higher-than-in-an-otherise-identical-month-i.e.identical-among-the-variables-in-the-model.-explanation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;When the president is Democratic, the odds of the fed funds rate increasing are 41.6% higher than in an otherise identical month (i.e. identical among the variables in the model). EXPLANATION:&lt;/h4&gt;
&lt;p&gt;The coefficients of the model are the log odds associated with that variable; so we see that the odds of being sold are exp(0.347829)=1.41599 those of an otherwise identical month. This means the month is predicted to have 41.6% higher odds of being sold.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-8---obtaining-test-set-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 8 - Obtaining Test Set Predictions&lt;/h3&gt;
&lt;p&gt;Using our logistic regression model, obtain predictions on the test-set. Then, using a probability threshold of 0.5, create a confusion matrix for the test-set.&lt;/p&gt;
&lt;p&gt;On how many test-set observations does our logistic regression model make a different prediction than the prediction the naive baseline model would make?&lt;/p&gt;
&lt;p&gt;(Remember that the naive baseline model we use always predicts the most frequent outcome in the training set for all observations in the test-set.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(testing)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   175 obs. of  12 variables:
 $ Date               : Factor w/ 585 levels &amp;quot;1966-02-01&amp;quot;,&amp;quot;1966-03-01&amp;quot;,..: 14 15 16 18 19 31 32 37 38 39 ...
 $ Chairman           : Factor w/ 8 levels &amp;quot;Bernanke, Ben&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...
 $ PreviousRate       : num  5 4.53 4.05 3.98 3.79 6.02 6.03 6.3 6.61 6.79 ...
 $ Streak             : int  1 -1 -2 1 -1 -2 1 2 3 4 ...
 $ GDP                : num  4325 4325 4329 4329 4366 ...
 $ Unemployment       : num  3.8 3.8 3.8 3.9 3.8 3.7 3.5 3.4 3.4 3.4 ...
 $ CPI                : num  33 33 33.1 33.3 33.4 34.9 35 35.7 35.8 36.1 ...
 $ HomeownershipRate  : num  63.3 63.3 63.9 63.9 63.8 64.1 64.1 64.1 64.1 64.1 ...
 $ DebtAsPctGDP       : num  4324.9 4324.9 37.9 4328.7 38.8 ...
 $ DemocraticPres     : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 2 2 2 2 2 2 2 1 1 1 ...
 $ MonthsUntilElection: int  20 19 18 16 15 3 2 45 44 43 ...
 $ RaisedFedFunds     : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 1 1 1 1 2 2 1 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredProb &amp;lt;- predict(LogIntRate, newdata = testing, type = &amp;quot;response&amp;quot;)
table(testing$RaisedFedFunds, PredProb &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    60   27
  1    31   57&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(training$RaisedFedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
204 206 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;were-predicted-less-than-0.5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;91 (60 + 31 were predicted less than 0.5)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-9---computing-test-set-auc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 9 - Computing Test-Set AUC&lt;/h3&gt;
&lt;p&gt;What is the test-set AUC of the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:stats&amp;#39;:

    lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredTestLogROCR &amp;lt;- prediction(PredProb, testing$RaisedFedFunds)
performance(PredTestLogROCR, &amp;quot;auc&amp;quot;)@y.values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 0.704023&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-10---interpreting-auc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 10 - Interpreting AUC&lt;/h3&gt;
&lt;p&gt;What is the meaning of the AUC?
#### The proportion of the time the model can differentiate between a randomly selected month during which the fed funds were raised and a randomly selected month during which the federal funds were not raised.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-11---roc-curves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 11 - ROC Curves&lt;/h3&gt;
&lt;p&gt;Which logistic regression threshold is associated with the upper-right corner of the ROC plot (true positive rate 1 and false positive rate 1)?
#### 0&lt;/p&gt;
&lt;div id=&#34;explanation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;EXPLANATION&lt;/h4&gt;
&lt;p&gt;A model with threshold 0 predicts 1 for all observations, yielding a 100% true positive rate and a 100% false positive rate.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-12---roc-curves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 12 - ROC Curves&lt;/h3&gt;
&lt;p&gt;Plot the colorized ROC curve for the logistic regression model’s performance on the test-set. At roughly which logistic regression cut-off does the model achieve a true positive rate of 85% and a false positive rate of 60%?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRperf &amp;lt;- performance(PredTestLogROCR, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(ROCRperf, colorize = TRUE, 
     print.cutoffs.at = seq(0, 1, by = 0.1), 
     text.adj = c(-0.2, 1.7))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/interest_forecast/interest_forecast_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.37&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-13---cross-validation-to-select-parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 13 - Cross-Validation to Select Parameters&lt;/h3&gt;
&lt;p&gt;Which of the following best describes how 10-fold cross-validation works when selecting between 2 different parameter values?
#### 20 models are trained on subsets of the training set and evaluated on a portion of the training set&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-14---cross-validation-for-a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 14 - Cross-Validation for a CART Model&lt;/h3&gt;
&lt;p&gt;Set the random seed to 201 (even though you have already done so earlier in the problem).&lt;/p&gt;
&lt;p&gt;Then use the caret package and the train function to perform 10-fold cv with the training data set to select the best cp value for a CART model that predicts the dependent variable “RaisedFedFunds” using the independent variables “PreviousRate,” “Streak,” “Unemployment,” “HomeownershipRate,” “DemocraticPres,” and “MonthsUntilElection.” Select the cp value from a grid consisting of the 50 values 0.001, 0.002, …, 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)
set.seed(201)

# define cross-validation experiment
numFolds &amp;lt;- trainControl(method = &amp;quot;cv&amp;quot;, number = 10)
cpGrid &amp;lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Define cv experiment&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numFolds &amp;lt;- trainControl(method = &amp;quot;cv&amp;quot;, number = 10)
cpGrid &amp;lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perform the cv&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainCV &amp;lt;- train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + 
                     HomeownershipRate + DemocraticPres + MonthsUntilElection, 
                 data = training, 
                 method = &amp;quot;rpart&amp;quot;, 
                 trControl = numFolds, 
                 tuneGrid = cpGrid)
trainCV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;CART 

410 samples
  6 predictor
  2 classes: &amp;#39;0&amp;#39;, &amp;#39;1&amp;#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 369, 368, 368, 369, 370, 370, ... 
Resampling results across tuning parameters:

  cp     Accuracy   Kappa    
  0.001  0.6248461  0.2498288
  0.002  0.6366928  0.2737781
  0.003  0.6465099  0.2940348
  0.004  0.6465099  0.2940348
  0.005  0.6465099  0.2940348
  0.006  0.6513298  0.3037278
  0.007  0.6513298  0.3037278
  0.008  0.6488298  0.2987278
  0.009  0.6462108  0.2934897
  0.010  0.6437718  0.2887290
  0.011  0.6532956  0.3075752
  0.012  0.6532956  0.3075752
  0.013  0.6532956  0.3075752
  0.014  0.6386527  0.2782894
  0.015  0.6386527  0.2782894
  0.016  0.6386527  0.2782894
  0.017  0.6287718  0.2585275
  0.018  0.6287718  0.2585275
  0.019  0.6287718  0.2585275
  0.020  0.6385918  0.2780499
  0.021  0.6385918  0.2780499
  0.022  0.6385918  0.2784431
  0.023  0.6385918  0.2784431
  0.024  0.6432956  0.2882859
  0.025  0.6432956  0.2882859
  0.026  0.6605575  0.3228097
  0.027  0.6605575  0.3228097
  0.028  0.6680575  0.3378097
  0.029  0.6680575  0.3378097
  0.030  0.6680575  0.3381290
  0.031  0.6680575  0.3381290
  0.032  0.6680575  0.3381290
  0.033  0.6680575  0.3381290
  0.034  0.6680575  0.3381290
  0.035  0.6680575  0.3381290
  0.036  0.6729355  0.3474661
  0.037  0.6729355  0.3474661
  0.038  0.6729355  0.3474661
  0.039  0.6729355  0.3474661
  0.040  0.6729355  0.3474661
  0.041  0.6729355  0.3474661
  0.042  0.6729355  0.3474661
  0.043  0.6729355  0.3474661
  0.044  0.6729355  0.3474661
  0.045  0.6729355  0.3474661
  0.046  0.6729355  0.3474661
  0.047  0.6729355  0.3474661
  0.048  0.6729355  0.3474661
  0.049  0.6729355  0.3474661
  0.050  0.6729355  0.3474661

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.05.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What cp value maximizes the cv accuracy?
#### 0.016&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-15---train-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 15 - Train CART Model&lt;/h3&gt;
&lt;p&gt;Create and plot the CART model trained with the parameter identified in Problem 14, again predicting the dependent variable using “PreviousRate”, “Streak”, “Unemployment”, “HomeownershipRate”, “DemocraticPres”, and “MonthsUntilElection”.&lt;/p&gt;
&lt;p&gt;What variable is used as the first (upper-most) split in the tree?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
TreeIntRate &amp;lt;- trainCV$finalModel
prp(TreeIntRate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/interest_forecast/interest_forecast_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TreeIntRate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;n= 410 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 410 204 1 (0.4975610 0.5024390)  
  2) Streak&amp;lt; 2.5 300 115 0 (0.6166667 0.3833333) *
  3) Streak&amp;gt;=2.5 110  19 1 (0.1727273 0.8272727) *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TreeIntRate2 &amp;lt;- rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + 
                          HomeownershipRate + DemocraticPres + 
                          MonthsUntilElection, 
                      data = training, 
                      method = &amp;quot;class&amp;quot;, 
                      cp = 0.016)
prp(TreeIntRate2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/interest_forecast/interest_forecast_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;streak&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Streak&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-16---predicting-using-a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 16 - Predicting Using a CART Model&lt;/h3&gt;
&lt;p&gt;If you were to use the CART model you created in Problem 15 to explore the question asked of the analyst in Problem 6, what would you predict for next month?&lt;/p&gt;
&lt;p&gt;Remember: The rate has been lowered for 3 straight months (Streak = -3). The previous month’s rate was 1.7%. The unemployment rate is 5.1%. The homeownership rate is 65.3%. The current U.S. president is a Republican and the next election will be held in 18 months.
#### The Fed will not raise the federal funds rate. The Fed will not raise the fed funds rate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-17---test-set-accuracy-for-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 17 - Test-Set Accuracy for CART Model&lt;/h3&gt;
&lt;p&gt;Using the CART model you created in Problem 15, obtain predictions on the test-set (using the parameter type=“class” with the predict function).&lt;/p&gt;
&lt;p&gt;Then, create a confusion matrix for the test-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredClassTree &amp;lt;- predict(TreeIntRate2, newdata = testing, type = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of your CART model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(PredClassTree, testing$RaisedFedFunds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             
PredClassTree  0  1
            0 64 40
            1 23 48&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(64 + 48) / nrow(testing)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.64&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Patterns In Renewable Energy Generation</title>
      <link>/project/energy_patterns/energy_patterns/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/energy_patterns/energy_patterns/</guid>
      <description>


&lt;p&gt;The use of coal in the United States peaked in 2005, and since then has decreased by 25%, being replaced by renewable energy sources and more efficient use (Lovins, 2014). As the US pursues a portfolio of more diverse, sustainable and secure energy sources, there are many questions to consider.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What are effective factors in incentivizing states to adopt more environmentally friendly energy generation methods?&lt;/li&gt;
&lt;li&gt;How do these factors vary by state?&lt;/li&gt;
&lt;li&gt;How can we direct resources to different places in the country and ensure that they effectively drive renewable energy sources adoption?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To derive insights and explore these questions, I’ll take a combination of generation, usage, and greenhouse emission data by state and combine it with macro-economic and political information.&lt;/p&gt;
&lt;p&gt;For this analysis, I’ve gathered data from various sources to include the following information for each state within the U.S. for the years spanning year 2000 to year 2013. The aggregated dataset energy.csv.xz results in a total of 27 variables and 699 observations. Each observation contains one record per state per year. Here’s a detailed description of the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GenTotal: Annual generation of energy using all types of energy sources (coal, nuclear, hydroelectric, solar, etc.) normalized by the state population at a given year.&lt;/li&gt;
&lt;li&gt;GenTotalRenewable: Annual generation of energy using all renewable energy sources normalized by the state population at a given year.&lt;/li&gt;
&lt;li&gt;GenHydro, GenSolar: Annual generation of energy using each type of energy source as a percent of the total energy generation.&lt;/li&gt;
&lt;li&gt;GenTotalRenewableBinary, GenSolarBinary: 1 if generation from solar or other renewable energy sources increased between a year n and a year n+1. 0 if it did not increase.&lt;/li&gt;
&lt;li&gt;AllSourcesCO2, AllSourcesSO2 and AllSourcesNOx: Annual emissions per state in metric tons, normalized by the respective state population at a given year and caused by all energy generation sources.&lt;/li&gt;
&lt;li&gt;EPriceResidential, EPriceCommercial, EPriceIndustrial, EPriceTransportation, EPriceTotal: Average electricity price per state, per sector (residential, industrial, commercial, etc.)&lt;/li&gt;
&lt;li&gt;ESalesResidential, ESalesCommercial, ESalesIndustrial, ESalesTransportation, ESalesTotal: Annual normalized sales of electricity per state, per sector.&lt;/li&gt;
&lt;li&gt;CumlRegulatory, CumlFinancial: Number of energy-related financial incentives and regulations created by a state per year.&lt;/li&gt;
&lt;li&gt;Demographic data such as annual wages per capita and presidential results (0 if a state voted republican, 1 is democrat).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1---total-renewable-energy-generation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1 - Total Renewable Energy Generation&lt;/h3&gt;
&lt;p&gt;Load energy.csv into a data frame called energy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;energy &amp;lt;- read.csv(&amp;quot;energy.csv.xz&amp;quot;)
str(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   699 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
 $ YEAR                   : int  2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 ...
 $ GenTotal               : num  9.81 10.64 10.54 9.79 9.9 ...
 $ GenHydro               : num  0.163 0.2 0.213 0.25 0.23 ...
 $ GenSolar               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.163 0.2 0.214 0.25 0.231 ...
 $ GenSolarBinary         : int  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewableBinary: int  1 1 1 0 0 0 1 0 1 1 ...
 $ AllSourcesCO2          : num  7.26 7.31 6.94 6.15 7.25 ...
 $ AllSourcesSO2          : num  0.0224 0.01193 0.01148 0.00675 0.00659 ...
 $ AllSourcesNOx          : num  0.0289 0.0277 0.0294 0.0242 0.0377 ...
 $ EPriceResidential      : num  11.4 12.1 12.1 12 12.4 ...
 $ EPriceCommercial       : num  9.77 10.29 10.13 10.49 10.99 ...
 $ EPriceIndustrial       : num  7.56 7.61 7.65 7.86 8.33 ...
 $ EPriceTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EPriceTotal            : num  10.1 10.5 10.5 10.5 11 ...
 $ EsalesResidential      : num  0.349 0.347 0.354 0.357 0.356 ...
 $ EsalesCommercial       : num  0.421 0.42 0.41 0.444 0.449 ...
 $ EsalesIndustrial       : num  0.195 0.198 0.199 0.198 0.195 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  0.0343 0.0356 0.0379 0 0 ...
 $ EsalesTotal            : num  8.46 8.61 8.51 8.59 8.78 ...
 $ CumlFinancial          : int  1 1 1 1 1 2 7 7 10 10 ...
 $ CumlRegulatory         : int  1 1 1 1 1 1 2 2 2 3 ...
 $ Total.salary           : num  17.6 18.7 19.6 20.4 21.4 ...
 $ presidential.results   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     STATE          YEAR         GenTotal         GenHydro      
 AK     : 14   Min.   :2000   Min.   : 4.591   Min.   :0.00000  
 AL     : 14   1st Qu.:2003   1st Qu.:10.037   1st Qu.:0.00888  
 AR     : 14   Median :2006   Median :14.647   Median :0.02291  
 AZ     : 14   Mean   :2006   Mean   :16.956   Mean   :0.09853  
 CA     : 14   3rd Qu.:2010   3rd Qu.:18.018   3rd Qu.:0.07041  
 CO     : 14   Max.   :2013   Max.   :92.097   Max.   :0.92076  
 (Other):615                                                    
    GenSolar          GenTotalRenewable GenSolarBinary  
 Min.   :-1.300e-08   Min.   :0.00000   Min.   :0.0000  
 1st Qu.: 0.000e+00   1st Qu.:0.02027   1st Qu.:0.0000  
 Median : 0.000e+00   Median :0.04475   Median :0.0000  
 Mean   : 3.344e-04   Mean   :0.12245   Mean   :0.2318  
 3rd Qu.: 0.000e+00   3rd Qu.:0.10771   3rd Qu.:0.0000  
 Max.   : 2.045e-02   Max.   :0.92076   Max.   :1.0000  
                                                        
 GenTotalRenewableBinary AllSourcesCO2      AllSourcesSO2    
 Min.   :0.0000          Min.   : 0.01059   Min.   :0.00003  
 1st Qu.:0.0000          1st Qu.: 4.84645   1st Qu.:0.00925  
 Median :1.0000          Median : 8.34005   Median :0.02375  
 Mean   :0.5923          Mean   :11.61430   Mean   :0.03687  
 3rd Qu.:1.0000          3rd Qu.:12.99889   3rd Qu.:0.04197  
 Max.   :1.0000          Max.   :93.96429   Max.   :0.34568  
                         NA&amp;#39;s   :50         NA&amp;#39;s   :50       
 AllSourcesNOx     EPriceResidential EPriceCommercial EPriceIndustrial
 Min.   :0.00067   Min.   : 5.130    Min.   : 4.240   Min.   : 3.010  
 1st Qu.:0.00604   1st Qu.: 7.965    1st Qu.: 6.760   1st Qu.: 4.695  
 Median :0.01428   Median : 9.490    Median : 8.100   Median : 5.820  
 Mean   :0.02058   Mean   :10.424    Mean   : 8.969   Mean   : 6.660  
 3rd Qu.:0.02288   3rd Qu.:11.825    3rd Qu.:10.065   3rd Qu.: 7.500  
 Max.   :0.35610   Max.   :37.340    Max.   :34.880   Max.   :30.820  
 NA&amp;#39;s   :50                                                           
 EPriceTransportation  EPriceTotal     EsalesResidential EsalesCommercial
 Min.   : 0.000       Min.   : 4.170   Min.   :0.1594    Min.   :0.1690  
 1st Qu.: 0.000       1st Qu.: 6.515   1st Qu.:0.3297    1st Qu.:0.2816  
 Median : 1.615       Median : 7.860   Median :0.3563    Median :0.3385  
 Mean   : 4.131       Mean   : 8.809   Mean   :0.3595    Mean   :0.3411  
 3rd Qu.: 7.940       3rd Qu.:10.095   3rd Qu.:0.3926    3rd Qu.:0.3877  
 Max.   :15.440       Max.   :34.040   Max.   :0.5287    Max.   :0.5381  
 NA&amp;#39;s   :1                                                               
 EsalesIndustrial  EsalesTransportation  EsalesOther      EsalesTotal    
 Min.   :0.06372   Min.   :0.0000000    Min.   :0.0000   Min.   : 6.745  
 1st Qu.:0.21331   1st Qu.:0.0000000    1st Qu.:0.0000   1st Qu.:10.336  
 Median :0.29978   Median :0.0000000    Median :0.0092   Median :13.153  
 Mean   :0.29232   Mean   :0.0010349    Mean   :0.0170   Mean   :13.145  
 3rd Qu.:0.35791   3rd Qu.:0.0002786    3rd Qu.:0.0216   3rd Qu.:15.353  
 Max.   :0.59561   Max.   :0.0229279    Max.   :0.1074   Max.   :31.336  
                                        NA&amp;#39;s   :449                      
 CumlFinancial    CumlRegulatory    Total.salary   presidential.results
 Min.   :  0.00   Min.   : 0.000   Min.   :10.65   Min.   :0.0000      
 1st Qu.:  2.00   1st Qu.: 3.000   1st Qu.:16.84   1st Qu.:0.0000      
 Median :  8.00   Median : 6.000   Median :19.51   Median :0.0000      
 Mean   : 17.58   Mean   : 6.838   Mean   :20.01   Mean   :0.4578      
 3rd Qu.: 23.50   3rd Qu.:10.000   3rd Qu.:22.83   3rd Qu.:1.0000      
 Max.   :154.00   Max.   :42.000   Max.   :33.85   Max.   :1.0000      
                                                                       
     Import      
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.0000  
 Mean   :0.3433  
 3rd Qu.:1.0000  
 Max.   :1.0000  
                 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Renewable energy sources are considered to include geothermal, hydroelectric, biomass, solar and wind.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which state in the US seems to have the highest total generation of energy from renewable sources (using the variable GenTotalRenewable)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;energy[which.max(energy$GenTotalRenewable), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    STATE YEAR GenTotal  GenHydro GenSolar GenTotalRenewable
169    ID 2000 9.165016 0.9207631        0         0.9207631
    GenSolarBinary GenTotalRenewableBinary AllSourcesCO2 AllSourcesSO2
169              0                       0     0.6368657   0.004590802
    AllSourcesNOx EPriceResidential EPriceCommercial EPriceIndustrial
169   0.002714775              5.39             4.24             3.11
    EPriceTransportation EPriceTotal EsalesResidential EsalesCommercial
169                    0        4.17         0.3068433        0.3095307
    EsalesIndustrial EsalesTransportation EsalesOther EsalesTotal
169         0.368208                    0  0.01541804    17.57071
    CumlFinancial CumlRegulatory Total.salary presidential.results Import
169             2              2     12.81243                    0      1&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;idaho-id&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Idaho (ID)&lt;/h4&gt;
&lt;p&gt;Which year did the above state produce the highest energy generation from renewable resources?
#### 2000&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2---relationship-between-politics-and-greenhouse-emissions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2 - Relationship Between Politics and Greenhouse Emissions&lt;/h3&gt;
&lt;p&gt;What is the average CO2 emissions from all sources of energy for: Note Using na.rm = TRUE in our calculations!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;states during years in which they voted republican?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(subset(energy, presidential.results == 0)$AllSourcesCO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 16.44296&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;states during years in which they voted democrat?&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(subset(energy, presidential.results == 1)$AllSourcesCO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5.783781&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;States that voted democrat have on average higher NOx emissions than states that voted republican across all years.&lt;/p&gt;
&lt;p&gt;Is this statement true or false?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(subset(energy, presidential.results == 0)$AllSourcesNOx, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.02985461&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(subset(energy, presidential.results == 1)$AllSourcesNOx, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.009377028&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;false&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;False&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3---relationship-between-greenhouse-emissions-and-energy-sales&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3 - Relationship Between Greenhouse Emissions and Energy Sales&lt;/h3&gt;
&lt;p&gt;What is the correlation between overall CO2 emissions and energy sales made to industrial facilities?&lt;/p&gt;
&lt;p&gt;Note that the variables AllSourcesCO2 and EsalesIndustrial contain NAs. Using the parameter: use=“complete” to handle NAs in this question!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(energy$AllSourcesCO2, energy$EsalesIndustrial, use = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5385867&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Choose the correct answers from the following statements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(energy$AllSourcesSO2, energy$EsalesIndustrial, use = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4812317&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(energy$AllSourcesNOx, energy$EsalesResidential, use = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -0.5038829&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(energy$AllSourcesCO2, energy$EsalesCommercial, use = &amp;quot;complete&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -0.373383&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;overall-so2-emissions-are-likely-higher-with-increased-industrial-energy-sales&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Overall SO2 emissions are likely higher with increased industrial energy sales&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4---boxplot-of-energy-prices-per-state&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4 - Boxplot of Energy Prices per State&lt;/h3&gt;
&lt;p&gt;Creating a boxplot of the total energy price (EPriceTotal) by State across the data, and a table summarizing the mean of EPriceTotal by State.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;priceBoxplot &amp;lt;- ggplot(energy, aes(STATE, EPriceTotal)) 
priceBoxplot + geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/energy_patterns/energy_patterns_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What observations do we make?
#### - The boxplot shows a clear outlier, the state of Hawaii, with much higher energy price compared to the rest of the U.S.
#### - When looking at the average energy prices, there seems to be three price tiers ($5-$9, $10-$14, and $20+)&lt;/p&gt;
&lt;p&gt;Which state has the lowest average energy price of all? Using a table to explore this question.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(energy$EPriceTotal))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(energy$EPriceTotal)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8.809213&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avgPriceByState &amp;lt;- aggregate(EPriceTotal ~ STATE, energy, mean)
avgPriceByState[which.min(avgPriceByState$EPriceTotal), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   STATE EPriceTotal
50    WY        5.51&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;wyoming-wy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Wyoming (WY)&lt;/h4&gt;
&lt;p&gt;Is this state associated with the highest mean total energy generation (GenTotal)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avgGenByState &amp;lt;- aggregate(GenTotal ~ STATE, energy, mean)
avgGenByState&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   STATE  GenTotal
1     AK  9.810218
2     AL 30.582000
3     AR 19.090133
4     AZ 17.211604
5     CA  5.576285
6     CO 10.367634
7     CT  9.358409
8     DE  8.256761
9     FL 12.027089
10    GA 13.969187
11    HI  8.623563
12    IA 16.270149
13    ID  8.358471
14    IL 15.193858
15    IN 19.664150
16    KS 16.732968
17    KY 22.399999
18    LA 21.384516
19    MA  6.434295
20    MD  8.352530
21    ME 13.148792
22    MI 11.201083
23    MN 10.199283
24    MO 15.062760
25    MS 16.377409
26    MT 28.942351
27    NC 13.915444
28    ND 49.909462
29    NE 18.470352
30    NH 15.671056
31    NJ  7.108700
32    NM 18.130563
33    NV 14.311625
34    NY  7.197383
35    OH 12.664461
36    OK 18.787146
37    OR 14.537263
38    PA 17.326080
39    RI  6.488628
40    SC 22.476570
41    SD 10.719863
42    TN 14.751132
43    TX 17.020082
44    UT 15.587630
45    VA  9.720367
46    VT 10.177005
47    WA 16.426394
48    WI 11.079302
49    WV 47.417141
50    WY 88.393083&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avgGenByState[which.max(avgGenByState$GenTotal), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   STATE GenTotal
50    WY 88.39308&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;true&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;True&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5---prediction-model-for-solar-generation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5 - Prediction Model for Solar Generation&lt;/h3&gt;
&lt;p&gt;We are interested in predicting whether states are going to increase their solar energy generation over the next year.&lt;/p&gt;
&lt;p&gt;Let’s subset our dataset into a training and a testing set by using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
spl &amp;lt;- sample(1 : nrow(energy), size = 0.7 * nrow(energy))
train &amp;lt;- energy[spl, ]
test &amp;lt;- energy[-spl, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s create a logistic regression model “mod” using the train set to predict the binary variable GenSolarBinary.&lt;/p&gt;
&lt;p&gt;To do so, consider the following as potential predictive variables: GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary, Import.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
             CumlRegulatory + Total.salary + Import, 
           data = train, 
           family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which variable is most predictive in the model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
    CumlRegulatory + Total.salary + Import, family = binomial, 
    data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-4.0165  -0.4374  -0.2398  -0.0719   2.6342  

Coefficients:
                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)    -6.345e+00  9.148e-01  -6.935 4.05e-12 ***
GenHydro       -3.652e+00  1.114e+00  -3.279  0.00104 ** 
GenSolar        1.076e+03  3.322e+02   3.238  0.00121 ** 
CumlFinancial   2.814e-02  9.563e-03   2.943  0.00325 ** 
CumlRegulatory  1.996e-01  5.045e-02   3.955 7.65e-05 ***
Total.salary    1.393e-01  4.465e-02   3.119  0.00181 ** 
Import         -3.142e-01  3.447e-01  -0.912  0.36190    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 528.69  on 488  degrees of freedom
Residual deviance: 282.82  on 482  degrees of freedom
AIC: 296.82

Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;cumlregulatory&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;CumlRegulatory&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;Computing the predictions on the test-set. Using a threshold of 0.5, what is the accuracy of our model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest &amp;lt;- predict(mod, newdata = test, type = &amp;quot;response&amp;quot;)
table(test$GenSolarBinary, predTest &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   154    7
  1    31   18&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(154 + 18) / nrow(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8190476&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Cool!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;What is the accuracy for states voting republican?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testWithPred &amp;lt;- test
testWithPred$predTest &amp;lt;- predTest
str(testWithPred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   210 obs. of  28 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 1 1 1 1 2 2 2 2 3 3 ...
 $ YEAR                   : int  2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ...
 $ GenTotal               : num  10.64 10.03 9.88 9.5 28.08 ...
 $ GenHydro               : num  0.1995 0.1893 0.173 0.2267 0.0667 ...
 $ GenSolar               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.1997 0.1909 0.1737 0.2325 0.0668 ...
 $ GenSolarBinary         : int  0 0 0 0 0 0 0 1 0 0 ...
 $ GenTotalRenewableBinary: int  1 0 1 1 0 0 1 1 0 1 ...
 $ AllSourcesCO2          : num  7.31 6.39 6.38 5.89 17.65 ...
 $ AllSourcesSO2          : num  0.01193 0.00631 0.00514 0.0037 0.10361 ...
 $ AllSourcesNOx          : num  0.0277 0.0253 0.0215 0.0233 0.0384 ...
 $ EPriceResidential      : num  12.12 15.18 16.56 17.88 7.01 ...
 $ EPriceCommercial       : num  10.29 12.19 13.64 14.93 6.53 ...
 $ EPriceIndustrial       : num  7.61 12.63 14.17 16.82 3.79 ...
 $ EPriceTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EPriceTotal            : num  10.5 13.3 14.7 16.3 5.6 ...
 $ EsalesResidential      : num  0.347 0.334 0.337 0.337 0.35 ...
 $ EsalesCommercial       : num  0.42 0.447 0.451 0.448 0.238 ...
 $ EsalesIndustrial       : num  0.198 0.219 0.213 0.215 0.403 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  0.0356 NA NA NA 0.00931 ...
 $ EsalesTotal            : num  8.61 9.31 9.23 8.78 17.78 ...
 $ CumlFinancial          : int  1 7 10 12 1 8 8 16 0 0 ...
 $ CumlRegulatory         : int  1 2 2 4 0 1 1 2 0 1 ...
 $ Total.salary           : num  18.7 25.7 27.1 31.2 13.2 ...
 $ presidential.results   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 0 0 ...
 $ predTest               : num  0.01417 0.0541 0.07459 0.15572 0.00879 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testWithPredRep &amp;lt;- subset(testWithPred, presidential.results == 0)
table(testWithPredRep$GenSolarBinary, testWithPredRep$predTest &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    90    0
  1    18    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(90 + 2) / nrow(testWithPredRep)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8363636&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy for states voting democrat?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testWithPredDem &amp;lt;- subset(testWithPred, presidential.results == 1)
table(testWithPredDem$GenSolarBinary, testWithPredDem$predTest &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    64    7
  1    13   16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(64 + 16) / nrow(testWithPredDem)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-7---clustering-of-the-observations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 7 - Clustering of the Observations&lt;/h3&gt;
&lt;p&gt;We can perhaps improve our accuracy if we implement a cluster-the-predict approach. I’m interested in clustering the observations based on information about the regulatory and financial incentives, the elections outcome and the population wealth in each state across the years, in addition to whether the state was an energy importer or not.&lt;/p&gt;
&lt;p&gt;Let’s create a train.limited and test.limited datasets, where we only keep the variables CumlRegulatory, CumlFinancial, presidential.results, Total.salary, and Import.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   489 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 3 36 37 25 16 32 2 37 4 41 ...
 $ YEAR                   : int  2006 2003 2002 2012 2010 2009 2011 2000 2011 2007 ...
 $ GenTotal               : num  18.5 17.3 13.4 18.3 16.9 ...
 $ GenHydro               : num  0.029722 0.029664 0.73065 0 0.000276 ...
 $ GenSolar               : num  0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.030363 0.030562 0.739667 0.000301 0.07246 ...
 $ GenSolarBinary         : int  0 0 0 0 0 1 0 0 1 0 ...
 $ GenTotalRenewableBinary: int  1 1 0 0 1 1 0 0 0 0 ...
 $ AllSourcesCO2          : num  10.28 14.09 2.05 8.13 12.78 ...
 $ AllSourcesSO2          : num  0.0291 0.0316 0.004 0.0144 0.0144 ...
 $ AllSourcesNOx          : num  0.01358 0.02472 0.00324 0.00786 0.01617 ...
 $ EPriceResidential      : num  8.85 7.47 7.12 10.26 10.03 ...
 $ EPriceCommercial       : num  6.96 6.38 6.59 9.33 8.25 ...
 $ EPriceIndustrial       : num  5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 6.55 5.09 ...
 $ EPriceTransportation   : num  0 0 6.5 0 0 0 0 6.68 0 0 ...
 $ EPriceTotal            : num  6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 9.71 6.89 ...
 $ EsalesResidential      : num  0.366 0.4 0.388 0.372 0.355 ...
 $ EsalesCommercial       : num  0.248 0.336 0.329 0.281 0.382 ...
 $ EsalesIndustrial       : num  0.386 0.264 0.272 0.347 0.263 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  NA 0 0.0111 NA NA ...
 $ EsalesTotal            : num  16.6 14.4 12.9 16.2 14.2 ...
 $ CumlFinancial          : int  4 1 12 19 3 23 15 5 42 5 ...
 $ CumlRegulatory         : int  4 1 9 1 6 8 2 5 21 1 ...
 $ Total.salary           : num  15.4 13.4 16.3 15.3 20.8 ...
 $ presidential.results   : int  0 0 1 0 0 1 0 1 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 0 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train.limited &amp;lt;- subset(train, select = CumlFinancial:Import)
str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   210 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 1 1 1 1 2 2 2 2 3 3 ...
 $ YEAR                   : int  2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ...
 $ GenTotal               : num  10.64 10.03 9.88 9.5 28.08 ...
 $ GenHydro               : num  0.1995 0.1893 0.173 0.2267 0.0667 ...
 $ GenSolar               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.1997 0.1909 0.1737 0.2325 0.0668 ...
 $ GenSolarBinary         : int  0 0 0 0 0 0 0 1 0 0 ...
 $ GenTotalRenewableBinary: int  1 0 1 1 0 0 1 1 0 1 ...
 $ AllSourcesCO2          : num  7.31 6.39 6.38 5.89 17.65 ...
 $ AllSourcesSO2          : num  0.01193 0.00631 0.00514 0.0037 0.10361 ...
 $ AllSourcesNOx          : num  0.0277 0.0253 0.0215 0.0233 0.0384 ...
 $ EPriceResidential      : num  12.12 15.18 16.56 17.88 7.01 ...
 $ EPriceCommercial       : num  10.29 12.19 13.64 14.93 6.53 ...
 $ EPriceIndustrial       : num  7.61 12.63 14.17 16.82 3.79 ...
 $ EPriceTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EPriceTotal            : num  10.5 13.3 14.7 16.3 5.6 ...
 $ EsalesResidential      : num  0.347 0.334 0.337 0.337 0.35 ...
 $ EsalesCommercial       : num  0.42 0.447 0.451 0.448 0.238 ...
 $ EsalesIndustrial       : num  0.198 0.219 0.213 0.215 0.403 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  0.0356 NA NA NA 0.00931 ...
 $ EsalesTotal            : num  8.61 9.31 9.23 8.78 17.78 ...
 $ CumlFinancial          : int  1 7 10 12 1 8 8 16 0 0 ...
 $ CumlRegulatory         : int  1 2 2 4 0 1 1 2 0 1 ...
 $ Total.salary           : num  18.7 25.7 27.1 31.2 13.2 ...
 $ presidential.results   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test.limited &amp;lt;- subset(test, select = CumlFinancial:Import)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the “preProcess” function on the train.limited set, I can compute the train.norm and test.norm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preprocTrain &amp;lt;- preProcess(train.limited)
train.norm &amp;lt;- predict(preprocTrain, train.limited)
preprocTest &amp;lt;- preProcess(test.limited)
test.norm &amp;lt;- predict(preprocTest, test.limited)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why didn’t I include the dependent variable GenSolarBinary in this clustering phase?
#### - Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the cluster-then-predict methodology&lt;/p&gt;
&lt;p&gt;Let’s use kmeans clustering for this problem with a seed of 144, k=2 and keep the maximum number of iterations at 1,000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
k &amp;lt;- 2
trainKMC &amp;lt;- kmeans(train.norm, centers = k, iter.max = 1000)
set.seed(144)
testKMC &amp;lt;- kmeans(test.norm, centers = k, iter.max = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the flexclust package, identifying the clusters and call train1 the subset of train corresponding to the first cluster, and train2 the subset of train corresponding to the second cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainKMC.kcca &amp;lt;- as.kcca(trainKMC, train.norm)

# clusters(trainKMC.kcca)
trainClusters &amp;lt;- predict(trainKMC.kcca)
table(trainClusters)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;trainClusters
  1   2 
308 181 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train1 &amp;lt;- subset(train, trainClusters == 1)
train2 &amp;lt;- subset(train, trainClusters == 2)
str(train1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   308 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 3 36 37 25 16 32 2 37 41 28 ...
 $ YEAR                   : int  2006 2003 2002 2012 2010 2009 2011 2000 2007 2009 ...
 $ GenTotal               : num  18.5 17.3 13.4 18.3 16.9 ...
 $ GenHydro               : num  0.029722 0.029664 0.73065 0 0.000276 ...
 $ GenSolar               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.030363 0.030562 0.739667 0.000301 0.07246 ...
 $ GenSolarBinary         : int  0 0 0 0 0 1 0 0 0 0 ...
 $ GenTotalRenewableBinary: int  1 1 0 0 1 1 0 0 0 1 ...
 $ AllSourcesCO2          : num  10.28 14.09 2.05 8.13 12.78 ...
 $ AllSourcesSO2          : num  0.0291 0.0316 0.004 0.0144 0.0144 ...
 $ AllSourcesNOx          : num  0.01358 0.02472 0.00324 0.00786 0.01617 ...
 $ EPriceResidential      : num  8.85 7.47 7.12 10.26 10.03 ...
 $ EPriceCommercial       : num  6.96 6.38 6.59 9.33 8.25 ...
 $ EPriceIndustrial       : num  5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 5.09 5.25 ...
 $ EPriceTransportation   : num  0 0 6.5 0 0 0 0 6.68 0 0 ...
 $ EPriceTotal            : num  6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 6.89 6.63 ...
 $ EsalesResidential      : num  0.366 0.4 0.388 0.372 0.355 ...
 $ EsalesCommercial       : num  0.248 0.336 0.329 0.281 0.382 ...
 $ EsalesIndustrial       : num  0.386 0.264 0.272 0.347 0.263 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  NA 0 0.0111 NA NA ...
 $ EsalesTotal            : num  16.6 14.4 12.9 16.2 14.2 ...
 $ CumlFinancial          : int  4 1 12 19 3 23 15 5 5 8 ...
 $ CumlRegulatory         : int  4 1 9 1 6 8 2 5 1 4 ...
 $ Total.salary           : num  15.4 13.4 16.3 15.3 20.8 ...
 $ presidential.results   : int  0 0 1 0 0 1 0 1 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 1 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   181 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 4 9 8 19 14 8 30 10 48 22 ...
 $ YEAR                   : int  2011 2011 2002 2007 2006 2006 2010 2011 2004 2011 ...
 $ GenTotal               : num  16.7 11.61 7.46 7.24 15.13 ...
 $ GenHydro               : num  0.08485 0.00082 0 0.01694 0.0009 ...
 $ GenSolar               : num  0.000771 0.000567 0 0 0 ...
 $ GenTotalRenewable      : num  0.08841 0.01191 0 0.04075 0.00531 ...
 $ GenSolarBinary         : int  1 1 0 1 0 0 0 1 0 0 ...
 $ GenTotalRenewableBinary: int  0 0 0 1 1 1 1 0 0 1 ...
 $ AllSourcesCO2          : num  8.27 5.99 7.54 3.96 7.95 ...
 $ AllSourcesSO2          : num  0.00459 0.00592 0.03869 0.00787 0.0243 ...
 $ AllSourcesNOx          : num  0.00815 0.00434 0.01426 0.00307 0.00962 ...
 $ EPriceResidential      : num  11.08 11.51 8.7 16.23 8.42 ...
 $ EPriceCommercial       : num  9.5 9.85 7.15 15.2 7.95 ...
 $ EPriceIndustrial       : num  6.55 8.55 4.85 13.03 4.69 ...
 $ EPriceTransportation   : num  0 8.81 0 9.24 5.59 0 0 7.94 0 8.53 ...
 $ EPriceTotal            : num  9.71 10.61 6.91 15.16 7.07 ...
 $ EsalesResidential      : num  0.441 0.517 0.335 0.352 0.326 ...
 $ EsalesCommercial       : num  0.394 0.408 0.315 0.475 0.355 ...
 $ EsalesIndustrial       : num  0.165 0.075 0.345 0.165 0.315 ...
 $ EsalesTransportation   : num  0 0.00038 0 0.00705 0.00364 ...
 $ EsalesOther            : num  NA NA 0.00498 NA NA ...
 $ EsalesTotal            : num  11.58 11.78 14.94 8.79 11.2 ...
 $ CumlFinancial          : int  42 59 2 33 6 2 32 39 3 37 ...
 $ CumlRegulatory         : int  21 14 4 12 6 7 9 7 7 12 ...
 $ Total.salary           : num  22 20.7 21.3 28.9 22.7 ...
 $ presidential.results   : int  0 1 1 1 1 1 1 0 1 1 ...
 $ Import                 : int  0 1 1 1 0 1 0 1 1 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testKMC.kcca &amp;lt;- as.kcca(testKMC, test.norm)
# clusters(testKMC.kcca)
testClusters &amp;lt;- predict(testKMC.kcca)
table(testClusters)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;testClusters
  1   2 
130  80 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test1 &amp;lt;- subset(test, testClusters == 1)
test2 &amp;lt;- subset(test, testClusters == 2)
str(test1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   130 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 1 1 1 1 2 2 2 2 3 3 ...
 $ YEAR                   : int  2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ...
 $ GenTotal               : num  10.64 10.03 9.88 9.5 28.08 ...
 $ GenHydro               : num  0.1995 0.1893 0.173 0.2267 0.0667 ...
 $ GenSolar               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ GenTotalRenewable      : num  0.1997 0.1909 0.1737 0.2325 0.0668 ...
 $ GenSolarBinary         : int  0 0 0 0 0 0 0 1 0 0 ...
 $ GenTotalRenewableBinary: int  1 0 1 1 0 0 1 1 0 1 ...
 $ AllSourcesCO2          : num  7.31 6.39 6.38 5.89 17.65 ...
 $ AllSourcesSO2          : num  0.01193 0.00631 0.00514 0.0037 0.10361 ...
 $ AllSourcesNOx          : num  0.0277 0.0253 0.0215 0.0233 0.0384 ...
 $ EPriceResidential      : num  12.12 15.18 16.56 17.88 7.01 ...
 $ EPriceCommercial       : num  10.29 12.19 13.64 14.93 6.53 ...
 $ EPriceIndustrial       : num  7.61 12.63 14.17 16.82 3.79 ...
 $ EPriceTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EPriceTotal            : num  10.5 13.3 14.7 16.3 5.6 ...
 $ EsalesResidential      : num  0.347 0.334 0.337 0.337 0.35 ...
 $ EsalesCommercial       : num  0.42 0.447 0.451 0.448 0.238 ...
 $ EsalesIndustrial       : num  0.198 0.219 0.213 0.215 0.403 ...
 $ EsalesTransportation   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ EsalesOther            : num  0.0356 NA NA NA 0.00931 ...
 $ EsalesTotal            : num  8.61 9.31 9.23 8.78 17.78 ...
 $ CumlFinancial          : int  1 7 10 12 1 8 8 16 0 0 ...
 $ CumlRegulatory         : int  1 2 2 4 0 1 1 2 0 1 ...
 $ Total.salary           : num  18.7 25.7 27.1 31.2 13.2 ...
 $ presidential.results   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ Import                 : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   80 obs. of  27 variables:
 $ STATE                  : Factor w/ 50 levels &amp;quot;AK&amp;quot;,&amp;quot;AL&amp;quot;,&amp;quot;AR&amp;quot;,..: 4 5 5 6 6 6 7 7 7 7 ...
 $ YEAR                   : int  2009 2004 2010 2008 2010 2013 2001 2003 2005 2007 ...
 $ GenTotal               : num  17 5.48 5.48 10.84 9.95 ...
 $ GenHydro               : num  0.0574 0.1753 0.1638 0.0382 0.0311 ...
 $ GenSolar               : num  0.000126 0.002931 0.003769 0.000343 0.000838 ...
 $ GenTotalRenewable      : num  0.058 0.2787 0.2711 0.0996 0.1012 ...
 $ GenSolarBinary         : int  1 0 1 1 1 1 0 0 0 0 ...
 $ GenTotalRenewableBinary: int  1 1 1 1 1 1 0 0 1 1 ...
 $ AllSourcesCO2          : num  8.12 1.69 1.49 8.45 7.95 ...
 $ AllSourcesSO2          : num  4.99e-03 6.28e-04 6.77e-05 1.11e-02 8.81e-03 ...
 $ AllSourcesNOx          : num  0.00935 0.00276 0.00214 0.01272 0.01081 ...
 $ EPriceResidential      : num  10.7 12.2 14.8 10.1 11 ...
 $ EPriceCommercial       : num  9.35 11.64 13.09 8.57 9.13 ...
 $ EPriceIndustrial       : num  6.65 9.27 9.8 6.65 6.9 ...
 $ EPriceTransportation   : num  0 6.42 8.27 8.32 9.34 ...
 $ EPriceTotal            : num  9.56 11.35 13.01 8.59 9.15 ...
 $ EsalesResidential      : num  0.447 0.331 0.338 0.34 0.342 ...
 $ EsalesCommercial       : num  0.4 0.472 0.469 0.394 0.37 ...
 $ EsalesIndustrial       : num  0.153 0.194 0.191 0.265 0.287 ...
 $ EsalesTransportation   : num  0 0.003572 0.003177 0.000932 0.000876 ...
 $ EsalesOther            : num  NA 0 NA NA NA ...
 $ EsalesTotal            : num  11.15 7.09 6.94 10.58 10.39 ...
 $ CumlFinancial          : int  33 17 138 30 58 70 3 3 4 10 ...
 $ CumlRegulatory         : int  17 20 28 18 19 20 5 5 7 12 ...
 $ Total.salary           : num  21.2 20.8 24 27 26.3 ...
 $ presidential.results   : int  0 1 1 1 1 1 1 1 1 1 ...
 $ Import                 : int  0 1 1 0 1 1 1 1 0 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Select the correct statement(s) below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(train1$presidential.results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
248  60 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(train2$presidential.results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
 21 160 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train1$CumlFinancial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7.766234&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train2$CumlFinancial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 34.87845&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train1$CumlRegulatory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3.983766&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train2$CumlRegulatory)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12.28729&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train1$AllSourcesCO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 15.09418&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train2$AllSourcesCO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5.841604&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train1$AllSourcesSO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.04923232&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train2$AllSourcesSO2, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.01533239&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train1$AllSourcesNOx, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0282891&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(train2$AllSourcesNOx, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.007871944&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;on-average-train1-contains-more-republican-states-than-train2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;- On average, train1 contains more republican states than train2&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;on-average-train1-contains-states-that-have-recorded-more-co2-so2-and-nox-emissions-than-train2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;- On average, train1 contains states that have recorded more CO2, SO2 and NOx emissions than train2&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-8---creating-the-model-on-the-first-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 8 - Creating the Model on the First Cluster&lt;/h3&gt;
&lt;p&gt;Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod1 using a logistic regression on train1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
               CumlRegulatory + Total.salary + Import, 
           data = train1,
           family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What variable is most predictive?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
    CumlRegulatory + Total.salary + Import, family = binomial, 
    data = train1)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8654  -0.2715  -0.1692  -0.1194   2.8802  

Coefficients:
                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)    -7.574e+00  1.945e+00  -3.893 9.89e-05 ***
GenHydro       -1.201e+00  2.082e+00  -0.577  0.56398    
GenSolar        2.324e+04  1.104e+04   2.106  0.03524 *  
CumlFinancial   6.836e-02  2.159e-02   3.166  0.00154 ** 
CumlRegulatory  1.589e-01  1.145e-01   1.388  0.16521    
Total.salary    1.549e-01  9.190e-02   1.686  0.09188 .  
Import         -3.323e-01  9.628e-01  -0.345  0.72995    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 131.538  on 307  degrees of freedom
Residual deviance:  90.376  on 301  degrees of freedom
AIC: 104.38

Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;cumlfinancial&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;CumlFinancial&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-9---evaluating-the-model-obtained-using-the-first-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 9 - Evaluating the Model Obtained Using the First Cluster&lt;/h3&gt;
&lt;p&gt;What is the accuracy on test1, the subset of test corresponding to the first cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest1 &amp;lt;- predict(mod1, newdata = test1, type = &amp;quot;response&amp;quot;)
table(test1$GenSolarBinary, predTest1 &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   114    1
  1    11    4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(114 + 4) / nrow(test1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9076923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Wow!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’d like to know if mod1 gives us an edge over mod on the dataset test1.&lt;/p&gt;
&lt;p&gt;Using mod, predicting GenSolarBinary for the observation in test1 and report the accuracy below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest1WithMod &amp;lt;- predict(mod, newdata = test1, type = &amp;quot;response&amp;quot;)
table(test1$GenSolarBinary, predTest1WithMod &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE
  0   115
  1    15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;115 / nrow(test1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8846154&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-10---creating-the-model-on-the-second-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 10 - Creating the Model on the Second Cluster&lt;/h3&gt;
&lt;p&gt;Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod2 using a logistic regression on train2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
                CumlRegulatory + Total.salary + Import, 
            data = train2,
            family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Select the correct statement(s) below?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
    CumlRegulatory + Total.salary + Import, family = binomial, 
    data = train2)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.6321  -0.7964   0.0083   0.8536   1.7143  

Coefficients:
                Estimate Std. Error z value Pr(&amp;gt;|z|)   
(Intercept)     -4.14627    1.43969  -2.880  0.00398 **
GenHydro        -3.88698    1.23196  -3.155  0.00160 **
GenSolar       951.59397  322.86360   2.947  0.00321 **
CumlFinancial    0.02077    0.01008   2.060  0.03939 * 
CumlRegulatory   0.17315    0.06541   2.647  0.00812 **
Total.salary     0.08472    0.05576   1.519  0.12869   
Import          -0.72189    0.40427  -1.786  0.07416 . 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 250.25  on 180  degrees of freedom
Residual deviance: 173.65  on 174  degrees of freedom
AIC: 187.65

Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + 
    CumlRegulatory + Total.salary + Import, family = binomial, 
    data = train1)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8654  -0.2715  -0.1692  -0.1194   2.8802  

Coefficients:
                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)    -7.574e+00  1.945e+00  -3.893 9.89e-05 ***
GenHydro       -1.201e+00  2.082e+00  -0.577  0.56398    
GenSolar        2.324e+04  1.104e+04   2.106  0.03524 *  
CumlFinancial   6.836e-02  2.159e-02   3.166  0.00154 ** 
CumlRegulatory  1.589e-01  1.145e-01   1.388  0.16521    
Total.salary    1.549e-01  9.190e-02   1.686  0.09188 .  
Import         -3.323e-01  9.628e-01  -0.345  0.72995    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 131.538  on 307  degrees of freedom
Residual deviance:  90.376  on 301  degrees of freedom
AIC: 104.38

Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;unlike-mod1-the-number-of-regulatory-policies-is-more-predictive-than-the-number-of-financial-incentives-in-mod2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;- Unlike mod1, the number of regulatory policies is more predictive than the number of financial incentives in mod2&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-11---evaluating-the-model-obtained-using-the-second-cluster&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 11 - Evaluating the Model Obtained Using the Second Cluster&lt;/h3&gt;
&lt;p&gt;Using the threshold of 0.5, what is the accuracy on test2, the subset of test corresponding to the second cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest2 &amp;lt;- predict(mod2, newdata = test2, type = &amp;quot;response&amp;quot;)
table(test2$GenSolarBinary, predTest2 &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    40    6
  1    14   20&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(40 + 20) / nrow(test2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would like to know if mod2 gives us an edge over mod on the dataset test2.&lt;/p&gt;
&lt;p&gt;Using mod, predicting GenSolarBinary for the observation in test2 and report the accuracy below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest2WithMod &amp;lt;- predict(mod, newdata = test2, type = &amp;quot;response&amp;quot;)
table(test2$GenSolarBinary, predTest2WithMod &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    39    7
  1    16   18&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(39 + 18) / nrow(test2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7125&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-12---evaluating-the-performance-of-the-cluster-the-predict-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 12 - Evaluating the Performance of the Cluster-the-Predict Algorithm&lt;/h3&gt;
&lt;p&gt;To compute the overall test-set accuracy of the cluster-the-predict approach, I can combine all the test-set predictions into a single vector “AllPredictions” and all the true outcomes into a single vector “AllOutcomes”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AllPredictions &amp;lt;- c(predTest1, predTest2)
AllOutcomes &amp;lt;- c(test1$GenSolarBinary, test2$GenSolarBinary)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall accuracy on the test-set, using the cluster-then-predict approach, again using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(AllOutcomes, AllPredictions &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           
AllOutcomes FALSE TRUE
          0   154    7
          1    25   24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(AllOutcomes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 210&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(AllPredictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 210&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(154 + 24) / length(AllOutcomes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.847619&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Retail Consumers Behaviour</title>
      <link>/project/retail_consumers/retail_consumers/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/retail_consumers/retail_consumers/</guid>
      <description>


&lt;p&gt;Clustering can be used for market segmentation, the idea of dividing airline passengers into small, more similar groups, and then designing a marketing strategy specifically for each group. In this analysis, I’ll see how this idea can be applied to &lt;strong&gt;retail consumer data.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I’ll use the dataset Households.csv.xz, which contains data collected over two years for a group of 2,500 households. Each row (observation) in our dataset represents a unique household. The dataset contains the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NumVisits = the number of times the household visited the retailer&lt;/li&gt;
&lt;li&gt;AvgProdCount = the average number of products purchased per transaction&lt;/li&gt;
&lt;li&gt;AvgDiscount = the average discount per transaction from coupon usage
(in %) - NOTE: Do not divide this value by 100!&lt;/li&gt;
&lt;li&gt;AvgSalesValue = the average sales value per transaction&lt;/li&gt;
&lt;li&gt;MorningPct = the percentage of visits in the morning (8am - 1:59pm)&lt;/li&gt;
&lt;li&gt;AfternoonPct = the percentage of visits in the afternoon (2pm - 7:59pm)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that some visits can occur outside of morning and afternoon hours. That is, visits from 8pm - 7:59am are possible.&lt;/p&gt;
&lt;p&gt;This dataset was derived from source files provided by &lt;strong&gt;dunnhumby,&lt;/strong&gt; a customer science company based in the UK.&lt;/p&gt;
&lt;div id=&#34;problem-1---reading-in-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1 - Reading in the data&lt;/h3&gt;
&lt;p&gt;Read the dataset Households.csv.xz into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HouseHolds &amp;lt;- read.csv(&amp;quot;Households.csv.xz&amp;quot;)
str(HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2500 obs. of  6 variables:
 $ NumVisits    : int  86 45 47 30 40 250 59 113 20 9 ...
 $ AvgProdCount : num  20.08 15.87 19.62 10.03 5.55 ...
 $ AvgDiscount  : num  8.11 7.44 14.37 3.85 2.96 ...
 $ AvgSalesValue: num  50.4 43.4 56.5 40 19.5 ...
 $ MorningPct   : num  46.51 8.89 14.89 13.33 2.5 ...
 $ AfternoonPct : num  51.2 60 76.6 56.7 67.5 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   NumVisits       AvgProdCount     AvgDiscount     AvgSalesValue    
 Min.   :   1.0   Min.   : 1.186   Min.   : 0.089   Min.   :  2.388  
 1st Qu.:  39.0   1st Qu.: 6.123   1st Qu.: 3.006   1st Qu.: 18.329  
 Median :  79.0   Median : 8.979   Median : 4.865   Median : 27.417  
 Mean   : 110.6   Mean   :10.291   Mean   : 5.713   Mean   : 31.621  
 3rd Qu.: 142.2   3rd Qu.:13.116   3rd Qu.: 7.327   3rd Qu.: 40.546  
 Max.   :1300.0   Max.   :56.600   Max.   :47.176   Max.   :165.829  
   MorningPct      AfternoonPct   
 Min.   :  0.00   Min.   :  0.00  
 1st Qu.: 16.67   1st Qu.: 42.20  
 Median : 26.09   Median : 52.00  
 Mean   : 28.73   Mean   : 51.45  
 3rd Qu.: 37.17   3rd Qu.: 61.29  
 Max.   :100.00   Max.   :100.00  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct AfternoonPct
1        86     20.08140    8.105116      50.35070  46.511628     51.16279
2        45     15.86667    7.444222      43.42978   8.888889     60.00000
3        47     19.61702   14.365106      56.45128  14.893617     76.59574
4        30     10.03333    3.855000      40.00367  13.333333     56.66667
5        40      5.55000    2.958250      19.47650   2.500000     67.50000
6       250      7.16400    3.313360      23.98464  25.600000     61.20000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many households have logged transactions at the retailer only in the morning?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(HouseHolds, MorningPct == 100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many households have logged transactions at the retailer only in the afternoon?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(HouseHolds, AfternoonPct == 100))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 13&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2---descriptive-statistics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2 - Descriptive statistics&lt;/h3&gt;
&lt;p&gt;Of the households that spend more than $150 per transaction on average, what is the minimum average discount per transaction?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(subset(HouseHolds, AvgSalesValue &amp;gt; 150)$AvgDiscount)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 15.64607&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of the households who have an average discount per transaction greater than 25%, what is the minimum average sales value per transaction?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(subset(HouseHolds, AvgDiscount &amp;gt; 25)$AvgSalesValue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 50.1175&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of households visited the retailer at least 300 times?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(HouseHolds, NumVisits &amp;gt;= 300)) / nrow(HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0592&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3---importance-of-normalizing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3 - Importance of Normalizing&lt;/h3&gt;
&lt;p&gt;When clustering data, its often important to normalize the variables so that they are all on the same scale.&lt;/p&gt;
&lt;p&gt;If you clustered this dataset without normalizing, which variable would you expect to dominate in the distance calculations?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   NumVisits       AvgProdCount     AvgDiscount     AvgSalesValue    
 Min.   :   1.0   Min.   : 1.186   Min.   : 0.089   Min.   :  2.388  
 1st Qu.:  39.0   1st Qu.: 6.123   1st Qu.: 3.006   1st Qu.: 18.329  
 Median :  79.0   Median : 8.979   Median : 4.865   Median : 27.417  
 Mean   : 110.6   Mean   :10.291   Mean   : 5.713   Mean   : 31.621  
 3rd Qu.: 142.2   3rd Qu.:13.116   3rd Qu.: 7.327   3rd Qu.: 40.546  
 Max.   :1300.0   Max.   :56.600   Max.   :47.176   Max.   :165.829  
   MorningPct      AfternoonPct   
 Min.   :  0.00   Min.   :  0.00  
 1st Qu.: 16.67   1st Qu.: 42.20  
 Median : 26.09   Median : 52.00  
 Mean   : 28.73   Mean   : 51.45  
 3rd Qu.: 37.17   3rd Qu.: 61.29  
 Max.   :100.00   Max.   :100.00  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;numvisits&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;NumVisits&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4---normalizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4 - Normalizing the Data&lt;/h3&gt;
&lt;p&gt;Normalize all of the variables in the dataset (Note that these codes assume that our dataset is called “Households”, and create the normalized dataset “HouseholdsNorm”. You can change the names to anything you want by editing the codes.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preproc &amp;lt;- preProcess(HouseHolds)
HouseHoldsNorm &amp;lt;- predict(preproc, HouseHolds)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Remember that for each variable, the normalization process subtracts the mean and divides by the standard deviation. In our normalized dataset, all of the variables should have mean 0 and standard deviation 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(HouseHoldsNorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   NumVisits        AvgProdCount      AvgDiscount      AvgSalesValue    
 Min.   :-0.9475   Min.   :-1.5239   Min.   :-1.4010   Min.   :-1.5342  
 1st Qu.:-0.6190   1st Qu.:-0.6976   1st Qu.:-0.6743   1st Qu.:-0.6976  
 Median :-0.2731   Median :-0.2197   Median :-0.2112   Median :-0.2206  
 Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000  
 3rd Qu.: 0.2737   3rd Qu.: 0.4728   3rd Qu.: 0.4021   3rd Qu.: 0.4684  
 Max.   :10.2828   Max.   : 7.7500   Max.   :10.3293   Max.   : 7.0432  
   MorningPct       AfternoonPct     
 Min.   :-1.6779   Min.   :-3.22843  
 1st Qu.:-0.7047   1st Qu.:-0.58006  
 Median :-0.1546   Median : 0.03458  
 Mean   : 0.0000   Mean   : 0.00000  
 3rd Qu.: 0.4926   3rd Qu.: 0.61755  
 Max.   : 4.1617   Max.   : 3.04658  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the maximum value of NumVisits in the normalized dataset?
#### 10.2828&lt;/p&gt;
&lt;p&gt;What is the minimum value of AfternoonPct in the normalized dataset?
#### -3.22843&lt;/p&gt;
&lt;p&gt;Create a dendrogram of our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(200)
distances &amp;lt;- dist(HouseHoldsNorm, method = &amp;quot;euclidean&amp;quot;)
ClusterShoppers &amp;lt;- hclust(distances, method = &amp;quot;ward.D&amp;quot;)
plot(ClusterShoppers, labels = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/retail_consumers/retail_consumers_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5---interpreting-the-dendrogram&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5 - Interpreting the Dendrogram&lt;/h3&gt;
&lt;p&gt;Based on the dendrogram, how many clusters do you think would be appropriate for this problem?
#### 2, 3, 5&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6---k-means-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6 - K-means Clustering&lt;/h3&gt;
&lt;p&gt;Run the k-means clustering algorithm on our normalized dataset, selecting 10 clusters. Right before using the kmeans function, Remember “set.seed(200)”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(200)
k &amp;lt;- 10
KMC &amp;lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000)
str(KMC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 9
 $ cluster     : int [1:2500] 7 3 1 3 5 6 1 1 3 8 ...
 $ centers     : num [1:10, 1:6] -0.248 -0.483 -0.234 -0.18 -0.246 ...
  ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
  .. ..$ : chr [1:10] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
  .. ..$ : chr [1:6] &amp;quot;NumVisits&amp;quot; &amp;quot;AvgProdCount&amp;quot; &amp;quot;AvgDiscount&amp;quot; &amp;quot;AvgSalesValue&amp;quot; ...
 $ totss       : num 14994
 $ withinss    : num [1:10] 628 449 700 282 580 ...
 $ tot.withinss: num 4828
 $ betweenss   : num 10166
 $ size        : int [1:10] 246 51 490 118 504 226 141 284 52 388
 $ iter        : int 5
 $ ifault      : int 0
 - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;kmeans&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeansGroups &amp;lt;- KMC$cluster
table(kmeansGroups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;kmeansGroups
  1   2   3   4   5   6   7   8   9  10 
246  51 490 118 504 226 141 284  52 388 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are in the smallest cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(table(kmeansGroups))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 51&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;51&lt;/h4&gt;
&lt;p&gt;How many observations are in the largest cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(table(kmeansGroups))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 504&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;504&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-7---understanding-the-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 7 - Understanding the Clusters&lt;/h3&gt;
&lt;p&gt;Now, use the cluster assignments from k-means clustering together with the cluster centroids to explore the next few questions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeans1 &amp;lt;- subset(HouseHolds, kmeansGroups == 1)
kmeans2 &amp;lt;- subset(HouseHolds, kmeansGroups == 2)
kmeans3 &amp;lt;- subset(HouseHolds, kmeansGroups == 3)
kmeans4 &amp;lt;- subset(HouseHolds, kmeansGroups == 4)
kmeans5 &amp;lt;- subset(HouseHolds, kmeansGroups == 5)
kmeans6 &amp;lt;- subset(HouseHolds, kmeansGroups == 6)
kmeans7 &amp;lt;- subset(HouseHolds, kmeansGroups == 7)
kmeans8 &amp;lt;- subset(HouseHolds, kmeansGroups == 8)
kmeans9 &amp;lt;- subset(HouseHolds, kmeansGroups == 9)
kmeans10 &amp;lt;- subset(HouseHolds, kmeansGroups == 10)

colMeans(kmeans1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
     81.89431      19.11594      10.92924      59.49868      22.76746 
 AfternoonPct 
     61.93939 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
     54.70588      32.62351      19.65784      99.73684      32.15593 
 AfternoonPct 
     49.41508 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    83.508163     12.081068      6.881078     37.391552     25.609449 
 AfternoonPct 
    51.185788 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    89.788136      7.053082      3.877403     21.175564     71.391580 
 AfternoonPct 
    22.584436 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
     82.18254       5.89608       2.90764      17.51880      19.36659 
 AfternoonPct 
     55.03936 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
   281.796460      8.117065      4.297144     25.446228     29.851517 
 AfternoonPct 
    51.583481 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    99.886525     15.469831      9.343551     50.447122     53.462000 
 AfternoonPct 
    35.836861 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    80.288732      9.992614      5.288399     29.327693     13.350751 
 AfternoonPct 
    74.066827 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
   626.903846      5.203533      2.632325     16.278150     24.641085 
 AfternoonPct 
    48.731981 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    70.729381      6.479314      3.527893     19.688575     37.377204 
 AfternoonPct 
    38.916046 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which cluster best fits the description “morning shoppers stopping in to make a quick purchase”?
#### Cluster 4&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-8---understanding-the-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 8 - Understanding the Clusters&lt;/h3&gt;
&lt;p&gt;Which cluster best fits the description “shoppers with high average product count and high average value per visit”?
#### Cluster 2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-9---understanding-the-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 9 - Understanding the Clusters&lt;/h3&gt;
&lt;p&gt;Which cluster best fits the description “frequent shoppers with low value per visit”?
#### Cluster 9&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-10---random-behavior&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 10 - Random Behavior&lt;/h3&gt;
&lt;p&gt;If we ran hierarchical clustering a second time without making any additional calls to set.seed, we would expect:
#### Identical results to the first hierarchical clustering&lt;/p&gt;
&lt;p&gt;If we ran k-means clustering a second time without making any additional calls to set.seed, we would expect:
#### Different results from the first k-means clustering&lt;/p&gt;
&lt;p&gt;If we ran k-means clustering a second time, again running the code set.seed(200) right before doing the clustering, we would expect:
#### Identical results to the first k-means clustering&lt;/p&gt;
&lt;p&gt;If we ran k-means clustering a second time, running the code set.seed(100) right before doing the clustering, we would expect:
#### Different results from the first k-means clustering&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-11---the-number-of-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 11 - The Number of Clusters&lt;/h3&gt;
&lt;p&gt;Suppose the marketing department at the retail store decided that the 10 clusters were too specific, and they wanted more general clusters to describe the consumer base.&lt;/p&gt;
&lt;p&gt;Would they want to increase or decrease the number of clusters?
#### Decrease the number of clusters&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-12---increasing-the-number-of-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 12 - Increasing the Number of Clusters&lt;/h3&gt;
&lt;p&gt;Run the k-means clustering algorithm again, this time selecting 5 clusters. Right before the “kmeans” function, set the random seed to 5000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(5000)
k &amp;lt;- 5
KMC.5 &amp;lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000)
str(KMC.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 9
 $ cluster     : int [1:2500] 5 5 1 5 2 2 5 5 5 2 ...
 $ centers     : num [1:5, 1:6] -0.398 -0.193 -0.169 2.695 -0.176 ...
  ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
  .. ..$ : chr [1:5] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
  .. ..$ : chr [1:6] &amp;quot;NumVisits&amp;quot; &amp;quot;AvgProdCount&amp;quot; &amp;quot;AvgDiscount&amp;quot; &amp;quot;AvgSalesValue&amp;quot; ...
 $ totss       : num 14994
 $ withinss    : num [1:5] 1264 1838 1336 754 1604
 $ tot.withinss: num 6795
 $ betweenss   : num 8199
 $ size        : int [1:5] 182 994 428 172 724
 $ iter        : int 7
 $ ifault      : int 0
 - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;kmeans&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeansGroups.5 &amp;lt;- KMC.5$cluster
table(kmeansGroups.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;kmeansGroups.5
  1   2   3   4   5 
182 994 428 172 724 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are in the smallest cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(table(kmeansGroups.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 172&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;172&lt;/h4&gt;
&lt;p&gt;How many observations are in the largest cluster?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(table(kmeansGroups.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 994&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-3&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;994&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-13---describing-the-clusters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 13 - Describing the Clusters&lt;/h3&gt;
&lt;p&gt;Use the cluster assignments from k-means clustering with 5 clusters, which cluster best fits the description “frequent shoppers with low value per visit”?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeans1.5 &amp;lt;- subset(HouseHolds, kmeansGroups.5 == 1)
kmeans2.5 &amp;lt;- subset(HouseHolds, kmeansGroups.5 == 2)
kmeans3.5 &amp;lt;- subset(HouseHolds, kmeansGroups.5 == 3)
kmeans4.5 &amp;lt;- subset(HouseHolds, kmeansGroups.5 == 4)
kmeans5.5 &amp;lt;- subset(HouseHolds, kmeansGroups.5 == 5)

colMeans(kmeans1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
     64.60989      24.47666      14.69849      76.09221      31.49236 
 AfternoonPct 
     53.37519 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans2.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    88.274648      6.573304      3.355126     19.679329     21.516913 
 AfternoonPct 
    53.900113 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans3.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    91.095794      8.541218      4.910884     26.956499     54.594773 
 AfternoonPct 
    32.958057 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans4.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
   422.273256      7.074807      3.710112     22.317813     27.386172 
 AfternoonPct 
    51.043708 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(kmeans5.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    NumVisits  AvgProdCount   AvgDiscount AvgSalesValue    MorningPct 
    90.276243     13.628410      7.640804     41.803608     22.980428 
 AfternoonPct 
    58.626883 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;cluster-4&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Cluster 4&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-14---understanding-centroids&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 14 - Understanding Centroids&lt;/h3&gt;
&lt;p&gt;Why do we typically use cluster centroids to describe the clusters?
#### The cluster centroid captures the average behavior in the cluster, and can be used to summarize the general pattern in the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-15---using-a-visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 15 - Using a Visualization&lt;/h3&gt;
&lt;p&gt;Which of the following visualizations could be used to observe the distribution of NumVisits, broken down by cluster?
#### - A box plot of the variable NumVisits, subdivided by cluster
#### - ggplot with NumVisits on the x-axis and the cluster number on the y-axis, plotting with geom_point()&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Network Data Visualizing</title>
      <link>/project/visualizing_network/visualizing_network/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/visualizing_network/visualizing_network/</guid>
      <description>


&lt;p&gt;The cliche goes that the world is an increasingly interconnected place, and the connections between different entities are often best represented with a graph. Graphs are comprised of vertices (also often called “nodes”) and edges connecting those nodes. In this analysis, I’ll explore how to visualize networks using the igraph package in R.&lt;/p&gt;
&lt;p&gt;I’ll visualize social networking data using anonymized data from &lt;strong&gt;Facebook;&lt;/strong&gt; this data was originally curated in a recent paper about computing social circles in social networks. In our visualizations, the vertices in our network will represent &lt;strong&gt;Facebook users&lt;/strong&gt; and the edges will represent these users being Facebook friends with each other.&lt;/p&gt;
&lt;p&gt;The first file I’ll use, edges.csv, contains variables V1 and V2, which label the endpoints of edges in our network. Each row represents a pair of users in our graph who are Facebook friends. For a pair of friends A and B, edges.csv will only contain a single row – the smaller identifier will be listed first in this row. From this row, I’ll know that A is friends with B and B is friends with A.&lt;/p&gt;
&lt;p&gt;The second file, users.csv, contains information about the &lt;strong&gt;Facebook users,&lt;/strong&gt; who are the vertices in our network. This file contains the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;id: A unique identifier for this user; this is the value that appears in the rows of edges.csv&lt;/li&gt;
&lt;li&gt;gender: An identifier for the gender of a user taking the values A and B. Because the data is anonymized, we don’t know which value refers to males and which value refers to females.&lt;/li&gt;
&lt;li&gt;school: An identifier for the school the user attended taking the values A and AB (users with AB attended school A as well as another school B). Because the data is anonymized, we don’t know the schools represented by A and B.&lt;/li&gt;
&lt;li&gt;locale: An identifier for the locale of the user taking the values A and B. Because the data is anonymized, we don’t know which value refers to what locale.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---summarizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Summarizing the Data&lt;/h3&gt;
&lt;p&gt;Load the data from edges.csv into a dataframe called edges, and load the data from users.csv into a dataframe called users.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges &amp;lt;- read.csv(&amp;quot;edges.csv&amp;quot;)
users &amp;lt;- read.csv(&amp;quot;users.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many Facebook users are there in our dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(users)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   59 obs. of  4 variables:
 $ id    : int  3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 ...
 $ gender: Factor w/ 3 levels &amp;quot;&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;: 2 3 3 3 3 3 2 3 3 2 ...
 $ school: Factor w/ 3 levels &amp;quot;&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;AB&amp;quot;: 2 1 1 1 1 2 1 1 2 1 ...
 $ locale: Factor w/ 3 levels &amp;quot;&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;: 3 3 3 3 3 3 2 3 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In our dataset, what is the average number of friends per user? Hint: this question is tricky, and it might help to start by thinking about a small example with two users who are friends.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(edges)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    V1   V2
1 4019 4026
2 4023 4031
3 4023 4030
4 4027 4032
5 3988 4021
6 3982 3986&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges[1,] # users 4019 and 4026 are friends&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    V1   V2
1 4019 4026&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(subset(edges, V1 == 4019)) # user 4019 has 2 connections as V1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2 obs. of  2 variables:
 $ V1: int  4019 4019
 $ V2: int  4026 4030&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(subset(edges, V2 == 4019)) # user 4019 has 5 connections as V2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   5 obs. of  2 variables:
 $ V1: int  3997 3994 3998 4009 3981
 $ V2: int  4019 4019 4019 4019 4019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(subset(edges, V1 == 4026)) # user 4026 has 1 connections as V1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1 obs. of  2 variables:
 $ V1: int 4026
 $ V2: int 4030&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(subset(edges, V2 == 4026)) # user 4026 has 7 connections as V2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   7 obs. of  2 variables:
 $ V1: int  4019 4000 3995 4017 3986 3982 4021
 $ V2: int  4026 4026 4026 4026 4026 4026 4026&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges2 &amp;lt;- edges
edges2$PK &amp;lt;- row.names(edges2)
edges2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      V1   V2  PK
1   4019 4026   1
2   4023 4031   2
3   4023 4030   3
4   4027 4032   4
5   3988 4021   5
6   3982 3986   6
7   3994 3998   7
8   3998 3999   8
9   3993 3995   9
10  3982 4021  10
11  3982 4037  11
12  3997 4019  12
13  3994 4019  13
14  3992 4017  14
15  3981 3998  15
16  3997 4018  16
17  4009 4030  17
18  3994 4018  18
19  3995 4000  19
20  4000 4026  20
21  4027 4038  21
22  4031 4038  22
23  4000 4021  23
24  3986 4030  24
25  3985 4014  25
26  3994 4030  26
27  3998 4021  27
28  3994 4009  28
29  3982 4023  29
30  3998 4019  30
31  4020 4031  31
32  4009 4023  32
33  3994 3997  33
34  3981 4023  34
35  3997 4030  35
36  3997 4021  36
37  4023 4034  37
38  3993 4004  38
39  3994 3996  39
40  4000 4030  40
41  3998 4014  41
42  4004 4013  42
43  4016 4025  43
44  3990 4016  44
45  3999 4005  45
46  4004 4023  46
47  4002 4020  47
48  3998 4018  48
49  3985 3995  49
50  3989 3991  50
51  4000 4017  51
52  4003 4009  52
53  3982 4030  53
54  3982 3994  54
55  3998 4005  55
56  3995 4014  56
57  4021 4030  57
58   594 4011  58
59  3993 4030  59
60  4020 4030  60
61  3989 4038  61
62  3989 4011  62
63  4009 4019  63
64  4004 4020  64
65  3995 4026  65
66  4017 4026  66
67  3989 4013  67
68  4020 4037  68
69  3998 4002  69
70  3995 4023  70
71  3983 4017  71
72  3999 4036  72
73  3982 3997  73
74  3990 4007  74
75  3985 3988  75
76  4018 4030  76
77  4026 4030  77
78  3997 4023  78
79  3996 4028  79
80  3982 3988  80
81  3988 4030  81
82  4013 4023  82
83  4014 4021  83
84  4014 4037  84
85  3986 4021  85
86  4017 4021  86
87  3982 4009  87
88  3998 4023  88
89  3998 4009  89
90   594 3989  90
91  3992 4000  91
92  4011 4031  92
93  4019 4030  93
94  4020 4038  94
95  3997 3998  95
96  4023 4038  96
97  4004 4031  97
98  4027 4031  98
99  4014 4038  99
100 3986 4000 100
101 3982 4003 101
102 3986 4033 102
103 3981 3994 103
104 4004 4038 104
105 3985 3993 105
106 4000 4033 106
107 4013 4038 107
108 4018 4023 108
109 4003 4030 109
110 3990 4025 110
111 3986 4026 111
112 3996 4002 112
113 4001 4029 113
114 4014 4030 114
115 4020 4027 115
116 3982 3998 116
117 3988 3993 117
118 4002 4031 118
119 3988 3995 119
120 3986 4014 120
121 4003 4023 121
122 3981 4019 122
123 3997 4009 123
124 4014 4023 124
125 4004 4030 125
126 4006 4027 126
127  594 4031 127
128 4007 4025 128
129 3981 4018 129
130 3981 3997 130
131 3982 4026 131
132 4014 4017 132
133 3991 4031 133
134 3987 4012 134
135 4007 4016 135
136 3995 4004 136
137 4017 4030 137
138 4002 4023 138
139 3994 4023 139
140 3982 4014 140
141 3981 4009 141
142 4021 4026 142
143 4013 4031 143
144 3986 4017 144
145 4002 4027 145
146 3985 4004 146&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---summarizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Summarizing the Data&lt;/h3&gt;
&lt;p&gt;Out of all the students who listed a school, what was the most common locale?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(users)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       id       gender school  locale
 Min.   : 594    : 2     :40    : 3  
 1st Qu.:3994   A:15   A :17   A: 6  
 Median :4009   B:42   AB: 2   B:50  
 Mean   :3952                        
 3rd Qu.:4024                        
 Max.   :4038                        &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(users$school, users$locale)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    
         A  B
      3  6 31
  A   0  0 17
  AB  0  0  2&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;locale-b&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Locale B&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---summarizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Summarizing the Data&lt;/h3&gt;
&lt;p&gt;Is it possible that either school A or B is an all-girls or all-boys school?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(users$gender, users$school)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
        A AB
     1  1  0
  A 11  3  1
  B 28 13  1&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;no&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---creating-a-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Creating a Network&lt;/h3&gt;
&lt;p&gt;We can create a new graph object using the graph.data.frame() function. Based on ?graph.data.frame, using the following code we will create a graph g describing our social network, with the attributes of each user correctly loaded.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?graph.data.frame
g &amp;lt;- graph.data.frame(edges, FALSE, users)
g&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;IGRAPH 097ec57 UN-- 59 146 -- 
+ attr: name (v/c), gender (v/c), school (v/c), locale (v/c)
+ edges from 097ec57 (vertex names):
 [1] 4019--4026 4023--4031 4023--4030 4027--4032 3988--4021 3982--3986
 [7] 3994--3998 3998--3999 3993--3995 3982--4021 3982--4037 3997--4019
[13] 3994--4019 3992--4017 3981--3998 3997--4018 4009--4030 3994--4018
[19] 3995--4000 4000--4026 4027--4038 4031--4038 4000--4021 3986--4030
[25] 3985--4014 3994--4030 3998--4021 3994--4009 3982--4023 3998--4019
[31] 4020--4031 4009--4023 3994--3997 3981--4023 3997--4030 3997--4021
[37] 4023--4034 3993--4004 3994--3996 4000--4030 3998--4014 4004--4013
[43] 4016--4025 3990--4016 3999--4005 4004--4023 4002--4020 3998--4018
+ ... omitted several edges&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: A directed graph is one where the edges only go one way – they point from one vertex to another. The other option is an undirected graph, which means that the relations between the vertices are symmetric.&lt;/p&gt;
&lt;p&gt;Now, we want to plot our graph. By default, the vertices are large and have text labels of a user’s identifier, this would clutter the output.&lt;/p&gt;
&lt;p&gt;We will plot with no text labels and smaller vertices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(g, vertex.size=5, vertex.label=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/visualizing_network/visualizing_network_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this graph, there are a number of groups of nodes where all the nodes in each group are connected but, the groups are disjoint from one another, forming “islands” in the graph. Such groups are called “connected components,” or “components” for short.&lt;/p&gt;
&lt;p&gt;How many connected components with at least 2 nodes are there in the graph?
#### 4&lt;/p&gt;
&lt;p&gt;How many users are there with no friends in the network?
#### 7&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---creating-a-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Creating a Network&lt;/h3&gt;
&lt;p&gt;In our graph, the “degree” of a node is its number of friends. We have already seen that some nodes in our graph have degree 0 (these are the nodes with no friends), while others have much higher degree. We can use degree(g) to compute the degree of all the nodes in our graph g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;degree(g)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 
   7   13    1    0    5    8    1    6    5    3    2    2    5   10    8 
 594 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 
   3    3   10   13    3    8    1    6    4    9    2    1    3    0    9 
4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023 4024 
   0    3    1    5   11    0    3    8    6    7    7   10    0   17    0 
4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 
   3    8    6    1    1   18   10    1    2    1    0    1    3    8 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many users are friends with 10 or more other Facebook users in this network?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(degree(g) &amp;gt;= 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---creating-a-network&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Creating a Network&lt;/h3&gt;
&lt;p&gt;In a network, it’s often visually useful to draw attention to “important” nodes in the network. While this might mean different things in different contexts, in a social network we might consider a user with a large number of friends to be an important user. From the previous problem, we know this is the same as saying that nodes with a high degree are important users.&lt;/p&gt;
&lt;p&gt;To visually draw attention to these nodes, we will change the size of the vertices so the vertices with high degrees are larger. To do this, we will change the “size” attribute of the vertices of our graph to be an increasing function of their degrees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V(g)$size &amp;lt;- degree(g)/2+2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, that we have specified the vertex size of each vertex, we will no longer use the vertex.size parameter when we plot our graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(g, vertex.label=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/visualizing_network/visualizing_network_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the largest size we assigned to any node in our graph?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(V(g)$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the smallest size we assigned to any node in our graph?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(V(g)$size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---coloring-vertices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Coloring Vertices&lt;/h3&gt;
&lt;p&gt;Thus far, we have changed the “size” attributes of our vertices. However, we can also change the colors of vertices to capture additional information about the &lt;strong&gt;Facebook users&lt;/strong&gt; we are depicting.&lt;/p&gt;
&lt;p&gt;When changing the size of nodes, we first obtained the vertices of our graph with V(g) and then accessed the the size attribute with V(g)&lt;span class=&#34;math inline&#34;&gt;\(size. To change the color, we will update the attribute V(g)\)&lt;/span&gt;color.&lt;/p&gt;
&lt;p&gt;To color the vertices based on the gender of the user, we will need access to that variable. When we created our graph g, we provided it with the dataframe users, which had variables gender, school, and locale. These are now stored as attributes V(g)&lt;span class=&#34;math inline&#34;&gt;\(gender, V(g)\)&lt;/span&gt;school, and V(g)$locale.&lt;/p&gt;
&lt;p&gt;We can update the colors by setting the color to black for all vertices, than setting it to red for the vertices with gender A and setting it to gray for the vertices with gender B:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V(g)$color = &amp;quot;black&amp;quot;
V(g)$color[V(g)$gender == &amp;quot;A&amp;quot;] = &amp;quot;red&amp;quot;
V(g)$color[V(g)$gender == &amp;quot;B&amp;quot;] = &amp;quot;gray&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ploting the resulting graph.&lt;/p&gt;
&lt;p&gt;What is the gender of the users with the highest degree in the graph?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(g, vertex.label=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/visualizing_network/visualizing_network_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Gender B&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---coloring-vertices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Coloring Vertices&lt;/h3&gt;
&lt;p&gt;Now, color the vertices based on the school that each user in our network attended.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(V(g)$school)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    A AB 
40 17  2 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V(g)$color = &amp;quot;black&amp;quot;
V(g)$color[V(g)$school == &amp;quot;A&amp;quot;] = &amp;quot;red&amp;quot;
V(g)$color[V(g)$school == &amp;quot;AB&amp;quot;] = &amp;quot;gray&amp;quot;
plot(g, vertex.label=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/visualizing_network/visualizing_network_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Are the two users who attended both schools A and B Facebook friends with each other?
#### Yes&lt;/p&gt;
&lt;p&gt;What best describes the users with highest degree?
#### Some, but not all, of the high-degree users attended school A&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---coloring-vertices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Coloring Vertices&lt;/h3&gt;
&lt;p&gt;Now, color the vertices based on the locale of the user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(V(g)$locale)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    A  B 
 3  6 50 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V(g)$color = &amp;quot;black&amp;quot;
V(g)$color[V(g)$locale == &amp;quot;A&amp;quot;] = &amp;quot;red&amp;quot;
V(g)$color[V(g)$locale == &amp;quot;B&amp;quot;] = &amp;quot;gray&amp;quot;
plot(g, vertex.label=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/visualizing_network/visualizing_network_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The large connected component is most associated with which locale?
#### Locale B&lt;/p&gt;
&lt;p&gt;The 4-user connected component is most associated with which locale?
#### Locale A&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4---other-plotting-options&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4 - Other Plotting Options&lt;/h3&gt;
&lt;p&gt;The help page is a helpful tool when making visualizations. The following questions with the help of ?igraph.plotting and experimentation in our R console.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?igraph.plotting&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which igraph plotting function would enable us to plot our graph in 3-D?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?rglplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;rglplot&lt;/p&gt;
&lt;p&gt;What parameter to the plot() function would we use to change the edge width when plotting g?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?plot.igraph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;edge.width&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Revisiting Election Forecasting</title>
      <link>/project/election_forecasting/election_forecasting/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/election_forecasting/election_forecasting/</guid>
      <description>


&lt;p&gt;Prevously, we used logistic regression on polling data in order to construct US presidential election predictions. We separated our data into a training set, containing data from 2004 and 2008 polls, and a test-set, containing the data from 2012 polls. We then proceeded to develop a logistic regression model to forecast the 2012 US presidential election.&lt;/p&gt;
&lt;p&gt;In this problem, we’ll revisit our logistic regression model, and learn how to plot the output on a map of the United States. Unlike what we did prevously, this time we’ll be plotting predictions rather than data!&lt;/p&gt;
&lt;p&gt;First, load the ggplot2, maps, and ggmap packages using the library function. All three packages should be installed on your computer from lecture, but if not, you may need to install them too using the install.packages function.&lt;/p&gt;
&lt;p&gt;Then, load the US map and save it to the variable statesMap:&lt;/p&gt;
&lt;p&gt;statesMap = map_data(“state”)&lt;/p&gt;
&lt;p&gt;The maps package contains other built-in maps, including a US county map, a world map, and maps for France and Italy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load states map
statesMap = map_data(&amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;problem-1.1---drawing-a-map-of-the-us&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Drawing a Map of the US&lt;/h3&gt;
&lt;p&gt;If you look at the structure of the statesMap dataframe using the str function, you should see that there are 6 variables. One of the variables, group, defines the different shapes or polygons on the map. Sometimes a state may have multiple groups, for example, if it includes islands.&lt;/p&gt;
&lt;p&gt;How many different groups are there?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(statesMap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   15537 obs. of  6 variables:
 $ long     : num  -87.5 -87.5 -87.5 -87.5 -87.6 ...
 $ lat      : num  30.4 30.4 30.4 30.3 30.3 ...
 $ group    : num  1 1 1 1 1 1 1 1 1 1 ...
 $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
 $ region   : chr  &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; ...
 $ subregion: chr  NA NA NA NA ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(table(statesMap$group))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 63&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable “order” defines the order to connect the points within each group, and the variable “region” gives the name of the state.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---drawing-a-map-of-the-us&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Drawing a Map of the US&lt;/h3&gt;
&lt;p&gt;You can draw a map of the US by typing the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(statesMap, aes(x = long, y = lat, group = group)) + 
    geom_polygon(fill = &amp;quot;white&amp;quot;, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We specified two colors in geom_polygon – fill and color. Which one defined the color of the outline of the states?
#### color&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---coloring-the-states-by-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Coloring the States by Predictions&lt;/h3&gt;
&lt;p&gt;Now, let’s color the map of the US according to our 2012 US presidential election predictions from the Unit 3 Recitation. We’ll rebuild the model here, using the dataset PollingImputed.csv. Be sure to use this file so that you don’t have to redo the imputation to fill in the missing values, like we did in the Unit 3 Recitation.&lt;/p&gt;
&lt;p&gt;Load the data using the read.csv function, and call it “polling”. Then split the data using the subset function into a training set called “Train” that has observations from 2004 and 2008, and a testing set called “Test” that has observations from 2012.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polling &amp;lt;- read.csv(&amp;quot;PollingImputed.csv&amp;quot;)
str(polling)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   145 obs. of  7 variables:
 $ State     : Factor w/ 50 levels &amp;quot;Alabama&amp;quot;,&amp;quot;Alaska&amp;quot;,..: 1 1 2 2 3 3 3 4 4 4 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ...
 $ Rasmussen : int  11 21 19 16 5 5 8 7 10 13 ...
 $ SurveyUSA : int  18 25 21 18 15 3 5 5 7 21 ...
 $ DiffCount : int  5 5 1 6 8 9 4 8 5 2 ...
 $ PropR     : num  1 1 1 1 1 ...
 $ Republican: int  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(polling$Year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
2004 2008 2012 
  50   50   45 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Train &amp;lt;- subset(polling, Year &amp;lt;= 2008)
Test &amp;lt;- subset(polling, Year &amp;gt; 2008)
str(Train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   100 obs. of  7 variables:
 $ State     : Factor w/ 50 levels &amp;quot;Alabama&amp;quot;,&amp;quot;Alaska&amp;quot;,..: 1 1 2 2 3 3 4 4 5 5 ...
 $ Year      : int  2004 2008 2004 2008 2004 2008 2004 2008 2004 2008 ...
 $ Rasmussen : int  11 21 19 16 5 5 7 10 -11 -27 ...
 $ SurveyUSA : int  18 25 21 18 15 3 5 7 -11 -24 ...
 $ DiffCount : int  5 5 1 6 8 9 8 5 -8 -5 ...
 $ PropR     : num  1 1 1 1 1 1 1 1 0 0 ...
 $ Republican: int  1 1 1 1 1 1 1 1 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(Test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   45 obs. of  7 variables:
 $ State     : Factor w/ 50 levels &amp;quot;Alabama&amp;quot;,&amp;quot;Alaska&amp;quot;,..: 3 4 5 6 7 9 10 11 12 13 ...
 $ Year      : int  2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ...
 $ Rasmussen : int  8 13 -12 3 -7 2 5 -22 31 -22 ...
 $ SurveyUSA : int  5 21 -14 -2 -13 0 8 -24 24 -16 ...
 $ DiffCount : int  4 2 -6 -5 -8 6 4 -2 1 -5 ...
 $ PropR     : num  0.833 1 0 0.308 0 ...
 $ Republican: int  1 1 0 0 0 0 1 0 1 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we only have 45 states in our testing set, since we are missing observations for Alaska, Delaware, Alabama, Wyoming, and Vermont, so these states will not appear colored in our map.&lt;/p&gt;
&lt;p&gt;Then, create a logistic regression model and make predictions on the test-set using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- glm(Republican ~ SurveyUSA + DiffCount, 
            data = Train, family = &amp;quot;binomial&amp;quot;)
TestPrediction &amp;lt;- predict(mod2, newdata = Test, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;TestPrediction gives the predicted probabilities for each state, but let’s also create a vector of Republican/Democrat predictions by using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TestPredictionBinary &amp;lt;- as.numeric(TestPrediction &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, put the predictions and state labels in a data.frame so that we can use ggplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictionDataFrame &amp;lt;- 
    data.frame(TestPrediction, TestPredictionBinary, Test$State)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make sure everything went smoothly, answer the following.&lt;/p&gt;
&lt;p&gt;For how many states is our binary prediction 1 (for 2012), corresponding to Republican?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(TestPredictionBinary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; num [1:45] 1 1 0 0 0 1 1 0 1 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(TestPredictionBinary)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;TestPredictionBinary
 0  1 
23 22 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;22&lt;/p&gt;
&lt;p&gt;What is the average predicted probability of our model (on the Test set, for 2012)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(TestPrediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4852626&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---coloring-the-states-by-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Coloring the States by Predictions&lt;/h3&gt;
&lt;p&gt;Now, we need to merge “predictionDataFrame” with the map data “statesMap”. Before doing so, we need to convert the Test.State variable to lowercase, so that it matches the region variable in statesMap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictionDataFrame$region &amp;lt;- tolower(predictionDataFrame$Test.State)
# Now, merge the two data frames using the following command:
predictionMap &amp;lt;- merge(statesMap, predictionDataFrame, by = &amp;quot;region&amp;quot;)
# Lastly, we need to make sure the observations are in order so that the map is
# drawn properly, by typing the following:
predictionMap &amp;lt;- predictionMap[order(predictionMap$order),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are there in predictionMap?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(predictionMap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   15034 obs. of  9 variables:
 $ region              : chr  &amp;quot;arizona&amp;quot; &amp;quot;arizona&amp;quot; &amp;quot;arizona&amp;quot; &amp;quot;arizona&amp;quot; ...
 $ long                : num  -115 -115 -115 -115 -115 ...
 $ lat                 : num  35 35.1 35.1 35.2 35.2 ...
 $ group               : num  2 2 2 2 2 2 2 2 2 2 ...
 $ order               : int  204 205 206 207 208 209 210 211 212 213 ...
 $ subregion           : chr  NA NA NA NA ...
 $ TestPrediction      : num  0.974 0.974 0.974 0.974 0.974 ...
 $ TestPredictionBinary: num  1 1 1 1 1 1 1 1 1 1 ...
 $ Test.State          : Factor w/ 50 levels &amp;quot;Alabama&amp;quot;,&amp;quot;Alaska&amp;quot;,..: 3 3 3 3 3 3 3 3 3 3 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are there in statesMap?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(statesMap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   15537 obs. of  6 variables:
 $ long     : num  -87.5 -87.5 -87.5 -87.5 -87.6 ...
 $ lat      : num  30.4 30.4 30.4 30.3 30.3 ...
 $ group    : num  1 1 1 1 1 1 1 1 1 1 ...
 $ order    : int  1 2 3 4 5 6 7 8 9 10 ...
 $ region   : chr  &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; &amp;quot;alabama&amp;quot; ...
 $ subregion: chr  NA NA NA NA ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---coloring-the-states-by-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Coloring the States by Predictions&lt;/h3&gt;
&lt;p&gt;When we merged the data in the previous problem, it caused the number of observations to change. Why? Check out the help page for merge by typing ?merge to help us answer this question.
#### Because we only make predictions for 45 states, we no longer have observations for some of the states. These observations were removed in the merging process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---coloring-the-states-by-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Coloring the States by Predictions&lt;/h3&gt;
&lt;p&gt;Now we are ready to color the US map with our predictions! You can color the states according to our binary predictions by typing the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + 
    geom_polygon(color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The states appear light blue and dark blue in this map. Which color represents a Republican prediction?
#### Light blue&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---coloring-the-states-by-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - Coloring the States by Predictions&lt;/h3&gt;
&lt;p&gt;We see that the legend displays a blue gradient for outcomes between 0 and 1. However, when plotting the binary predictions there are only two possible outcomes: 0 or 1.&lt;/p&gt;
&lt;p&gt;Let’s replot the map with discrete outcomes. We can also change the color scheme to blue and red, to match the blue color associated with the Democratic Party in the US and the red color associated with the Republican Party in the US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) +
    geom_polygon(color = &amp;quot;black&amp;quot;) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we could plot the probabilities instead of the binary predictions. Change the plot code above to instead color the states by the variable TestPrediction. You should see a gradient of colors ranging from red to blue.&lt;/p&gt;
&lt;p&gt;Do the colors of the states in the map for TestPrediction look different from the colors of the states in the map with TestPredictionBinary? Why or why not?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPrediction)) +
    geom_polygon(color = &amp;quot;black&amp;quot;) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(TestPrediction, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   7   10   13   16   19   24   27   30   33   36   39   42   45   48   51 
0.97 1.00 0.00 0.01 0.00 0.96 0.99 0.00 1.00 0.00 1.00 0.06 0.95 0.99 1.00 
  54   57   60   63   66   69   72   75   78   81   84   87   90   93   96 
0.00 0.00 0.00 0.00 0.00 0.93 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.95 
  99  102  105  108  111  114  117  120  123  126  129  134  137  140  143 
1.00 0.00 1.00 0.00 0.00 0.00 1.00 0.99 1.00 1.00 1.00 0.02 0.00 1.00 0.00 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-two-maps-look-very-similar.-this-is-because-most-of-our-predicted-probabilities-are-close-to-0-or-close-to-1.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The two maps look very similar. This is because most of our predicted probabilities are close to 0 or close to 1.&lt;/h4&gt;
&lt;p&gt;NOTE: If you have a hard time seeing the red/blue gradient, feel free to change the color scheme, by changing the arguments low = “blue” and high = “red” to colors of your choice (to see all of the color options in R, type colors() in your R console). You can even change it to a gray scale, by changing the low and high colors to “gray” and “black”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---understanding-the-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Understanding the Predictions&lt;/h3&gt;
&lt;p&gt;In the 2012 election, the state of Florida ended up being a very close race. It was ultimately won by the Democratic party. Did we predict this state correctly or incorrectly? To see the names and locations of the different states, take a look at the World Atlas map here.
#### We incorrectly predicted this state by predicting that it would be won by the Republican party.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---understanding-the-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Understanding the Predictions&lt;/h3&gt;
&lt;p&gt;What was our predicted probability for the state of Florida?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictionDataFrame[predictionDataFrame$region == &amp;quot;florida&amp;quot;, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   TestPrediction TestPredictionBinary Test.State  region
24      0.9640395                    1    Florida florida&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this imply?
#### Our prediction model did not do a very good job of correctly predicting the state of Florida, and we were very confident in our incorrect prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4---parameter-settings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;PROBLEM 4 - PARAMETER SETTINGS&lt;/h3&gt;
&lt;p&gt;In this part, we’ll explore what the different parameter settings of geom_polygon do. Throughout the problem, use the help page for geom_polygon, which can be accessed by ?geom_polygon. To see more information about a certain parameter, just type a question mark and then the parameter name to get the help page for that parameter. Experiment with different parameter settings to try and replicate the plots!&lt;/p&gt;
&lt;p&gt;We’ll be asking questions about the following three plots:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---parameter-settings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Parameter Settings&lt;/h3&gt;
&lt;p&gt;Plots (1) and (2) were created by setting different parameters of geom_polygon to the value 3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPrediction)) +
    geom_polygon(color = &amp;quot;black&amp;quot;) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?geom_polygon&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the name of the parameter we set to have value 3 to create plot (1)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPrediction)) +
    geom_polygon(color = &amp;quot;black&amp;quot;, linetype = 3) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;linetype&lt;/p&gt;
&lt;p&gt;What is the name of the parameter we set to have value 3 to create plot (2)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPrediction)) +
    geom_polygon(color = &amp;quot;black&amp;quot;, size = 3) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;size&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---parameter-settings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Parameter Settings&lt;/h3&gt;
&lt;p&gt;Plot (3) was created by changing the value of a different geom_polygon parameter to have value 0.3. Which parameter did we use?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predictionMap, 
       aes(x = long, y = lat, group = group, fill = TestPrediction)) +
    geom_polygon(color = &amp;quot;black&amp;quot;, alpha = 0.3) + 
    scale_fill_gradient(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, guide = &amp;quot;legend&amp;quot;, 
                        breaks= c(0,1), labels = c(&amp;quot;Democrat&amp;quot;, &amp;quot;Republican&amp;quot;), 
                        name = &amp;quot;Prediction 2012&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/election_forecasting/election_forecasting_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;alpha&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Market Segmentation For Airlines</title>
      <link>/project/market_segmentation_for_airlines/market/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/market_segmentation_for_airlines/market/</guid>
      <description>


&lt;p&gt;Market segmentation is a strategy that divides a broad target market of customers into smaller, more similar groups, and then designs a marketing strategy specifically for each group. Clustering is a common technique for market segmentation since it automatically finds similar groups given a dataset.&lt;/p&gt;
&lt;p&gt;In this analysis, I’ll see how clustering can be used to find similar groups of customers who belong to an airline’s frequent flyer program. The airline is trying to learn more about its customers so that it can target different customer segments with different types of mileage offers.&lt;/p&gt;
&lt;p&gt;The file AirlinesCluster.csv contains information on 3,999 members of the frequent flyer program. This data comes from the textbook &lt;strong&gt;Data Mining for Business Intelligence,&lt;/strong&gt; by Galit Shmueli, Nitin R. Patel, and Peter C. Bruce. For more information, see the website for the book.&lt;/p&gt;
&lt;p&gt;There are seven different variables in the dataset, described below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Balance = number of miles eligible for award travel&lt;/li&gt;
&lt;li&gt;QualMiles = number of miles qualifying for TopFlight status&lt;/li&gt;
&lt;li&gt;BonusMiles = number of miles earned from non-flight bonus transactions in the past 12 months&lt;/li&gt;
&lt;li&gt;BonusTrans = number of non-flight bonus transactions in the past 12 months&lt;/li&gt;
&lt;li&gt;FlightMiles = number of flight miles in the past 12 months&lt;/li&gt;
&lt;li&gt;FlightTrans = number of flight transactions in the past 12 months&lt;/li&gt;
&lt;li&gt;DaysSinceEnroll = number of days since enrolled in the frequent flyer program&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---normalizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Normalizing the Data&lt;/h3&gt;
&lt;p&gt;Read the dataset AirlinesCluster.csv into R and call it “airlines”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;airlines &amp;lt;- read.csv(&amp;quot;AirlinesCluster.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the summary of airlines, which TWO variables have (on average) the smallest values?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(airlines)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Balance          QualMiles         BonusMiles       BonusTrans  
 Min.   :      0   Min.   :    0.0   Min.   :     0   Min.   : 0.0  
 1st Qu.:  18528   1st Qu.:    0.0   1st Qu.:  1250   1st Qu.: 3.0  
 Median :  43097   Median :    0.0   Median :  7171   Median :12.0  
 Mean   :  73601   Mean   :  144.1   Mean   : 17145   Mean   :11.6  
 3rd Qu.:  92404   3rd Qu.:    0.0   3rd Qu.: 23800   3rd Qu.:17.0  
 Max.   :1704838   Max.   :11148.0   Max.   :263685   Max.   :86.0  
  FlightMiles       FlightTrans     DaysSinceEnroll
 Min.   :    0.0   Min.   : 0.000   Min.   :   2   
 1st Qu.:    0.0   1st Qu.: 0.000   1st Qu.:2330   
 Median :    0.0   Median : 0.000   Median :4096   
 Mean   :  460.1   Mean   : 1.374   Mean   :4119   
 3rd Qu.:  311.0   3rd Qu.: 1.000   3rd Qu.:5790   
 Max.   :30817.0   Max.   :53.000   Max.   :8296   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BonusTrans and FlightTrans&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---normalizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Normalizing the Data&lt;/h3&gt;
&lt;p&gt;In this problem, we will normalize our data before we run the clustering algorithms.&lt;/p&gt;
&lt;p&gt;Why is it important to normalize the data before clustering?
#### If we don’t normalize the data, the clustering will be dominated by the variables that are on a larger scale.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---normalizing-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Normalizing the Data&lt;/h3&gt;
&lt;p&gt;Let’s go ahead and normalize our data. You can normalize the variables in a dataframe by using the preProcess function in the “caret” package.&lt;/p&gt;
&lt;p&gt;Now, create a normalized dataframe called “airlinesNorm” by running the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preproc &amp;lt;- preProcess(airlines)
airlinesNorm &amp;lt;- predict(preproc, airlines)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first code pre-processes the data, and the second code performs the normalization. If you look at the summary of airlinesNorm, you should see that all of the variables now have mean zero. You can also see that each of the variables has standard deviation 1 by using the sd() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(airlinesNorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Balance          QualMiles         BonusMiles        BonusTrans      
 Min.   :-0.7303   Min.   :-0.1863   Min.   :-0.7099   Min.   :-1.20805  
 1st Qu.:-0.5465   1st Qu.:-0.1863   1st Qu.:-0.6581   1st Qu.:-0.89568  
 Median :-0.3027   Median :-0.1863   Median :-0.4130   Median : 0.04145  
 Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.00000  
 3rd Qu.: 0.1866   3rd Qu.:-0.1863   3rd Qu.: 0.2756   3rd Qu.: 0.56208  
 Max.   :16.1868   Max.   :14.2231   Max.   :10.2083   Max.   : 7.74673  
  FlightMiles       FlightTrans       DaysSinceEnroll   
 Min.   :-0.3286   Min.   :-0.36212   Min.   :-1.99336  
 1st Qu.:-0.3286   1st Qu.:-0.36212   1st Qu.:-0.86607  
 Median :-0.3286   Median :-0.36212   Median :-0.01092  
 Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  
 3rd Qu.:-0.1065   3rd Qu.:-0.09849   3rd Qu.: 0.80960  
 Max.   :21.6803   Max.   :13.61035   Max.   : 2.02284  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(airlinesNorm$Balance)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(airlinesNorm$FlightTrans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the normalized data, which variable has the largest maximum value?
#### FlightMiles&lt;/p&gt;
&lt;p&gt;In the normalized data, which variable has the smallest minimum value?
#### DaysSinceEnroll&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Compute the distances between data points (using euclidean distance) and then run the Hierarchical clustering algorithm (using method=“ward.D”) on the normalized data. It may take a few minutes for the code to finish since the dataset has a large number of observations for hierarchical clustering.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distAirlines &amp;lt;- dist(airlinesNorm, method = &amp;quot;euclidean&amp;quot;)
hclustAirlines &amp;lt;- hclust(distAirlines, method = &amp;quot;ward.D&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, plot the dendrogram of the hierarchical clustering process. Suppose the airline is looking for somewhere between 2 and 10 clusters. According to the dendrogram, which of the following is NOT a good choice for the number of clusters?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(hclustAirlines)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/market_segmentation_for_airlines/market_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;6&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Suppose that after looking at the dendrogram and discussing with the marketing department, the airline decides to proceed with 5 clusters. Divide the data points into 5 clusters by using the cutree function.&lt;/p&gt;
&lt;p&gt;How many data points are in Cluster 1?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hclustGroups &amp;lt;- cutree(hclustAirlines, k = 5)
table(hclustGroups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;hclustGroups
   1    2    3    4    5 
 776  519  494  868 1342 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;776&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Now, use tapply to compare the average values in each of the variables for the 5 clusters (the centroids of the clusters). You may want to compute the average values of the unnormalized data so that it is easier to interpret. You can do this for the variable “Balance”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(airlines)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3999 obs. of  7 variables:
 $ Balance        : int  28143 19244 41354 14776 97752 16420 84914 20856 443003 104860 ...
 $ QualMiles      : int  0 0 0 0 0 0 0 0 0 0 ...
 $ BonusMiles     : int  174 215 4123 500 43300 0 27482 5250 1753 28426 ...
 $ BonusTrans     : int  1 2 4 1 26 0 25 4 43 28 ...
 $ FlightMiles    : int  0 0 0 0 2077 0 0 250 3850 1150 ...
 $ FlightTrans    : int  0 0 0 0 4 0 0 1 12 3 ...
 $ DaysSinceEnroll: int  7000 6968 7034 6952 6935 6942 6994 6938 6948 6931 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$Balance, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
 57866.90 110669.27 198191.57  52335.91  36255.91 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$QualMiles, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           1            2            3            4            5 
   0.6443299 1065.9826590   30.3461538    4.8479263    2.5111773 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$BonusMiles, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
10360.124 22881.763 55795.860 20788.766  2264.788 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$BonusTrans, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
10.823454 18.229287 19.663968 17.087558  2.973174 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$FlightMiles, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         1          2          3          4          5 
  83.18428 2613.41811  327.67611  111.57373  119.32191 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$FlightTrans, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
0.3028351 7.4026975 1.0688259 0.3444700 0.4388972 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$DaysSinceEnroll, hclustGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       1        2        3        4        5 
6235.365 4402.414 5615.709 2840.823 3060.081 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compared to the other clusters, Cluster 1 has the largest average values in which variables (if any)?
#### DaysSinceEnroll&lt;/p&gt;
&lt;p&gt;How would you describe the customers in Cluster 1?
#### Infrequent but loyal customers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Compared to the other clusters, Cluster 2 has the largest average values in which variables?
#### QualMiles, FlightMiles, FlightTrans&lt;/p&gt;
&lt;p&gt;How would you describe the customers in Cluster 2?
#### Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Compared to the other clusters, Cluster 3 has the largest average values in which variables?
#### Balance, BonusMiles, BonusTrans&lt;/p&gt;
&lt;p&gt;How would you describe the customers in Cluster 3?
#### Customers who have accumulated a large amount of miles, mostly through non-flight transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.6---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.6 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Compared to the other clusters, Cluster 4 has the largest average values in which variables?
#### None&lt;/p&gt;
&lt;p&gt;How would you describe the customers in Cluster 4?
#### Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.7---hierarchical-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.7 - Hierarchical Clustering&lt;/h3&gt;
&lt;p&gt;Compared to the other clusters, Cluster 5 has the largest average values in which variables?
#### None&lt;/p&gt;
&lt;p&gt;How would you describe the customers in Cluster 5?
#### Relatively new customers who don’t use the airline very often.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---k-means-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - K-Means Clustering&lt;/h3&gt;
&lt;p&gt;Now run the k-means clustering algorithm on the normalized data, again creating 5 clusters. Set the seed to 88 right before running the clustering algorithm, and set the argument iter.max to 1000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(88)
kmeansAirlines &amp;lt;- kmeans(airlinesNorm, centers = 5, iter.max = 1000)
str(kmeansAirlines)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;List of 9
 $ cluster     : int [1:3999] 4 4 4 4 1 4 3 4 2 3 ...
 $ centers     : num [1:5, 1:7] 1.4444 1.0005 -0.0558 -0.1333 -0.4058 ...
  ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
  .. ..$ : chr [1:5] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...
  .. ..$ : chr [1:7] &amp;quot;Balance&amp;quot; &amp;quot;QualMiles&amp;quot; &amp;quot;BonusMiles&amp;quot; &amp;quot;BonusTrans&amp;quot; ...
 $ totss       : num 27986
 $ withinss    : num [1:5] 4948 3624 2054 2040 2321
 $ tot.withinss: num 14987
 $ betweenss   : num 12999
 $ size        : int [1:5] 408 141 993 1182 1275
 $ iter        : int 4
 $ ifault      : int 0
 - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;kmeans&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many clusters have more than 1,000 observations?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(kmeansAirlines$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   1    2    3    4    5 
 408  141  993 1182 1275 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---k-means-clustering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - K-Means Clustering&lt;/h3&gt;
&lt;p&gt;Now, compare the cluster centroids to each other either by dividing the data points into groups and then using tapply, or by looking at the output of kmeansClust&lt;span class=&#34;math inline&#34;&gt;\(centers, where &amp;quot;kmeansClust&amp;quot; is the name of the output of the kmeans function. (Note that the output of kmeansClust\)&lt;/span&gt;centers will be for the normalized data. If you want to look at the average values for the unnormalized data, you need to use tapply like we did for hierarchical clustering.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kmeansGroups &amp;lt;- kmeansAirlines$cluster
tapply(airlines$Balance, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
219161.40 174431.51  67977.44  60166.18  32706.67 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$QualMiles, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
539.57843 673.16312  34.99396  55.20812 126.46667 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$BonusMiles, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
62474.483 31985.085 24490.019  8709.712  3097.478 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$BonusTrans, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
21.524510 28.134752 18.429003  8.362098  4.284706 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$FlightMiles, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5 
 623.8725 5859.2340  289.4713  203.2589  181.4698 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$FlightTrans, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         1          2          3          4          5 
 1.9215686 17.0000000  0.8851964  0.6294416  0.5403922 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(airlines$DaysSinceEnroll, kmeansGroups, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       1        2        3        4        5 
5605.051 4684.901 3416.783 6109.540 2281.055 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do you expect Cluster 1 of the K-Means clustering output to necessarily be similar to Cluster 1 of the Hierarchical clustering output?
#### No, because cluster ordering is not meaningful in either k-means clustering or hierarchical clustering.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stock Returns Prediction</title>
      <link>/project/stock_returns/stock/</link>
      <pubDate>Tue, 09 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/stock_returns/stock/</guid>
      <description>


&lt;p&gt;About cluster-then-predict, a methodology in which you first cluster observations and then build cluster-specific prediction models. In this problem, I’ll use cluster-then-predict to predict future stock prices using historical stock data.&lt;/p&gt;
&lt;p&gt;When selecting which stocks to invest in, investors seek to obtain good future returns. In this analysis, I’ll first use clustering to identify clusters of stocks that have similar returns over time. Then, use logistic regression to predict whether or not the stocks will have positive future returns.&lt;/p&gt;
&lt;p&gt;For this problem, I’ll use StocksCluster.csv, which contains monthly stock returns from the NASDAQ stock exchange. The NASDAQ is the second-largest stock exchange in the world, and it lists many tech companies. The stock price data used in this analysis was obtained from &lt;strong&gt;infochimps,&lt;/strong&gt; a website providing access to many datasets.&lt;/p&gt;
&lt;p&gt;Each observation in the dataset is the monthly returns of a particular company in a particular year. The years included are 2000-2009. The companies are limited to tickers that were listed on the exchange for the entire period 2000-2009, and whose stock price never fell below $1. So, for example, one observation is for Yahoo in 2000, and another observation is for Yahoo in 2001. Our goal will be to predict whether or not the stock return in December will be positive, using the stock returns for the first 11 months of the year.&lt;/p&gt;
&lt;p&gt;This dataset contains the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ReturnJan = the return for the company’s stock during January (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnFeb = the return for the company’s stock during February (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnMar = the return for the company’s stock during March (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnApr = the return for the company’s stock during April (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnMay = the return for the company’s stock during May (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnJune = the return for the company’s stock during June (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnJuly = the return for the company’s stock during July (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnAug = the return for the company’s stock during August (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnSep = the return for the company’s stock during September (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnOct = the return for the company’s stock during October (in the year of the observation).&lt;/li&gt;
&lt;li&gt;ReturnNov = the return for the company’s stock during November (in the year of the observation).&lt;/li&gt;
&lt;li&gt;PositiveDec = whether or not the company’s stock had a positive return in December (in the year of the observation). This variable takes value 1 if the return was positive, and value 0 if the return was not positive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the first 11 variables, the value stored is a proportional change in stock value during that month. For instance, a value of 0.05 means the stock increased in value 5% during the month, while a value of -0.02 means the stock decreased in value 2% during the month.&lt;/p&gt;
&lt;div id=&#34;problem-1.1---exploring-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Exploring the Dataset&lt;/h3&gt;
&lt;p&gt;Load StocksCluster.csv into a dataframe called “stocks”.&lt;/p&gt;
&lt;p&gt;How many observations are in the dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stocks &amp;lt;- read.csv(&amp;quot;StocksCluster.csv&amp;quot;)
str(stocks)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   11580 obs. of  12 variables:
 $ ReturnJan  : num  0.0807 -0.0107 0.0477 -0.074 -0.031 ...
 $ ReturnFeb  : num  0.0663 0.1021 0.036 -0.0482 -0.2127 ...
 $ ReturnMar  : num  0.0329 0.1455 0.0397 0.0182 0.0915 ...
 $ ReturnApr  : num  0.1831 -0.0844 -0.1624 -0.0247 0.1893 ...
 $ ReturnMay  : num  0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ...
 $ ReturnJune : num  -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ...
 $ ReturnJuly : num  -0.0205 -0.0253 -0.1354 -0.094 0.3553 ...
 $ ReturnAug  : num  0.0247 0.2113 0.0334 0.0953 0.0568 ...
 $ ReturnSep  : num  -0.0204 -0.58 0 0.0567 0.0336 ...
 $ ReturnOct  : num  -0.1733 -0.2671 0.0917 -0.0963 0.0363 ...
 $ ReturnNov  : num  -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ...
 $ PositiveDec: int  0 0 0 1 1 1 1 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;11580&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---exploring-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Exploring the Dataset&lt;/h3&gt;
&lt;p&gt;What proportion of the observations have positive returns in December?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocks$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
5256 6324 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;6324 / (5256 + 6324)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.546114&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---exploring-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Exploring the Dataset&lt;/h3&gt;
&lt;p&gt;What is the maximum correlation between any two return variables in the dataset? You should look at the pairwise correlations between ReturnJan, ReturnFeb, ReturnMar, ReturnApr, ReturnMay, ReturnJune, ReturnJuly, ReturnAug, ReturnSep, ReturnOct, and ReturnNov.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(stocks[, 1:11])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             ReturnJan   ReturnFeb    ReturnMar    ReturnApr    ReturnMay
ReturnJan   1.00000000  0.06677458 -0.090496798 -0.037678006 -0.044411417
ReturnFeb   0.06677458  1.00000000 -0.155983263 -0.191351924 -0.095520920
ReturnMar  -0.09049680 -0.15598326  1.000000000  0.009726288 -0.003892789
ReturnApr  -0.03767801 -0.19135192  0.009726288  1.000000000  0.063822504
ReturnMay  -0.04441142 -0.09552092 -0.003892789  0.063822504  1.000000000
ReturnJune  0.09223831  0.16999448 -0.085905486 -0.011027752 -0.021074539
ReturnJuly -0.08142976 -0.06177851  0.003374160  0.080631932  0.090850264
ReturnAug  -0.02279202  0.13155979 -0.022005400 -0.051756051 -0.033125658
ReturnSep  -0.02643715  0.04350177  0.076518327 -0.028920972  0.021962862
ReturnOct   0.14297723 -0.08732427 -0.011923758  0.048540025  0.017166728
ReturnNov   0.06763233 -0.15465828  0.037323535  0.031761837  0.048046590
            ReturnJune    ReturnJuly     ReturnAug     ReturnSep
ReturnJan   0.09223831 -0.0814297650 -0.0227920187 -0.0264371526
ReturnFeb   0.16999448 -0.0617785094  0.1315597863  0.0435017706
ReturnMar  -0.08590549  0.0033741597 -0.0220053995  0.0765183267
ReturnApr  -0.01102775  0.0806319317 -0.0517560510 -0.0289209718
ReturnMay  -0.02107454  0.0908502642 -0.0331256580  0.0219628623
ReturnJune  1.00000000 -0.0291525996  0.0107105260  0.0447472692
ReturnJuly -0.02915260  1.0000000000  0.0007137558  0.0689478037
ReturnAug   0.01071053  0.0007137558  1.0000000000  0.0007407139
ReturnSep   0.04474727  0.0689478037  0.0007407139  1.0000000000
ReturnOct  -0.02263599 -0.0547089088 -0.0755945614 -0.0580792362
ReturnNov  -0.06527054 -0.0483738369 -0.1164890345 -0.0197197998
             ReturnOct   ReturnNov
ReturnJan   0.14297723  0.06763233
ReturnFeb  -0.08732427 -0.15465828
ReturnMar  -0.01192376  0.03732353
ReturnApr   0.04854003  0.03176184
ReturnMay   0.01716673  0.04804659
ReturnJune -0.02263599 -0.06527054
ReturnJuly -0.05470891 -0.04837384
ReturnAug  -0.07559456 -0.11648903
ReturnSep  -0.05807924 -0.01971980
ReturnOct   1.00000000  0.19167279
ReturnNov   0.19167279  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ReturnOct vs ReturnNov = 0.19167279&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---exploring-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Exploring the Dataset&lt;/h3&gt;
&lt;p&gt;Which month (from January through November) has the largest mean return across all observations in the dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(stocks)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   11580 obs. of  12 variables:
 $ ReturnJan  : num  0.0807 -0.0107 0.0477 -0.074 -0.031 ...
 $ ReturnFeb  : num  0.0663 0.1021 0.036 -0.0482 -0.2127 ...
 $ ReturnMar  : num  0.0329 0.1455 0.0397 0.0182 0.0915 ...
 $ ReturnApr  : num  0.1831 -0.0844 -0.1624 -0.0247 0.1893 ...
 $ ReturnMay  : num  0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ...
 $ ReturnJune : num  -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ...
 $ ReturnJuly : num  -0.0205 -0.0253 -0.1354 -0.094 0.3553 ...
 $ ReturnAug  : num  0.0247 0.2113 0.0334 0.0953 0.0568 ...
 $ ReturnSep  : num  -0.0204 -0.58 0 0.0567 0.0336 ...
 $ ReturnOct  : num  -0.1733 -0.2671 0.0917 -0.0963 0.0363 ...
 $ ReturnNov  : num  -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ...
 $ PositiveDec: int  0 0 0 1 1 1 1 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colMeans(stocks[, 1:11])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   ReturnJan    ReturnFeb    ReturnMar    ReturnApr    ReturnMay 
 0.012631602 -0.007604784  0.019402336  0.026308147  0.024736591 
  ReturnJune   ReturnJuly    ReturnAug    ReturnSep    ReturnOct 
 0.005937902  0.003050863  0.016198265 -0.014720768  0.005650844 
   ReturnNov 
 0.011387440 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ReturnApr = 0.026308147&lt;/p&gt;
&lt;p&gt;Which month (from January through November) has the smallest mean return across all observations in the dataset?
#### ReturnSep = -0.014720768&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---initial-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Initial Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Split the data into a training set and testing set, putting 70% of the data in the training set and 30% of the data in the testing set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
set.seed(144)
spl &amp;lt;- sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain &amp;lt;- subset(stocks, spl == TRUE)
stocksTest &amp;lt;- subset(stocks, spl == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, use the stocksTrain dataframe to train a logistic regression model (name it StocksModel) to predict PositiveDec using all the other variables as independent variables.&lt;/p&gt;
&lt;p&gt;Not forgetting to add the argument family=binomial to our glm code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModel &amp;lt;- glm(PositiveDec ~ ., data = stocksTrain, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall accuracy on the training set, using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModelTrainPred &amp;lt;- predict(StocksModel, type = &amp;quot;response&amp;quot;)
head(StocksModelTrainPred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         4         6         7         8 
0.6333193 0.3804326 0.5432996 0.6485711 0.5991750 0.4372892 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTrain$PositiveDec, StocksModelTrainPred &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   990 2689
  1   787 3640&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(990 + 3640) / nrow(stocksTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5711818&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---initial-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Initial Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Now obtain test-set predictions from StocksModel.&lt;/p&gt;
&lt;p&gt;What is the overall accuracy of the model on the test, again using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModelTestPred &amp;lt;- 
    predict(StocksModel, newdata = stocksTest, type = &amp;quot;response&amp;quot;)
head(StocksModelTestPred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        3         5        15        17        23        26 
0.4506152 0.6470609 0.6089785 0.5708036 0.4758428 0.3631213 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTest$PositiveDec, StocksModelTestPred &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   417 1160
  1   344 1553&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(417 + 1553) / nrow(stocksTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5670697&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---initial-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Initial Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What is the accuracy on the test-set of a baseline model that always predicts the most common outcome (PositiveDec = 1)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTest$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
1577 1897 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1897 / (1577 + 1897)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5460564&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---clustering-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Clustering Stocks&lt;/h3&gt;
&lt;p&gt;Now, let’s cluster the stocks. The first step in this process is to remove the dependent variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limitedTrain &amp;lt;- stocksTrain
limitedTrain$PositiveDec &amp;lt;- NULL
limitedTest &amp;lt;- stocksTest
limitedTest$PositiveDec &amp;lt;- NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do we need to remove the dependent variable in the clustering phase of the cluster-then-predict methodology?
#### Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the methodology&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---clustering-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Clustering Stocks&lt;/h3&gt;
&lt;p&gt;preProcess code from the caret package, which normalizes variables by subtracting by the mean and dividing by the standard deviation.&lt;/p&gt;
&lt;p&gt;In cases where we have a training and testing set, we’ll want to normalize by the mean and standard deviation of the variables in the training set. We can do this by passing just the training set to the preProcess function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preproc &amp;lt;- preProcess(limitedTrain)
normTrain &amp;lt;- predict(preproc, limitedTrain)
normTest &amp;lt;- predict(preproc, limitedTest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the mean of the ReturnJan variable in normTrain?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(normTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   ReturnJan          ReturnFeb          ReturnMar       
 Min.   :-4.57682   Min.   :-3.43004   Min.   :-4.54609  
 1st Qu.:-0.48271   1st Qu.:-0.35589   1st Qu.:-0.40758  
 Median :-0.07055   Median :-0.01875   Median :-0.05778  
 Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  
 3rd Qu.: 0.35898   3rd Qu.: 0.25337   3rd Qu.: 0.36106  
 Max.   :18.06234   Max.   :34.92751   Max.   :24.77296  
   ReturnApr         ReturnMay          ReturnJune      
 Min.   :-5.0227   Min.   :-4.96759   Min.   :-4.82957  
 1st Qu.:-0.4757   1st Qu.:-0.43045   1st Qu.:-0.45602  
 Median :-0.1104   Median :-0.06983   Median :-0.04354  
 Mean   : 0.0000   Mean   : 0.00000   Mean   : 0.00000  
 3rd Qu.: 0.3400   3rd Qu.: 0.35906   3rd Qu.: 0.37273  
 Max.   :14.6959   Max.   :42.69158   Max.   :10.84515  
   ReturnJuly         ReturnAug          ReturnSep       
 Min.   :-5.19139   Min.   :-5.60378   Min.   :-5.47078  
 1st Qu.:-0.51832   1st Qu.:-0.47163   1st Qu.:-0.39604  
 Median :-0.02372   Median :-0.07393   Median : 0.04767  
 Mean   : 0.00000   Mean   : 0.00000   Mean   : 0.00000  
 3rd Qu.: 0.47735   3rd Qu.: 0.39967   3rd Qu.: 0.42287  
 Max.   :17.33975   Max.   :27.14273   Max.   :39.05435  
   ReturnOct          ReturnNov       
 Min.   :-3.53719   Min.   :-4.31684  
 1st Qu.:-0.42176   1st Qu.:-0.43564  
 Median :-0.01891   Median :-0.01878  
 Mean   : 0.00000   Mean   : 0.00000  
 3rd Qu.: 0.37451   3rd Qu.: 0.42560  
 Max.   :31.25996   Max.   :17.18255  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the mean of the ReturnJan variable in normTest?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(normTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   ReturnJan           ReturnFeb           ReturnMar       
 Min.   :-3.743836   Min.   :-3.251044   Min.   :-4.07731  
 1st Qu.:-0.485690   1st Qu.:-0.348951   1st Qu.:-0.40662  
 Median :-0.066856   Median :-0.006860   Median :-0.05674  
 Mean   :-0.000419   Mean   :-0.003862   Mean   : 0.00583  
 3rd Qu.: 0.357729   3rd Qu.: 0.264647   3rd Qu.: 0.35653  
 Max.   : 8.412973   Max.   : 9.552365   Max.   : 9.00982  
   ReturnApr          ReturnMay          ReturnJune      
 Min.   :-4.47865   Min.   :-5.84445   Min.   :-4.73628  
 1st Qu.:-0.51121   1st Qu.:-0.43819   1st Qu.:-0.44968  
 Median :-0.11414   Median :-0.05346   Median :-0.02678  
 Mean   :-0.03638   Mean   : 0.02651   Mean   : 0.04315  
 3rd Qu.: 0.32742   3rd Qu.: 0.42290   3rd Qu.: 0.43010  
 Max.   : 6.84589   Max.   : 7.21362   Max.   :29.00534  
   ReturnJuly          ReturnAug          ReturnSep       
 Min.   :-5.201454   Min.   :-4.62097   Min.   :-3.57222  
 1st Qu.:-0.512039   1st Qu.:-0.51546   1st Qu.:-0.38067  
 Median :-0.026576   Median :-0.10277   Median : 0.08215  
 Mean   : 0.006016   Mean   :-0.04973   Mean   : 0.02939  
 3rd Qu.: 0.457193   3rd Qu.: 0.38781   3rd Qu.: 0.45847  
 Max.   :12.790901   Max.   : 6.66889   Max.   : 7.09106  
   ReturnOct           ReturnNov        
 Min.   :-3.807577   Min.   :-4.881463  
 1st Qu.:-0.393856   1st Qu.:-0.396764  
 Median : 0.006783   Median :-0.002337  
 Mean   : 0.029672   Mean   : 0.017128  
 3rd Qu.: 0.419005   3rd Qu.: 0.424617  
 Max.   : 7.428466   Max.   :21.007786  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---clustering-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Clustering Stocks&lt;/h3&gt;
&lt;p&gt;Why is the mean ReturnJan variable much closer to 0 in normTrain than in normTest?
#### The distribution of the ReturnJan variable is different in the training and testing set&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---clustering-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Clustering Stocks&lt;/h3&gt;
&lt;p&gt;Set the random seed to 144 (it is important to do this again, even though we did it earlier). Run k-means clustering with 3 clusters on normTrain, storing the result in an object called km.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
km &amp;lt;- kmeans(normTrain, centers = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which cluster has the largest number of observations?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(km$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   1    2    3 
3157 4696  253 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cluster 2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---clustering-stocks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Clustering Stocks&lt;/h3&gt;
&lt;p&gt;Recall from the recitation that we can use the flexclust package to obtain training set and testing set cluster assignments for our observations (note that the call to as.kcca may take a while to complete):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(flexclust)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: modeltools&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: stats4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.kcca &amp;lt;- as.kcca(km, normTrain)
clusterTrain = predict(km.kcca)
clusterTest = predict(km.kcca, newdata=normTest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many test-set observations were assigned to Cluster 2?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(clusterTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;clusterTest
   1    2    3 
1298 2080   96 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2080&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---cluster-specific-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Cluster-Specific Predictions&lt;/h3&gt;
&lt;p&gt;Using the subset function, build dataframes stocksTrain1, stocksTrain2, and stocksTrain3, containing the elements in the stocksTrain dataframe assigned to clusters 1, 2, and 3, respectively (be careful to take subsets of stocksTrain, not of normTrain). Similarly build stocksTest1, stocksTest2, and stocksTest3 from the stocksTest dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stocksTrain1 &amp;lt;- subset(stocksTrain, clusterTrain == 1)
stocksTrain2 &amp;lt;- subset(stocksTrain, clusterTrain == 2)
stocksTrain3 &amp;lt;- subset(stocksTrain, clusterTrain == 3)
stocksTest1 &amp;lt;- subset(stocksTest, clusterTest == 1)
stocksTest2 &amp;lt;- subset(stocksTest, clusterTest == 2)
stocksTest3 &amp;lt;- subset(stocksTest, clusterTest == 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which training set dataframe has the highest average value of the dependent variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(stocksTrain1$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6024707&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(stocksTrain2$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5140545&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(stocksTrain3$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4387352&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;stocksTrain1&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---cluster-specific-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Cluster-Specific Predictions&lt;/h3&gt;
&lt;p&gt;Build logistic regression models StocksModel1, StocksModel2, and StocksModel3, which predict PositiveDec using all the other variables as independent variables. StocksModel1 should be trained on stocksTrain1, StocksModel2 should be trained on stocksTrain2, and StocksModel3 should be trained on stocksTrain3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModel1 &amp;lt;- glm(PositiveDec ~ ., data = stocksTrain1, family = binomial)
StocksModel2 &amp;lt;- glm(PositiveDec ~ ., data = stocksTrain2, family = binomial)
StocksModel3 &amp;lt;- glm(PositiveDec ~ ., data = stocksTrain3, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which variables have a positive sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3 and a negative sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3? Select all that apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModel1$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Intercept)   ReturnJan   ReturnFeb   ReturnMar   ReturnApr   ReturnMay 
 0.17223985  0.02498357 -0.37207369  0.59554957  1.19047752  0.30420906 
 ReturnJune  ReturnJuly   ReturnAug   ReturnSep   ReturnOct   ReturnNov 
-0.01165375  0.19769226  0.51272941  0.58832685 -1.02253506 -0.74847186 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModel2$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(Intercept)   ReturnJan   ReturnFeb   ReturnMar   ReturnApr   ReturnMay 
  0.1029318   0.8845148   0.3176221  -0.3797811   0.4929105   0.8965492 
 ReturnJune  ReturnJuly   ReturnAug   ReturnSep   ReturnOct   ReturnNov 
  1.5008787   0.7831487  -0.2448602   0.7368522  -0.2775631  -0.7874737 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;StocksModel3$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; (Intercept)    ReturnJan    ReturnFeb    ReturnMar    ReturnApr 
-0.181895809 -0.009789345 -0.046883260  0.674179495  1.281466189 
   ReturnMay   ReturnJune   ReturnJuly    ReturnAug    ReturnSep 
 0.762511555  0.329433917  0.774164370  0.982605385  0.363806823 
   ReturnOct    ReturnNov 
 0.782242086 -0.873752144 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind(StocksModel1$coefficients &amp;gt; 0,
      StocksModel2$coefficients &amp;gt; 0,
      StocksModel3$coefficients &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay
[1,]        TRUE      TRUE     FALSE      TRUE      TRUE      TRUE
[2,]        TRUE      TRUE      TRUE     FALSE      TRUE      TRUE
[3,]       FALSE     FALSE     FALSE      TRUE      TRUE      TRUE
     ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov
[1,]      FALSE       TRUE      TRUE      TRUE     FALSE     FALSE
[2,]       TRUE       TRUE     FALSE      TRUE     FALSE     FALSE
[3,]       TRUE       TRUE      TRUE      TRUE      TRUE     FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ReturnJan, ReturnFeb, ReturnMar, ReturnJune, ReturnAug, ReturnOct&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---cluster-specific-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Cluster-Specific Predictions&lt;/h3&gt;
&lt;p&gt;Using StocksModel1, make test-set predictions called PredictTest1 on the dataframe stocksTest1. Using StocksModel2, make test-set predictions called PredictTest2 on the dataframe stocksTest2. Using StocksModel3, make test-set predictions called PredictTest3 on the dataframe stocksTest3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredictTest1 &amp;lt;- predict(StocksModel1, newdata = stocksTest1, type = &amp;quot;response&amp;quot;)
PredictTest2 &amp;lt;- predict(StocksModel2, newdata = stocksTest2, type = &amp;quot;response&amp;quot;)
PredictTest3 &amp;lt;- predict(StocksModel3, newdata = stocksTest3, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall accuracy of StocksModel1 on the test-set stocksTest1, using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTest1$PositiveDec, PredictTest1 &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    30  471
  1    23  774&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(30 + 774) / nrow(stocksTest1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6194145&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall accuracy of StocksModel2 on the test-set stocksTest2, using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTest2$PositiveDec, PredictTest2 &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   388  626
  1   309  757&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(388 + 757) / nrow(stocksTest2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5504808&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall accuracy of StocksModel3 on the test-set stocksTest3, using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(stocksTest3$PositiveDec, PredictTest3 &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0    49   13
  1    21   13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(49 + 13) / nrow(stocksTest3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6458333&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---cluster-specific-predictions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Cluster-Specific Predictions&lt;/h3&gt;
&lt;p&gt;To compute the overall test-set accuracy of the cluster-then-predict approach, we can combine all the test-set predictions into a single vector and all the true outcomes into a single vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AllPredictions &amp;lt;- c(PredictTest1, PredictTest2, PredictTest3)
AllOutcomes &amp;lt;- c(stocksTest1$PositiveDec, 
                 stocksTest2$PositiveDec, 
                 stocksTest3$PositiveDec)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the overall test-set accuracy of the cluster-then-predict approach, again using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(AllOutcomes, AllPredictions &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           
AllOutcomes FALSE TRUE
          0   467 1110
          1   353 1544&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(AllOutcomes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3474&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(467 + 1544) / length(AllOutcomes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5788716&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We see a modest improvement over the original logistic regression model. Since predicting stock returns is a notoriously hard problem, this is a good increase in accuracy. By investing in stocks for which we are more confident that they will have positive returns (by selecting the ones with higher predicted probabilities), this cluster-then-predict model can give us an edge over the original logistic regression model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automating Reviews In Medicine</title>
      <link>/project/medicine/medicine/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/medicine/medicine/</guid>
      <description>


&lt;p&gt;The medical literature is enormous! &lt;strong&gt;Pubmed,&lt;/strong&gt; a database of medical publications maintained by the U.S. National Library of Medicine, has indexed over 23 million medical publications. Further, the rate of medical publication has increased over time, and now there are nearly 1 million new publications in the field each year, or more than one per minute.&lt;/p&gt;
&lt;p&gt;The large size and fast-changing nature of the medical literature has increased the need for reviews, which search databases like &lt;strong&gt;Pubmed&lt;/strong&gt; for papers on a particular topic and then report results from the papers found. While such reviews are often performed manually, with multiple people reviewing each search result, this is tedious and time consuming. In this analysis, I’ll see how &lt;strong&gt;text analytics can be used to automate the process of information retrieval.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The dataset consists of the titles (variable title) and abstracts (variable abstract) of papers retrieved in a &lt;strong&gt;Pubmed&lt;/strong&gt; search. Each search result is labeled with whether the paper is a clinical trial testing a drug therapy for cancer (variable trial). These labels were obtained by two people reviewing each search result and accessing the actual paper if necessary, as part of a literature review of clinical trials testing drug therapies for advanced and metastatic breast cancer.&lt;/p&gt;
&lt;div id=&#34;loading-the-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the packages&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Loading the Data&lt;/h3&gt;
&lt;p&gt;Load clinical_trial.csv into a dataframe called trials (remembering to add the argument stringsAsFactors=FALSE when working with text analytics, so that the text is read in properply), and investigate the dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trials &amp;lt;- read.csv(&amp;quot;clinical_trial.csv&amp;quot;, stringsAsFactors = FALSE)
str(trials)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  3 variables:
 $ title   : chr  &amp;quot;Treatment of Hodgkin&amp;#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).&amp;quot; &amp;quot;Cell mediated immune status in malignancy--pretherapy and post-therapy assessment.&amp;quot; &amp;quot;Neoadjuvant vinorelbine-capecitabine versus docetaxel-doxorubicin-cyclophosphamide in early nonresponsive breas&amp;quot;| __truncated__ &amp;quot;Randomized phase 3 trial of fluorouracil, epirubicin, and cyclophosphamide alone or followed by Paclitaxel for &amp;quot;| __truncated__ ...
 $ abstract: chr  &amp;quot;&amp;quot; &amp;quot;Twenty-eight cases of malignancies of different kinds were studied to assess T-cell activity and population bef&amp;quot;| __truncated__ &amp;quot;BACKGROUND: Among breast cancer patients, nonresponse to initial neoadjuvant chemotherapy is associated with un&amp;quot;| __truncated__ &amp;quot;BACKGROUND: Taxanes are among the most active drugs for the treatment of metastatic breast cancer, and, as a co&amp;quot;| __truncated__ ...
 $ trial   : int  1 0 1 1 1 0 1 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(trials)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    title             abstract             trial       
 Length:1860        Length:1860        Min.   :0.0000  
 Class :character   Class :character   1st Qu.:0.0000  
 Mode  :character   Mode  :character   Median :0.0000  
                                       Mean   :0.4392  
                                       3rd Qu.:1.0000  
                                       Max.   :1.0000  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;IMPORTANT NOTE: Should you get an error like “invalid multibyte string” when performing certain parts of this analysis, use the argument fileEncoding=“latin1” when reading in the file with read.csv. This should cause those errors to go away. We can use R’s string functions to learn more about the titles and abstracts of the located papers. The nchar() function counts the number of characters in a piece of text.&lt;/p&gt;
&lt;p&gt;Using the nchar() function on the variables in the dataframe. How many characters are there in the longest abstract? (Longest here is defined as the abstract with the largest number of characters.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(nchar(trials$abstract))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3708&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which.max(nchar(trials$abstract))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 664&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trials[664, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                title
664 Five versus more than five years of tamoxifen therapy for breast cancer patients with negative lymph nodes and estrogen receptor-positive tumors.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        abstract
664 BACKGROUND: In 1982, the National Surgical Adjuvant Breast and Bowel Project initiated a randomized, double-blinded, placebo-controlled trial (B-14) to determine the effectiveness of adjuvant tamoxifen therapy in patients with primary operable breast cancer who had estrogen receptor-positive tumors and no axillary lymph node involvement. The findings indicated that tamoxifen therapy provided substantial benefit to patients with early stage disease. However, questions arose about how long the observed benefit would persist, about the duration of therapy necessary to maintain maximum benefit, and about the nature and severity of adverse effects from prolonged treatment.PURPOSE: We evaluated the outcome of patients in the B-14 trial through 10 years of follow-up. In addition, the effects of 5 years versus more than 5 years of tamoxifen therapy were compared.METHODS: In the trial, patients were initially assigned to receive either tamoxifen at 20 mg/day (n = 1404) or placebo (n = 1414). Tamoxifen-treated patients who remained disease free after 5 years of therapy were then reassigned to receive either another 5 years of tamoxifen (n = 322) or 5 years of placebo (n = 321). After the study began, another group of patients who met the same protocol eligibility requirements as the randomly assigned patients were registered to receive tamoxifen (n = 1211). Registered patients who were disease free after 5 years of treatment were also randomly assigned to another 5 years of tamoxifen (n = 261) or to 5 years of placebo (n = 249). To compare 5 years with more than 5 years of tamoxifen therapy, data relating to all patients reassigned to an additional 5 years of the drug were combined. Patients who were not reassigned to either tamoxifen or placebo continued to be followed in the study. Survival, disease-free survival, and distant disease-free survival (relating to failure at distant sites) were estimated by use of the Kaplan-Meier method; differences between the treatment groups were assessed by use of the logrank test. The relative risks of failure (with 95% confidence intervals [CIs]) were determined by use of the Cox proportional hazards model. Reported P values are two-sided.RESULTS: Through 10 years of follow-up, a significant advantage in disease-free survival (69% versus 57%, P &amp;lt; .0001; relative risk = 0.66; 95% CI = 0.58-0.74), distant disease-free survival (76% versus 67%, P &amp;lt; .0001; relative risk = 0.70; 95% CI = 0.61-0.81), and survival (80% versus 76%, P = .02; relative risk = 0.84; 95% CI = 0.71-0.99) was found for patients in the group first assigned to receive tamoxifen. The survival benefit extended to those 49 years of age or younger and to those 50 years of age or older. Tamoxifen therapy was associated with a 37% reduction in the incidence of contralateral (opposite) breast cancer (P = .007). Through 4 years after the reassignment of tamoxifen-treated patients to either continued-therapy or placebo groups, advantages in disease-free survival (92% versus 86%, P = .003) and distant disease-free survival (96% versus 90%, P = .01) were found for those who discontinued tamoxifen treatment. Survival was 96% for those who discontinued tamoxifen compared with 94% for those who continued tamoxifen treatment (P = .08). A higher incidence of thromboembolic events was seen in tamoxifen-treated patients (through 5 years, 1.7% versus 0.4%). Except for endometrial cancer, the incidence of second cancers was not increased with tamoxifen therapy.CONCLUSIONS AND IMPLICATIONS: The benefit from 5 years of tamoxifen therapy persists through 10 years of follow-up. No additional advantage is obtained from continuing tamoxifen therapy for more than 5 years.
    trial
664     1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nchar(trials[664, ]$abstract)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3708&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3708&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Loading the Data&lt;/h3&gt;
&lt;p&gt;How many search results provided no abstract? (HINT: A search result provided no abstract if the number of characters in the abstract field is zero.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(nchar(trials$abstract) == 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 112&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Loading the Data&lt;/h3&gt;
&lt;p&gt;Find the observation with the minimum number of characters in the title (the variable “title”) out of all of the observations in this dataset. What is the text of the title of this article?&lt;/p&gt;
&lt;p&gt;Include capitalization and punctuation in our response, but don’t include the quotes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(nchar(trials$title))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 28&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which.min(nchar(trials$title))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trials[1258,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                            title
1258 A decade of letrozole: FACE.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstract
1258 Third-generation nonsteroidal aromatase inhibitors (AIs), letrozole and anastrozole, are superior to tamoxifen as initial therapy for early breast cancer but have not been directly compared in a head-to-head adjuvant trial. Cumulative evidence suggests that AIs are not equivalent in terms of potency of estrogen suppression and that there may be differences in clinical efficacy. Thus, with no data from head-to-head comparisons of the AIs as adjuvant therapy yet available, the question of whether there are efficacy differences between the AIs remains. To help answer this question, the Femara versus Anastrozole Clinical Evaluation (FACE) is a phase IIIb open-label, randomized, multicenter trial designed to test whether letrozole or anastrozole has superior efficacy as adjuvant treatment of postmenopausal women with hormone receptor (HR)- and lymph node-positive breast cancer. Eligible patients (target accrual, N=4,000) are randomized to receive either letrozole 2.5 mg or anastrozole 1 mg daily for up to 5 years. The primary objective is to compare disease-free survival at 5 years. Secondary end points include safety, overall survival, time to distant metastases, and time to contralateral breast cancer. The FACE trial will determine whether or not letrozole offers a greater clinical benefit to postmenopausal women with HR+ early breast cancer at increased risk of early recurrence compared with anastrozole.
     trial
1258     0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trials[1258,]$title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;A decade of letrozole: FACE.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---preparing-the-corpus&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Preparing the Corpus&lt;/h3&gt;
&lt;p&gt;Because we have both title and abstract information for trials, we need to build two corpera instead of one. Naming them corpusTitle and corpusAbstract.&lt;/p&gt;
&lt;p&gt;The code performs the following tasks (you might need to load the “tm” package first if it isn’t already loaded). Making sure to perform them in this order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1) Convert the title variable to corpusTitle and the abstract variable to corpusAbstract
corpusTitle &amp;lt;- Corpus(VectorSource(trials$title))
corpusTitle[[1]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;Treatment of Hodgkin&amp;#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract &amp;lt;- Corpus(VectorSource(trials$abstract))
corpusAbstract[[1]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2) Convert corpusTitle and corpusAbstract to lowercase
corpusTitle = tm_map(corpusTitle, content_transformer(tolower))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusTitle, content_transformer(tolower)):
transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAbstract,
content_transformer(tolower)): transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#corpusTitle = tm_map(corpusTitle, PlainTextDocument)
#corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3) Remove the punctuation in corpusTitle and corpusAbstract
corpusTitle = tm_map(corpusTitle, removePunctuation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusTitle, removePunctuation):
transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusTitle[[2]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;cell mediated immune status in malignancypretherapy and posttherapy assessment&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract = tm_map(corpusAbstract, removePunctuation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAbstract, removePunctuation):
transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4) Remove the English language stop words from corpusTitle and corpusAbstract
corpusTitle &amp;lt;- tm_map(corpusTitle, removeWords, stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusTitle, removeWords,
stopwords(&amp;quot;english&amp;quot;)): transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusTitle[[2]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;cell mediated immune status  malignancypretherapy  posttherapy assessment&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract &amp;lt;- tm_map(corpusAbstract, removeWords, stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAbstract, removeWords,
stopwords(&amp;quot;english&amp;quot;)): transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract[[2]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;twentyeight cases  malignancies  different kinds  studied  assess tcell activity  population    institution  therapy fifteen cases  diagnosed  nonmetastasising squamous cell carcinoma  larynx pharynx laryngopharynx hypopharynx  tonsils seven cases  nonmetastasising infiltrating duct carcinoma  breast  6 cases  nonhodgkins lymphoma nhl   observed  3   15 cases 20  squamous cell carcinoma cases  mantoux test mt negative   tcell population  less  40 2   7 cases 286  infiltrating duct carcinoma  breast  mt negative   tcell population  less  40  3   6 cases 50  nhl  mt negative   tcell population  less  40  normal controls consisting  apparently normal healthy adults   tcell population    40    mt positive  patients  showed  negative skin test   tcell population less  40   subjected  assessment  tcell population  activity  appropriate therapy  clinical cure   disease   observed  2   3 cases 6666  squamous cell carcinomas 2   2 cases 100  adenocarcinomas  one   3 cases 3333  nhl showed positive conversion   tcell population    40&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5) Stem the words in corpusTitle and corpusAbstract (each stemming might take a few minutes)
corpusTitle = tm_map(corpusTitle, stemDocument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusTitle, stemDocument): transformation
drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusTitle[[2]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;cell mediat immun status malignancypretherapi posttherapi assess&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract = tm_map(corpusAbstract, stemDocument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAbstract, stemDocument):
transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAbstract[[2]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;twentyeight case malign differ kind studi assess tcell activ popul institut therapi fifteen case diagnos nonmetastasis squamous cell carcinoma larynx pharynx laryngopharynx hypopharynx tonsil seven case nonmetastasis infiltr duct carcinoma breast 6 case nonhodgkin lymphoma nhl observ 3 15 case 20 squamous cell carcinoma case mantoux test mt negat tcell popul less 40 2 7 case 286 infiltr duct carcinoma breast mt negat tcell popul less 40 3 6 case 50 nhl mt negat tcell popul less 40 normal control consist appar normal healthi adult tcell popul 40 mt posit patient show negat skin test tcell popul less 40 subject assess tcell popul activ appropri therapi clinic cure diseas observ 2 3 case 6666 squamous cell carcinoma 2 2 case 100 adenocarcinoma one 3 case 3333 nhl show posit convers tcell popul 40&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 6) Build a document term matrix called dtmTitle from corpusTitle and dtmAbstract from corpusAbstract
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 1860, terms: 2836)&amp;gt;&amp;gt;
Non-/sparse entries: 23416/5251544
Sparsity           : 100%
Maximal term length: 49
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmAbstract&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 1860, terms: 12451)&amp;gt;&amp;gt;
Non-/sparse entries: 153290/23005570
Sparsity           : 99%
Maximal term length: 67
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 7) Limit dtmTitle and dtmAbstract to terms with sparseness of at most 95% (aka terms that appear in at least 5% of documents)
dtmTitle &amp;lt;- removeSparseTerms(dtmTitle, 0.95)
dtmTitle&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 1860, terms: 31)&amp;gt;&amp;gt;
Non-/sparse entries: 10683/46977
Sparsity           : 81%
Maximal term length: 15
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtmAbstract &amp;lt;- removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 1860, terms: 335)&amp;gt;&amp;gt;
Non-/sparse entries: 91969/531131
Sparsity           : 85%
Maximal term length: 15
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 8) Convert dtmTitle and dtmAbstract to data frames (keep the names dtmTitle and dtmAbstract)
dtmTitle &amp;lt;- as.data.frame(as.matrix(dtmTitle))
dtmAbstract &amp;lt;- as.data.frame(as.matrix(dtmAbstract))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When removing stop words, use tm_map(corpusTitle, removeWords, sw) and tm_map(corpusAbstract, removeWords, sw) instead of tm_map(corpusTitle, removeWords, stopwords(“english”)) and tm_map(corpusAbstract, removeWords, stopwords(“english”)).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many terms remain in dtmTitle after removing sparse terms (aka how many columns does it have)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(dtmTitle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  31 variables:
 $ cancer         : num  1 0 1 1 1 1 0 1 1 2 ...
 $ treatment      : num  1 0 0 0 1 0 0 0 0 1 ...
 $ breast         : num  0 0 1 1 1 1 0 1 1 1 ...
 $ earli          : num  0 0 1 1 0 0 0 1 0 0 ...
 $ iii            : num  0 0 1 0 0 0 0 0 0 1 ...
 $ phase          : num  0 0 1 1 0 0 0 0 0 1 ...
 $ random         : num  0 0 1 1 1 0 0 0 0 1 ...
 $ trial          : num  0 0 1 1 1 0 0 1 1 1 ...
 $ versus         : num  0 0 1 0 0 0 0 1 0 0 ...
 $ cyclophosphamid: num  0 0 0 1 0 0 0 0 0 0 ...
 $ chemotherapi   : num  0 0 0 0 1 1 0 0 0 0 ...
 $ combin         : num  0 0 0 0 1 0 1 0 0 0 ...
 $ effect         : num  0 0 0 0 1 0 0 1 0 1 ...
 $ metastat       : num  0 0 0 0 1 0 0 0 0 0 ...
 $ patient        : num  0 0 0 0 1 0 1 0 1 1 ...
 $ respons        : num  0 0 0 0 0 1 0 0 0 0 ...
 $ advanc         : num  0 0 0 0 0 0 1 0 0 0 ...
 $ postmenopaus   : num  0 0 0 0 0 0 0 1 1 0 ...
 $ randomis       : num  0 0 0 0 0 0 0 1 1 0 ...
 $ studi          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ tamoxifen      : num  0 0 0 0 0 0 0 2 1 0 ...
 $ women          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ adjuv          : num  0 0 0 0 0 0 0 0 1 0 ...
 $ group          : num  0 0 0 0 0 0 0 0 1 1 ...
 $ therapi        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ compar         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ doxorubicin    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ docetaxel      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ result         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ plus           : num  0 0 0 0 0 0 0 0 0 0 ...
 $ clinic         : num  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;31&lt;/h4&gt;
&lt;p&gt;How many terms remain in dtmAbstract?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(dtmAbstract)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  335 variables:
 $ 100            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ activ          : num  0 2 0 1 0 0 1 0 0 0 ...
 $ assess         : num  0 2 1 2 0 1 0 0 0 3 ...
 $ breast         : num  0 2 3 3 3 4 2 2 2 3 ...
 $ carcinoma      : num  0 5 0 0 0 0 0 0 0 2 ...
 $ case           : num  0 11 0 0 1 0 0 0 0 0 ...
 $ cell           : num  0 3 0 0 0 1 0 0 0 0 ...
 $ clinic         : num  0 1 0 1 0 0 0 0 0 0 ...
 $ consist        : num  0 1 0 0 0 0 0 0 0 0 ...
 $ control        : num  0 1 0 0 0 0 0 0 1 0 ...
 $ differ         : num  0 1 2 1 3 0 0 1 0 1 ...
 $ diseas         : num  0 1 0 1 3 0 0 1 0 0 ...
 $ less           : num  0 4 1 0 0 0 0 0 0 6 ...
 $ negat          : num  0 4 0 0 0 3 0 0 0 0 ...
 $ observ         : num  0 2 1 0 1 0 0 0 0 0 ...
 $ one            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ patient        : num  0 1 9 5 5 6 8 3 2 5 ...
 $ popul          : num  0 8 0 0 0 0 0 0 0 0 ...
 $ posit          : num  0 2 0 1 0 5 0 0 0 0 ...
 $ seven          : num  0 1 0 0 0 0 0 0 0 0 ...
 $ show           : num  0 2 0 0 1 0 1 0 0 3 ...
 $ studi          : num  0 1 1 1 0 1 3 2 0 1 ...
 $ test           : num  0 2 1 1 0 0 0 0 0 0 ...
 $ therapi        : num  0 2 0 1 0 0 0 0 0 0 ...
 $ 500            : num  0 0 1 0 2 0 0 0 0 0 ...
 $ addit          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ among          : num  0 0 2 4 0 0 0 0 1 0 ...
 $ arm            : num  0 0 7 4 2 0 0 1 0 1 ...
 $ assign         : num  0 0 2 1 0 0 0 1 1 1 ...
 $ associ         : num  0 0 1 3 0 2 0 1 2 0 ...
 $ background     : num  0 0 1 1 1 0 0 1 1 0 ...
 $ better         : num  0 0 1 0 1 1 0 0 0 0 ...
 $ cancer         : num  0 0 2 3 3 3 2 2 3 0 ...
 $ chang          : num  0 0 1 0 0 0 0 0 0 0 ...
 $ chemotherapi   : num  0 0 1 2 3 5 2 0 1 0 ...
 $ compar         : num  0 0 1 2 0 0 0 1 1 1 ...
 $ complet        : num  0 0 3 0 1 1 1 2 0 0 ...
 $ conclus        : num  0 0 1 0 1 0 0 0 0 0 ...
 $ confid         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ continu        : num  0 0 2 0 1 0 0 2 0 0 ...
 $ cycl           : num  0 0 6 0 2 0 1 0 0 0 ...
 $ cyclophosphamid: num  0 0 1 1 1 1 3 0 0 0 ...
 $ day            : num  0 0 1 0 0 0 3 0 0 0 ...
 $ decreas        : num  0 0 1 0 0 0 0 0 1 0 ...
 $ defin          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ demonstr       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ docetaxel      : num  0 0 1 0 0 0 3 0 0 0 ...
 $ doxorubicin    : num  0 0 1 0 0 1 0 0 0 0 ...
 $ effect         : num  0 0 2 0 0 1 0 2 0 1 ...
 $ efficaci       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ enrol          : num  0 0 1 0 0 0 1 0 0 1 ...
 $ four           : num  0 0 4 0 0 0 0 0 0 0 ...
 $ hematolog      : num  0 0 1 0 0 0 0 0 0 0 ...
 $ initi          : num  0 0 3 0 0 0 0 1 0 0 ...
 $ interv         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ least          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ lymph          : num  0 0 1 4 0 3 0 0 1 0 ...
 $ method         : num  0 0 1 0 1 0 0 1 1 0 ...
 $ mgm2           : num  0 0 5 0 4 0 9 0 0 0 ...
 $ neoadjuv       : num  0 0 2 0 0 0 0 0 0 0 ...
 $ node           : num  0 0 1 3 0 0 0 0 1 0 ...
 $ number         : num  0 0 1 1 0 2 1 0 0 0 ...
 $ outcom         : num  0 0 2 0 0 0 0 0 0 2 ...
 $ patholog       : num  0 0 3 0 0 1 0 0 0 0 ...
 $ per            : num  0 0 1 0 0 0 0 0 0 0 ...
 $ previous       : num  0 0 1 0 1 0 2 0 0 0 ...
 $ random         : num  0 0 2 1 1 0 0 1 1 2 ...
 $ rate           : num  0 0 1 1 2 0 1 0 1 0 ...
 $ receiv         : num  0 0 2 0 1 1 3 0 0 2 ...
 $ reduct         : num  0 0 1 2 0 0 0 1 0 0 ...
 $ regimen        : num  0 0 2 0 0 0 1 0 0 0 ...
 $ respond        : num  0 0 2 0 0 0 0 0 0 0 ...
 $ respons        : num  0 0 7 0 4 2 2 0 0 0 ...
 $ result         : num  0 0 1 0 1 0 1 0 0 0 ...
 $ similar        : num  0 0 2 0 0 0 0 0 1 0 ...
 $ size           : num  0 0 1 3 0 1 0 0 0 0 ...
 $ statist        : num  0 0 1 3 0 0 0 0 0 0 ...
 $ surgeri        : num  0 0 1 1 0 0 0 0 0 0 ...
 $ toler          : num  0 0 1 0 0 0 1 0 0 0 ...
 $ toxic          : num  0 0 2 0 1 0 1 0 0 5 ...
 $ treatment      : num  0 0 3 6 14 0 0 4 1 1 ...
 $ tumor          : num  0 0 2 4 0 0 2 0 0 0 ...
 $ two            : num  0 0 3 0 1 0 0 1 0 0 ...
 $ 001            : num  0 0 0 1 0 0 0 1 0 0 ...
 $ adjuv          : num  0 0 0 2 0 1 0 2 4 0 ...
 $ age            : num  0 0 0 1 0 0 0 0 0 0 ...
 $ also           : num  0 0 0 1 0 0 0 0 2 0 ...
 $ analysi        : num  0 0 0 2 0 0 0 1 0 0 ...
 $ analyz         : num  0 0 0 1 0 0 0 0 0 0 ...
 $ axillari       : num  0 0 0 1 0 0 0 0 1 0 ...
 $ death          : num  0 0 0 2 0 0 0 0 1 0 ...
 $ dfs            : num  0 0 0 3 0 0 0 0 0 0 ...
 $ diseasefre     : num  0 0 0 1 0 2 0 0 3 0 ...
 $ drug           : num  0 0 0 1 0 0 0 0 0 0 ...
 $ elig           : num  0 0 0 1 0 0 0 0 0 0 ...
 $ endpoint       : num  0 0 0 2 0 0 0 0 1 3 ...
 $ epirubicin     : num  0 0 0 1 1 0 0 0 0 0 ...
 $ estim          : num  0 0 0 1 1 0 0 0 0 0 ...
 $ fluorouracil   : num  0 0 0 1 1 0 0 0 0 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;335&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---preparing-the-corpus&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Preparing the Corpus&lt;/h3&gt;
&lt;p&gt;What is the most likely reason why dtmAbstract has so many more terms than dtmTitle?
#### Abstracts tend to have many more words than titles&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---preparing-the-corpus&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Preparing the Corpus&lt;/h3&gt;
&lt;p&gt;What is the most frequent word stem across all the abstracts? Hint: you can use colSums() to compute the frequency of a word across all the abstracts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?colSums
colSums(dtmAbstract)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;            100           activ          assess          breast 
            225             509             668            3859 
      carcinoma            case            cell          clinic 
            251             233             359             944 
        consist         control          differ          diseas 
            200             621            1176             950 
           less           negat          observ             one 
            351             258             700             570 
        patient           popul           posit           seven 
           8381             162             511             108 
           show           studi            test         therapi 
            516            1965             282            1564 
            500           addit           among             arm 
            169             420             365            1038 
         assign          associ      background          better 
            435             604             397             186 
         cancer           chang    chemotherapi          compar 
           3726             431            2344            1359 
        complet         conclus          confid         continu 
            628             842             241             281 
           cycl cyclophosphamid             day         decreas 
            962             632            1245             350 
          defin        demonstr       docetaxel     doxorubicin 
            123             251             514             486 
         effect        efficaci           enrol            four 
           1340             591             221             369 
      hematolog           initi          interv           least 
            117             275             349             177 
          lymph          method            mgm2        neoadjuv 
            249             892            1093             293 
           node          number          outcom        patholog 
            477             296             335             254 
            per        previous          random            rate 
            218             355            1520            1253 
         receiv          reduct         regimen         respond 
           1908             301             807             200 
        respons          result         similar            size 
           2051            1485             438             177 
        statist         surgeri           toler           toxic 
            384             407             373            1065 
      treatment           tumor             two             001 
           2894            1122             889             162 
          adjuv             age            also         analysi 
           1162             429             364             587 
         analyz        axillari           death             dfs 
            124             292             215             310 
     diseasefre            drug            elig        endpoint 
            364             332             196             213 
     epirubicin           estim    fluorouracil          follow 
            339             139             215             675 
          found          hazard            her2          hormon 
            238             301             314             428 
         includ          involv          marker        metastat 
            529             180             189             755 
          model       multivari       nodeposit            oper 
            180             154             199             193 
         overal      paclitaxel         predict         primari 
            962             397             369             718 
       prognost         proport           ratio        receptor 
            242             125             344             573 
          reduc          relaps         respect            risk 
            400             254             758             635 
          sampl       secondari        signific          status 
            172             158            2043             538 
         surviv            type            valu            week 
           1927             126             256            1074 
          women            year           agent         benefit 
           1484            1335             240             551 
         combin          detect        determin           durat 
            926             148             352             344 
         either           evalu           everi            evid 
            532             926             487             150 
       firstlin           howev            life             may 
            182             339             178             413 
         measur          object         partial         perform 
            411             400             295             342 
           plus         potenti        progress         prolong 
            622             156             622             125 
        qualiti           score           singl           stabl 
            189             254             149             154 
       subgroup            term            time           total 
            192             153             881             397 
            use           vomit         whether            0001 
           1053             174             235             249 
  5fluorouracil             aim          correl         express 
            208             185             203             356 
         factor        followup           grade           group 
            552             494             580            2668 
           high        independ            larg           level 
            378             149             108             743 
         longer             low          median           month 
            193             196            1180            1575 
    premenopaus     progesteron        randomis          remain 
            303             114             264             158 
          trend          tumour          wherea          achiev 
            115             320             173             245 
       administ          advanc           daili            dose 
            322             556             412            1123 
          indic           infus        intraven     neutropenia 
            269             237             192             234 
          phase           prior         support           treat 
            481             305             183             893 
          trial        aromatas            caus             due 
           1417             171             115             154 
          earli             end        estrogen         increas 
            325             221             421             729 
      inhibitor           lower        particip           point 
            182             236             144             202 
   postmenopaus   receptorposit       tamoxifen          versus 
            590             152            1632             570 
         within            alon             can         distant 
            172             472             191             149 
           find           first          higher             iii 
            177             421             415             266 
         improv           limit      mastectomi        metastas 
            562             127             165             352 
          occur          postop    radiotherapi          recurr 
            312             177             244             465 
           site           stage          system          advers 
            183             286             193             256 
        baselin          common     doubleblind           event 
            340             191             149             409 
        greater            mean         placebo          purpos 
            279             310             475             434 
         obtain            oral         present           relat 
            147             422             218             351 
        suggest            bone       administr           cours 
            274             514             218             283 
          given          safeti         schedul            seen 
            374             265             215             199 
       standard         without        although             new 
            305             306             191             171 
         accord    anthracyclin            base           eight 
            182             207             124             124 
       histolog        investig           local             set 
            127             295             300             191 
         analys           sever           three          import 
            177             288             564             138 
          shown             005           avail            data 
            117             124             108             405 
          incid          period          profil          report 
            300             170             158             357 
       endocrin           hundr       multicent          design 
            266             195             126             219 
         growth           human        pretreat            well 
            208             144             175             328 
         examin             six          appear        identifi 
            190             261             164             148 
         nausea          provid         regress            five 
            239             155             120             173 
        conduct        prospect         develop        sequenti 
            177             239             259             168 
          serum      comparison        frequent             cmf 
            315             116             153             586 
         consid            rang          select         possibl 
            131             248             124             130 
         inform           major            need        function 
            124             122             128             188 
         failur         confirm          requir       experienc 
            262             178             168             167 
   characterist       methotrex  progressionfre         general 
            119             265             158             126 
        prevent           andor             mbc            main 
            143             128             276             131 
           side        superior           start           tissu 
            168             161             131             197 
         second          regard           enter 
            138             105             117 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(colSums(dtmAbstract))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8381&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which.max(colSums(dtmAbstract))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;patient 
     17 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtmAbstract$patient&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   [1]  0  1  9  5  5  6  8  3  2  5  2  4  2  1  1  3  0  8  6  0  3  7  7
  [24]  4  2  5  2  5  0  0  3  2  3  5 12  3  5  4  7  2  0  1  2  3  6  5
  [47]  1  4  7  6  2  5  6  5  5  6  9  5  1  5 10  6  3  3  6  1  4  4  0
  [70]  6  9  5  9  1 11  5  0  3  5  6  3  8  8  2  6  9  3  7  4  1  6 13
  [93]  3 12  0  5  5  3  0  1  8 16  5 13  0  7  2  7  0  7  3  2  5  1  6
 [116] 13  4  5  4  4  4  6  5  6  5  3  1  3  3  0  5  1  3  9  8  2  3  7
 [139]  5  3  4  4 11  2  9  1  1  6  8  1  2  1  1  7  0  2  4  6 16  0  3
 [162]  0  3  8  3  5  2 11  5  0  0  2  8  8  1  4  7 11  6  5  6  1  7  2
 [185] 14 13  4 23  7  3  6  3  3  2  3  4 11  2  4 12  3  6  4  3  6  9  4
 [208]  8  2  4  2  6  1  7  6  5  5 14  5 13  0 11  1  7  6  3  5  4  3  7
 [231]  0  0  7  2  4  5  7  0  4  4 16  0  8  3  0  3  7  6  6  2  2  1  5
 [254]  4  7  8  1  1  5  4  2  3 16  2  1  5  7  3  4  0  6  3  2  1  5  6
 [277]  6  2  9  6  2  6  5  7  9 10  9  5  2  4  5  0  0  3  3  6  4  6  3
 [300]  9  6  3  3  8  5  0  7  9  4  2  7  3  5  7  4  0  2  6  8  1  4  5
 [323]  2  1  2  2  4  6  5 13  2  0 10  4  4  4  9  7  2  5  3  4  4  4  4
 [346]  3  9  2  5  0  9  0  4  5  2  7  5  1  4  5  5  5  4  2  0  2  9  9
 [369]  1  0  6  3  7  8  0  2  6  5  2  1  2  1  8  3  6  6  7 17  3  7  6
 [392]  3  8  9  1  8  5 10  0  7  4  6  8  3  8  0  9  8  3  1  0  0  7  4
 [415]  8  5  4  4  5  5  4  4  0  3  3  5  1  9  3  2  3  3  8  4  9  7  0
 [438]  5  1  0 10  7  6  5  7  4  5  2 13  1  6  7  3  4  5  7  0  5  4  4
 [461]  2  6  4  3  2  2  5  5  5  0  1  0  8  2  0  2  0  2  8 11  0  2  3
 [484]  5  7  6  4  3  6  4  7 10 12  7 11  7  2  8  6  2  4  4  5  5  3  8
 [507]  3  0  0  5  4  5  1  4  0  0  6  3  3  3  6  2  3  2  3  4 11  3  5
 [530]  7  2  6  6  2  4  6  1 13  5  3  7  4  3  3  4  3  8  3  5  1  5  3
 [553]  0 11  5  7  2  1  0  6  4  5  5  4  3  8  4  4  1  4  2  7  9 14 10
 [576]  0  6  2  1  8  0  2  5  2  2  1  5  0 10  6  5  0  1  1  4  4  4  0
 [599]  0  4  8  3  3  8  7  5  7  1  2  3  3  9  5 11  8  8  6  4  3  2  0
 [622]  2  6  9  1  0 12  3  2 12  9  6  3  6  0  4  5  4  1 13  3  0  2  5
 [645]  3  1  8  5  9  5  5  9  5  5  7  6  5  6  6  3  0  6  7 13  7  9  5
 [668]  6  7  5  2  6  1  3  0  3  4  3  3  0 10  0  7  0  6  3  9  4  7 12
 [691]  0  0  7  9  0  7  6  6  3  2  5  2  0  5  5  7  4  5  5  3  6  2  7
 [714]  1  2  1  6  4 11  9  2  0  8 10  2  7  2  6  1  8  0  0  4  3  9  5
 [737]  2  3  1  7  4  0  0  0  6  3 13  3  5  5  0  4  2  3  6  4  5  0  1
 [760]  4  9 12  4  0 10  5  9  4  3 10  6  2  3  5  3  9  3  6  1  5  7  0
 [783]  4  5  3  4  1  0  1  3 10 17  0  7  5  2  0  9  0  1  5  8  0  6  9
 [806]  2  0  9 10  0  3  3  4  6  5  5  1  1 12 10  4  0  5  4  6  3  7  2
 [829]  1  0  0  4  0 10  5  8  9  8  1  3  2  6  1 12  2  0  5  2  9  3  4
 [852]  7  5  3 10  6  6  9  0  4  7  4  1  6  0  6  5  8  3  3  0  4  9  8
 [875]  2  8  3 11  7  1  5  4  7  6  8  3  1  5  6  6  0  5 20  5  4 13  2
 [898]  1  0  2  1  3  4 10  4  5  5  1  9  0  1  0  0 11  3 12  4  6  7  4
 [921]  0  3  3  5  6 11  0  1  4 10  2  5  8  1  0  7  6  2  5  4  0  2  1
 [944]  6  5 10  3  0  0  3 12  3  4 10  0  5  3  3  9  0  1  3 17  1  8 14
 [967]  6  3  3  2  8  0  6  8  1  0 11 10 10 18  0  7  9  2  5  1  7  5  5
 [990]  5  6  4  3  7  8  2 10  5  0 10  5  5  0  5  3  1  4  4  3  2  6  1
[1013]  2  6  3  6 14  2  0  6  5  3  4  9  2  5  8  4  5  3  4  7  5  6  2
[1036]  3  4  7  8 11  8  6  9  4  7  5 14  7  2 12  0  5  3  6  1  4  0  1
[1059]  6  5  4  3  0  2  2  1  6  5  2  1  1  9  6 11  4  1  5  5  0  5 10
[1082]  0  5  2  2  4  5  7  1  4  5  2 10  0  0  5  0  2  3  3  6  2  8  3
[1105]  3  3  1  5  6  0  0  2  4  5  2  8 10  7  9  5 10  4  7 10  6 10  6
[1128]  3  0  0  7  6  8  4  4  4  0  8  7  7  1  6  6 10  8  4  4  5 11  7
[1151]  5  7  4  6  4  4  7  3 12 12  0  3  0  4  2  7  4  4  6  0  6  6  1
[1174]  9  5  6 11  6 15  3  2  3  6  4  5  4  1  3  0  3  0  2  6  0  1  7
[1197]  2  8  0  0  0  4 11  9  2 10  3  6  5  2  1 11  0  6  1  5  4  3  2
[1220]  2  6  7  1 11  6 10  5  4  7 10  4  0  4  4  0  2  8  7  5  4  6  4
[1243]  0  1  7  6  5  0 12  7  7  7  5  7  0 11  5  1  8 11  3  1  8  4  3
[1266]  3  7  2  8  2  4  0  7  5  0  1  0  9  7  1  7  4  7  8  3  1  6  3
[1289]  2  4  3  2  3  9  6  2  3  6  5 11  5  7  8  0  6  4  5  4  1  1  3
[1312]  5  2  0  6  3  3  0  6  6  0  7 11  0  3  3  7  5  2  0  3  5  2  0
[1335]  4  4  3  6  4  3  4  5  0  7  1  6  6  6  5  5 13 15  2  7 12  3  2
[1358]  6  5  5  5  9  7  0  5  7  4  8  2  0  2  2  7  8  2  4  0  7  9  4
[1381]  4  6  2  8  4  0  1 10  4  4  5  3  4  6  4  8  2  6 12  5  5  5  4
[1404]  5 11  4  4  7  5  6  7  2  3  6  3  3  2  5  2  4  1  2  2  7  7  4
[1427] 13  4 12  4  7  7  4  2  0  7  6  7  6  7  4  5  5  8  5  2  5  1  3
[1450]  0 10  2  0  5  3  3 11  1  0  3  1  5  0  2  0  3  2  0  6  0  3  2
[1473]  4 11  0  0  1  4  1  3  2  1  5  0  2  3  2  3 10  3  4  2  0  0  9
[1496]  4  8  4  0  2  5  5  0  0  2  0  8  3  0  9  4  3  2  2  4  1  0  6
[1519]  2  2  2  0  3  0  0  6  0  0  4  2  4 12  2  8  6  2  1  0  6  3  4
[1542]  5  0  6  1  1  2  5  1  3  4 11  9  0  6  2  7  5  3  7  5  0  2  7
[1565] 10  2  3  3  5  3  4  2  3  4  3  1  6  2  1  3  1  3  2  3  5  4  3
[1588]  0  1  0  5  1  3  9  1  6  4  1  1  7  2  0  1  7  3  0  5  2  5  2
[1611]  2  2  0  6  0  5  1  4  0  0  1  7  1  4  6  2  0  1  8  2  3  5  0
[1634]  7  7  7  4  7  7  3  5  6  1  6  4 16  5  8  7  3  1  7  3 10  4  7
[1657] 10  4  9  1 10  4  6  9  0  4  5  2  9  5  1  8  4 12  8  6  3  6 10
[1680]  2  3  1  1 13  4  2  5  7 10  7 10  5  4  3  8  4  3  6  5  2  9  6
[1703]  7  4  4  7  3  6  9  5  3  4  4  6  8  4  6  1  8  7 10  6  7  7  1
[1726]  9  6 15 10 10  9  8  6  5  8  5  6  7  4  8  9  8  8  9 10  6  5  6
[1749]  6  9 11  4  4 11  4  5  6  7  4  6  3  7  4  7  9  8  4  5 10  6  2
[1772]  0  8  5  5  6  3  7  4  0  2  4  1  0  3  6 10  3 14  3 10  0  5  0
[1795]  8  0  5  0  9  0  3  0  0  8  3  5  5  2  0  0  4 16  2  7  3  0  0
[1818]  0  7  6  0  2  5  5  0  4  5 10  9  3  5  9  4  1  4  4  2  0  3  2
[1841]  4  4  5  0  9  8  4  0  0  0  8  2  4  4  0  0  0  0  0  0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Building a model&lt;/h3&gt;
&lt;p&gt;We want to combine dtmTitle and dtmAbstract into a single dataframe to make predictions. However, some of the variables in these dataframes have the same names. To fix this issue, run the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(dtmTitle) &amp;lt;- paste0(&amp;quot;T&amp;quot;, colnames(dtmTitle))
str(dtmTitle)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  31 variables:
 $ Tcancer         : num  1 0 1 1 1 1 0 1 1 2 ...
 $ Ttreatment      : num  1 0 0 0 1 0 0 0 0 1 ...
 $ Tbreast         : num  0 0 1 1 1 1 0 1 1 1 ...
 $ Tearli          : num  0 0 1 1 0 0 0 1 0 0 ...
 $ Tiii            : num  0 0 1 0 0 0 0 0 0 1 ...
 $ Tphase          : num  0 0 1 1 0 0 0 0 0 1 ...
 $ Trandom         : num  0 0 1 1 1 0 0 0 0 1 ...
 $ Ttrial          : num  0 0 1 1 1 0 0 1 1 1 ...
 $ Tversus         : num  0 0 1 0 0 0 0 1 0 0 ...
 $ Tcyclophosphamid: num  0 0 0 1 0 0 0 0 0 0 ...
 $ Tchemotherapi   : num  0 0 0 0 1 1 0 0 0 0 ...
 $ Tcombin         : num  0 0 0 0 1 0 1 0 0 0 ...
 $ Teffect         : num  0 0 0 0 1 0 0 1 0 1 ...
 $ Tmetastat       : num  0 0 0 0 1 0 0 0 0 0 ...
 $ Tpatient        : num  0 0 0 0 1 0 1 0 1 1 ...
 $ Trespons        : num  0 0 0 0 0 1 0 0 0 0 ...
 $ Tadvanc         : num  0 0 0 0 0 0 1 0 0 0 ...
 $ Tpostmenopaus   : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Trandomis       : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Tstudi          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Ttamoxifen      : num  0 0 0 0 0 0 0 2 1 0 ...
 $ Twomen          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Tadjuv          : num  0 0 0 0 0 0 0 0 1 0 ...
 $ Tgroup          : num  0 0 0 0 0 0 0 0 1 1 ...
 $ Ttherapi        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tcompar         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdoxorubicin    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdocetaxel      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tresult         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tplus           : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tclinic         : num  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(dtmAbstract) &amp;lt;- paste0(&amp;quot;A&amp;quot;, colnames(dtmAbstract))
str(dtmAbstract)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  335 variables:
 $ A100            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Aactiv          : num  0 2 0 1 0 0 1 0 0 0 ...
 $ Aassess         : num  0 2 1 2 0 1 0 0 0 3 ...
 $ Abreast         : num  0 2 3 3 3 4 2 2 2 3 ...
 $ Acarcinoma      : num  0 5 0 0 0 0 0 0 0 2 ...
 $ Acase           : num  0 11 0 0 1 0 0 0 0 0 ...
 $ Acell           : num  0 3 0 0 0 1 0 0 0 0 ...
 $ Aclinic         : num  0 1 0 1 0 0 0 0 0 0 ...
 $ Aconsist        : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Acontrol        : num  0 1 0 0 0 0 0 0 1 0 ...
 $ Adiffer         : num  0 1 2 1 3 0 0 1 0 1 ...
 $ Adiseas         : num  0 1 0 1 3 0 0 1 0 0 ...
 $ Aless           : num  0 4 1 0 0 0 0 0 0 6 ...
 $ Anegat          : num  0 4 0 0 0 3 0 0 0 0 ...
 $ Aobserv         : num  0 2 1 0 1 0 0 0 0 0 ...
 $ Aone            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Apatient        : num  0 1 9 5 5 6 8 3 2 5 ...
 $ Apopul          : num  0 8 0 0 0 0 0 0 0 0 ...
 $ Aposit          : num  0 2 0 1 0 5 0 0 0 0 ...
 $ Aseven          : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Ashow           : num  0 2 0 0 1 0 1 0 0 3 ...
 $ Astudi          : num  0 1 1 1 0 1 3 2 0 1 ...
 $ Atest           : num  0 2 1 1 0 0 0 0 0 0 ...
 $ Atherapi        : num  0 2 0 1 0 0 0 0 0 0 ...
 $ A500            : num  0 0 1 0 2 0 0 0 0 0 ...
 $ Aaddit          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Aamong          : num  0 0 2 4 0 0 0 0 1 0 ...
 $ Aarm            : num  0 0 7 4 2 0 0 1 0 1 ...
 $ Aassign         : num  0 0 2 1 0 0 0 1 1 1 ...
 $ Aassoci         : num  0 0 1 3 0 2 0 1 2 0 ...
 $ Abackground     : num  0 0 1 1 1 0 0 1 1 0 ...
 $ Abetter         : num  0 0 1 0 1 1 0 0 0 0 ...
 $ Acancer         : num  0 0 2 3 3 3 2 2 3 0 ...
 $ Achang          : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Achemotherapi   : num  0 0 1 2 3 5 2 0 1 0 ...
 $ Acompar         : num  0 0 1 2 0 0 0 1 1 1 ...
 $ Acomplet        : num  0 0 3 0 1 1 1 2 0 0 ...
 $ Aconclus        : num  0 0 1 0 1 0 0 0 0 0 ...
 $ Aconfid         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Acontinu        : num  0 0 2 0 1 0 0 2 0 0 ...
 $ Acycl           : num  0 0 6 0 2 0 1 0 0 0 ...
 $ Acyclophosphamid: num  0 0 1 1 1 1 3 0 0 0 ...
 $ Aday            : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adecreas        : num  0 0 1 0 0 0 0 0 1 0 ...
 $ Adefin          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Ademonstr       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Adocetaxel      : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adoxorubicin    : num  0 0 1 0 0 1 0 0 0 0 ...
 $ Aeffect         : num  0 0 2 0 0 1 0 2 0 1 ...
 $ Aefficaci       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aenrol          : num  0 0 1 0 0 0 1 0 0 1 ...
 $ Afour           : num  0 0 4 0 0 0 0 0 0 0 ...
 $ Ahematolog      : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Ainiti          : num  0 0 3 0 0 0 0 1 0 0 ...
 $ Ainterv         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Aleast          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Alymph          : num  0 0 1 4 0 3 0 0 1 0 ...
 $ Amethod         : num  0 0 1 0 1 0 0 1 1 0 ...
 $ Amgm2           : num  0 0 5 0 4 0 9 0 0 0 ...
 $ Aneoadjuv       : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Anode           : num  0 0 1 3 0 0 0 0 1 0 ...
 $ Anumber         : num  0 0 1 1 0 2 1 0 0 0 ...
 $ Aoutcom         : num  0 0 2 0 0 0 0 0 0 2 ...
 $ Apatholog       : num  0 0 3 0 0 1 0 0 0 0 ...
 $ Aper            : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aprevious       : num  0 0 1 0 1 0 2 0 0 0 ...
 $ Arandom         : num  0 0 2 1 1 0 0 1 1 2 ...
 $ Arate           : num  0 0 1 1 2 0 1 0 1 0 ...
 $ Areceiv         : num  0 0 2 0 1 1 3 0 0 2 ...
 $ Areduct         : num  0 0 1 2 0 0 0 1 0 0 ...
 $ Aregimen        : num  0 0 2 0 0 0 1 0 0 0 ...
 $ Arespond        : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Arespons        : num  0 0 7 0 4 2 2 0 0 0 ...
 $ Aresult         : num  0 0 1 0 1 0 1 0 0 0 ...
 $ Asimilar        : num  0 0 2 0 0 0 0 0 1 0 ...
 $ Asize           : num  0 0 1 3 0 1 0 0 0 0 ...
 $ Astatist        : num  0 0 1 3 0 0 0 0 0 0 ...
 $ Asurgeri        : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Atoler          : num  0 0 1 0 0 0 1 0 0 0 ...
 $ Atoxic          : num  0 0 2 0 1 0 1 0 0 5 ...
 $ Atreatment      : num  0 0 3 6 14 0 0 4 1 1 ...
 $ Atumor          : num  0 0 2 4 0 0 2 0 0 0 ...
 $ Atwo            : num  0 0 3 0 1 0 0 1 0 0 ...
 $ A001            : num  0 0 0 1 0 0 0 1 0 0 ...
 $ Aadjuv          : num  0 0 0 2 0 1 0 2 4 0 ...
 $ Aage            : num  0 0 0 1 0 0 0 0 0 0 ...
 $ Aalso           : num  0 0 0 1 0 0 0 0 2 0 ...
 $ Aanalysi        : num  0 0 0 2 0 0 0 1 0 0 ...
 $ Aanalyz         : num  0 0 0 1 0 0 0 0 0 0 ...
 $ Aaxillari       : num  0 0 0 1 0 0 0 0 1 0 ...
 $ Adeath          : num  0 0 0 2 0 0 0 0 1 0 ...
 $ Adfs            : num  0 0 0 3 0 0 0 0 0 0 ...
 $ Adiseasefre     : num  0 0 0 1 0 2 0 0 3 0 ...
 $ Adrug           : num  0 0 0 1 0 0 0 0 0 0 ...
 $ Aelig           : num  0 0 0 1 0 0 0 0 0 0 ...
 $ Aendpoint       : num  0 0 0 2 0 0 0 0 1 3 ...
 $ Aepirubicin     : num  0 0 0 1 1 0 0 0 0 0 ...
 $ Aestim          : num  0 0 0 1 1 0 0 0 0 0 ...
 $ Afluorouracil   : num  0 0 0 1 1 0 0 0 0 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What was the effect of these functions?
#### Adding the letter T in front of all the title variable names and adding the letter A in front of all the abstract variable names.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Building a Model&lt;/h3&gt;
&lt;p&gt;Using cbind(), combine dtmTitle and dtmAbstract into a single dataframe called dtm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm &amp;lt;- cbind(dtmTitle, dtmAbstract)
str(dtm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  366 variables:
 $ Tcancer         : num  1 0 1 1 1 1 0 1 1 2 ...
 $ Ttreatment      : num  1 0 0 0 1 0 0 0 0 1 ...
 $ Tbreast         : num  0 0 1 1 1 1 0 1 1 1 ...
 $ Tearli          : num  0 0 1 1 0 0 0 1 0 0 ...
 $ Tiii            : num  0 0 1 0 0 0 0 0 0 1 ...
 $ Tphase          : num  0 0 1 1 0 0 0 0 0 1 ...
 $ Trandom         : num  0 0 1 1 1 0 0 0 0 1 ...
 $ Ttrial          : num  0 0 1 1 1 0 0 1 1 1 ...
 $ Tversus         : num  0 0 1 0 0 0 0 1 0 0 ...
 $ Tcyclophosphamid: num  0 0 0 1 0 0 0 0 0 0 ...
 $ Tchemotherapi   : num  0 0 0 0 1 1 0 0 0 0 ...
 $ Tcombin         : num  0 0 0 0 1 0 1 0 0 0 ...
 $ Teffect         : num  0 0 0 0 1 0 0 1 0 1 ...
 $ Tmetastat       : num  0 0 0 0 1 0 0 0 0 0 ...
 $ Tpatient        : num  0 0 0 0 1 0 1 0 1 1 ...
 $ Trespons        : num  0 0 0 0 0 1 0 0 0 0 ...
 $ Tadvanc         : num  0 0 0 0 0 0 1 0 0 0 ...
 $ Tpostmenopaus   : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Trandomis       : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Tstudi          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Ttamoxifen      : num  0 0 0 0 0 0 0 2 1 0 ...
 $ Twomen          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Tadjuv          : num  0 0 0 0 0 0 0 0 1 0 ...
 $ Tgroup          : num  0 0 0 0 0 0 0 0 1 1 ...
 $ Ttherapi        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tcompar         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdoxorubicin    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdocetaxel      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tresult         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tplus           : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tclinic         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A100            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Aactiv          : num  0 2 0 1 0 0 1 0 0 0 ...
 $ Aassess         : num  0 2 1 2 0 1 0 0 0 3 ...
 $ Abreast         : num  0 2 3 3 3 4 2 2 2 3 ...
 $ Acarcinoma      : num  0 5 0 0 0 0 0 0 0 2 ...
 $ Acase           : num  0 11 0 0 1 0 0 0 0 0 ...
 $ Acell           : num  0 3 0 0 0 1 0 0 0 0 ...
 $ Aclinic         : num  0 1 0 1 0 0 0 0 0 0 ...
 $ Aconsist        : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Acontrol        : num  0 1 0 0 0 0 0 0 1 0 ...
 $ Adiffer         : num  0 1 2 1 3 0 0 1 0 1 ...
 $ Adiseas         : num  0 1 0 1 3 0 0 1 0 0 ...
 $ Aless           : num  0 4 1 0 0 0 0 0 0 6 ...
 $ Anegat          : num  0 4 0 0 0 3 0 0 0 0 ...
 $ Aobserv         : num  0 2 1 0 1 0 0 0 0 0 ...
 $ Aone            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Apatient        : num  0 1 9 5 5 6 8 3 2 5 ...
 $ Apopul          : num  0 8 0 0 0 0 0 0 0 0 ...
 $ Aposit          : num  0 2 0 1 0 5 0 0 0 0 ...
 $ Aseven          : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Ashow           : num  0 2 0 0 1 0 1 0 0 3 ...
 $ Astudi          : num  0 1 1 1 0 1 3 2 0 1 ...
 $ Atest           : num  0 2 1 1 0 0 0 0 0 0 ...
 $ Atherapi        : num  0 2 0 1 0 0 0 0 0 0 ...
 $ A500            : num  0 0 1 0 2 0 0 0 0 0 ...
 $ Aaddit          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Aamong          : num  0 0 2 4 0 0 0 0 1 0 ...
 $ Aarm            : num  0 0 7 4 2 0 0 1 0 1 ...
 $ Aassign         : num  0 0 2 1 0 0 0 1 1 1 ...
 $ Aassoci         : num  0 0 1 3 0 2 0 1 2 0 ...
 $ Abackground     : num  0 0 1 1 1 0 0 1 1 0 ...
 $ Abetter         : num  0 0 1 0 1 1 0 0 0 0 ...
 $ Acancer         : num  0 0 2 3 3 3 2 2 3 0 ...
 $ Achang          : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Achemotherapi   : num  0 0 1 2 3 5 2 0 1 0 ...
 $ Acompar         : num  0 0 1 2 0 0 0 1 1 1 ...
 $ Acomplet        : num  0 0 3 0 1 1 1 2 0 0 ...
 $ Aconclus        : num  0 0 1 0 1 0 0 0 0 0 ...
 $ Aconfid         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Acontinu        : num  0 0 2 0 1 0 0 2 0 0 ...
 $ Acycl           : num  0 0 6 0 2 0 1 0 0 0 ...
 $ Acyclophosphamid: num  0 0 1 1 1 1 3 0 0 0 ...
 $ Aday            : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adecreas        : num  0 0 1 0 0 0 0 0 1 0 ...
 $ Adefin          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Ademonstr       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Adocetaxel      : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adoxorubicin    : num  0 0 1 0 0 1 0 0 0 0 ...
 $ Aeffect         : num  0 0 2 0 0 1 0 2 0 1 ...
 $ Aefficaci       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aenrol          : num  0 0 1 0 0 0 1 0 0 1 ...
 $ Afour           : num  0 0 4 0 0 0 0 0 0 0 ...
 $ Ahematolog      : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Ainiti          : num  0 0 3 0 0 0 0 1 0 0 ...
 $ Ainterv         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Aleast          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Alymph          : num  0 0 1 4 0 3 0 0 1 0 ...
 $ Amethod         : num  0 0 1 0 1 0 0 1 1 0 ...
 $ Amgm2           : num  0 0 5 0 4 0 9 0 0 0 ...
 $ Aneoadjuv       : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Anode           : num  0 0 1 3 0 0 0 0 1 0 ...
 $ Anumber         : num  0 0 1 1 0 2 1 0 0 0 ...
 $ Aoutcom         : num  0 0 2 0 0 0 0 0 0 2 ...
 $ Apatholog       : num  0 0 3 0 0 1 0 0 0 0 ...
 $ Aper            : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aprevious       : num  0 0 1 0 1 0 2 0 0 0 ...
 $ Arandom         : num  0 0 2 1 1 0 0 1 1 2 ...
 $ Arate           : num  0 0 1 1 2 0 1 0 1 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, add the dependent variable “trial” to dtm, copying it from the original dataframe called trials.&lt;/p&gt;
&lt;p&gt;How many columns are in this combined dataframe?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dtm$trial &amp;lt;- trials$trial
str(dtm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1860 obs. of  367 variables:
 $ Tcancer         : num  1 0 1 1 1 1 0 1 1 2 ...
 $ Ttreatment      : num  1 0 0 0 1 0 0 0 0 1 ...
 $ Tbreast         : num  0 0 1 1 1 1 0 1 1 1 ...
 $ Tearli          : num  0 0 1 1 0 0 0 1 0 0 ...
 $ Tiii            : num  0 0 1 0 0 0 0 0 0 1 ...
 $ Tphase          : num  0 0 1 1 0 0 0 0 0 1 ...
 $ Trandom         : num  0 0 1 1 1 0 0 0 0 1 ...
 $ Ttrial          : num  0 0 1 1 1 0 0 1 1 1 ...
 $ Tversus         : num  0 0 1 0 0 0 0 1 0 0 ...
 $ Tcyclophosphamid: num  0 0 0 1 0 0 0 0 0 0 ...
 $ Tchemotherapi   : num  0 0 0 0 1 1 0 0 0 0 ...
 $ Tcombin         : num  0 0 0 0 1 0 1 0 0 0 ...
 $ Teffect         : num  0 0 0 0 1 0 0 1 0 1 ...
 $ Tmetastat       : num  0 0 0 0 1 0 0 0 0 0 ...
 $ Tpatient        : num  0 0 0 0 1 0 1 0 1 1 ...
 $ Trespons        : num  0 0 0 0 0 1 0 0 0 0 ...
 $ Tadvanc         : num  0 0 0 0 0 0 1 0 0 0 ...
 $ Tpostmenopaus   : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Trandomis       : num  0 0 0 0 0 0 0 1 1 0 ...
 $ Tstudi          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Ttamoxifen      : num  0 0 0 0 0 0 0 2 1 0 ...
 $ Twomen          : num  0 0 0 0 0 0 0 1 0 0 ...
 $ Tadjuv          : num  0 0 0 0 0 0 0 0 1 0 ...
 $ Tgroup          : num  0 0 0 0 0 0 0 0 1 1 ...
 $ Ttherapi        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tcompar         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdoxorubicin    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tdocetaxel      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tresult         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tplus           : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Tclinic         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A100            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Aactiv          : num  0 2 0 1 0 0 1 0 0 0 ...
 $ Aassess         : num  0 2 1 2 0 1 0 0 0 3 ...
 $ Abreast         : num  0 2 3 3 3 4 2 2 2 3 ...
 $ Acarcinoma      : num  0 5 0 0 0 0 0 0 0 2 ...
 $ Acase           : num  0 11 0 0 1 0 0 0 0 0 ...
 $ Acell           : num  0 3 0 0 0 1 0 0 0 0 ...
 $ Aclinic         : num  0 1 0 1 0 0 0 0 0 0 ...
 $ Aconsist        : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Acontrol        : num  0 1 0 0 0 0 0 0 1 0 ...
 $ Adiffer         : num  0 1 2 1 3 0 0 1 0 1 ...
 $ Adiseas         : num  0 1 0 1 3 0 0 1 0 0 ...
 $ Aless           : num  0 4 1 0 0 0 0 0 0 6 ...
 $ Anegat          : num  0 4 0 0 0 3 0 0 0 0 ...
 $ Aobserv         : num  0 2 1 0 1 0 0 0 0 0 ...
 $ Aone            : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Apatient        : num  0 1 9 5 5 6 8 3 2 5 ...
 $ Apopul          : num  0 8 0 0 0 0 0 0 0 0 ...
 $ Aposit          : num  0 2 0 1 0 5 0 0 0 0 ...
 $ Aseven          : num  0 1 0 0 0 0 0 0 0 0 ...
 $ Ashow           : num  0 2 0 0 1 0 1 0 0 3 ...
 $ Astudi          : num  0 1 1 1 0 1 3 2 0 1 ...
 $ Atest           : num  0 2 1 1 0 0 0 0 0 0 ...
 $ Atherapi        : num  0 2 0 1 0 0 0 0 0 0 ...
 $ A500            : num  0 0 1 0 2 0 0 0 0 0 ...
 $ Aaddit          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Aamong          : num  0 0 2 4 0 0 0 0 1 0 ...
 $ Aarm            : num  0 0 7 4 2 0 0 1 0 1 ...
 $ Aassign         : num  0 0 2 1 0 0 0 1 1 1 ...
 $ Aassoci         : num  0 0 1 3 0 2 0 1 2 0 ...
 $ Abackground     : num  0 0 1 1 1 0 0 1 1 0 ...
 $ Abetter         : num  0 0 1 0 1 1 0 0 0 0 ...
 $ Acancer         : num  0 0 2 3 3 3 2 2 3 0 ...
 $ Achang          : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Achemotherapi   : num  0 0 1 2 3 5 2 0 1 0 ...
 $ Acompar         : num  0 0 1 2 0 0 0 1 1 1 ...
 $ Acomplet        : num  0 0 3 0 1 1 1 2 0 0 ...
 $ Aconclus        : num  0 0 1 0 1 0 0 0 0 0 ...
 $ Aconfid         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Acontinu        : num  0 0 2 0 1 0 0 2 0 0 ...
 $ Acycl           : num  0 0 6 0 2 0 1 0 0 0 ...
 $ Acyclophosphamid: num  0 0 1 1 1 1 3 0 0 0 ...
 $ Aday            : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adecreas        : num  0 0 1 0 0 0 0 0 1 0 ...
 $ Adefin          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Ademonstr       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Adocetaxel      : num  0 0 1 0 0 0 3 0 0 0 ...
 $ Adoxorubicin    : num  0 0 1 0 0 1 0 0 0 0 ...
 $ Aeffect         : num  0 0 2 0 0 1 0 2 0 1 ...
 $ Aefficaci       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aenrol          : num  0 0 1 0 0 0 1 0 0 1 ...
 $ Afour           : num  0 0 4 0 0 0 0 0 0 0 ...
 $ Ahematolog      : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Ainiti          : num  0 0 3 0 0 0 0 1 0 0 ...
 $ Ainterv         : num  0 0 1 1 0 0 0 0 0 0 ...
 $ Aleast          : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Alymph          : num  0 0 1 4 0 3 0 0 1 0 ...
 $ Amethod         : num  0 0 1 0 1 0 0 1 1 0 ...
 $ Amgm2           : num  0 0 5 0 4 0 9 0 0 0 ...
 $ Aneoadjuv       : num  0 0 2 0 0 0 0 0 0 0 ...
 $ Anode           : num  0 0 1 3 0 0 0 0 1 0 ...
 $ Anumber         : num  0 0 1 1 0 2 1 0 0 0 ...
 $ Aoutcom         : num  0 0 2 0 0 0 0 0 0 2 ...
 $ Apatholog       : num  0 0 3 0 0 1 0 0 0 0 ...
 $ Aper            : num  0 0 1 0 0 0 0 0 0 0 ...
 $ Aprevious       : num  0 0 1 0 1 0 2 0 0 0 ...
 $ Arandom         : num  0 0 2 1 1 0 0 1 1 2 ...
 $ Arate           : num  0 0 1 1 2 0 1 0 1 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;367&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Building a Model&lt;/h3&gt;
&lt;p&gt;Now that we have prepared our dataframe, it’s time to split it into a training and testing set and to build regression models. Set the random seed to 144 and use the sample.split function from the caTools package to split dtm into dataframes named “train” and “test”, putting 70% of the data in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
trialSplit &amp;lt;- sample.split(dtm$trial, 0.7)
train &amp;lt;- subset(dtm, trialSplit == TRUE)
test &amp;lt;- subset(dtm, trialSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the baseline model on the training set? (Remember that the baseline model predicts the most frequent outcome in the training set for all observations.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(train$trial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
730 572 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;730 / (730 + 572)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5606759&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Building a Model&lt;/h3&gt;
&lt;p&gt;Build a CART model called trialCART, using all the independent variables in the training set to train the model, and then plot the CART model. Just use the default parameters to build the model (don’t add a minbucket or cp value). Remember to add the method=“class” argument, since this is a classification problem.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trialCART &amp;lt;- rpart(trial ~ ., data = train, method = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the name of the first variable the model split on?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(trialCART)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/medicine/medicine_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;tphase&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tphase&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Building a Model&lt;/h3&gt;
&lt;p&gt;Obtain the training set predictions for the model (do not yet predict on the test-set). Extract the predicted probability of a result being a trial (recall that this involves not setting a type argument, and keeping only the second column of the predict output).&lt;/p&gt;
&lt;p&gt;What is the maximum predicted probability for any result?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTrain &amp;lt;- predict(trialCART)
predTrain[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           0          1
1  0.8636364 0.13636364
2  0.8636364 0.13636364
3  0.1281139 0.87188612
5  0.2176871 0.78231293
6  0.9454545 0.05454545
7  0.2176871 0.78231293
10 0.1281139 0.87188612
12 0.7125000 0.28750000
13 0.1281139 0.87188612
14 0.7125000 0.28750000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTrainProb &amp;lt;- predTrain[, 2]
max(predTrainProb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8718861&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(predTrainProb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.05455 0.13636 0.28750 0.43932 0.78231 0.87189 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;problem-3.6---building-a-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Problem 3.6 - Building a Model&lt;/h4&gt;
&lt;p&gt;Without running the analysis, how do you expect the maximum predicted probability to differ in the testing set?
#### The maximum predicted probability will likely be exactly the same in the testing set. Because the CART tree assigns the same predicted probability to each leaf node and there are a small number of leaf nodes compared to data points, we expect exactly the same maximum predicted probability.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.7---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.7 - Building a Model&lt;/h3&gt;
&lt;p&gt;For these questions, use a threshold probability of 0.5 to predict that an observation is a clinical trial.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(train$trial, predTrainProb &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   631   99
  1   131  441&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the training set accuracy of the CART model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(631 + 441) / nrow(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8233487&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the training set sensitivity of the CART model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;441 / (441 + 131)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.770979&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the training set specificity of the CART model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;631 / (631 + 99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8643836&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Evaluating the model on the testing set&lt;/h3&gt;
&lt;p&gt;Evaluate the CART model on the testing set using the predict function and creating a vector of predicted probabilities predTest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred &amp;lt;- predict(trialCART, newdata = test)
pred[1:10,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;           0         1
4  0.1281139 0.8718861
8  0.8636364 0.1363636
9  0.7125000 0.2875000
11 0.7125000 0.2875000
19 0.7125000 0.2875000
31 0.8636364 0.1363636
40 0.8636364 0.1363636
42 0.8636364 0.1363636
43 0.8636364 0.1363636
48 0.7125000 0.2875000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest &amp;lt;- pred[, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the testing set accuracy, assuming a probability threshold of 0.5 for predicting that a result is a clinical trial?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$trial, predTest &amp;gt;= 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   261   52
  1    83  162&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(261 + 162) / nrow(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7580645&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Using the ROCR package, what is the testing set AUC of the prediction model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predROCR &amp;lt;- prediction(predTest, test$trial)
performance(predROCR, &amp;quot;auc&amp;quot;)@y.values&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
[1] 0.8371063&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---decision-maker-tradeoffs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - Decision-Maker Tradeoffs&lt;/h3&gt;
&lt;p&gt;What is the cost associated with the model in Step 1 making a false negative prediction?
#### A paper that should have been included in Set A will be missed, affecting the quality of the results of Step 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.2---decision-maker-tradeoffs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.2 - Decision-Maker Tradeoffs&lt;/h3&gt;
&lt;p&gt;What is the cost associated with the model in Step 1 making a false positive prediction?
#### A paper will be mistakenly added to Set A, yielding additional work in Step 2 of the process but not affecting the quality of the results of Step 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.3---decision-maker-tradeoffs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.3 - Decision-Maker Tradeoffs&lt;/h3&gt;
&lt;p&gt;Given the costs associated with false positives and false negatives, which of the following is most accurate?
#### A false negative is more costly than a false positive; the decision maker should use a probability threshold less than 0.5 for the machine learning model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Earnings Prediction From Census Data</title>
      <link>/project/earnings/earnings/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/earnings/earnings/</guid>
      <description>


&lt;p&gt;The United States government periodically collects demographic information by conducting a census.&lt;/p&gt;
&lt;p&gt;In this analysis, I am are going to use census information about individuals to predict how much a person earns – in particular, whether the person earns more than $50,000 per year. This data comes from the UCI Machine Learning Repository.&lt;/p&gt;
&lt;p&gt;The file census.csv contains 1994 census data for 31,978 individuals in the U.S. The dataset includes the following 13 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;age = the age of the individual in years&lt;/li&gt;
&lt;li&gt;workclass = the classification of the individual’s working status (does the person work for the federal government, work for the local government, work without pay, and so on)
education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on)&lt;/li&gt;
&lt;li&gt;maritalstatus = the marital status of the individual&lt;/li&gt;
&lt;li&gt;occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on)&lt;/li&gt;
&lt;li&gt;relationship = relationship of individual to his/her household&lt;/li&gt;
&lt;li&gt;race = the individual’s race&lt;/li&gt;
&lt;li&gt;sex = the individual’s sex&lt;/li&gt;
&lt;li&gt;capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price)&lt;/li&gt;
&lt;li&gt;capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price)&lt;/li&gt;
&lt;li&gt;hoursperweek = the number of hours the individual works per week&lt;/li&gt;
&lt;li&gt;nativecountry = the native country of the individual&lt;/li&gt;
&lt;li&gt;over50k = whether or not the individual earned more than $50,000 in 1994&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Let’s begin by building a logistic regression model to predict whether an individual’s earnings are above $50,000 (the variable “over50k”) using all of the other variables as independent variables.&lt;/p&gt;
&lt;p&gt;First, read the dataset census.csv into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census &amp;lt;- read.csv(&amp;quot;census.csv&amp;quot;)
str(census)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   31978 obs. of  13 variables:
 $ age          : int  39 50 38 53 28 37 49 52 31 42 ...
 $ workclass    : Factor w/ 9 levels &amp;quot; ?&amp;quot;,&amp;quot; Federal-gov&amp;quot;,..: 8 7 5 5 5 5 5 7 5 5 ...
 $ education    : Factor w/ 16 levels &amp;quot; 10th&amp;quot;,&amp;quot; 11th&amp;quot;,..: 10 10 12 2 10 13 7 12 13 10 ...
 $ maritalstatus: Factor w/ 7 levels &amp;quot; Divorced&amp;quot;,&amp;quot; Married-AF-spouse&amp;quot;,..: 5 3 1 3 3 3 4 3 5 3 ...
 $ occupation   : Factor w/ 15 levels &amp;quot; ?&amp;quot;,&amp;quot; Adm-clerical&amp;quot;,..: 2 5 7 7 11 5 9 5 11 5 ...
 $ relationship : Factor w/ 6 levels &amp;quot; Husband&amp;quot;,&amp;quot; Not-in-family&amp;quot;,..: 2 1 2 1 6 6 2 1 2 1 ...
 $ race         : Factor w/ 5 levels &amp;quot; Amer-Indian-Eskimo&amp;quot;,..: 5 5 5 3 3 5 3 5 5 5 ...
 $ sex          : Factor w/ 2 levels &amp;quot; Female&amp;quot;,&amp;quot; Male&amp;quot;: 2 2 2 2 1 1 1 2 1 2 ...
 $ capitalgain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
 $ capitalloss  : int  0 0 0 0 0 0 0 0 0 0 ...
 $ hoursperweek : int  40 13 40 40 40 40 16 45 50 40 ...
 $ nativecountry: Factor w/ 41 levels &amp;quot; Cambodia&amp;quot;,&amp;quot; Canada&amp;quot;,..: 39 39 39 39 5 39 23 39 39 39 ...
 $ over50k      : Factor w/ 2 levels &amp;quot; &amp;lt;=50K&amp;quot;,&amp;quot; &amp;gt;50K&amp;quot;: 1 1 1 1 1 1 1 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(census)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      age                    workclass             education    
 Min.   :17.00    Private         :22286    HS-grad     :10368  
 1st Qu.:28.00    Self-emp-not-inc: 2499    Some-college: 7187  
 Median :37.00    Local-gov       : 2067    Bachelors   : 5210  
 Mean   :38.58    ?               : 1809    Masters     : 1674  
 3rd Qu.:48.00    State-gov       : 1279    Assoc-voc   : 1366  
 Max.   :90.00    Self-emp-inc    : 1074    11th        : 1167  
                 (Other)          :  964   (Other)      : 5006  
                maritalstatus              occupation  
  Divorced             : 4394    Prof-specialty :4038  
  Married-AF-spouse    :   23    Craft-repair   :4030  
  Married-civ-spouse   :14692    Exec-managerial:3992  
  Married-spouse-absent:  397    Adm-clerical   :3721  
  Never-married        :10488    Sales          :3584  
  Separated            : 1005    Other-service  :3212  
  Widowed              :  979   (Other)         :9401  
          relationship                    race            sex       
  Husband       :12947    Amer-Indian-Eskimo:  311    Female:10608  
  Not-in-family : 8156    Asian-Pac-Islander:  956    Male  :21370  
  Other-relative:  952    Black             : 3028                  
  Own-child     : 5005    Other             :  253                  
  Unmarried     : 3384    White             :27430                  
  Wife          : 1534                                              
                                                                    
  capitalgain     capitalloss       hoursperweek          nativecountry  
 Min.   :    0   Min.   :   0.00   Min.   : 1.00    United-States:29170  
 1st Qu.:    0   1st Qu.:   0.00   1st Qu.:40.00    Mexico       :  643  
 Median :    0   Median :   0.00   Median :40.00    Philippines  :  198  
 Mean   : 1064   Mean   :  86.74   Mean   :40.42    Germany      :  137  
 3rd Qu.:    0   3rd Qu.:   0.00   3rd Qu.:45.00    Canada       :  121  
 Max.   :99999   Max.   :4356.00   Max.   :99.00    Puerto-Rico  :  114  
                                                   (Other)       : 1595  
   over50k     
  &amp;lt;=50K:24283  
  &amp;gt;50K : 7695  
               
               
               
               
               &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60% of the observations, while the testing set contains 40% of the observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
set.seed(2000)
censusSplit = sample.split(census$over50k, SplitRatio = 0.6)
censusTrain = subset(census, censusSplit == TRUE)
censusTest = subset(census, censusSplit == FALSE)
nrow(censusTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 19187&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12791&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, build a logistic regression model to predict the dependent variable “over50k”, using all of the other variables in the dataset as independent variables. Use the training set to build the model.&lt;/p&gt;
&lt;p&gt;Which variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CensusLog &amp;lt;- glm(over50k ~ ., data = censusTrain, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(CensusLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = over50k ~ ., family = binomial, data = censusTrain)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1065  -0.5037  -0.1804  -0.0008   3.3383  

Coefficients: (1 not defined because of singularities)
                                           Estimate Std. Error z value
(Intercept)                              -8.658e+00  1.379e+00  -6.279
age                                       2.548e-02  2.139e-03  11.916
workclass Federal-gov                     1.105e+00  2.014e-01   5.489
workclass Local-gov                       3.675e-01  1.821e-01   2.018
workclass Never-worked                   -1.283e+01  8.453e+02  -0.015
workclass Private                         6.012e-01  1.626e-01   3.698
workclass Self-emp-inc                    7.575e-01  1.950e-01   3.884
workclass Self-emp-not-inc                1.855e-01  1.774e-01   1.046
workclass State-gov                       4.012e-01  1.961e-01   2.046
workclass Without-pay                    -1.395e+01  6.597e+02  -0.021
education 11th                            2.225e-01  2.867e-01   0.776
education 12th                            6.380e-01  3.597e-01   1.774
education 1st-4th                        -7.075e-01  7.760e-01  -0.912
education 5th-6th                        -3.170e-01  4.880e-01  -0.650
education 7th-8th                        -3.498e-01  3.126e-01  -1.119
education 9th                            -1.258e-01  3.539e-01  -0.355
education Assoc-acdm                      1.602e+00  2.427e-01   6.601
education Assoc-voc                       1.541e+00  2.368e-01   6.506
education Bachelors                       2.177e+00  2.218e-01   9.817
education Doctorate                       2.761e+00  2.893e-01   9.544
education HS-grad                         1.006e+00  2.169e-01   4.638
education Masters                         2.421e+00  2.353e-01  10.289
education Preschool                      -2.237e+01  6.864e+02  -0.033
education Prof-school                     2.938e+00  2.753e-01  10.672
education Some-college                    1.365e+00  2.195e-01   6.219
maritalstatus Married-AF-spouse           2.540e+00  7.145e-01   3.555
maritalstatus Married-civ-spouse          2.458e+00  3.573e-01   6.880
maritalstatus Married-spouse-absent      -9.486e-02  3.204e-01  -0.296
maritalstatus Never-married              -4.515e-01  1.139e-01  -3.962
maritalstatus Separated                   3.609e-02  1.984e-01   0.182
maritalstatus Widowed                     1.858e-01  1.962e-01   0.947
occupation Adm-clerical                   9.470e-02  1.288e-01   0.735
occupation Armed-Forces                  -1.008e+00  1.487e+00  -0.677
occupation Craft-repair                   2.174e-01  1.109e-01   1.960
occupation Exec-managerial                9.400e-01  1.138e-01   8.257
occupation Farming-fishing               -1.068e+00  1.908e-01  -5.599
occupation Handlers-cleaners             -6.237e-01  1.946e-01  -3.204
occupation Machine-op-inspct             -1.862e-01  1.376e-01  -1.353
occupation Other-service                 -8.183e-01  1.641e-01  -4.987
occupation Priv-house-serv               -1.297e+01  2.267e+02  -0.057
occupation Prof-specialty                 6.331e-01  1.222e-01   5.180
occupation Protective-serv                6.267e-01  1.710e-01   3.664
occupation Sales                          3.276e-01  1.175e-01   2.789
occupation Tech-support                   6.173e-01  1.533e-01   4.028
occupation Transport-moving                      NA         NA      NA
relationship Not-in-family                7.881e-01  3.530e-01   2.233
relationship Other-relative              -2.194e-01  3.137e-01  -0.699
relationship Own-child                   -7.489e-01  3.507e-01  -2.136
relationship Unmarried                    7.041e-01  3.720e-01   1.893
relationship Wife                         1.324e+00  1.331e-01   9.942
race Asian-Pac-Islander                   4.830e-01  3.548e-01   1.361
race Black                                3.644e-01  2.882e-01   1.265
race Other                                2.204e-01  4.513e-01   0.488
race White                                4.108e-01  2.737e-01   1.501
sex Male                                  7.729e-01  1.024e-01   7.545
capitalgain                               3.280e-04  1.372e-05  23.904
capitalloss                               6.445e-04  4.854e-05  13.277
hoursperweek                              2.897e-02  2.101e-03  13.791
nativecountry Canada                      2.593e-01  1.308e+00   0.198
nativecountry China                      -9.695e-01  1.327e+00  -0.730
nativecountry Columbia                   -1.954e+00  1.526e+00  -1.280
nativecountry Cuba                        5.735e-02  1.323e+00   0.043
nativecountry Dominican-Republic         -1.435e+01  3.092e+02  -0.046
nativecountry Ecuador                    -3.550e-02  1.477e+00  -0.024
nativecountry El-Salvador                -6.095e-01  1.395e+00  -0.437
nativecountry England                    -6.707e-02  1.327e+00  -0.051
nativecountry France                      5.301e-01  1.419e+00   0.374
nativecountry Germany                     5.474e-02  1.306e+00   0.042
nativecountry Greece                     -2.646e+00  1.714e+00  -1.544
nativecountry Guatemala                  -1.293e+01  3.345e+02  -0.039
nativecountry Haiti                      -9.221e-01  1.615e+00  -0.571
nativecountry Holand-Netherlands         -1.282e+01  2.400e+03  -0.005
nativecountry Honduras                   -9.584e-01  3.412e+00  -0.281
nativecountry Hong                       -2.362e-01  1.492e+00  -0.158
nativecountry Hungary                     1.412e-01  1.555e+00   0.091
nativecountry India                      -8.218e-01  1.314e+00  -0.625
nativecountry Iran                       -3.299e-02  1.366e+00  -0.024
nativecountry Ireland                     1.579e-01  1.473e+00   0.107
nativecountry Italy                       6.100e-01  1.333e+00   0.458
nativecountry Jamaica                    -2.279e-01  1.387e+00  -0.164
nativecountry Japan                       5.072e-01  1.375e+00   0.369
nativecountry Laos                       -6.831e-01  1.661e+00  -0.411
nativecountry Mexico                     -9.182e-01  1.303e+00  -0.705
nativecountry Nicaragua                  -1.987e-01  1.507e+00  -0.132
nativecountry Outlying-US(Guam-USVI-etc) -1.373e+01  8.502e+02  -0.016
nativecountry Peru                       -9.660e-01  1.678e+00  -0.576
nativecountry Philippines                 4.393e-02  1.281e+00   0.034
nativecountry Poland                      2.410e-01  1.383e+00   0.174
nativecountry Portugal                    7.276e-01  1.477e+00   0.493
nativecountry Puerto-Rico                -5.769e-01  1.357e+00  -0.425
nativecountry Scotland                   -1.188e+00  1.719e+00  -0.691
nativecountry South                      -8.183e-01  1.341e+00  -0.610
nativecountry Taiwan                     -2.590e-01  1.350e+00  -0.192
nativecountry Thailand                   -1.693e+00  1.737e+00  -0.975
nativecountry Trinadad&amp;amp;Tobago            -1.346e+00  1.721e+00  -0.782
nativecountry United-States              -8.594e-02  1.269e+00  -0.068
nativecountry Vietnam                    -1.008e+00  1.523e+00  -0.662
nativecountry Yugoslavia                  1.402e+00  1.648e+00   0.851
                                         Pr(&amp;gt;|z|)    
(Intercept)                              3.41e-10 ***
age                                       &amp;lt; 2e-16 ***
workclass Federal-gov                    4.03e-08 ***
workclass Local-gov                      0.043641 *  
workclass Never-worked                   0.987885    
workclass Private                        0.000218 ***
workclass Self-emp-inc                   0.000103 ***
workclass Self-emp-not-inc               0.295646    
workclass State-gov                      0.040728 *  
workclass Without-pay                    0.983134    
education 11th                           0.437738    
education 12th                           0.076064 .  
education 1st-4th                        0.361897    
education 5th-6th                        0.516008    
education 7th-8th                        0.263152    
education 9th                            0.722228    
education Assoc-acdm                     4.10e-11 ***
education Assoc-voc                      7.74e-11 ***
education Bachelors                       &amp;lt; 2e-16 ***
education Doctorate                       &amp;lt; 2e-16 ***
education HS-grad                        3.52e-06 ***
education Masters                         &amp;lt; 2e-16 ***
education Preschool                      0.973996    
education Prof-school                     &amp;lt; 2e-16 ***
education Some-college                   5.00e-10 ***
maritalstatus Married-AF-spouse          0.000378 ***
maritalstatus Married-civ-spouse         6.00e-12 ***
maritalstatus Married-spouse-absent      0.767155    
maritalstatus Never-married              7.42e-05 ***
maritalstatus Separated                  0.855672    
maritalstatus Widowed                    0.343449    
occupation Adm-clerical                  0.462064    
occupation Armed-Forces                  0.498170    
occupation Craft-repair                  0.049972 *  
occupation Exec-managerial                &amp;lt; 2e-16 ***
occupation Farming-fishing               2.15e-08 ***
occupation Handlers-cleaners             0.001353 ** 
occupation Machine-op-inspct             0.176061    
occupation Other-service                 6.14e-07 ***
occupation Priv-house-serv               0.954385    
occupation Prof-specialty                2.22e-07 ***
occupation Protective-serv               0.000248 ***
occupation Sales                         0.005282 ** 
occupation Tech-support                  5.63e-05 ***
occupation Transport-moving                    NA    
relationship Not-in-family               0.025562 *  
relationship Other-relative              0.484263    
relationship Own-child                   0.032716 *  
relationship Unmarried                   0.058392 .  
relationship Wife                         &amp;lt; 2e-16 ***
race Asian-Pac-Islander                  0.173504    
race Black                               0.206001    
race Other                               0.625263    
race White                               0.133356    
sex Male                                 4.52e-14 ***
capitalgain                               &amp;lt; 2e-16 ***
capitalloss                               &amp;lt; 2e-16 ***
hoursperweek                              &amp;lt; 2e-16 ***
nativecountry Canada                     0.842879    
nativecountry China                      0.465157    
nativecountry Columbia                   0.200470    
nativecountry Cuba                       0.965432    
nativecountry Dominican-Republic         0.962972    
nativecountry Ecuador                    0.980829    
nativecountry El-Salvador                0.662181    
nativecountry England                    0.959686    
nativecountry France                     0.708642    
nativecountry Germany                    0.966572    
nativecountry Greece                     0.122527    
nativecountry Guatemala                  0.969180    
nativecountry Haiti                      0.568105    
nativecountry Holand-Netherlands         0.995736    
nativecountry Honduras                   0.778775    
nativecountry Hong                       0.874155    
nativecountry Hungary                    0.927653    
nativecountry India                      0.531661    
nativecountry Iran                       0.980736    
nativecountry Ireland                    0.914628    
nativecountry Italy                      0.647194    
nativecountry Jamaica                    0.869467    
nativecountry Japan                      0.712179    
nativecountry Laos                       0.680866    
nativecountry Mexico                     0.481103    
nativecountry Nicaragua                  0.895132    
nativecountry Outlying-US(Guam-USVI-etc) 0.987115    
nativecountry Peru                       0.564797    
nativecountry Philippines                0.972640    
nativecountry Poland                     0.861624    
nativecountry Portugal                   0.622327    
nativecountry Puerto-Rico                0.670837    
nativecountry Scotland                   0.489616    
nativecountry South                      0.541809    
nativecountry Taiwan                     0.847878    
nativecountry Thailand                   0.329678    
nativecountry Trinadad&amp;amp;Tobago            0.434105    
nativecountry United-States              0.946020    
nativecountry Vietnam                    0.507799    
nativecountry Yugoslavia                 0.394874    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 21175  on 19186  degrees of freedom
Residual deviance: 12104  on 19090  degrees of freedom
AIC: 12298

Number of Fisher Scoring iterations: 15&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;age-workclass-education-maritalstatus-occupation-relationship-sex-capitalgain-capitalloss-housperweek&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;age, workclass, education, maritalstatus, occupation, relationship, sex, capitalgain, capitalloss, housperweek&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictLog &amp;lt;- predict(CensusLog, newdata = censusTest, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
ifelse(type == : prediction from a rank-deficient fit may be misleading&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k, predictLog &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        
         FALSE TRUE
   &amp;lt;=50K  9051  662
   &amp;gt;50K   1190 1888&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9051 + 1888) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8552107&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What is the baseline accuracy for the testing set? table(censusTest$over50k)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 &amp;lt;=50K   &amp;gt;50K 
  9713   3078 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;9713 / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7593621&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;problem-1.4---a-logistic-regression-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Problem 1.4 - A Logistic Regression Model&lt;/h4&gt;
&lt;p&gt;What is the area-under-the-curve (AUC) for this model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:stats&amp;#39;:

    lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpredLog = prediction(predictLog, censusTest$over50k)
as.numeric(performance(ROCRpredLog, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9061598&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - A CART Model&lt;/h3&gt;
&lt;p&gt;I have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.&lt;/p&gt;
&lt;p&gt;Let’s now build a classification tree to predict “over50k”. Using the training set to build the model, and all of the other variables as independent variables.&lt;/p&gt;
&lt;p&gt;Using the default parameters, so don’t set a value for minbucket or cp. Remember to specify method=“class” as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.&lt;/p&gt;
&lt;p&gt;How many splits does the tree have in total?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
censusTree &amp;lt;- rpart(over50k ~ ., data = censusTrain, method=&amp;quot;class&amp;quot;)
prp(censusTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - A CART Model&lt;/h3&gt;
&lt;p&gt;Which variable does the tree split on at the first level (the very first split of the tree)?
#### relationship&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - A CART Model&lt;/h3&gt;
&lt;p&gt;Which variables does the tree split on at the second level (immediately after the first split of the tree)?
#### capitalgain, education&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - A CART Model&lt;/h3&gt;
&lt;p&gt;What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type=“class”, or generate probabilities and use a threshold of 0.5 like in logistic regression.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictTree &amp;lt;- 
    as.vector(predict(censusTree, newdata = censusTest, type = &amp;quot;class&amp;quot;))
head(predictTree)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot; &amp;gt;50K&amp;quot;  &amp;quot; &amp;gt;50K&amp;quot;  &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;gt;50K&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k, predictTree)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        predictTree
          &amp;lt;=50K  &amp;gt;50K
   &amp;lt;=50K   9243   470
   &amp;gt;50K    1482  1596&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9243 + 1596) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8473927&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - A CART Model&lt;/h3&gt;
&lt;p&gt;Let’s now consider the ROC curve and AUC for the CART model on the test-set. I will need to get predicted probabilities for the observations in the test-set to build the ROC curve and compute the AUC. Remember that you can do this by removing the type=“class” argument when making predictions, and taking the second column of the resulting object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictTreeProb &amp;lt;- predict(censusTree, newdata = censusTest)
head(predictTreeProb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       &amp;lt;=50K       &amp;gt;50K
2  0.2794982 0.72050176
5  0.2794982 0.72050176
7  0.9490143 0.05098572
8  0.6972807 0.30271934
11 0.6972807 0.30271934
12 0.2794982 0.72050176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predictTreeProb[, 2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         2          5          7          8         11         12 
0.72050176 0.72050176 0.05098572 0.30271934 0.30271934 0.72050176 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the ROC curve for the CART model you have estimated. Observe that compared to the logistic regression ROC curve, the CART ROC curve is less smooth than the logistic regression ROC curve.&lt;/p&gt;
&lt;p&gt;Which of the following explanations for this behavior is most correct? (HINT: Think about what the ROC curve is plotting and what changing the threshold does.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpredTree = prediction(predictTreeProb[, 2], censusTest$over50k)
ROCRperfTree = performance(ROCRpredTree, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(ROCRperfTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-probabilities-from-the-cart-model-take-only-a-handful-of-values-five-one-for-each-end-bucketleaf-of-the-tree-the-changes-in-the-roc-curve-correspond-to-setting-the-threshold-to-one-of-those-values.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The probabilities from the CART model take only a handful of values (five, one for each end bucket/leaf of the tree); the changes in the ROC curve correspond to setting the threshold to one of those values.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.6---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.6 - A CART Model&lt;/h3&gt;
&lt;p&gt;What is the AUC of the CART model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(performance(ROCRpredTree, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8470256&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;Before building a random forest model, I’ll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression.&lt;/p&gt;
&lt;p&gt;For this reason, before continuing I’ll define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
trainSmall &amp;lt;- censusTrain[sample(nrow(censusTrain), 2000), ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now build a random forest model to predict “over50k”, using the dataset “trainSmall” as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest “cannot handle categorical predictors with more than 32 categories”, re-build the model without the nativecountry variable as one of the independent variables.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
CensusForest &amp;lt;- randomForest(over50k ~ ., data = trainSmall)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, make predictions using this model on the entire test-set.&lt;/p&gt;
&lt;p&gt;What is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that you don’t need a “type” argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that our accuracy might be different from the one reported here, since random forest models can still differ depending on our operating system, even when the random seed is set.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictForest &amp;lt;- predict(CensusForest, newdata = censusTest)
table(censusTest$over50k, predictForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        predictForest
          &amp;lt;=50K  &amp;gt;50K
   &amp;lt;=50K   8843   870
   &amp;gt;50K    1029  2049&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9586 + 1093) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8348839&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;As we discussed, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important.&lt;/p&gt;
&lt;p&gt;However, we can still compute metrics that give us insight into which variables are important. One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split.&lt;/p&gt;
&lt;p&gt;To view this metric, run the following lines of code (replace “MODEL” with the name of your random forest model):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vu &amp;lt;- varUsed(CensusForest, count=TRUE)
vusorted &amp;lt;- sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(CensusForest$forest$xlevels[vusorted$ix]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis).&lt;/p&gt;
&lt;p&gt;Which of the following variables is the most important in terms of the number of splits?
#### age&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;A different metric we can look at is related to “impurity”, which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased.&lt;/p&gt;
&lt;p&gt;Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest.&lt;/p&gt;
&lt;p&gt;To compute this metric, run the following code (replace “MODEL” with the name of your random forest model):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(CensusForest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which one of the following variables is the most important in terms of mean reduction in impurity?
#### occupation&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;We now conclude our analysis of this dataset by looking at how CART behaves with different choices of its parameters. Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments.&lt;/p&gt;
&lt;p&gt;Also, remember we using the entire training set “train” when building this model. The train function might take some time to run.&lt;/p&gt;
&lt;p&gt;Which value of cp does the train function recommend?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;ggplot2&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:randomForest&amp;#39;:

    margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)
# number of folds
tr.control = trainControl(method = &amp;quot;cv&amp;quot;, number = 10)

# cp values
cartGrid &amp;lt;- expand.grid( .cp = seq(0.002,0.1,0.002))
cartGrid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     .cp
1  0.002
2  0.004
3  0.006
4  0.008
5  0.010
6  0.012
7  0.014
8  0.016
9  0.018
10 0.020
11 0.022
12 0.024
13 0.026
14 0.028
15 0.030
16 0.032
17 0.034
18 0.036
19 0.038
20 0.040
21 0.042
22 0.044
23 0.046
24 0.048
25 0.050
26 0.052
27 0.054
28 0.056
29 0.058
30 0.060
31 0.062
32 0.064
33 0.066
34 0.068
35 0.070
36 0.072
37 0.074
38 0.076
39 0.078
40 0.080
41 0.082
42 0.084
43 0.086
44 0.088
45 0.090
46 0.092
47 0.094
48 0.096
49 0.098
50 0.100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cross-validation
set.seed(2)
tr = train(over50k ~ ., data = censusTrain, method = &amp;quot;rpart&amp;quot;, 
           trControl = tr.control, tuneGrid = cartGrid)
tr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;CART 

19187 samples
   12 predictor
    2 classes: &amp;#39; &amp;lt;=50K&amp;#39;, &amp;#39; &amp;gt;50K&amp;#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 17268, 17268, 17269, 17269, 17269, 17268, ... 
Resampling results across tuning parameters:

  cp     Accuracy   Kappa     
  0.002  0.8510972  0.55404931
  0.004  0.8482829  0.55537475
  0.006  0.8452078  0.53914084
  0.008  0.8442176  0.53817486
  0.010  0.8433317  0.53305978
  0.012  0.8433317  0.53305978
  0.014  0.8433317  0.53305978
  0.016  0.8413510  0.52349296
  0.018  0.8400480  0.51528594
  0.020  0.8381193  0.50351272
  0.022  0.8381193  0.50351272
  0.024  0.8381193  0.50351272
  0.026  0.8381193  0.50351272
  0.028  0.8381193  0.50351272
  0.030  0.8381193  0.50351272
  0.032  0.8381193  0.50351272
  0.034  0.8352011  0.48749911
  0.036  0.8326470  0.47340390
  0.038  0.8267570  0.44688035
  0.040  0.8248289  0.43893150
  0.042  0.8248289  0.43893150
  0.044  0.8248289  0.43893150
  0.046  0.8248289  0.43893150
  0.048  0.8248289  0.43893150
  0.050  0.8231084  0.42467058
  0.052  0.8174798  0.37478096
  0.054  0.8138837  0.33679015
  0.056  0.8118514  0.30751485
  0.058  0.8118514  0.30751485
  0.060  0.8118514  0.30751485
  0.062  0.8118514  0.30751485
  0.064  0.8118514  0.30751485
  0.066  0.8099233  0.29697206
  0.068  0.7971025  0.22226318
  0.070  0.7958512  0.21465656
  0.072  0.7958512  0.21465656
  0.074  0.7958512  0.21465656
  0.076  0.7689601  0.05701508
  0.078  0.7593684  0.00000000
  0.080  0.7593684  0.00000000
  0.082  0.7593684  0.00000000
  0.084  0.7593684  0.00000000
  0.086  0.7593684  0.00000000
  0.088  0.7593684  0.00000000
  0.090  0.7593684  0.00000000
  0.092  0.7593684  0.00000000
  0.094  0.7593684  0.00000000
  0.096  0.7593684  0.00000000
  0.098  0.7593684  0.00000000
  0.100  0.7593684  0.00000000

Accuracy was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.002.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;Fit a CART model to the training data using this value of cp. What is the prediction accuracy on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;censusTree2 &amp;lt;- 
    rpart(over50k ~ ., data = censusTrain, method=&amp;quot;class&amp;quot;, cp = 0.002)
predictTree2 &amp;lt;- 
    as.vector(predict(censusTree2, newdata = censusTest, type = &amp;quot;class&amp;quot;))
table(censusTest$over50k, predictTree2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        predictTree2
          &amp;lt;=50K  &amp;gt;50K
   &amp;lt;=50K   9178   535
   &amp;gt;50K    1240  1838&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9178 + 1838) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8612306&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;Compared to the original accuracy using the default value of cp, this new CART model is an improvement, and so we should clearly favor this new model over the old one – or should we?&lt;/p&gt;
&lt;p&gt;Plot the CART tree for this model. How many splits are there?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(censusTree2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;18&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This highlights one important trade-off in building predictive models. By tuning cp, we improved our accuracy by over 1%, but our tree became significantly more complicated. In some applications, such an improvement in accuracy would be worth the loss in interpretability. In others, we may prefer a less accurate model that is simpler to understand and describe over a more accurate – but more complicated – model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Letter Reecognition</title>
      <link>/project/letter_recognition/letters/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/letter_recognition/letters/</guid>
      <description>


&lt;p&gt;One of the earliest applications of predictive analytics methods applied so far was to automatically recognize letters, which post office machines use to sort mail. In this analysis, I’ll build a model that uses statistics of images of four letters in the Roman alphabet – A, B, P, and R – to predict which letter a particular image corresponds to.&lt;/p&gt;
&lt;p&gt;Note, that this is a multiclass classification problem. We have mostly focused on binary classification problems (e.g., predicting whether an individual voted or not, whether the Supreme Court will affirm or reverse a case, whether or not a person is at risk for a certain disease, etc.). In this problem, we have more than two classifications that are possible for each observation.&lt;/p&gt;
&lt;p&gt;The file letters_ABPR.csv contains 3116 observations, each of which corresponds to a certain image of one of the four letters A, B, P and R. The images came from 20 different fonts, which were then randomly distorted to produce the final images; each such distorted image is represented as a collection of pixels, each of which is “on” or “off”.&lt;/p&gt;
&lt;p&gt;For each such distorted image, we have available certain statistics of the image in terms of these pixels, as well as which of the four letters the image is. This data comes from the UCI ML Repository.&lt;/p&gt;
&lt;p&gt;This dataset contains the following 17 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;letter = the letter that the image corresponds to (A, B, P or R)&lt;/li&gt;
&lt;li&gt;xbox = the horizontal position of where the smallest box covering the letter shape begins.&lt;/li&gt;
&lt;li&gt;ybox = the vertical position of where the smallest box covering the letter shape begins.&lt;/li&gt;
&lt;li&gt;width = the width of this smallest box.&lt;/li&gt;
&lt;li&gt;height = the height of this smallest box.&lt;/li&gt;
&lt;li&gt;onpix = the total number of “on” pixels in the character image&lt;/li&gt;
&lt;li&gt;xbar = the mean horizontal position of all of the “on” pixels&lt;/li&gt;
&lt;li&gt;ybar = the mean vertical position of all of the “on” pixels&lt;/li&gt;
&lt;li&gt;x2bar = the mean squared horizontal position of all of the “on” pixels in the image&lt;/li&gt;
&lt;li&gt;y2bar = the mean squared vertical position of all of the “on” pixels in the image&lt;/li&gt;
&lt;li&gt;xybar = the mean of the product of the horizontal and vertical position of all of the “on” pixels in the image&lt;/li&gt;
&lt;li&gt;x2ybar = the mean of the product of the squared horizontal position and the vertical position of all of the “on” pixels&lt;/li&gt;
&lt;li&gt;xy2bar = the mean of the product of the horizontal position and the squared vertical position of all of the “on” pixels&lt;/li&gt;
&lt;li&gt;xedge = the mean number of edges (the number of times an “off” pixel is followed by an “on” pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image&lt;/li&gt;
&lt;li&gt;xedgeycor = the mean of the product of the number of horizontal edges at each vertical position and the vertical position&lt;/li&gt;
&lt;li&gt;yedge = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image&lt;/li&gt;
&lt;li&gt;yedgexcor = the mean of the product of the number of vertical edges at each horizontal position and the horizontal position&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;load-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load the dataset&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters &amp;lt;- read.csv(&amp;quot;letters_ABPR.csv&amp;quot;)
summary(letters)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; letter       xbox             ybox            width       
 A:789   Min.   : 0.000   Min.   : 0.000   Min.   : 1.000  
 B:766   1st Qu.: 3.000   1st Qu.: 5.000   1st Qu.: 4.000  
 P:803   Median : 4.000   Median : 7.000   Median : 5.000  
 R:758   Mean   : 3.915   Mean   : 7.051   Mean   : 5.186  
         3rd Qu.: 5.000   3rd Qu.: 9.000   3rd Qu.: 6.000  
         Max.   :13.000   Max.   :15.000   Max.   :11.000  
     height           onpix             xbar             ybar       
 Min.   : 0.000   Min.   : 0.000   Min.   : 3.000   Min.   : 0.000  
 1st Qu.: 4.000   1st Qu.: 2.000   1st Qu.: 6.000   1st Qu.: 6.000  
 Median : 6.000   Median : 4.000   Median : 7.000   Median : 7.000  
 Mean   : 5.276   Mean   : 3.869   Mean   : 7.469   Mean   : 7.197  
 3rd Qu.: 7.000   3rd Qu.: 5.000   3rd Qu.: 8.000   3rd Qu.: 9.000  
 Max.   :12.000   Max.   :12.000   Max.   :14.000   Max.   :15.000  
     x2bar            y2bar           xybar            x2ybar     
 Min.   : 0.000   Min.   :0.000   Min.   : 3.000   Min.   : 0.00  
 1st Qu.: 3.000   1st Qu.:2.000   1st Qu.: 7.000   1st Qu.: 3.00  
 Median : 4.000   Median :4.000   Median : 8.000   Median : 5.00  
 Mean   : 4.706   Mean   :3.903   Mean   : 8.491   Mean   : 4.52  
 3rd Qu.: 6.000   3rd Qu.:5.000   3rd Qu.:10.000   3rd Qu.: 6.00  
 Max.   :11.000   Max.   :8.000   Max.   :14.000   Max.   :10.00  
     xy2bar           xedge          xedgeycor          yedge     
 Min.   : 0.000   Min.   : 0.000   Min.   : 1.000   Min.   : 0.0  
 1st Qu.: 6.000   1st Qu.: 2.000   1st Qu.: 7.000   1st Qu.: 3.0  
 Median : 7.000   Median : 2.000   Median : 8.000   Median : 4.0  
 Mean   : 6.711   Mean   : 2.913   Mean   : 7.763   Mean   : 4.6  
 3rd Qu.: 8.000   3rd Qu.: 4.000   3rd Qu.: 9.000   3rd Qu.: 6.0  
 Max.   :14.000   Max.   :10.000   Max.   :13.000   Max.   :12.0  
   yedgexcor     
 Min.   : 1.000  
 1st Qu.: 7.000  
 Median : 8.000  
 Mean   : 8.418  
 3rd Qu.:10.000  
 Max.   :13.000  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(letters)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3116 obs. of  17 variables:
 $ letter   : Factor w/ 4 levels &amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;,&amp;quot;P&amp;quot;,&amp;quot;R&amp;quot;: 2 1 4 2 3 4 4 1 3 3 ...
 $ xbox     : int  4 1 5 5 3 8 2 3 8 6 ...
 $ ybox     : int  2 1 9 9 6 10 6 7 14 10 ...
 $ width    : int  5 3 5 7 4 8 4 5 7 8 ...
 $ height   : int  4 2 7 7 4 6 4 5 8 8 ...
 $ onpix    : int  4 1 6 10 2 6 3 3 4 7 ...
 $ xbar     : int  8 8 6 9 4 7 6 12 5 8 ...
 $ ybar     : int  7 2 11 8 14 7 7 2 10 5 ...
 $ x2bar    : int  6 2 7 4 8 3 5 3 6 7 ...
 $ y2bar    : int  6 2 3 4 1 5 5 2 3 5 ...
 $ xybar    : int  7 8 7 6 11 8 6 10 12 7 ...
 $ x2ybar   : int  6 2 3 8 6 4 5 2 5 6 ...
 $ xy2bar   : int  6 8 9 6 3 8 7 9 4 6 ...
 $ xedge    : int  2 1 2 6 0 6 3 2 4 3 ...
 $ xedgeycor: int  8 6 7 11 10 6 7 6 10 9 ...
 $ yedge    : int  7 2 5 8 4 7 5 3 4 8 ...
 $ yedgexcor: int  10 7 11 7 8 7 8 8 8 9 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---predicting-b-or-not-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Predicting B or not B&lt;/h3&gt;
&lt;p&gt;Let’s warm up by attempting to predict just whether a letter is B or not. To begin, load the file letters_ABPR.csv into R, and call it letters. Then, create a new variable isB in the dataframe, which takes the value “TRUE” if the observation corresponds to the letter B, and “FALSE” if it does not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters$isB &amp;lt;- as.factor(letters$letter == &amp;quot;B&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, split the dataset into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. The first argument to sample.split should be the dependent variable “letters$isB”. Remember that TRUE values from sample.split should go in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
set.seed(1000)
lettersSplit = sample.split(letters$isB, SplitRatio = 0.5)
lettersTrain = subset(letters, lettersSplit == TRUE)
lettersTest = subset(letters, lettersSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before building models, let’s consider a baseline method that always predicts the most frequent outcome, which is “not B”.&lt;/p&gt;
&lt;p&gt;What is the accuracy of this baseline method on the test set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(letters$isB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
FALSE  TRUE 
 2350   766 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(lettersTest$isB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
FALSE  TRUE 
 1175   383 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1175 / (1175 + 383)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.754172&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---predicting-b-or-not-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Predicting B or not B&lt;/h3&gt;
&lt;p&gt;Now, build a classification tree to predict whether a letter is a B or not, using the training set to build the model. Remember to remove the variable “letter” out of the model, as this is related to what we are trying to predict!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
CARTb &amp;lt;- rpart(isB ~ . - letter, data = lettersTrain, method=&amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are just using the default parameters in our CART model, so we don’t need to add the minbucket or cp arguments at all. We also added the argument method=“class” since this is a classification problem.&lt;/p&gt;
&lt;p&gt;What is the accuracy of the CART model on the test-set? (Use type=“class” when making predictions on the test set.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bPredict &amp;lt;- predict(CARTb, newdata = lettersTest, type = &amp;quot;class&amp;quot;)
table(lettersTest$isB, bPredict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       bPredict
        FALSE TRUE
  FALSE  1118   57
  TRUE     43  340&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1126 + 342) / nrow(lettersTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9422336&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---predicting-b-or-not-b&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Predicting B or Not B&lt;/h3&gt;
&lt;p&gt;Now, build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. Using all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). Using the default settings for ntree and nodesize (don’t include these arguments at all). Right before building the model, set the seed to 1000. (NOTE: You might get a slightly different answer on this problem, even if you set the random seed. This has to do with your operating system and the implementation of the random forest algorithm.)&lt;/p&gt;
&lt;p&gt;What is the accuracy of the model on the test set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bForestPredict &amp;lt;- predict(bForest, newdata = lettersTest, type = &amp;quot;class&amp;quot;)
table(lettersTest$isB, bForestPredict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       bForestPredict
        FALSE TRUE
  FALSE  1163   12
  TRUE      9  374&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(1164 + 372) / nrow(lettersTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9858793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Random forests tends to improve on CART in terms of predictive accuracy. Sometimes, this improvement can be quite significant, as it is here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---predicting-the-letters-a-b-p-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Predicting the letters A, B, P, R&lt;/h3&gt;
&lt;p&gt;Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.&lt;/p&gt;
&lt;p&gt;As we saw earlier, building a multiclass classification CART model is no harder than building the models for binary classification problems. Fortunately, building a random forest model is just as easy. The variable in our dataframe which we will be trying to predict is “letter”.&lt;/p&gt;
&lt;p&gt;Start by converting letter in the original dataset (letters) to a factor by running the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters$letter &amp;lt;- as.factor(letters$letter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, generate new training and testing sets of the letters dataframe using letters$letter as the first input to the sample.split function. Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2000)
lettersAllSplit &amp;lt;- sample.split(letters$letter, SplitRatio = 0.5)
lettersAllTrain &amp;lt;- subset(letters, lettersAllSplit == TRUE)
lettersAllTest &amp;lt;- subset(letters, lettersAllSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.&lt;/p&gt;
&lt;p&gt;What is the baseline accuracy on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(letters$letter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  A   B   P   R 
789 766 803 758 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(lettersAllTest$letter)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  A   B   P   R 
395 383 401 379 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;401 / nrow(lettersAllTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2573813&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;p-is-the-most-frequent-class-in-the-test-set&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;P is the most frequent class in the test set&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---predicting-the-letters-a-b-p-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Predicting the letters A, B, P, R&lt;/h3&gt;
&lt;p&gt;Now build a classification tree to predict “letter”, using the training set to build our model. You should use all of the other variables as independent variables, except “isB”, since it is related to what we are trying to predict!&lt;/p&gt;
&lt;p&gt;Just use the default parameters in your CART model. Add the argument method=“class” since this is a classification problem. Even though we have multiple classes here, nothing changes in how we build the model from the binary case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTletters &amp;lt;- rpart(letter ~ . - isB, data = lettersAllTrain, method=&amp;quot;class&amp;quot;)
summary(CARTletters)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Call:
rpart(formula = letter ~ . - isB, data = lettersAllTrain, method = &amp;quot;class&amp;quot;)
  n= 1558 

          CP nsplit rel error    xerror       xstd
1 0.31920415      0 1.0000000 1.0346021 0.01442043
2 0.25865052      1 0.6807958 0.6323529 0.01704001
3 0.18685121      2 0.4221453 0.4238754 0.01585412
4 0.02595156      3 0.2352941 0.2370242 0.01299919
5 0.02076125      4 0.2093426 0.2162630 0.01253234
6 0.01730104      5 0.1885813 0.1980969 0.01209034
7 0.01384083      6 0.1712803 0.1894464 0.01186782
8 0.01211073      7 0.1574394 0.1678201 0.01127370
9 0.01000000      8 0.1453287 0.1608997 0.01107113

Variable importance
     ybar xedgeycor    x2ybar    xy2bar     yedge     y2bar     xedge 
       17        16        14        12        11         8         7 
    xybar     x2bar      xbar 
        5         5         3 

Node number 1: 1558 observations,    complexity param=0.3192042
  predicted class=P  expected loss=0.7419769  P(node) =1
    class counts:   394   383   402   379
   probabilities: 0.253 0.246 0.258 0.243 
  left son=2 (1088 obs) right son=3 (470 obs)
  Primary splits:
      xedgeycor &amp;lt; 8.5  to the left,  improve=293.2010, (0 missing)
      ybar      &amp;lt; 5.5  to the left,  improve=287.8322, (0 missing)
      xy2bar    &amp;lt; 5.5  to the right, improve=278.1742, (0 missing)
      x2ybar    &amp;lt; 2.5  to the left,  improve=262.6356, (0 missing)
      yedge     &amp;lt; 4.5  to the left,  improve=177.0582, (0 missing)
  Surrogate splits:
      xy2bar &amp;lt; 5.5  to the right, agree=0.892, adj=0.643, (0 split)
      ybar   &amp;lt; 8.5  to the left,  agree=0.821, adj=0.406, (0 split)
      xedge  &amp;lt; 1.5  to the right, agree=0.816, adj=0.391, (0 split)
      xybar  &amp;lt; 10.5 to the left,  agree=0.785, adj=0.287, (0 split)
      x2ybar &amp;lt; 6.5  to the left,  agree=0.777, adj=0.262, (0 split)

Node number 2: 1088 observations,    complexity param=0.2586505
  predicted class=A  expected loss=0.6488971  P(node) =0.6983312
    class counts:   382   338    13   355
   probabilities: 0.351 0.311 0.012 0.326 
  left son=4 (344 obs) right son=5 (744 obs)
  Primary splits:
      ybar      &amp;lt; 5.5  to the left,  improve=275.7625, (0 missing)
      x2ybar    &amp;lt; 2.5  to the left,  improve=240.6702, (0 missing)
      y2bar     &amp;lt; 2.5  to the left,  improve=226.4519, (0 missing)
      yedge     &amp;lt; 3.5  to the left,  improve=215.2610, (0 missing)
      xedgeycor &amp;lt; 7.5  to the right, improve=171.4917, (0 missing)
  Surrogate splits:
      x2ybar &amp;lt; 2.5  to the left,  agree=0.904, adj=0.698, (0 split)
      y2bar  &amp;lt; 2.5  to the left,  agree=0.892, adj=0.657, (0 split)
      yedge  &amp;lt; 3.5  to the left,  agree=0.881, adj=0.625, (0 split)
      x2bar  &amp;lt; 2.5  to the left,  agree=0.820, adj=0.430, (0 split)
      xbar   &amp;lt; 9.5  to the right, agree=0.779, adj=0.302, (0 split)

Node number 3: 470 observations,    complexity param=0.01730104
  predicted class=P  expected loss=0.1723404  P(node) =0.3016688
    class counts:    12    45   389    24
   probabilities: 0.026 0.096 0.828 0.051 
  left son=6 (91 obs) right son=7 (379 obs)
  Primary splits:
      xybar  &amp;lt; 7.5  to the left,  improve=59.48719, (0 missing)
      xy2bar &amp;lt; 6.5  to the right, improve=54.86112, (0 missing)
      ybar   &amp;lt; 7.5  to the left,  improve=49.49367, (0 missing)
      yedge  &amp;lt; 6.5  to the right, improve=48.42295, (0 missing)
      xedge  &amp;lt; 5.5  to the left,  improve=30.83057, (0 missing)
  Surrogate splits:
      xy2bar &amp;lt; 6.5  to the right, agree=0.936, adj=0.670, (0 split)
      ybar   &amp;lt; 7.5  to the left,  agree=0.902, adj=0.495, (0 split)
      xedge  &amp;lt; 5.5  to the right, agree=0.889, adj=0.429, (0 split)
      yedge  &amp;lt; 6.5  to the right, agree=0.885, adj=0.407, (0 split)
      onpix  &amp;lt; 6.5  to the right, agree=0.838, adj=0.165, (0 split)

Node number 4: 344 observations
  predicted class=A  expected loss=0.04360465  P(node) =0.2207959
    class counts:   329     9     3     3
   probabilities: 0.956 0.026 0.009 0.009 

Node number 5: 744 observations,    complexity param=0.1868512
  predicted class=R  expected loss=0.5268817  P(node) =0.4775353
    class counts:    53   329    10   352
   probabilities: 0.071 0.442 0.013 0.473 
  left son=10 (342 obs) right son=11 (402 obs)
  Primary splits:
      xedgeycor &amp;lt; 7.5  to the right, improve=139.70670, (0 missing)
      xy2bar    &amp;lt; 7.5  to the left,  improve= 92.43059, (0 missing)
      x2ybar    &amp;lt; 5.5  to the right, improve= 81.07422, (0 missing)
      y2bar     &amp;lt; 4.5  to the right, improve= 56.45671, (0 missing)
      yedgexcor &amp;lt; 10.5 to the left,  improve= 52.58754, (0 missing)
  Surrogate splits:
      x2ybar &amp;lt; 5.5  to the right, agree=0.738, adj=0.430, (0 split)
      xy2bar &amp;lt; 6.5  to the left,  agree=0.675, adj=0.292, (0 split)
      xedge  &amp;lt; 2.5  to the left,  agree=0.675, adj=0.292, (0 split)
      yedge  &amp;lt; 5.5  to the right, agree=0.644, adj=0.225, (0 split)
      ybar   &amp;lt; 7.5  to the left,  agree=0.625, adj=0.184, (0 split)

Node number 6: 91 observations,    complexity param=0.01384083
  predicted class=B  expected loss=0.5604396  P(node) =0.05840822
    class counts:    10    40    20    21
   probabilities: 0.110 0.440 0.220 0.231 
  left son=12 (55 obs) right son=13 (36 obs)
  Primary splits:
      x2bar     &amp;lt; 3.5  to the right, improve=14.308240, (0 missing)
      xy2bar    &amp;lt; 7.5  to the left,  improve= 9.472092, (0 missing)
      yedge     &amp;lt; 4.5  to the left,  improve= 9.449763, (0 missing)
      x2ybar    &amp;lt; 7.5  to the right, improve= 8.053076, (0 missing)
      yedgexcor &amp;lt; 6.5  to the right, improve= 7.478284, (0 missing)
  Surrogate splits:
      yedgexcor &amp;lt; 5.5  to the right, agree=0.736, adj=0.333, (0 split)
      x2ybar    &amp;lt; 7.5  to the left,  agree=0.725, adj=0.306, (0 split)
      yedge     &amp;lt; 5.5  to the right, agree=0.725, adj=0.306, (0 split)
      xy2bar    &amp;lt; 8.5  to the left,  agree=0.714, adj=0.278, (0 split)
      ybar      &amp;lt; 7.5  to the left,  agree=0.681, adj=0.194, (0 split)

Node number 7: 379 observations
  predicted class=P  expected loss=0.02638522  P(node) =0.2432606
    class counts:     2     5   369     3
   probabilities: 0.005 0.013 0.974 0.008 

Node number 10: 342 observations,    complexity param=0.02595156
  predicted class=B  expected loss=0.2192982  P(node) =0.2195122
    class counts:    14   267    10    51
   probabilities: 0.041 0.781 0.029 0.149 
  left son=20 (283 obs) right son=21 (59 obs)
  Primary splits:
      xy2bar    &amp;lt; 7.5  to the left,  improve=48.65030, (0 missing)
      xedge     &amp;lt; 2.5  to the left,  improve=33.98799, (0 missing)
      y2bar     &amp;lt; 4.5  to the right, improve=27.13499, (0 missing)
      yedgexcor &amp;lt; 6.5  to the left,  improve=15.49245, (0 missing)
      ybar      &amp;lt; 8.5  to the left,  improve=15.03303, (0 missing)
  Surrogate splits:
      xedge     &amp;lt; 5.5  to the left,  agree=0.871, adj=0.254, (0 split)
      yedgexcor &amp;lt; 4.5  to the right, agree=0.854, adj=0.153, (0 split)
      ybar      &amp;lt; 9.5  to the left,  agree=0.848, adj=0.119, (0 split)
      xbox      &amp;lt; 6.5  to the left,  agree=0.842, adj=0.085, (0 split)
      ybox      &amp;lt; 11.5 to the left,  agree=0.842, adj=0.085, (0 split)

Node number 11: 402 observations,    complexity param=0.02076125
  predicted class=R  expected loss=0.2512438  P(node) =0.2580231
    class counts:    39    62     0   301
   probabilities: 0.097 0.154 0.000 0.749 
  left son=22 (26 obs) right son=23 (376 obs)
  Primary splits:
      yedge     &amp;lt; 2.5  to the left,  improve=35.46191, (0 missing)
      x2ybar    &amp;lt; 0.5  to the left,  improve=34.14932, (0 missing)
      y2bar     &amp;lt; 1.5  to the left,  improve=33.87850, (0 missing)
      x2bar     &amp;lt; 3.5  to the left,  improve=19.57685, (0 missing)
      yedgexcor &amp;lt; 8.5  to the left,  improve=19.07812, (0 missing)
  Surrogate splits:
      y2bar  &amp;lt; 1.5  to the left,  agree=0.993, adj=0.885, (0 split)
      x2ybar &amp;lt; 0.5  to the left,  agree=0.993, adj=0.885, (0 split)

Node number 12: 55 observations
  predicted class=B  expected loss=0.3090909  P(node) =0.03530167
    class counts:     1    38    13     3
   probabilities: 0.018 0.691 0.236 0.055 

Node number 13: 36 observations
  predicted class=R  expected loss=0.5  P(node) =0.02310655
    class counts:     9     2     7    18
   probabilities: 0.250 0.056 0.194 0.500 

Node number 20: 283 observations
  predicted class=B  expected loss=0.08480565  P(node) =0.1816431
    class counts:     3   259     8    13
   probabilities: 0.011 0.915 0.028 0.046 

Node number 21: 59 observations
  predicted class=R  expected loss=0.3559322  P(node) =0.03786906
    class counts:    11     8     2    38
   probabilities: 0.186 0.136 0.034 0.644 

Node number 22: 26 observations
  predicted class=A  expected loss=0.03846154  P(node) =0.01668806
    class counts:    25     0     0     1
   probabilities: 0.962 0.000 0.000 0.038 

Node number 23: 376 observations,    complexity param=0.01211073
  predicted class=R  expected loss=0.2021277  P(node) =0.241335
    class counts:    14    62     0   300
   probabilities: 0.037 0.165 0.000 0.798 
  left son=46 (26 obs) right son=47 (350 obs)
  Primary splits:
      yedge  &amp;lt; 7.5  to the right, improve=19.73450, (0 missing)
      x2ybar &amp;lt; 5.5  to the right, improve=16.32647, (0 missing)
      xybar  &amp;lt; 8.5  to the right, improve=15.20779, (0 missing)
      xedge  &amp;lt; 3.5  to the right, improve=14.35240, (0 missing)
      onpix  &amp;lt; 4.5  to the right, improve=12.94437, (0 missing)
  Surrogate splits:
      xedgeycor &amp;lt; 4.5  to the left,  agree=0.939, adj=0.115, (0 split)

Node number 46: 26 observations
  predicted class=B  expected loss=0.3076923  P(node) =0.01668806
    class counts:     4    18     0     4
   probabilities: 0.154 0.692 0.000 0.154 

Node number 47: 350 observations
  predicted class=R  expected loss=0.1542857  P(node) =0.224647
    class counts:    10    44     0   296
   probabilities: 0.029 0.126 0.000 0.846 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(CARTletters)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/letter_recognition/letters_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is the test-set accuracy of our CART model? Use the argument type=“class” when making predictions. (HINT: When you are computing the test-set accuracy using the confusion matrix, you want to add everything on the main diagonal and divide by the total number of observations in the test-set, which can be computed with nrow(test), where test is the name of our test-set).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lettersPredict &amp;lt;- 
    as.vector(predict(CARTletters, newdata = lettersAllTest, type = &amp;quot;class&amp;quot;))
length(lettersPredict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1558&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lettersPredict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   [1] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
  [18] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
  [35] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
  [52] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
  [69] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
  [86] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
 [103] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [120] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [137] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [154] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [171] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [188] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [205] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [222] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [239] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [256] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
 [273] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [290] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [307] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [324] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [341] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [358] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [375] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [392] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [409] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [426] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
 [443] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [460] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [477] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [494] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [511] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [528] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [545] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [562] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [579] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [596] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [613] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [630] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [647] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [664] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
 [681] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [698] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [715] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [732] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [749] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [766] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
 [783] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [800] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [817] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [834] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [851] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
 [868] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [885] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
 [902] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [919] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [936] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [953] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [970] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [987] &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
[1004] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
[1021] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1038] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
[1055] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
[1072] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
[1089] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
[1106] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
[1123] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
[1140] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1157] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1174] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;
[1191] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
[1208] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1225] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1242] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
[1259] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1276] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
[1293] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
[1310] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;
[1327] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
[1344] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
[1361] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
[1378] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1395] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
[1412] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1429] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
[1446] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
[1463] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
[1480] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1497] &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1514] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
[1531] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1548] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(lettersAllTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1558&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(lettersAllTest$letter, lettersPredict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   lettersPredict
      A   B   P   R
  A 348   4   0  43
  B   8 318  12  45
  P   2  21 363  15
  R  10  24   5 340&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(348 + 318 + 363 + 340) / nrow(lettersAllTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8786906&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---predicting-the-letters-a-b-p-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Predicting the letters A, B, P, R&lt;/h3&gt;
&lt;p&gt;Now build a random forest model on the training data, using the same independent variables as in the previous problem – again, don’t forget to remove the isB variable.&lt;/p&gt;
&lt;p&gt;Just use the default parameter values for ntree and node size (you don’t need to include these arguments at all). Set the seed to 1000 right before building our model. (Remember that you might get a slightly different result even if you set the random seed.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1000)
lettersForest &amp;lt;- randomForest(letter ~ . - isB, data = lettersAllTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the test-set accuracy of your random forest model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lettersForestPredict &amp;lt;- 
    as.vector(predict(lettersForest, newdata = lettersAllTest, type = &amp;quot;class&amp;quot;))
lettersForestPredict&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   [1] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
  [18] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
  [35] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
  [52] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
  [69] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
  [86] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
 [103] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [120] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [137] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [154] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [171] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [188] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [205] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [222] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [239] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [256] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
 [273] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [290] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [307] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [324] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [341] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
 [358] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [375] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [392] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [409] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [426] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
 [443] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [460] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
 [477] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [494] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [511] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [528] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [545] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [562] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
 [579] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
 [596] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [613] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [630] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [647] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [664] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
 [681] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [698] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
 [715] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
 [732] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
 [749] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [766] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [783] &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
 [800] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
 [817] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
 [834] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot;
 [851] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
 [868] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [885] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
 [902] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [919] &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [936] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
 [953] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
 [970] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
 [987] &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
[1004] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
[1021] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1038] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1055] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
[1072] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1089] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
[1106] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
[1123] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
[1140] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1157] &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1174] &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;
[1191] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot;
[1208] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot;
[1225] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1242] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
[1259] &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot;
[1276] &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;
[1293] &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;
[1310] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot;
[1327] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot;
[1344] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
[1361] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
[1378] &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot;
[1395] &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot;
[1412] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1429] &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot;
[1446] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot;
[1463] &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot;
[1480] &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot;
[1497] &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;R&amp;quot;
[1514] &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot;
[1531] &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;B&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;B&amp;quot;
[1548] &amp;quot;P&amp;quot; &amp;quot;P&amp;quot; &amp;quot;A&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot; &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;A&amp;quot; &amp;quot;P&amp;quot; &amp;quot;R&amp;quot; &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(lettersAllTest$letter, lettersForestPredict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   lettersForestPredict
      A   B   P   R
  A 391   0   3   1
  B   0 380   1   2
  P   0   6 394   1
  R   3  14   0 362&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(390 + 380 + 393 + 364) / nrow(lettersAllTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9801027&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;You should find this value rather striking, for several reasons. The first is that it is significantly higher than the value for CART, highlighting the gain in accuracy that is possible from using random forest models.&lt;/p&gt;
&lt;p&gt;The second is that while the accuracy of CART decreased significantly as we transitioned from the problem of predicting B or not B (a relatively simple problem) to the problem of predicting the four letters (certainly a harder problem), the accuracy of the random forest model decreased by a tiny amount.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Why People Vote?</title>
      <link>/project/understanding_votes/votes/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/understanding_votes/votes/</guid>
      <description>


&lt;p&gt;In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is &lt;strong&gt;social, or extrinsic, pressure.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To quote the first paragraph of their 2008 research paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Among the most striking features of a democratic political system is the participation of millions of voters in elections. Why do large numbers of people vote, despite the fact that …”the casting of a single vote is of no significance where there is a multitude of electors“? One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail to participate in elections. Voters’ sense of civic duty has long been a leading explanation of voters turnout…”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this analysis, I’ll use both logistic regression and classification trees to analyze the data they collected.&lt;/p&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data&lt;/h3&gt;
&lt;p&gt;The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a “control” group, and the rest were categorized into one of four “treatment” groups. These five groups correspond to five binary variables in the dataset.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Civic Duty” (variable civicduty) group members were sent a letter that simply said “DO YOUR CIVIC DUTY - VOTE!”&lt;/li&gt;
&lt;li&gt;“Hawthorne Effect” (variable hawthorne) group members were sent a letter that had the “Civic Duty” message plus the additional message “YOU ARE BEING STUDIED” and they were informed that their voting behavior would be examined by means of public records.&lt;/li&gt;
&lt;li&gt;“Self” (variable self) group members received the “Civic Duty” message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records.&lt;/li&gt;
&lt;li&gt;“Neighbors” (variable neighbors) group members were given the same message as that for the “Self” group, except the message not only had the household voting records but, also that of neighbors - maximizing social pressure.&lt;/li&gt;
&lt;li&gt;“Control” (variable control) group members were not sent anything, and represented the typical voting situation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;We will first get familiar with the data.&lt;/p&gt;
&lt;p&gt;What proportion of people in this dataset voted in this election?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gerber &amp;lt;- read.csv(&amp;quot;gerber.csv&amp;quot;)
str(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   344084 obs. of  8 variables:
 $ sex      : int  0 1 1 1 0 1 0 0 1 0 ...
 $ yob      : int  1941 1947 1982 1950 1951 1959 1956 1981 1968 1967 ...
 $ voting   : int  0 0 1 1 1 1 1 0 0 0 ...
 $ hawthorne: int  0 0 1 1 1 0 0 0 0 0 ...
 $ civicduty: int  1 1 0 0 0 0 0 0 0 0 ...
 $ neighbors: int  0 0 0 0 0 0 0 0 0 0 ...
 $ self     : int  0 0 0 0 0 0 0 0 0 0 ...
 $ control  : int  0 0 0 0 0 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     0      1 
235388 108696 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;108696 / (235388 + 108696)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3158996&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Which of the four “treatment groups” had the largest percentage of people who actually voted (voting = 1)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# howthorne
table(gerber$voting, gerber$hawthorne)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
         0      1
  0 209500  25888
  1  96380  12316&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12316 / (25888 + 12316)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3223746&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# civicduty
table(gerber$voting, gerber$civicduty)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
         0      1
  0 209191  26197
  1  96675  12021&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12021 / (26197 + 12021)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3145377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# neighbors
table(gerber$voting, gerber$neighbors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
         0      1
  0 211625  23763
  1  94258  14438&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14438 / (23763 + 14438)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3779482&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# self
table(gerber$voting, gerber$self)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
         0      1
  0 210361  25027
  1  95505  13191&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;13191 / (25027 + 13191)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3451515&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;neighbors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Neighbors&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Using all the data to build the model (NOT spliting the data into a training set and testing set).&lt;/p&gt;
&lt;p&gt;Which of the following coefficients are significant in the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VotingLog &amp;lt;- glm(voting ~ civicduty + hawthorne + self + neighbors, 
                 data = gerber, family = binomial)
summary(VotingLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = voting ~ civicduty + hawthorne + self + neighbors, 
    family = binomial, data = gerber)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9744  -0.8691  -0.8389   1.4586   1.5590  

Coefficients:
             Estimate Std. Error  z value Pr(&amp;gt;|z|)    
(Intercept) -0.863358   0.005006 -172.459  &amp;lt; 2e-16 ***
civicduty    0.084368   0.012100    6.972 3.12e-12 ***
hawthorne    0.120477   0.012037   10.009  &amp;lt; 2e-16 ***
self         0.222937   0.011867   18.786  &amp;lt; 2e-16 ***
neighbors    0.365092   0.011679   31.260  &amp;lt; 2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 429238  on 344083  degrees of freedom
Residual deviance: 428090  on 344079  degrees of freedom
AIC: 428100

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;all-coefficients-are-significant&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;All coefficients are significant&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don’t need to use the new data argument since we didn’t split our data.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictVoting &amp;lt;- predict(VotingLog, type = &amp;quot;response&amp;quot;)
table(gerber$voting, predictVoting &amp;gt; 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
     FALSE   TRUE
  0 134513 100875
  1  56730  51966&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(134513 + 51966) / nrow(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5419578&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.5---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.5 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Using a threshold of 0.5, what is the accuracy of the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting, predictVoting &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
     FALSE
  0 235388
  1 108696&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(235388) / nrow(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6841004&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
     0      1 
235388 108696 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;235388 / (235388 + 108696)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6841004&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;equal-to-accuracy-of-threshold-of-0.5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.6841004 =&amp;gt; equal to accuracy of threshold of 0.5&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.6---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.6 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Compare our previous two answers to the percentage of people who did not vote (the baseline accuracy) and computing the AUC of the model. What is happening here?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:stats&amp;#39;:

    lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred = prediction(predictVoting, gerber$voting)
as.numeric(performance(ROCRpred, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5308461&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;even-though-all-of-the-variables-are-significant-this-is-a-weak-predictive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Even though all of the variables are significant, this is a weak predictive model.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Trees&lt;/h3&gt;
&lt;p&gt;I’ll now try out trees! Building a CART tree for voting using all data and the same four treatment variables we used before. Don’t set the option method=“class” - we are actually going to create a &lt;strong&gt;regression tree&lt;/strong&gt; here.&lt;/p&gt;
&lt;p&gt;We are interested in building a tree to explore the fraction of people who vote, or the probability of voting.&lt;/p&gt;
&lt;p&gt;I’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different).&lt;/p&gt;
&lt;p&gt;However, with regression trees, CART will split even if both groups have probability less than 50%. Leave all the parameters at their default values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
CARTmodel &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors, data = gerber)
# plot the tree. What happens, and if relevant, why?
prp(CARTmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;no-variables-are-used-the-tree-is-only-a-root-node---none-of-the-variables-make-a-big-enough-effect-to-be-split-on.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No variables are used (the tree is only a root node) - none of the variables make a big enough effect to be split on.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Trees&lt;/h3&gt;
&lt;p&gt;Now build the tree:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTmodel2 &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors, 
          data=gerber, cp=0.0)
prp(CARTmodel2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What do we observe about the order of the splits?
#### Neighbor is the first split, civic duty is the last.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Trees&lt;/h3&gt;
&lt;p&gt;Using only the CART tree plot, we note that the fraction (a number between 0 and 1) of “Civic Duty” people voted amounted to:
#### 31%&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Trees&lt;/h3&gt;
&lt;p&gt;Building a new tree that includes the “sex” variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTmodel3 &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, 
          data=gerber, cp=0.0)
prp(CARTmodel3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the control group, which gender is more likely to vote?
#### Men (0)&lt;/p&gt;
&lt;p&gt;In the “Civic Duty” group, which gender is more likely to vote?
#### Men (0)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;We know trees can handle “nonlinear” relationships, e.g. “in the ‘Civic Duty’ group and female”, but as we will see in the next few questions, it is possible to do the same for logistic regression.&lt;/p&gt;
&lt;p&gt;Firstly, let’s explore what trees can tell us. Let’s just focus on the “Control” treatment group. Creating a regression tree using just the “control” variable, then creating another tree with the “control” and “sex” variables, both with cp=0.0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTcontrol &amp;lt;- rpart(voting ~ control, data = gerber, cp = 0.0)
CARTcontrolAndSex &amp;lt;- rpart(voting ~ control + sex, data = gerber, cp = 0.0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the “control” only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group?&lt;/p&gt;
&lt;p&gt;Using the absolute value function to get an answer, i.e. abs(Control Prediction - Non-Control Prediction). I’ll add the argument “digits = 6” to the prp code to get a more accurate estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(CARTcontrol, digits = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.296638 - 0.34)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.043362&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.043362&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group (being in any of the four treatment groups):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(CARTcontrolAndSex, digits = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;they-are-affected-about-the-same-change-in-probability-within-0.001-of-each-other.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;They are affected about the same (change in probability within 0.001 of each other).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Going back to logistic regression now, I’ll build a model using “sex” and “control”. Interpreting the coefficient for “sex”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VotingControlAndSexLog &amp;lt;- 
    glm(voting ~ control + sex, data = gerber, family = binomial)
summary(VotingControlAndSexLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = voting ~ control + sex, family = binomial, data = gerber)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9220  -0.9012  -0.8290   1.4564   1.5717  

Coefficients:
             Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept) -0.635538   0.006511 -97.616  &amp;lt; 2e-16 ***
control     -0.200142   0.007364 -27.179  &amp;lt; 2e-16 ***
sex         -0.055791   0.007343  -7.597 3.02e-14 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 429238  on 344083  degrees of freedom
Residual deviance: 428443  on 344081  degrees of freedom
AIC: 428449

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;coefficient-is-negative-reflecting-that-women-are-less-likely-to-vote&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Coefficient is negative, reflecting that women are less likely to vote!&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control).&lt;/p&gt;
&lt;p&gt;Logistic regression has attempted to do the same, although it wasn’t able to do as well because it can’t consider exactly the joint possibility of being a women and in the control group.&lt;/p&gt;
&lt;p&gt;I can quantify this precisely. By creating the following dataframe (this contains all of the possible values of sex and control), and evaluating our logistic regression using the predict function (where “LogModelSex” is the name of our logistic regression model that uses both control and sex):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Possibilities &amp;lt;- data.frame(sex=c(0,0,1,1), control=c(0,1,0,1))
predict(VotingControlAndSexLog, newdata=Possibilities, type=&amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4 
0.3462559 0.3024455 0.3337375 0.2908065 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ).&lt;/p&gt;
&lt;p&gt;What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case?&lt;/p&gt;
&lt;p&gt;The answer contains five numbers after the decimal point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.290456 - 0.2908065)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.0003505&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(0.0003505, digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.00035&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;So the difference is not too big for this dataset, but it’s there. I’m going to add a new term to our logistic regression now, that is the combination of the “sex” and “control” variables - so if this new variable is 1, that means the person is a woman AND in the control group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LogModel2 &amp;lt;- glm(voting ~ sex + control + sex:control, 
                 data = gerber, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do I interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LogModel2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = voting ~ sex + control + sex:control, family = &amp;quot;binomial&amp;quot;, 
    data = gerber)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9213  -0.9019  -0.8284   1.4573   1.5724  

Coefficients:
             Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept) -0.637471   0.007603 -83.843  &amp;lt; 2e-16 ***
sex         -0.051888   0.010801  -4.804 1.55e-06 ***
control     -0.196553   0.010356 -18.980  &amp;lt; 2e-16 ***
sex:control -0.007259   0.014729  -0.493    0.622    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 429238  on 344083  degrees of freedom
Residual deviance: 428442  on 344080  degrees of freedom
AIC: 428450

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;if-a-person-is-a-woman-and-in-the-control-group-the-chance-that-she-voted-goes-down.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;If a person is a woman and in the control group, the chance that she voted goes down.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.6---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.6 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Run the same code as before to calculate the average for each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(LogModel2, newdata=Possibilities, type=&amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4 
0.3458183 0.3027947 0.3341757 0.2904558 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, what is the difference between the logistic regression model and the CART model for the (Woman, Control) case?&lt;/p&gt;
&lt;p&gt;Again, our answer has five numbers after the decimal point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.290456 - 0.2904558)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(2e-07, digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion - Interaction Terms&lt;/h3&gt;
&lt;p&gt;This example has shown that trees can capture nonlinear relationships that logistic regression cannot, but that we can get around this sometimes by using variables that are the combination of two variables.&lt;/p&gt;
&lt;p&gt;Should we always include all possible interaction terms of the independent variables when building a logistic regression model?
#### No (because of overfitting)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Vandalism Detection On Wikipedia</title>
      <link>/project/vandalism/vandalism/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/vandalism/vandalism/</guid>
      <description>


&lt;p&gt;Wikipedia is a free online encyclopedia that anyone can edit and contribute to. It’s available in many languages and is growing all the time. On the English language version of Wikipedia:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are currently 4.7 million pages.&lt;/li&gt;
&lt;li&gt;There have been a total over 760 million edits (also called revisions) over its lifetime.&lt;/li&gt;
&lt;li&gt;There are approximately 130,000 edits per day.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the consequences of being editable by anyone is that some people vandalize pages.&lt;/p&gt;
&lt;p&gt;This can take the form of &lt;strong&gt;removing content, adding promotional or inappropriate content, or more subtle shifts that change the meaning of the article.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;With these many articles and edits per day it is difficult for humans to detect all instances of vandalism and revert (undo) them. As a result, Wikipedia uses bots - computer programs that automatically revert edits that look like vandalism. In this analysis I’ll attempt to develop a vandalism detector that uses ML to distinguish between a valid edit and vandalism.&lt;/p&gt;
&lt;p&gt;The data for this analysis is based on the revision history of the page Language. Wikipedia provides a history for each page that consists of the state of the page at each revision. Rather than manually considering each revision, a script was run that checked whether edits stayed or were reverted. If a change was eventually reverted then that revision is marked as vandalism. This may result in some misclassifications, but the script performs well enough for our needs.&lt;/p&gt;
&lt;p&gt;As a result of this preprocessing, some common processing tasks have already been done, including lower-casing and punctuation removal. The columns in the dataset are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vandal = 1 if this edit was vandalism, 0 if not.&lt;/li&gt;
&lt;li&gt;Minor = 1 if the user marked this edit as a “minor edit”, 0 if not.&lt;/li&gt;
&lt;li&gt;Loggedin = 1 if the user made this edit while using a Wikipedia account, 0 if they did not.&lt;/li&gt;
&lt;li&gt;Added = The unique words added.&lt;/li&gt;
&lt;li&gt;Removed = The unique words removed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice the repeated use of unique. The data we have available is not the traditional bag of words - rather it is the set of words that were removed or added. For example, if a word was removed multiple times in a revision it will only appear one time in the “Removed” column.&lt;/p&gt;
&lt;div id=&#34;loading-the-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the packages&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install packages if necessary
list.of.packages &amp;lt;- c(&amp;quot;SnowballC&amp;quot;)
new.packages &amp;lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&amp;quot;Package&amp;quot;])]
if(length(new.packages)) install.packages(new.packages)

## load packages
library(tm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: NLP&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(SnowballC)
library(caTools) # to use sample.split function.
library(rpart)
library(rpart.plot)
library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The following object is masked from &amp;#39;package:stats&amp;#39;:

    lowess&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;problem-1.1---bags-of-words&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Problem 1.1 - Bags of Words&lt;/h4&gt;
&lt;p&gt;Load the data wiki.csv with the option stringsAsFactors=FALSE, calling the dataframe “wiki”. Convert the “Vandal” column to a factor using the code wiki&lt;span class=&#34;math inline&#34;&gt;\(Vandal = as.factor(wiki\)&lt;/span&gt;Vandal).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wiki &amp;lt;- read.csv(&amp;quot;wiki.csv&amp;quot;, stringsAsFactors = FALSE)
wiki$Vandal &amp;lt;- as.factor(wiki$Vandal)
str(wiki)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3876 obs. of  7 variables:
 $ X.1     : int  1 2 3 4 5 6 7 8 9 10 ...
 $ X       : int  1 2 3 4 5 6 7 8 9 10 ...
 $ Vandal  : Factor w/ 2 levels &amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
 $ Minor   : int  1 1 0 1 1 0 0 0 1 0 ...
 $ Loggedin: int  1 1 1 0 1 1 1 1 1 0 ...
 $ Added   : chr  &amp;quot;  represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethno&amp;quot;| __truncated__ &amp;quot; website external links&amp;quot; &amp;quot; &amp;quot; &amp;quot; afghanistan used iran mostly that farsiis is countries some xmlspacepreservepersian parts tajikestan region&amp;quot; ...
 $ Removed : chr  &amp;quot; &amp;quot; &amp;quot; talklanguagetalk&amp;quot; &amp;quot; regarded as technologytechnologies human first&amp;quot; &amp;quot;  represent psycholinguisticspsycholinguistics orthographyorthography help all actions through ethnologue relat&amp;quot;| __truncated__ ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many cases of vandalism were detected in the history of this page?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wiki$Vandal)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
2061 1815 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1815&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Bags of Words&lt;/h3&gt;
&lt;p&gt;We will now use the bag of words approach to build a model. We have two columns of textual data, with different meanings. For example, adding rude words has a different meaning to removing rude words. The text already is lowercase and stripped of punctuation. So to pre-process the data, just complete the following four steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1) Create the corpus for the Added column, and call it &amp;quot;corpusAdded&amp;quot;.
corpusAdded &amp;lt;- Corpus(VectorSource(wiki$Added))
corpusAdded[[1]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;  represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethnologue relationships linguistics regarded writing languages to other listing xmlspacepreservelanguages metaverse formal term philology common each including phonologyphonology often ten list humans affiliation see computer are speechpathologyspeech our what for ways dialects please artificial written body be of quite hypothesis found alone refers by about language profanity study programming priorities rosenfelders technologytechnologies makes or first among useful languagephilosophy one sounds use area create phrases mark their genetic basic families complete but sapirwhorfhypothesissapirwhorf with talklanguagetalk population animals this science up vocal can concepts called at and topics locations as numbers have in pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy most important meaningful a historicallinguisticsorphilologyhistorical semanticssemantics patterns the oral&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2) Remove the English-language stopwords.
corpusAdded &amp;lt;- tm_map(corpusAdded, removeWords, stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAdded, removeWords,
stopwords(&amp;quot;english&amp;quot;)): transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAdded[[1]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;  represent psycholinguisticspsycholinguistics orthographyorthography help text  actions  human ethnologue relationships linguistics regarded writing languages   listing xmlspacepreservelanguages metaverse formal term philology common  including phonologyphonology often ten list humans affiliation see computer  speechpathologyspeech    ways dialects please artificial written body   quite hypothesis found alone refers   language profanity study programming priorities rosenfelders technologytechnologies makes  first among useful languagephilosophy one sounds use area create phrases mark  genetic basic families complete  sapirwhorfhypothesissapirwhorf  talklanguagetalk population animals  science  vocal can concepts called   topics locations  numbers   pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy  important meaningful  historicallinguisticsorphilologyhistorical semanticssemantics patterns  oral&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3) Stem the words.
corpusAdded &amp;lt;- tm_map(corpusAdded, stemDocument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusAdded, stemDocument): transformation
drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusAdded[[1]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;repres psycholinguisticspsycholinguist orthographyorthographi help text action human ethnologu relationship linguist regard write languag list xmlspacepreservelanguag metavers formal term philolog common includ phonologyphonolog often ten list human affili see comput speechpathologyspeech way dialect pleas artifici written bodi quit hypothesi found alon refer languag profan studi program prioriti rosenfeld technologytechnolog make first among use languagephilosophi one sound use area creat phrase mark genet basic famili complet sapirwhorfhypothesissapirwhorf talklanguagetalk popul anim scienc vocal can concept call topic locat number patholog differ develop 4000 thing idea group complex anim mathemat fair literatur httpwwwzompistcom philosophi import meaning historicallinguisticsorphilologyhistor semanticssemant pattern oral&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4) Build the DocumentTermMatrix, and call it dtmAdded.
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 3876, terms: 6675)&amp;gt;&amp;gt;
Non-/sparse entries: 15368/25856932
Sparsity           : 100%
Maximal term length: 784
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many terms appear in dtmAdded?
#### 6675&lt;/p&gt;
&lt;p&gt;If the code length(stopwords(“english”)) does not return 174 for you, then please run the line of code in this file, which will store the standard stop words in a variable called sw. When removing stop words, use tm_map(corpusAdded, removeWords, sw) instead of tm_map(corpusAdded, removeWords, stopwords(“english”)).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 174&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sparseAdded &amp;lt;- removeSparseTerms(dtmAdded, 0.997)
sparseAdded&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 3876, terms: 166)&amp;gt;&amp;gt;
Non-/sparse entries: 2681/640735
Sparsity           : 100%
Maximal term length: 28
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many terms appear in sparseAdded?
#### 166&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Convert sparseAdded to a dataframe called wordsAdded, and then prepend all the words with the letter A, by using the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wordsAdded &amp;lt;- as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) &amp;lt;- paste(&amp;quot;A&amp;quot;, colnames(wordsAdded))
str(wordsAdded)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3876 obs. of  166 variables:
 $ A bodi                        : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A call                        : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A complet                     : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A concept                     : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A creat                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A develop                     : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A differ                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A famili                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A first                       : num  1 0 0 0 0 0 0 0 0 0 ...
 $ A group                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A help                        : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A idea                        : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A import                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A includ                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A linguist                    : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A make                        : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A number                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A pattern                     : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A popul                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A refer                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A relationship                : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A repres                      : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A sound                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A studi                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A ten                         : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A use                         : num  2 0 0 1 2 0 0 0 0 0 ...
 $ A write                       : num  1 0 0 0 1 0 0 0 0 0 ...
 $ A part                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ A name                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A communic                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A defin                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A express                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A mean                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A must                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A set                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A featur                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A form                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A symbol                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A talk                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A type                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A learn                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A general                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A exampl                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A provid                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A speak                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A thought                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A cours                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A german                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A onlin                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A biolog                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A close                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A communiti                   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A distinct                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A dutch                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A intellig                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A least                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A note                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A parallel                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A possibl                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A principl                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A sentenc                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A standard                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A stem                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A understood                  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A utter                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A arbitrari                   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A combin                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A cultur                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A discuss                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A invent                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A properti                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A research                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A unit                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A work                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A direct                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A relat                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A agre                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A exist                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A wide                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A describ                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A literari                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A purpos                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A fantasi                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A promin                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A manipul                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A tolkien                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A follow                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A regular                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A gestur                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A similar                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A imposs                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A interact                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A person                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A reason                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A writer                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A phenomenon                  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A accord                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A individu                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ A object                      : num  0 0 0 0 0 0 0 0 0 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now repeat all of the steps we’ve done so far (create a corpus, remove stop words, stem the document, create a sparse document term matrix, and convert it to a dataframe) to create a Removed bag-of-words dataframe, called wordsRemoved, except this time, prepend all of the words with the letter R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1) Create the corpus for the Removed column, and call it &amp;quot;corpusRemoved&amp;quot;.
corpusRemoved &amp;lt;- Corpus(VectorSource(wiki$Removed))
corpusRemoved[[3]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot; regarded as technologytechnologies human first&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2) Remove the English-language stopwords.
corpusRemoved &amp;lt;- tm_map(corpusRemoved, removeWords, stopwords(&amp;quot;english&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusRemoved, removeWords,
stopwords(&amp;quot;english&amp;quot;)): transformation drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusRemoved[[3]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot; regarded  technologytechnologies human first&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 3) Stem the words.
corpusRemoved &amp;lt;- tm_map(corpusRemoved, stemDocument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tm_map.SimpleCorpus(corpusRemoved, stemDocument): transformation
drops documents&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corpusRemoved[[3]]$content&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;regard technologytechnolog human first&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4) Build the DocumentTermMatrix, and call it dtmRemoved.
dtmRemoved &amp;lt;- DocumentTermMatrix(corpusRemoved)
dtmRemoved&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 3876, terms: 5403)&amp;gt;&amp;gt;
Non-/sparse entries: 13293/20928735
Sparsity           : 100%
Maximal term length: 784
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5) Sparse document term matrix
sparseRemoved &amp;lt;- removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;&amp;lt;DocumentTermMatrix (documents: 3876, terms: 162)&amp;gt;&amp;gt;
Non-/sparse entries: 2552/625360
Sparsity           : 100%
Maximal term length: 28
Weighting          : term frequency (tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 6) Create dataframe and preppend the colnames with the letter &amp;quot;R&amp;quot;
wordsRemoved &amp;lt;- as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) &amp;lt;- paste(&amp;quot;R&amp;quot;, colnames(wordsRemoved))
str(wordsRemoved)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3876 obs. of  162 variables:
 $ R first                       : num  0 0 1 0 0 0 0 0 0 0 ...
 $ R bodi                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R call                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R complet                     : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R concept                     : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R creat                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R develop                     : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R differ                      : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R famili                      : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R group                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R idea                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R includ                      : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R linguist                    : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R make                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R pattern                     : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R popul                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R quit                        : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R refer                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R relationship                : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R repres                      : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R sound                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R studi                       : num  0 0 0 1 0 0 0 0 0 0 ...
 $ R use                         : num  0 0 0 2 1 0 0 0 0 0 ...
 $ R part                        : num  0 0 0 0 1 0 0 0 0 0 ...
 $ R communic                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R defin                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R express                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R mean                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R must                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R name                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R featur                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R set                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R symbol                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R type                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R know                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R method                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R quot                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R suggest                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R want                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R speak                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R agre                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R regular                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R actual                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R spanish                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R cours                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R onlin                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R fuck                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R person                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R believ                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R direct                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R experiment                  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R will                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R deriv                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R gestur                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R least                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R logic                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R mainlinguist                : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R possibl                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R reason                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R result                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R standard                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R thought                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R tri                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R say                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R origin                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R process                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R languageenglish             : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R analog                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R subject                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R learn                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R peopl                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R discuss                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R biologyanalog               : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R govern                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R linguisticssent             : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R object                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R sentenc                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R verb                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R compar                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R get                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R provid                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R serv                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R clear                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R intern                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R combin                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R distinct                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R relat                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R map                         : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R nation                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R care                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R geograph                    : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R notion                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R present                     : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R appar                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R close                       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R wide                        : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R bigger                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R follow                      : num  0 0 0 0 0 0 0 0 0 0 ...
 $ R recent                      : num  0 0 0 0 0 0 0 0 0 0 ...
  [list output truncated]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many words are in the wordsRemoved dataframe?
#### 162&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.5---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.5 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Combine the two dataframes into a dataframe called wikiWords with the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords &amp;lt;- cbind(wordsAdded, wordsRemoved)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The cbind function combines two sets of variables for the same observations into one dataframe. Then add the Vandal column. Set the random seed to 123 and then split the dataset using sample.split from the “caTools” package to put 70% in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords$Vandal &amp;lt;- wiki$Vandal
set.seed(123)
wikiSplit &amp;lt;- sample.split(wikiWords$Vandal, 0.7)
wikiTrain &amp;lt;- subset(wikiWords, wikiSplit == TRUE)
wikiTest &amp;lt;- subset(wikiWords, wikiSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wikiTest$Vandal)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
618 545 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;618 / (618 + 545)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5313844&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.6---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.6 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Build a CART model to predict Vandal, using all of the other variables as independent variables. Use the training set to build the model and the default parameters (don’t set values for minbucket or cp).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTree &amp;lt;- rpart(Vandal ~ ., data = wikiTrain, method = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that if you add the argument type=“class” when making predictions, the output of predict will automatically use a threshold of 0.5.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiPred &amp;lt;- as.vector(predict(wikiTree, newdata = wikiTest, type=&amp;quot;class&amp;quot;))
head(wikiPred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wikiTest$Vandal, wikiPred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   wikiPred
      0   1
  0 614   4
  1 526  19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(618 + 12) / nrow(wikiTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5417025&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.7---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.7 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Plot the CART tree.&lt;/p&gt;
&lt;p&gt;How many word stems does the CART model use?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(wikiTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/vandalism/vandalism_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.8---bags-of-words&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.8 - Bags of Words&lt;/h3&gt;
&lt;p&gt;Given the performance of the CART model relative to the baseline, what is the best explanation for these results?
#### Although it beats the baseline, bag of words is not very predictive for this problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---problem-specific-knowledge&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Problem-specific Knowledge&lt;/h3&gt;
&lt;p&gt;We weren’t able to improve on the baseline using the raw textual information. More specifically, the words themselves were not useful. There are other options though, and in this section we will try two techniques - identifying a key class of words, and counting words. The key class of words we will use are website addresses. “Website addresses” (also known as URLs - Uniform Resource Locators) are comprised of two main parts. An example would be “&lt;a href=&#34;http://www.google.com&#34; class=&#34;uri&#34;&gt;http://www.google.com&lt;/a&gt;”. The first part is the protocol, which is usually “http” (HyperText Transfer Protocol). The second part is the address of the site, e.g. “www.google.com”.&lt;/p&gt;
&lt;p&gt;We have stripped all punctuation so links to websites appear in the data as one word, e.g. “httpwwwgooglecom”. We hypothesize that given, given that a lot of vandalism seems to be adding links to promotional or irrelevant websites. The presence of a web address is a sign of vandalism. We can search for the presence of a web address in the words added by searching for “http” in the Added column. The grepl function returns TRUE if a string is found in another string, e.g.
* grepl(“cat”,“dogs and cats”,fixed=TRUE) # TRUE
* grepl(“cat”,“dogs and rats”,fixed=TRUE) # FALSE&lt;/p&gt;
&lt;p&gt;Create a copy of your dataframe from the previous question:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords2 &amp;lt;- wikiWords

# make a new column in wikiWords2 that is 1 if &amp;quot;http&amp;quot; was in Added:
wikiWords2$HTTP &amp;lt;- ifelse(grepl(&amp;quot;http&amp;quot;, wiki$Added, fixed = TRUE), 1, 0)

# based on this new column, how many revisions added a link?
table(wikiWords2$HTTP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
3659  217 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;217&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---problem-specific-knowledge&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Problem-Specific Knowledge&lt;/h3&gt;
&lt;p&gt;In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTrain2 &amp;lt;- subset(wikiWords2, wikiSplit == TRUE)
wikiTest2 &amp;lt;- subset(wikiWords2, wikiSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create a new CART model using this new variable as one of the independent variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTree2 &amp;lt;- rpart(Vandal ~ ., data = wikiTrain2, method = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new accuracy of the CART model on the test-set, using a threshold of 0.5?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiPred2 &amp;lt;- as.vector(predict(wikiTree2, newdata = wikiTest2, type=&amp;quot;class&amp;quot;))
head(wikiPred2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;1&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wikiTest2$Vandal, wikiPred2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   wikiPred2
      0   1
  0 605  13
  1 481  64&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(609 + 57) / nrow(wikiTest2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5726569&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---problem-specific-knowledge&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Problem-Specific Knowledge&lt;/h3&gt;
&lt;p&gt;Another possibility is that the number of words added and removed is predictive, perhaps more so than the actual words themselves. We already have a word count available in the form of the document-term matrices (DTMs). Sum the rows of dtmAdded and dtmRemoved and add them as new variables in our dataframe wikiWords2 (called NumWordsAdded and NumWordsRemoved):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords2$NumWordsAdded &amp;lt;- rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved &amp;lt;- rowSums(as.matrix(dtmRemoved))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the average number of words added?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(wikiWords2$NumWordsAdded)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 4.050052&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---problem-specific-knowledge&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Problem-Specific Knowledge&lt;/h3&gt;
&lt;p&gt;In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords2.&lt;/p&gt;
&lt;p&gt;Create the CART model again (using the training set and the default parameters).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTrain2b &amp;lt;- subset(wikiWords2, wikiSplit == TRUE)
wikiTest2b &amp;lt;- subset(wikiWords2, wikiSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create a new CART model using this new variable as one of the independent variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTree2b &amp;lt;- rpart(Vandal ~ ., data = wikiTrain2b, method = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new accuracy of the CART model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiPred2b &amp;lt;- as.vector(predict(wikiTree2b, newdata = wikiTest2b, type=&amp;quot;class&amp;quot;))
head(wikiPred2b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;1&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wikiTest2b$Vandal, wikiPred2b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   wikiPred2b
      0   1
  0 514 104
  1 297 248&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(514 + 248) / nrow(wikiTest2b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6552021&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---using-non-textual-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Using Non-Textual Data&lt;/h3&gt;
&lt;p&gt;We have two pieces of “metadata” (data about data) that we haven’t yet used.&lt;/p&gt;
&lt;p&gt;Make a copy of wikiWords2, and call it wikiWords3:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords3 &amp;lt;- wikiWords2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then add the two original variables Minor and Loggedin to this new dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiWords3$Minor &amp;lt;- wiki$Minor
wikiWords3$Loggedin &amp;lt;- wiki$Loggedin&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTrain3 &amp;lt;- subset(wikiWords3, wikiSplit == TRUE)
wikiTest3 &amp;lt;- subset(wikiWords3, wikiSplit == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Build a CART model using all the training data.&lt;/p&gt;
&lt;p&gt;What is the accuracy of the model on the test-set?&lt;/p&gt;
&lt;p&gt;Then create a new CART model using this new variable as one of the independentvariables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wikiTree3 &amp;lt;- rpart(Vandal ~ ., data = wikiTrain3, method = &amp;quot;class&amp;quot;)
wikiPred3 &amp;lt;- as.vector(predict(wikiTree3, newdata = wikiTest3, type=&amp;quot;class&amp;quot;))
head(wikiPred3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;0&amp;quot; &amp;quot;1&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(wikiTest3$Vandal, wikiPred3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   wikiPred3
      0   1
  0 595  23
  1 304 241&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(595 + 241) / nrow(wikiTest3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7188306&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---using-non-textual-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Using Non-Textual Data&lt;/h3&gt;
&lt;p&gt;There is a substantial difference in the accuracy of the model using the meta data. Is this because we made a more complicated model?&lt;/p&gt;
&lt;p&gt;Plot the CART tree. How many splits are there in the tree?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(wikiTree3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/vandalism/vandalism_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict Loan Repayments</title>
      <link>/project/loan_repayment/loan_repayments/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/loan_repayment/loan_repayments/</guid>
      <description>


&lt;p&gt;In the lending industry, investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, if the borrower is unable to repay the loan, then the lender loses money. Therefore, lenders face the problem of predicting the risk of a borrower being unable to repay a loan.&lt;/p&gt;
&lt;p&gt;To address this analysis, I’ll use publicly available data from &lt;strong&gt;LendingClub.com,&lt;/strong&gt; a website that connects borrowers and investors over the Internet. This dataset represents 9,578 3-year loans that were funded through the &lt;strong&gt;LendingClub.com&lt;/strong&gt; platform between May 2007 and February 2010.&lt;/p&gt;
&lt;p&gt;The binary dependent variable not.fully.paid indicates that the loan was not paid back in full (the borrower either defaulted or the loan was “charged off,” meaning the borrower was deemed unlikely to ever pay it back).&lt;/p&gt;
&lt;p&gt;To predict this dependent variable, I’ll use the following independent variables available to the investor when deciding whether to fund a loan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.
purpose: The purpose of the loan (takes values “credit_card”, “debt_consolidation”, “educational”, “major_purchase”, “small_business”, and “all_other”).&lt;/li&gt;
&lt;li&gt;int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.
installment: The monthly installments ($) owed by the borrower if the loan is funded.&lt;/li&gt;
&lt;li&gt;log.annual.inc: The natural log of the self-reported annual income of the borrower.&lt;/li&gt;
&lt;li&gt;dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).&lt;/li&gt;
&lt;li&gt;fico: The FICO credit score of the borrower.
days.with.cr.line: The number of days the borrower has had a credit line.
revol.bal: The borrower’s revolving balance (amount unpaid at the end of the credit card billing cycle).&lt;/li&gt;
&lt;li&gt;revol.util: The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available).&lt;/li&gt;
&lt;li&gt;inq.last.6mths: The borrower’s number of inquiries by creditors in the last 6 months.&lt;/li&gt;
&lt;li&gt;delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.&lt;/li&gt;
&lt;li&gt;pub.rec: The borrower’s number of derogatory public records (bankruptcy filings, tax liens, or judgments).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Load the dataset loans.csv into a dataframe called loans, and explore it using the str() and summary() functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loans &amp;lt;- read.csv(&amp;quot;loans.csv&amp;quot;)
str(loans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   9578 obs. of  14 variables:
 $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 3 2 3 3 2 2 3 1 5 3 ...
 $ int.rate         : num  0.119 0.107 0.136 0.101 0.143 ...
 $ installment      : num  829 228 367 162 103 ...
 $ log.annual.inc   : num  11.4 11.1 10.4 11.4 11.3 ...
 $ dti              : num  19.5 14.3 11.6 8.1 15 ...
 $ fico             : int  737 707 682 712 667 727 667 722 682 707 ...
 $ days.with.cr.line: num  5640 2760 4710 2700 4066 ...
 $ revol.bal        : int  28854 33623 3511 33667 4740 50807 3839 24220 69909 5630 ...
 $ revol.util       : num  52.1 76.7 25.6 73.2 39.5 51 76.8 68.6 51.1 23 ...
 $ inq.last.6mths   : int  0 0 1 1 0 0 0 0 1 1 ...
 $ delinq.2yrs      : int  0 0 0 0 1 0 0 0 0 0 ...
 $ pub.rec          : int  0 0 0 0 0 0 1 0 0 0 ...
 $ not.fully.paid   : int  0 0 0 0 0 0 1 1 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(loans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; credit.policy                 purpose        int.rate     
 Min.   :0.000   all_other         :2331   Min.   :0.0600  
 1st Qu.:1.000   credit_card       :1262   1st Qu.:0.1039  
 Median :1.000   debt_consolidation:3957   Median :0.1221  
 Mean   :0.805   educational       : 343   Mean   :0.1226  
 3rd Qu.:1.000   home_improvement  : 629   3rd Qu.:0.1407  
 Max.   :1.000   major_purchase    : 437   Max.   :0.2164  
                 small_business    : 619                   
  installment     log.annual.inc        dti              fico      
 Min.   : 15.67   Min.   : 7.548   Min.   : 0.000   Min.   :612.0  
 1st Qu.:163.77   1st Qu.:10.558   1st Qu.: 7.213   1st Qu.:682.0  
 Median :268.95   Median :10.928   Median :12.665   Median :707.0  
 Mean   :319.09   Mean   :10.932   Mean   :12.607   Mean   :710.8  
 3rd Qu.:432.76   3rd Qu.:11.290   3rd Qu.:17.950   3rd Qu.:737.0  
 Max.   :940.14   Max.   :14.528   Max.   :29.960   Max.   :827.0  
                  NA&amp;#39;s   :4                                        
 days.with.cr.line   revol.bal         revol.util     inq.last.6mths  
 Min.   :  179     Min.   :      0   Min.   :  0.00   Min.   : 0.000  
 1st Qu.: 2820     1st Qu.:   3187   1st Qu.: 22.70   1st Qu.: 0.000  
 Median : 4140     Median :   8596   Median : 46.40   Median : 1.000  
 Mean   : 4562     Mean   :  16914   Mean   : 46.87   Mean   : 1.572  
 3rd Qu.: 5730     3rd Qu.:  18250   3rd Qu.: 71.00   3rd Qu.: 2.000  
 Max.   :17640     Max.   :1207359   Max.   :119.00   Max.   :33.000  
 NA&amp;#39;s   :29                          NA&amp;#39;s   :62       NA&amp;#39;s   :29      
  delinq.2yrs         pub.rec       not.fully.paid  
 Min.   : 0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.: 0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
 Median : 0.0000   Median :0.0000   Median :0.0000  
 Mean   : 0.1638   Mean   :0.0621   Mean   :0.1601  
 3rd Qu.: 0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  
 Max.   :13.0000   Max.   :5.0000   Max.   :1.0000  
 NA&amp;#39;s   :29        NA&amp;#39;s   :29                       &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of the loans in the dataset were not paid in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(loans$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
8045 1533 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1533 / (8045 + 1533)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1600543&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Which of the following variables has at least one missing observation?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;log.annual.inc&lt;/li&gt;
&lt;li&gt;days.with.cr.line&lt;/li&gt;
&lt;li&gt;revol.util&lt;/li&gt;
&lt;li&gt;inq.last.6mths&lt;/li&gt;
&lt;li&gt;delinq.2yrs&lt;/li&gt;
&lt;li&gt;pub.rec&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Which of the following is the best reason to fill in the missing values for these variables instead of removing observations with missing data? (Hint: you can use the subset() function to build a dataframe with the observations missing at least one value. To test if a variable, for example pub.rec, is missing a value, use is.na(pub.rec).)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loansNA &amp;lt;- subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line)
                  | is.na(revol.util) | is.na(inq.last.6mths)
                  | is.na(delinq.2yrs) | is.na(pub.rec))
str(loansNA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   62 obs. of  14 variables:
 $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 1 4 3 3 6 2 1 4 1 1 ...
 $ int.rate         : num  0.113 0.11 0.113 0.123 0.106 ...
 $ installment      : num  98.7 52.4 263.2 23.4 182.4 ...
 $ log.annual.inc   : num  10.53 10.53 10.71 9.85 11.26 ...
 $ dti              : num  7.72 15.84 8.75 12.38 4.26 ...
 $ fico             : int  677 682 682 662 697 667 687 687 722 752 ...
 $ days.with.cr.line: num  1680 1830 2490 1200 4141 ...
 $ revol.bal        : int  0 0 0 0 0 0 0 0 0 0 ...
 $ revol.util       : num  NA NA NA NA NA NA NA NA NA NA ...
 $ inq.last.6mths   : int  1 0 1 1 0 0 1 0 1 0 ...
 $ delinq.2yrs      : int  0 0 1 0 0 0 0 0 0 0 ...
 $ pub.rec          : int  0 0 0 0 1 0 0 0 0 0 ...
 $ not.fully.paid   : int  1 0 1 0 0 1 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(loansNA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; credit.policy                  purpose      int.rate     
 Min.   :0.0000   all_other         :41   Min.   :0.0712  
 1st Qu.:0.0000   credit_card       : 3   1st Qu.:0.0933  
 Median :0.0000   debt_consolidation: 8   Median :0.1122  
 Mean   :0.3871   educational       : 3   Mean   :0.1187  
 3rd Qu.:1.0000   home_improvement  : 1   3rd Qu.:0.1456  
 Max.   :1.0000   major_purchase    : 5   Max.   :0.1913  
                  small_business    : 1                   
  installment     log.annual.inc        dti              fico      
 Min.   : 23.35   Min.   : 8.294   Min.   : 0.000   Min.   :642.0  
 1st Qu.: 78.44   1st Qu.:10.096   1st Qu.: 5.147   1st Qu.:682.0  
 Median :145.91   Median :10.639   Median :10.000   Median :707.0  
 Mean   :159.19   Mean   :10.558   Mean   : 9.184   Mean   :711.5  
 3rd Qu.:192.73   3rd Qu.:11.248   3rd Qu.:11.540   3rd Qu.:740.8  
 Max.   :859.57   Max.   :13.004   Max.   :22.720   Max.   :802.0  
                  NA&amp;#39;s   :4                                        
 days.with.cr.line   revol.bal        revol.util  inq.last.6mths 
 Min.   : 179      Min.   :     0   Min.   : NA   Min.   :0.000  
 1st Qu.:1830      1st Qu.:     0   1st Qu.: NA   1st Qu.:0.000  
 Median :2580      Median :     0   Median : NA   Median :1.000  
 Mean   :3158      Mean   :  5476   Mean   :NaN   Mean   :1.182  
 3rd Qu.:4621      3rd Qu.:     0   3rd Qu.: NA   3rd Qu.:2.000  
 Max.   :7890      Max.   :290291   Max.   : NA   Max.   :6.000  
 NA&amp;#39;s   :29                         NA&amp;#39;s   :62    NA&amp;#39;s   :29     
  delinq.2yrs        pub.rec       not.fully.paid  
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
 Median :0.0000   Median :0.0000   Median :0.0000  
 Mean   :0.2121   Mean   :0.0303   Mean   :0.1935  
 3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  
 Max.   :4.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :29       NA&amp;#39;s   :29                       &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(loansNA$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 0  1 
50 12 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12 / (50 + 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1935484&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;we-want-to-be-able-to-predict-risk-for-all-borrowers-instead-of-just-the-ones-with-all-data-reported.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;We want to be able to predict risk for all borrowers, instead of just the ones with all data reported.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;For the rest of this problem, I’ll be using a revised version of the dataset that has the missing values filled in with multiple imputation. To ensure everybody has the same dataframe going forward, you can either run the code below in your R console (if you haven’t already, run the code install.packages(“mice”) first), or you can download and load into R the dataset we created after running the imputation: loans_imputed.csv.&lt;/p&gt;
&lt;p&gt;IMPORTANT NOTE: On certain operating systems, the imputation results are not the same even if you set the random seed. If you decide to do the imputation yourself, please still read the provided imputed dataset (loans_imputed.csv) into R and compare your results, using the summary function. If the results are different, please make sure to use the data in loans_imputed.csv for the rest of the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;library(mice)&lt;/li&gt;
&lt;li&gt;set.seed(144)&lt;/li&gt;
&lt;li&gt;vars.for.imputation = setdiff(names(loans), “not.fully.paid”)&lt;/li&gt;
&lt;li&gt;imputed = complete(mice(loans[vars.for.imputation]))&lt;/li&gt;
&lt;li&gt;loans[vars.for.imputation] = imputed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loans &amp;lt;- read.csv(&amp;quot;loans_imputed.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, that to do this imputation, we set vars.for.imputation to all variables in the dataframe except for not.fully.paid, to impute the values using all of the other independent variables.&lt;/p&gt;
&lt;p&gt;What best describes the process we just used to handle missing values?
#### We predicted missing variable values using the available independent variables for each observation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Now that we have prepared the dataset, we need to split it into a training and testing set. To ensure everybody obtains the same split, set the random seed to 144 (even though you already did so earlier in the problem) and use the sample.split function to select the 70% of observations for the training set (the dependent variable for sample.split is not.fully.paid). Name the dataframes train and test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test = subset(loans, split == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, use logistic regression trained on the training set to predict the dependent variable not.fully.paid using all the independent variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LoansLog &amp;lt;- glm(not.fully.paid ~ ., data = train, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which independent variables are significant in our model? (Significant variables have at least one star, or a Pr(&amp;gt;|z|) value less than 0.05.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LoansLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = not.fully.paid ~ ., family = binomial, data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2049  -0.6205  -0.4951  -0.3606   2.6397  

Coefficients:
                            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)                9.187e+00  1.554e+00   5.910 3.42e-09 ***
credit.policy             -3.368e-01  1.011e-01  -3.332 0.000861 ***
purposecredit_card        -6.141e-01  1.344e-01  -4.568 4.93e-06 ***
purposedebt_consolidation -3.212e-01  9.183e-02  -3.498 0.000469 ***
purposeeducational         1.347e-01  1.753e-01   0.768 0.442201    
purposehome_improvement    1.727e-01  1.480e-01   1.167 0.243135    
purposemajor_purchase     -4.830e-01  2.009e-01  -2.404 0.016203 *  
purposesmall_business      4.120e-01  1.419e-01   2.905 0.003678 ** 
int.rate                   6.110e-01  2.085e+00   0.293 0.769446    
installment                1.275e-03  2.092e-04   6.093 1.11e-09 ***
log.annual.inc            -4.337e-01  7.148e-02  -6.067 1.30e-09 ***
dti                        4.638e-03  5.502e-03   0.843 0.399288    
fico                      -9.317e-03  1.710e-03  -5.448 5.08e-08 ***
days.with.cr.line          2.371e-06  1.588e-05   0.149 0.881343    
revol.bal                  3.085e-06  1.168e-06   2.641 0.008273 ** 
revol.util                 1.839e-03  1.535e-03   1.199 0.230722    
inq.last.6mths             8.437e-02  1.600e-02   5.275 1.33e-07 ***
delinq.2yrs               -8.320e-02  6.561e-02  -1.268 0.204762    
pub.rec                    3.300e-01  1.139e-01   2.898 0.003756 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 5896.6  on 6704  degrees of freedom
Residual deviance: 5485.2  on 6686  degrees of freedom
AIC: 5523.2

Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;credit.policy&lt;/li&gt;
&lt;li&gt;purposecredit_card&lt;/li&gt;
&lt;li&gt;purposedebt_consolidation&lt;/li&gt;
&lt;li&gt;purposemajor_purchase&lt;/li&gt;
&lt;li&gt;purposesmall_business&lt;/li&gt;
&lt;li&gt;installment&lt;/li&gt;
&lt;li&gt;log.annual.inc&lt;/li&gt;
&lt;li&gt;fico&lt;/li&gt;
&lt;li&gt;revol.bal&lt;/li&gt;
&lt;li&gt;inq.last.6mths&lt;/li&gt;
&lt;li&gt;pub.rec&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Consider two loan applications, which are identical other than the fact that the borrower in Application A has FICO credit score 700 while the borrower in Application B has FICO credit score 710.&lt;/p&gt;
&lt;p&gt;Let’s Logit(A) be the log odds of loan A not being paid back in full, according to our logistic regression model, and define Logit(B) similarly for loan B.&lt;/p&gt;
&lt;p&gt;What is the value of Logit(A) - Logit(B)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicationA &amp;lt;- train[1, ]
applicationB &amp;lt;- applicationA
applicationA$fico = 700
applicationB$fico = 710
applicationA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  credit.policy            purpose int.rate installment log.annual.inc
1             1 debt_consolidation   0.1189       829.1       11.35041
    dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
1 19.48  700          5639.958     28854       52.1              0
  delinq.2yrs pub.rec not.fully.paid
1           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  credit.policy            purpose int.rate installment log.annual.inc
1             1 debt_consolidation   0.1189       829.1       11.35041
    dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
1 19.48  710          5639.958     28854       52.1              0
  delinq.2yrs pub.rec not.fully.paid
1           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applications &amp;lt;- rbind(applicationA, applicationB)
applications&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  credit.policy            purpose int.rate installment log.annual.inc
1             1 debt_consolidation   0.1189       829.1       11.35041
2             1 debt_consolidation   0.1189       829.1       11.35041
    dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
1 19.48  700          5639.958     28854       52.1              0
2 19.48  710          5639.958     28854       52.1              0
  delinq.2yrs pub.rec not.fully.paid
1           0       0              0
2           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredApplications &amp;lt;- predict(LoansLog, type = &amp;quot;response&amp;quot;, newdata = applications)
PredApplications&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2 
0.1828795 0.1693660 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredApplications[1] - PredApplications[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;         1 
0.01351347 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationA &amp;lt;- 
    1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 700)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationB &amp;lt;- 
    1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 710)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationA - CalcApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.005902546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let O(A) be the odds of loan A not being paid back in full, according to our logistic regression model, and define O(B) similarly for loan B.&lt;/p&gt;
&lt;p&gt;What is the value of O(A)/O(B)? (HINT: Use the mathematical rule that exp(A + B + C) = exp(A)&lt;em&gt;exp(B)&lt;/em&gt;exp(C). Also, remember that exp() is the exponential function in R.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationA &amp;lt;- PredApplications[1] / (1 - PredApplications[1])
OddsApplicationA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1 
0.2238097 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationB &amp;lt;- PredApplications[2] / (1 - PredApplications[2])
OddsApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        2 
0.2038997 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationA / OddsApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       1 
1.097646 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;ok&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1.097646 =&amp;gt; OK!&lt;/h4&gt;
&lt;p&gt;After computing the logs, try log(odds) for previous problem&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1 
-1.496959 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        2 
-1.590127 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationA) - log(OddsApplicationB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1 
0.0931679 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ok-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.0931679 =&amp;gt; OK!&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Predict the probability of the test-set loans not being paid back in full (remember type=“response” for the predict function). Store these predicted probabilities in a variable named predicted.risk and add it to our test-set (we will use this variable in later parts of the problem). Compute the confusion matrix using a threshold of 0.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted.risk &amp;lt;- predict(LoansLog, type = &amp;quot;response&amp;quot;, newdata = test)
str(predicted.risk)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Named num [1:2873] 0.0771 0.1728 0.1087 0.1016 0.0681 ...
 - attr(*, &amp;quot;names&amp;quot;)= chr [1:2873] &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;10&amp;quot; &amp;quot;12&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2873 obs. of  14 variables:
 $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 2 3 3 3 1 3 1 2 7 2 ...
 $ int.rate         : num  0.107 0.136 0.122 0.132 0.08 ...
 $ installment      : num  228.2 366.9 84.1 253.6 188 ...
 $ log.annual.inc   : num  11.1 10.4 10.2 11.8 11.2 ...
 $ dti              : num  14.29 11.63 10 9.16 16.08 ...
 $ fico             : int  707 682 707 662 772 662 772 797 712 682 ...
 $ days.with.cr.line: num  2760 4710 2730 4298 4889 ...
 $ revol.bal        : int  33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ...
 $ revol.util       : num  76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ...
 $ inq.last.6mths   : int  0 1 1 2 1 0 0 0 0 3 ...
 $ delinq.2yrs      : int  0 0 0 1 0 1 0 0 0 0 ...
 $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
 $ not.fully.paid   : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$predicted.risk &amp;lt;- predicted.risk
summary(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; credit.policy                  purpose        int.rate     
 Min.   :0.0000   all_other         : 688   Min.   :0.0600  
 1st Qu.:1.0000   credit_card       : 411   1st Qu.:0.1028  
 Median :1.0000   debt_consolidation:1206   Median :0.1221  
 Mean   :0.8047   educational       :  93   Mean   :0.1225  
 3rd Qu.:1.0000   home_improvement  : 186   3rd Qu.:0.1393  
 Max.   :1.0000   major_purchase    : 105   Max.   :0.2121  
                  small_business    : 184                   
  installment     log.annual.inc        dti             fico      
 Min.   : 15.67   Min.   : 8.102   Min.   : 0.00   Min.   :612.0  
 1st Qu.:163.57   1st Qu.:10.560   1st Qu.: 7.16   1st Qu.:682.0  
 Median :267.74   Median :10.933   Median :12.85   Median :707.0  
 Mean   :316.99   Mean   :10.928   Mean   :12.75   Mean   :710.8  
 3rd Qu.:421.89   3rd Qu.:11.290   3rd Qu.:18.30   3rd Qu.:737.0  
 Max.   :926.83   Max.   :13.459   Max.   :29.96   Max.   :822.0  
                                                                  
 days.with.cr.line   revol.bal         revol.util     inq.last.6mths  
 Min.   :  179     Min.   :      0   Min.   :  0.00   Min.   : 0.000  
 1st Qu.: 2795     1st Qu.:   3362   1st Qu.: 23.40   1st Qu.: 0.000  
 Median : 4140     Median :   8712   Median : 46.90   Median : 1.000  
 Mean   : 4494     Mean   :  17198   Mean   : 47.02   Mean   : 1.576  
 3rd Qu.: 5670     3rd Qu.:  18728   3rd Qu.: 70.40   3rd Qu.: 2.000  
 Max.   :17640     Max.   :1207359   Max.   :108.80   Max.   :24.000  
                                                                      
  delinq.2yrs         pub.rec        not.fully.paid   predicted.risk   
 Min.   : 0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.02114  
 1st Qu.: 0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.09461  
 Median : 0.0000   Median :0.00000   Median :0.0000   Median :0.13697  
 Mean   : 0.1605   Mean   :0.05743   Mean   :0.1601   Mean   :0.15785  
 3rd Qu.: 0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.19658  
 Max.   :13.0000   Max.   :3.00000   Max.   :1.0000   Max.   :0.95373  
                                                                       &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid, test$predicted.risk &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0  2400   13
  1   457    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(2400 + 3) / nrow(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8364079&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the baseline model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1 
2413  460 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2413 / (2413 + 460)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8398886&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Use the ROCR package to compute the test-set AUC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred = prediction(test$predicted.risk, test$not.fully.paid)
as.numeric(performance(ROCRpred, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6720995&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has poor accuracy at the threshold 0.5. But, despite the poor accuracy, we will see later how an investor can still leverage this logistic regression model to make profitable investments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;In the previous problem, we built a logistic regression model that has an AUC significantly higher than the AUC of 0.5 that would be obtained by randomly ordering observations. However, &lt;strong&gt;LendingClub.com&lt;/strong&gt; assigns the interest rate to a loan based on their estimate of that loan’s risk.&lt;/p&gt;
&lt;p&gt;This variable, int.rate, is an independent variable in our dataset. In this part, we will investigate using the loan’s interest rate as a “smart baseline” to order the loans according to risk.&lt;/p&gt;
&lt;p&gt;Using the training set, build a bivariate logistic regression model (aka a logistic regression model with a single independent variable) that predicts the dependent variable not.fully.paid using only the variable int.rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LoansLog2 &amp;lt;- glm(not.fully.paid ~ int.rate, data = train, family = binomial)
summary(LoansLog2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = not.fully.paid ~ int.rate, family = binomial, data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0547  -0.6271  -0.5442  -0.4361   2.2914  

Coefficients:
            Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)  -3.6726     0.1688  -21.76   &amp;lt;2e-16 ***
int.rate     15.9214     1.2702   12.54   &amp;lt;2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 5896.6  on 6704  degrees of freedom
Residual deviance: 5734.8  on 6703  degrees of freedom
AIC: 5738.8

Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable int.rate is highly significant in the bivariate model, but it is not significant at the 0.05 level in the model trained with all the independent variables.&lt;/p&gt;
&lt;p&gt;What is the most likely explanation for this difference?
#### int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;Make test-set predictions for the bivariate model.&lt;/p&gt;
&lt;p&gt;What is the highest predicted probability of a loan not being paid in full on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted.risk2 &amp;lt;- predict(LoansLog2, type = &amp;quot;response&amp;quot;, newdata = test)
max(predicted.risk2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.426624&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.426624&lt;/h4&gt;
&lt;p&gt;With a logistic regression cut-off of 0.5, how many loans would be predicted as not being paid in full on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid, predicted.risk2 &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE
  0  2413
  1   460&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2873 obs. of  15 variables:
 $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 2 3 3 3 1 3 1 2 7 2 ...
 $ int.rate         : num  0.107 0.136 0.122 0.132 0.08 ...
 $ installment      : num  228.2 366.9 84.1 253.6 188 ...
 $ log.annual.inc   : num  11.1 10.4 10.2 11.8 11.2 ...
 $ dti              : num  14.29 11.63 10 9.16 16.08 ...
 $ fico             : int  707 682 707 662 772 662 772 797 712 682 ...
 $ days.with.cr.line: num  2760 4710 2730 4298 4889 ...
 $ revol.bal        : int  33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ...
 $ revol.util       : num  76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ...
 $ inq.last.6mths   : int  0 1 1 2 1 0 0 0 0 3 ...
 $ delinq.2yrs      : int  0 0 0 1 0 1 0 0 0 0 ...
 $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
 $ not.fully.paid   : int  0 0 0 0 0 0 0 0 0 0 ...
 $ predicted.risk   : num  0.0771 0.1728 0.1087 0.1016 0.0681 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;What is the test-set AUC of the bivariate model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred2 = prediction(predicted.risk2, test$not.fully.paid)
as.numeric(performance(ROCRpred2, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6239081&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;While thus far we have predicted if a loan will be paid back or not, an investor needs to identify loans that are expected to be profitable.&lt;/p&gt;
&lt;p&gt;If the loan is paid back in full, then the investor makes interest on the loan. However, if the loan is not paid back, the investor loses the money invested. Therefore, the investor should seek loans that best balance this risk and reward.&lt;/p&gt;
&lt;p&gt;To compute interest revenue, consider a $c investment in a loan that has an annual interest rate r over a period of t years.&lt;/p&gt;
&lt;p&gt;Using continuous compounding of interest, this investment pays back c * exp(rt) dollars by the end of the t years, where exp(rt) is e raised to the r*t power.&lt;/p&gt;
&lt;p&gt;How much does a $10 investment with an annual interest rate of 6% pay back after 3 years, using continuous compounding of interest? Hint: remember to convert the percentage to a proportion before doing the math.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;10 * exp(0.06 * 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 11.97217&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;While the investment has value c * exp(rt) dollars after collecting interest, the investor had to pay $c for the investment.&lt;/p&gt;
&lt;p&gt;What is the profit to the investor if the investment is paid back in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;10 * exp(0.06 * 3) - 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.972174&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;Now, consider the case where the investor made a $c investment, but it was not paid back in full. Assume, conservatively, that no money was received from the borrower (often a lender will receive some but not all of the value of the loan, making this a pessimistic assumption of how much is received).&lt;/p&gt;
&lt;p&gt;What is the loss to the investor in this scenario?
#### -10&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---a-simple-investment-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - A Simple Investment Strategy&lt;/h3&gt;
&lt;p&gt;In the previous subproblem, we concluded that an investor who invested c dollars in a loan with interest rate r for t years makes c * (exp(rt) - 1) dollars of profit if the loan is paid back in full and -c dollars of profit if the loan is not paid back in full (pessimistically).&lt;/p&gt;
&lt;p&gt;In order to evaluate the quality of an investment strategy, we need to compute this profit for each loan in the test-set.&lt;/p&gt;
&lt;p&gt;For this variable, we will assume a $1 investment (aka c=1). To create the variable, we first assign to the profit for a fully paid loan, exp(rt)-1, to every observation, and we then replace this value with -1 in the cases where the loan was not paid in full.&lt;/p&gt;
&lt;p&gt;All the loans in our dataset are 3-year loans, meaning t=3 in our calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$profit = exp(test$int.rate*3) - 1
test$profit[test$not.fully.paid == 1] = -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the maximum profit of a $10 investment in any loan in the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(test$profit) * 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 8.894769&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6.1---an-investment-strategy-based-on-risk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6.1 - An Investment Strategy Based on Risk&lt;/h3&gt;
&lt;p&gt;A simple investment strategy of equally investing in all the loans would yield profit $20.94 for a $100 investment. But this simple investment strategy does not leverage the prediction model we built earlier in this problem.&lt;/p&gt;
&lt;p&gt;As stated earlier, investors seek loans that balance reward with risk, in that they simultaneously have high interest rates and a low risk of not being paid back.&lt;/p&gt;
&lt;p&gt;To meet this objective, I’ll analyze an investment strategy in which the investor only purchases loans with a high interest rate (a rate of at least 15%), but amongst these loans selects the ones with the lowest predicted risk of not being paid back in full.&lt;/p&gt;
&lt;p&gt;We will model an investor who invests $1 in each of the most promising &lt;strong&gt;100 loans.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, use the subset() function to build a dataframe called highInterest consisting of the test-set loans with an interest rate of at least 15%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highInterest &amp;lt;- subset(test, int.rate &amp;gt;= 0.15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the average profit of a $1 investment in one of these high-interest loans?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(highInterest$profit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2251015&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of the high-interest loans were not paid back in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(highInterest$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
327 110 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;110 / (327 + 110)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2517162&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6.2---an-investment-strategy-based-on-risk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6.2 - An Investment Strategy Based on Risk&lt;/h3&gt;
&lt;p&gt;Next, I’ll determine the 100th smallest predicted probability of not paying in full by sorting the predicted risks in increasing order and selecting the 100th element of this sorted list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100]
cutoff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1763305&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the subset() function to build a dataframe called selectedLoans consisting of the high-interest loans with predicted risk not exceeding the cut-off we just computed. Check to make sure you have selected 100 loans for investment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selectedLoans &amp;lt;- subset(highInterest, highInterest$predicted.risk &amp;lt;= cutoff)
summary(selectedLoans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; credit.policy                purpose      int.rate       installment    
 Min.   :0.00   all_other         : 8   Min.   :0.1501   Min.   : 48.79  
 1st Qu.:1.00   credit_card       :17   1st Qu.:0.1533   1st Qu.:176.16  
 Median :1.00   debt_consolidation:60   Median :0.1570   Median :309.37  
 Mean   :0.93   educational       : 1   Mean   :0.1610   Mean   :358.30  
 3rd Qu.:1.00   home_improvement  : 1   3rd Qu.:0.1645   3rd Qu.:473.10  
 Max.   :1.00   major_purchase    : 7   Max.   :0.2052   Max.   :907.60  
                small_business    : 6                                    
 log.annual.inc        dti             fico       days.with.cr.line
 Min.   : 9.575   Min.   : 0.00   Min.   :642.0   Min.   : 1140    
 1st Qu.:10.776   1st Qu.: 6.05   1st Qu.:662.0   1st Qu.: 2162    
 Median :11.127   Median :12.35   Median :672.0   Median : 3630    
 Mean   :11.203   Mean   :12.19   Mean   :680.5   Mean   : 3911    
 3rd Qu.:11.670   3rd Qu.:18.23   3rd Qu.:692.0   3rd Qu.: 5010    
 Max.   :13.305   Max.   :28.15   Max.   :782.0   Max.   :13170    
                                                                   
   revol.bal        revol.util    inq.last.6mths   delinq.2yrs  
 Min.   :     0   Min.   : 0.00   Min.   : 0.00   Min.   :0.00  
 1st Qu.:  3768   1st Qu.:45.92   1st Qu.: 0.00   1st Qu.:0.00  
 Median :  9691   Median :71.65   Median : 0.00   Median :0.00  
 Mean   : 19923   Mean   :65.79   Mean   : 0.89   Mean   :0.33  
 3rd Qu.: 24534   3rd Qu.:93.80   3rd Qu.: 1.00   3rd Qu.:0.00  
 Max.   :168496   Max.   :99.70   Max.   :10.00   Max.   :4.00  
                                                                
    pub.rec     not.fully.paid predicted.risk        profit       
 Min.   :0.00   Min.   :0.00   Min.   :0.06871   Min.   :-1.0000  
 1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.13596   1st Qu.: 0.5823  
 Median :0.00   Median :0.00   Median :0.15327   Median : 0.5992  
 Mean   :0.03   Mean   :0.19   Mean   :0.14794   Mean   : 0.3128  
 3rd Qu.:0.00   3rd Qu.:0.00   3rd Qu.:0.16514   3rd Qu.: 0.6317  
 Max.   :1.00   Max.   :1.00   Max.   :0.17633   Max.   : 0.8508  
                                                                  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(selectedLoans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   100 obs. of  16 variables:
 $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
 $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 7 2 3 1 3 5 2 3 2 3 ...
 $ int.rate         : num  0.15 0.153 0.158 0.159 0.156 ...
 $ installment      : num  225 444 420 246 245 ...
 $ log.annual.inc   : num  12.3 11 11.5 11.5 10.8 ...
 $ dti              : num  6.45 19.52 18.55 24.19 2.72 ...
 $ fico             : int  677 667 667 667 672 702 667 672 662 682 ...
 $ days.with.cr.line: num  6240 2701 4560 5376 3010 ...
 $ revol.bal        : int  56411 33074 34841 590 3273 4980 15977 16473 22783 87502 ...
 $ revol.util       : num  75.3 68.8 89.6 84.3 69.6 55.3 83.6 94.1 93.7 96.4 ...
 $ inq.last.6mths   : int  0 2 0 0 1 1 0 2 3 0 ...
 $ delinq.2yrs      : int  0 0 0 0 0 0 0 2 1 1 ...
 $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
 $ not.fully.paid   : int  1 0 0 0 1 0 0 0 0 0 ...
 $ predicted.risk   : num  0.164 0.169 0.158 0.162 0.147 ...
 $ profit           : num  -1 0.584 0.604 0.61 -1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the profit of the investor, who invested $1 in each of these 100 loans?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(selectedLoans$profit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 31.27825&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many of 100 selected loans were not paid back in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(selectedLoans$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
 0  1 
81 19 &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;19&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We have now seen how analytics can be used to select a subset of the high-interest loans that were paid back at only a slightly lower rate than average, resulting in a significant increase in the profit from our investor’s $100 investment. Although the logistic regression models developed in this analysis did not have large AUC values, we see that they still provided the edge needed to improve the profitability of an investment portfolio.&lt;/p&gt;
&lt;p&gt;We conclude with a note of warning. Throughout this analysis I’ve assume that the loans we invest in will perform in the same way as the loans we used to train our model, even though our training set covers a relatively short period of time. If there is an economic shock like a large financial downturn, default rates might be significantly higher than those observed in the training set and we might end up losing money instead of profiting. &lt;strong&gt;Investors must pay careful attention to such risk when making investment decisions.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict Parole Violators</title>
      <link>/project/parole_violators/parole_violators/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/parole_violators/parole_violators/</guid>
      <description>


&lt;p&gt;In many criminal justice systems around the world, inmates deemed not to be a threat to society are released from prison under the parole system prior to completing their sentence. They are still considered to be serving their sentence while on parole, and they can be returned to prison if they violate the terms of their parole.&lt;/p&gt;
&lt;p&gt;Parole boards are charged with identifying which inmates are good candidates for release on parole. They seek to release inmates who will not commit additional crimes after release. In this analysis, I’ll build and validate a model that predicts if an inmate will violate the terms of his or her parole.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Such a model could be useful to a parole board when deciding to approve or deny an application for parole.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For this prediction task, I’ll use data from the U.S 2004 National Corrections Reporting Program, a nationwide census of parole releases that occurred during 2004.&lt;/p&gt;
&lt;p&gt;I’ve limited my focus to parolees who served no more than 6 months in prison and whose maximum sentence for all charges did not exceed 18 months.&lt;/p&gt;
&lt;p&gt;The dataset contains all such parolees who either successfully completed their term of parole during 2004 or those who violated the terms of their parole during that year. The dataset contains the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;male: 1 if the parolee is male, 0 if female&lt;/li&gt;
&lt;li&gt;race: 1 if the parolee is white, 2 otherwise&lt;/li&gt;
&lt;li&gt;age: the parolee’s age (in years) when he or she was released from prison&lt;/li&gt;
&lt;li&gt;state: a code for the parolee’s state. 2 is Kentucky, 3 is Louisiana, 4 is Virginia, and 1 is any other state. The three states were selected due to having a high representation in the dataset.&lt;/li&gt;
&lt;li&gt;time.served: the number of months the parolee served in prison (limited by the inclusion criteria to not exceed 6 months).&lt;/li&gt;
&lt;li&gt;max.sentence: the maximum sentence length for all charges, in months (limited by the inclusion criteria to not exceed 18 months).&lt;/li&gt;
&lt;li&gt;multiple.offenses: 1 if the parolee was incarcerated for multiple offenses, 0 otherwise.&lt;/li&gt;
&lt;li&gt;crime: a code for the parolee’s main crime leading to incarceration. 2 is larceny, 3 is drug-related crime, 4 is driving-related crime, and 1 is any other crime.&lt;/li&gt;
&lt;li&gt;violator: 1 if the parolee violated the parole, and 0 if the parolee completed the parole without violation.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;loading-the-dataset-eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the Dataset &amp;amp; EDA&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parole &amp;lt;- read.csv(&amp;quot;parole.csv&amp;quot;)
str(parole)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   675 obs. of  9 variables:
 $ male             : int  1 0 1 1 1 1 1 0 0 1 ...
 $ race             : int  1 1 2 1 2 2 1 1 1 2 ...
 $ age              : num  33.2 39.7 29.5 22.4 21.6 46.7 31 24.6 32.6 29.1 ...
 $ state            : int  1 1 1 1 1 1 1 1 1 1 ...
 $ time.served      : num  5.5 5.4 5.6 5.7 5.4 6 6 4.8 4.5 4.7 ...
 $ max.sentence     : int  18 12 12 18 12 18 18 12 13 12 ...
 $ multiple.offenses: int  0 0 0 0 0 0 0 0 0 0 ...
 $ crime            : int  4 3 3 1 1 4 3 1 3 2 ...
 $ violator         : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(parole)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      male             race            age            state      
 Min.   :0.0000   Min.   :1.000   Min.   :18.40   Min.   :1.000  
 1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:25.35   1st Qu.:2.000  
 Median :1.0000   Median :1.000   Median :33.70   Median :3.000  
 Mean   :0.8074   Mean   :1.424   Mean   :34.51   Mean   :2.887  
 3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:42.55   3rd Qu.:4.000  
 Max.   :1.0000   Max.   :2.000   Max.   :67.00   Max.   :4.000  
  time.served     max.sentence   multiple.offenses     crime      
 Min.   :0.000   Min.   : 1.00   Min.   :0.0000    Min.   :1.000  
 1st Qu.:3.250   1st Qu.:12.00   1st Qu.:0.0000    1st Qu.:1.000  
 Median :4.400   Median :12.00   Median :1.0000    Median :2.000  
 Mean   :4.198   Mean   :13.06   Mean   :0.5363    Mean   :2.059  
 3rd Qu.:5.200   3rd Qu.:15.00   3rd Qu.:1.0000    3rd Qu.:3.000  
 Max.   :6.000   Max.   :18.00   Max.   :1.0000    Max.   :4.000  
    violator     
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.0000  
 Mean   :0.1156  
 3rd Qu.:0.0000  
 Max.   :1.0000  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many parolees are contained in the dataset?
#### 675&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Which variables in this dataset are unordered factors with at least three levels?
#### state, crime&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;In the last subproblem, we identified variables that are unordered factors with at least 3 levels, so we need to convert them to factors for our prediction problem.&lt;/p&gt;
&lt;p&gt;Using the as.factor() function, we convert these variables to factors. Keep in mind that we are not changing the values, just the way R understands them (the values are still numbers).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parole$state &amp;lt;- as.factor(parole$state)
parole$crime &amp;lt;- as.factor(parole$crime)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the output of summary() change for a factor variable as compared to a numerical variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(parole)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      male             race            age        state    time.served   
 Min.   :0.0000   Min.   :1.000   Min.   :18.40   1:143   Min.   :0.000  
 1st Qu.:1.0000   1st Qu.:1.000   1st Qu.:25.35   2:120   1st Qu.:3.250  
 Median :1.0000   Median :1.000   Median :33.70   3: 82   Median :4.400  
 Mean   :0.8074   Mean   :1.424   Mean   :34.51   4:330   Mean   :4.198  
 3rd Qu.:1.0000   3rd Qu.:2.000   3rd Qu.:42.55           3rd Qu.:5.200  
 Max.   :1.0000   Max.   :2.000   Max.   :67.00           Max.   :6.000  
  max.sentence   multiple.offenses crime      violator     
 Min.   : 1.00   Min.   :0.0000    1:315   Min.   :0.0000  
 1st Qu.:12.00   1st Qu.:0.0000    2:106   1st Qu.:0.0000  
 Median :12.00   Median :1.0000    3:153   Median :0.0000  
 Mean   :13.06   Mean   :0.5363    4:101   Mean   :0.1156  
 3rd Qu.:15.00   3rd Qu.:1.0000            3rd Qu.:0.0000  
 Max.   :18.00   Max.   :1.0000            Max.   :1.0000  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-output-becomes-similar-to-that-of-the-table-function-applied-to-that-variable&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The output becomes similar to that of the table() function applied to that variable&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---splitting-into-a-training-and-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Splitting into a Training and Testing Set&lt;/h3&gt;
&lt;p&gt;To ensure consistent training/testing set splits, run the following 5 lines of code (do not include the line numbers at the beginning):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
# 70% to the training set, 30% to the testing set
split = sample.split(parole$violator, SplitRatio = 0.7)
train = subset(parole, split == TRUE)
test = subset(parole, split == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Roughly what proportion of parolees have been allocated to the training and testing sets?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   473 obs. of  9 variables:
 $ male             : int  1 1 1 1 1 0 0 1 1 1 ...
 $ race             : int  1 1 2 2 1 1 2 1 1 1 ...
 $ age              : num  33.2 22.4 21.6 46.7 31 32.6 28.4 20.5 30.1 37.8 ...
 $ state            : Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
 $ time.served      : num  5.5 5.7 5.4 6 6 4.5 4.5 5.9 5.3 5.3 ...
 $ max.sentence     : int  18 18 12 18 18 13 12 12 16 8 ...
 $ multiple.offenses: int  0 0 0 0 0 0 1 0 0 0 ...
 $ crime            : Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;: 4 1 1 4 3 3 1 1 3 3 ...
 $ violator         : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;473 / 675&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7007407&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   202 obs. of  9 variables:
 $ male             : int  0 1 0 1 1 1 1 1 1 1 ...
 $ race             : int  1 2 1 2 2 1 1 2 1 1 ...
 $ age              : num  39.7 29.5 24.6 29.1 24.5 32.8 36.7 36.5 33.5 37.3 ...
 $ state            : Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
 $ time.served      : num  5.4 5.6 4.8 4.7 6 5.9 0.9 3.9 4.2 4.6 ...
 $ max.sentence     : int  12 12 12 12 16 16 16 12 12 12 ...
 $ multiple.offenses: int  0 0 0 0 0 0 0 1 1 1 ...
 $ crime            : Factor w/ 4 levels &amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;,&amp;quot;3&amp;quot;,&amp;quot;4&amp;quot;: 3 3 1 2 3 3 3 4 1 1 ...
 $ violator         : int  0 0 0 0 0 0 0 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;202 / 675 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2992593&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---splitting-into-a-training-and-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Splitting into a Training and Testing Set&lt;/h3&gt;
&lt;p&gt;Now, suppose you re-ran lines [1]-[5] of Problem 3.1. What would you expect?
#### The exact same training/testing set split as the first execution of [1]-[5]&lt;/p&gt;
&lt;p&gt;If you instead ONLY re-ran lines [3]-[5], what would you expect?
#### A different training/testing set split from the first execution of [1]-[5]&lt;/p&gt;
&lt;p&gt;If you instead called set.seed() with a different number and then re-ran lines [3]-[5] of Problem 3.1, what would you expect?
#### A different training/testing set split from the first execution of [1]-[5]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?sample.split&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---building-a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Building a Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;If you tested other training/testing set splits in the previous section, please re-run the original 5 lines of code to obtain the original split. Using glm (and remembering the parameter family=“binomial”), train a logistic regression model on the training set. Your dependent variable is “violator”, and you should use all of the other variables as independent variables.&lt;/p&gt;
&lt;p&gt;What variables are significant in this model? Significant variables should have a least one star, or should have a probability less than 0.05 (the column Pr(&amp;gt;|z|) in the summary output).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ParoleViolatorLog &amp;lt;- glm(violator ~ ., data = train, family = binomial)
summary(ParoleViolatorLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = violator ~ ., family = binomial, data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7041  -0.4236  -0.2719  -0.1690   2.8375  

Coefficients:
                    Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)       -4.2411574  1.2938852  -3.278  0.00105 ** 
male               0.3869904  0.4379613   0.884  0.37690    
race               0.8867192  0.3950660   2.244  0.02480 *  
age               -0.0001756  0.0160852  -0.011  0.99129    
state2             0.4433007  0.4816619   0.920  0.35739    
state3             0.8349797  0.5562704   1.501  0.13335    
state4            -3.3967878  0.6115860  -5.554 2.79e-08 ***
time.served       -0.1238867  0.1204230  -1.029  0.30359    
max.sentence       0.0802954  0.0553747   1.450  0.14705    
multiple.offenses  1.6119919  0.3853050   4.184 2.87e-05 ***
crime2             0.6837143  0.5003550   1.366  0.17180    
crime3            -0.2781054  0.4328356  -0.643  0.52054    
crime4            -0.0117627  0.5713035  -0.021  0.98357    
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 340.04  on 472  degrees of freedom
Residual deviance: 251.48  on 460  degrees of freedom
AIC: 277.48

Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;race-state4-multiple.offenses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;race, state4, multiple.offenses&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---building-a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Building a Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What can we say based on the coefficient of the multiple.offenses variable? The following two properties might be useful to you when exploring this question:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If we have a coefficient c for a variable, then that means the log odds (or Logit) are increased by c for a unit increase in the variable.&lt;/li&gt;
&lt;li&gt;If we have a coefficient c for a variable, then that means the odds are multiplied by e^c for a unit increase in the variable.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(1.6119919)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5.012786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our model predicts that a parolee who committed multiple offenses has 5.01 times higher odds of being a violator than a parolee who did not commit multiple offenses but, is otherwise identical.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---building-a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Building a Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Consider a parolee who is male, of white race, aged 50 years at prison release, from the state of Maryland, served 3 months, had a maximum sentence of 12 months, did not commit multiple offenses, and committed a larceny.&lt;/p&gt;
&lt;p&gt;Explore the following questions based on the model’s predictions for this individual. (HINT: We should use the coefficients of our model, the Logistic Response Function, and the Odds equation to solve this problem.) According to the model, what are the odds this individual is a violator?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(-4.2411574 + # intercept
        0.3869904 * 1 + # male
        0.8867192 * 1 + # white race
        -0.0001756 * 50 + # aged 50
        0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland
        -0.1238867 * 3 + # served 3 months
        0.0802954 * 12 + # max sentence of 12 months
        1.6119919 * 0 + # did not commit multiple offenses
        0.6837143*1 + -0.2781054*0 + -0.0117627*0
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1825687&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 0.1825687

# according to the model, what is the probability this individual is a violator?
1 / (1 + exp(-1 * (-4.2411574 + # intercept
                       0.3869904 * 1 + # male
                       0.8867192 * 1 + # white race
                       -0.0001756 * 50 + # aged 50
                       0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland
                       -0.1238867 * 3 + # served 3 months
                       0.0802954 * 12 + # max sentence of 12 months
                       1.6119919 * 0 + # did not commit multiple offenses
                       0.6837143*1 + -0.2781054*0 + -0.0117627*0
                   )))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1543832&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Logistic Response Function -&amp;gt; P(y = 1) = 0.1543832&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Use the predict() function to obtain the model’s predicted probabilities for parolees in the testing set, remembering to pass type=“response”.&lt;/p&gt;
&lt;p&gt;What is the maximum predicted probability of a violation?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ParolePredTest &amp;lt;- predict(ParoleViolatorLog, type = &amp;quot;response&amp;quot;, newdata = test)
max(ParolePredTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9072791&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;In the following questions, evaluate the model’s predictions on the test-set using a threshold of 0.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$violator, ParolePredTest &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   167   12
  1    11   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the model&amp;#39;s sensitivity?
12 / (11 + 12) # TP / (TP + FN)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.5217391&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the model&amp;#39;s specificity?
167 / (167 + 12) # TN / (TN + FP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9329609&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the model&amp;#39;s accuracy?
(167 + 12) / nrow(test) # (TN + TP) / N&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8861386&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;What is the accuracy of a simple model that predicts that every parolee is a non-violator?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$violator)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
179  23 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;179 / (179 + 23)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8861386&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Consider a parole board using the model to predict whether parolees will be violators or not.&lt;/p&gt;
&lt;p&gt;The job of a parole board is to make sure that a prisoner is ready to be released into free society, and therefore &lt;strong&gt;parole boards tend to be particularily concerned about releasing prisoners who will violate their parole.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which of the following most likely describes their preferences and best course of action?
#### The board assigns more cost to a false negative than a false positive, and should therefore use a logistic regression cut-off less than 0.5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.5---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.5 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Which of the following is the most accurate assessment of the value of the logistic regression model with a cut-off 0.5 to a parole board, based on the model’s accuracy as compared to the simple baseline model?
#### The model is likely of value to the board, and using a different logistic regression cut-off is likely to improve the model’s value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.6---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.6 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Using the ROCR package, what is the AUC value for the model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred = prediction(ParolePredTest, test$violator)
as.numeric(performance(ROCRpred, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8945834&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.7---evaluating-the-model-on-the-testing-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.7 - Evaluating the Model on the Testing Set&lt;/h3&gt;
&lt;p&gt;Describe the meaning of AUC in this context.
#### The probability the model can correctly differentiate between a randomly selected parole violator and a randomly selected parole non-violator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---identifying-bias-in-observational-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - Identifying Bias in Observational Data&lt;/h3&gt;
&lt;p&gt;Our goal has been to predict the outcome of a parole decision, and we used a publicly available dataset of parole releases for predictions.&lt;/p&gt;
&lt;p&gt;In this final problem, we’ll evaluate a potential source of bias associated with our analysis. It is always important to evaluate a dataset for possible sources of bias.&lt;/p&gt;
&lt;p&gt;The dataset contains all individuals released from parole in 2004, either due to completing their parole term or violating the terms of their parole. However, it does not contain parolees who neither violated their parole nor completed their term in 2004, causing non-violators to be underrepresented.&lt;/p&gt;
&lt;p&gt;This is called “selection bias” or “selecting on the dependent variable,” because only a subset of all relevant parolees were included in our analysis, based on our dependent variable in this analysis (parole violation).&lt;/p&gt;
&lt;p&gt;How could we improve our dataset to best address selection bias?
#### We should use a dataset tracking a group of parolees from the start of their parole until either they violated parole or they completed their term.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict Popular Songs</title>
      <link>/project/music/music/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/music/music/</guid>
      <description>


&lt;p&gt;Popularity of music records&lt;/p&gt;
&lt;p&gt;The music industry has a well-developed market with a global annual revenue around $15 billion. The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales.&lt;/p&gt;
&lt;p&gt;Artists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale. A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.&lt;/p&gt;
&lt;p&gt;Unfortunately, the success of an artist’s release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads, while another single may turn out quite unpopular, and therefore unprofitable.&lt;/p&gt;
&lt;p&gt;Knowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success.&lt;/p&gt;
&lt;p&gt;How can we use analytics to predict the popularity of a song? In this project, we challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.&lt;/p&gt;
&lt;p&gt;Taking an analytics approach, we aim to use information about a song’s properties to predict its popularity. The dataset songs.csv consists of all songs which made it to the Top 10 of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn’t make the Top 10. This data comes from three sources: Wikipedia, Billboard.com, and EchoNest.&lt;/p&gt;
&lt;p&gt;The variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes: time signature, loudness, key, pitch, tempo, and timbre.&lt;/p&gt;
&lt;p&gt;Here’s a detailed description of the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;year = the year the song was released&lt;/li&gt;
&lt;li&gt;songtitle = the title of the song&lt;/li&gt;
&lt;li&gt;artistname = the name of the artist of the song&lt;/li&gt;
&lt;li&gt;songID and artistID = identifying variables for the song and artist&lt;/li&gt;
&lt;li&gt;timesignature and timesignature_confidence = a variable estimating the time signature of the song, and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;loudness = a continuous variable indicating the average amplitude of the audio in decibels&lt;/li&gt;
&lt;li&gt;tempo and tempo_confidence = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;key and key_confidence = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;energy = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness&lt;/li&gt;
&lt;li&gt;pitch = a continuous variable that indicates the pitch of the song
timbre_0_min, timbre_0_max, timbre_1_min, timbre_1_max, . . . , timbre_11_min, and timbre_11_max = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables)&lt;/li&gt;
&lt;li&gt;Top10 = a binary variable indicating whether or not the song made it to the Top 10 of the Billboard Hot 100 Chart (1 if it was in the top 10, and 0 if it was not)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Use the read.csv function to load the dataset “songs.csv” into R. How many observations (songs) are from the year 2010?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs &amp;lt;- read.csv(&amp;quot;songs.csv&amp;quot;)
str(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   7574 obs. of  39 variables:
 $ year                    : int  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...
 $ songtitle               : Factor w/ 7141 levels &amp;quot;̈́ l&amp;#39;or_e des bois&amp;quot;,..: 6204 5522 241 3115 48 608 255 4419 2886 6756 ...
 $ artistname              : Factor w/ 1032 levels &amp;quot;50 Cent&amp;quot;,&amp;quot;98 Degrees&amp;quot;,..: 3 3 3 3 3 3 3 3 3 12 ...
 $ songID                  : Factor w/ 7549 levels &amp;quot;SOAACNI1315CD4AC42&amp;quot;,..: 595 5439 5252 1716 3431 1020 1831 3964 6904 2473 ...
 $ artistID                : Factor w/ 1047 levels &amp;quot;AR00B1I1187FB433EB&amp;quot;,..: 671 671 671 671 671 671 671 671 671 507 ...
 $ timesignature           : int  3 4 4 4 4 4 4 4 4 4 ...
 $ timesignature_confidence: num  0.853 1 1 1 0.788 1 0.968 0.861 0.622 0.938 ...
 $ loudness                : num  -4.26 -4.05 -3.57 -3.81 -4.71 ...
 $ tempo                   : num  91.5 140 160.5 97.5 140.1 ...
 $ tempo_confidence        : num  0.953 0.921 0.489 0.794 0.286 0.347 0.273 0.83 0.018 0.929 ...
 $ key                     : int  11 10 2 1 6 4 10 5 9 11 ...
 $ key_confidence          : num  0.453 0.469 0.209 0.632 0.483 0.627 0.715 0.423 0.751 0.602 ...
 $ energy                  : num  0.967 0.985 0.99 0.939 0.988 ...
 $ pitch                   : num  0.024 0.025 0.026 0.013 0.063 0.038 0.026 0.033 0.027 0.004 ...
 $ timbre_0_min            : num  0.002 0 0.003 0 0 ...
 $ timbre_0_max            : num  57.3 57.4 57.4 57.8 56.9 ...
 $ timbre_1_min            : num  -6.5 -37.4 -17.2 -32.1 -223.9 ...
 $ timbre_1_max            : num  171 171 171 221 171 ...
 $ timbre_2_min            : num  -81.7 -149.6 -72.9 -138.6 -147.2 ...
 $ timbre_2_max            : num  95.1 180.3 157.9 173.4 166 ...
 $ timbre_3_min            : num  -285 -380.1 -204 -73.5 -128.1 ...
 $ timbre_3_max            : num  259 384 251 373 389 ...
 $ timbre_4_min            : num  -40.4 -48.7 -66 -55.6 -43.9 ...
 $ timbre_4_max            : num  73.6 100.4 152.1 119.2 99.3 ...
 $ timbre_5_min            : num  -104.7 -87.3 -98.7 -77.5 -96.1 ...
 $ timbre_5_max            : num  183.1 42.8 141.4 141.2 38.3 ...
 $ timbre_6_min            : num  -88.8 -86.9 -88.9 -70.8 -110.8 ...
 $ timbre_6_max            : num  73.5 75.5 66.5 64.5 72.4 ...
 $ timbre_7_min            : num  -71.1 -65.8 -67.4 -63.7 -55.9 ...
 $ timbre_7_max            : num  82.5 106.9 80.6 96.7 110.3 ...
 $ timbre_8_min            : num  -52 -61.3 -59.8 -78.7 -56.5 ...
 $ timbre_8_max            : num  39.1 35.4 46 41.1 37.6 ...
 $ timbre_9_min            : num  -35.4 -81.9 -46.3 -49.2 -48.6 ...
 $ timbre_9_max            : num  71.6 74.6 59.9 95.4 67.6 ...
 $ timbre_10_min           : num  -126.4 -103.8 -108.3 -102.7 -52.8 ...
 $ timbre_10_max           : num  18.7 121.9 33.3 46.4 22.9 ...
 $ timbre_11_min           : num  -44.8 -38.9 -43.7 -59.4 -50.4 ...
 $ timbre_11_max           : num  26 22.5 25.7 37.1 32.8 ...
 $ Top10                   : int  0 0 0 0 0 0 0 0 0 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      year          songtitle              artistname  
 Min.   :1990   Intro    :  15   Various artists: 162  
 1st Qu.:1997   Forever  :   8   Anal Cunt      :  49  
 Median :2002   Home     :   7   Various Artists:  44  
 Mean   :2001   Goodbye  :   6   Tori Amos      :  41  
 3rd Qu.:2006   Again    :   5   Eels           :  37  
 Max.   :2010   Beautiful:   5   Napalm Death   :  37  
                (Other)  :7528   (Other)        :7204  
                songID                   artistID    timesignature  
 SOALSZJ1370F1A7C75:   2   ARAGWS81187FB3F768: 222   Min.   :0.000  
 SOANPAC13936E0B640:   2   ARL14X91187FB4CF14:  49   1st Qu.:4.000  
 SOBDGMX12B0B80808E:   2   AR4KS8C1187FB4CF3D:  41   Median :4.000  
 SOBUDCZ12A58A80013:   2   AR0JZZ01187B9B2C99:  37   Mean   :3.894  
 SODFRLK13134387FB5:   2   ARZGTK71187B9AC7F5:  37   3rd Qu.:4.000  
 SOEJPOK12A6D4FAFE4:   2   AR95XYH1187FB53951:  31   Max.   :7.000  
 (Other)           :7562   (Other)           :7157                  
 timesignature_confidence    loudness           tempo       
 Min.   :0.0000           Min.   :-42.451   Min.   :  0.00  
 1st Qu.:0.8193           1st Qu.:-10.847   1st Qu.: 88.86  
 Median :0.9790           Median : -7.649   Median :103.27  
 Mean   :0.8533           Mean   : -8.817   Mean   :107.35  
 3rd Qu.:1.0000           3rd Qu.: -5.640   3rd Qu.:124.80  
 Max.   :1.0000           Max.   :  1.305   Max.   :244.31  
                                                            
 tempo_confidence      key         key_confidence       energy       
 Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.00002  
 1st Qu.:0.3720   1st Qu.: 2.000   1st Qu.:0.2040   1st Qu.:0.50014  
 Median :0.7015   Median : 6.000   Median :0.4515   Median :0.71816  
 Mean   :0.6229   Mean   : 5.385   Mean   :0.4338   Mean   :0.67547  
 3rd Qu.:0.8920   3rd Qu.: 9.000   3rd Qu.:0.6460   3rd Qu.:0.88740  
 Max.   :1.0000   Max.   :11.000   Max.   :1.0000   Max.   :0.99849  
                                                                     
     pitch          timbre_0_min     timbre_0_max    timbre_1_min    
 Min.   :0.00000   Min.   : 0.000   Min.   :12.58   Min.   :-333.72  
 1st Qu.:0.00300   1st Qu.: 0.000   1st Qu.:53.12   1st Qu.:-160.12  
 Median :0.00700   Median : 0.027   Median :55.53   Median :-107.75  
 Mean   :0.01082   Mean   : 4.123   Mean   :54.46   Mean   :-110.79  
 3rd Qu.:0.01400   3rd Qu.: 2.772   3rd Qu.:57.08   3rd Qu.: -59.71  
 Max.   :0.54100   Max.   :48.353   Max.   :64.01   Max.   : 123.73  
                                                                     
  timbre_1_max     timbre_2_min      timbre_2_max      timbre_3_min    
 Min.   :-74.37   Min.   :-324.86   Min.   : -0.832   Min.   :-495.36  
 1st Qu.:171.13   1st Qu.:-167.64   1st Qu.:100.519   1st Qu.:-226.87  
 Median :194.40   Median :-136.60   Median :129.908   Median :-170.61  
 Mean   :212.34   Mean   :-136.89   Mean   :136.673   Mean   :-186.11  
 3rd Qu.:239.24   3rd Qu.:-106.51   3rd Qu.:166.121   3rd Qu.:-131.56  
 Max.   :549.97   Max.   :  34.57   Max.   :397.095   Max.   : -21.55  
                                                                       
  timbre_3_max     timbre_4_min      timbre_4_max      timbre_5_min    
 Min.   : 12.85   Min.   :-207.07   Min.   : -0.651   Min.   :-262.48  
 1st Qu.:127.14   1st Qu.: -77.69   1st Qu.: 83.966   1st Qu.:-113.58  
 Median :189.50   Median : -63.83   Median :107.422   Median : -95.47  
 Mean   :211.81   Mean   : -65.28   Mean   :108.227   Mean   :-104.00  
 3rd Qu.:290.72   3rd Qu.: -51.34   3rd Qu.:130.286   3rd Qu.: -81.02  
 Max.   :499.62   Max.   :  51.43   Max.   :257.801   Max.   : -42.17  
                                                                       
  timbre_5_max     timbre_6_min       timbre_6_max     timbre_7_min     
 Min.   :-22.41   Min.   :-152.170   Min.   : 12.70   Min.   :-214.791  
 1st Qu.: 84.64   1st Qu.: -94.792   1st Qu.: 59.04   1st Qu.:-101.171  
 Median :119.90   Median : -80.418   Median : 70.47   Median : -81.797  
 Mean   :127.04   Mean   : -80.944   Mean   : 72.17   Mean   : -84.313  
 3rd Qu.:162.34   3rd Qu.: -66.521   3rd Qu.: 83.19   3rd Qu.: -64.301  
 Max.   :350.94   Max.   :   4.503   Max.   :208.39   Max.   :   5.153  
                                                                        
  timbre_7_max     timbre_8_min       timbre_8_max     timbre_9_min    
 Min.   : 15.70   Min.   :-158.756   Min.   :-25.95   Min.   :-149.51  
 1st Qu.: 76.50   1st Qu.: -73.051   1st Qu.: 40.58   1st Qu.: -70.28  
 Median : 94.63   Median : -62.661   Median : 49.22   Median : -58.65  
 Mean   : 95.65   Mean   : -63.704   Mean   : 50.06   Mean   : -59.52  
 3rd Qu.:112.71   3rd Qu.: -52.983   3rd Qu.: 58.46   3rd Qu.: -47.70  
 Max.   :214.82   Max.   :  -2.382   Max.   :144.99   Max.   :   1.14  
                                                                       
  timbre_9_max     timbre_10_min     timbre_10_max     timbre_11_min     
 Min.   :  8.415   Min.   :-208.82   Min.   : -6.359   Min.   :-145.599  
 1st Qu.: 53.037   1st Qu.:-105.13   1st Qu.: 39.196   1st Qu.: -58.058  
 Median : 65.935   Median : -83.07   Median : 50.895   Median : -50.892  
 Mean   : 68.028   Mean   : -87.34   Mean   : 55.521   Mean   : -50.868  
 3rd Qu.: 81.267   3rd Qu.: -64.52   3rd Qu.: 66.593   3rd Qu.: -43.292  
 Max.   :161.518   Max.   : -10.64   Max.   :192.417   Max.   :  -6.497  
                                                                         
 timbre_11_max        Top10       
 Min.   :  7.20   Min.   :0.0000  
 1st Qu.: 38.98   1st Qu.:0.0000  
 Median : 46.44   Median :0.0000  
 Mean   : 47.49   Mean   :0.1477  
 3rd Qu.: 55.03   3rd Qu.:0.0000  
 Max.   :110.27   Max.   :1.0000  
                                  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(songs$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 
 328  196  186  324  198  258  178  329  380  357  363  282  518  434  479 
2005 2006 2007 2008 2009 2010 
 392  479  622  415  483  373 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;373&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;How many songs does the dataset include for which the artist name is “Michael Jackson”?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(songs, artistname == &amp;quot;Michael Jackson&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 18&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Which of these songs by Michael Jackson made it to the Top 10? Select all that apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(songs, 
       artistname == &amp;quot;Michael Jackson&amp;quot; &amp;amp; Top10 == 1,
       select = c(artistname, songtitle))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          artistname         songtitle
4329 Michael Jackson You Rock My World
6207 Michael Jackson You Are Not Alone
6210 Michael Jackson    Black or White
6218 Michael Jackson Remember the Time
6915 Michael Jackson     In The Closet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You Rock My World, You Are Not Alone&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). What are the values of this variable that occur in our dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs$timesignature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   4.000   4.000   3.894   4.000   7.000 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(songs$timesignature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
   0    1    3    4    5    7 
  10  143  503 6787  112   19 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which timesignature value is the most frequent among songs in our dataset?
#### 4&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.5---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.5 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Out of all of the songs in our dataset, the song with the highest tempo is one of the following songs.&lt;/p&gt;
&lt;p&gt;Which one is it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs$tempo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00   88.86  103.27  107.35  124.80  244.31 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which.max(songs$tempo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 6206&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs$tempo[6206]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 244.307&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(songs, tempo == 244.307))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs$songtitle[6206]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] Wanna Be Startin&amp;#39; Somethin&amp;#39;
7141 Levels: ̈́ l&amp;#39;or_e des bois _\x84_ _\x84\x8d ... Zumbi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wanna Be Startin’ Somethin’&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set “SongsTrain” consisting of all the observations up to and including 2009 song releases, and a testing set “SongsTest”, consisting of the 2010 song releases.&lt;/p&gt;
&lt;p&gt;How many observations (songs) are in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsTrain &amp;lt;- subset(songs, year &amp;lt;= 2009)
SongsTest &amp;lt;- subset(songs, year == 2010)
nrow(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7574&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(SongsTrain) + nrow(SongsTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7574&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Creating our Prediction Model&lt;/h3&gt;
&lt;p&gt;In this problem, our outcome variable is “Top10” - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart.&lt;/p&gt;
&lt;p&gt;Since the outcome variable is binary, we will build a &lt;strong&gt;logistic regression&lt;/strong&gt; model. We’ll start by using all song attributes as our independent variables, which we’ll call Model 1. We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model.&lt;/p&gt;
&lt;p&gt;So we won’t use the variables “year”, “songtitle”, “artistname”, “songID” or “artistID”. We have seen in the lecture that, to build the logistic regression model, we would normally explicitly input the formula including all the independent variables in R. However, in this case, this is a tedious amount of work since we have a large number of independent variables. There is a nice trick to avoid doing so. Let’s suppose that, except for the outcome variable Top10, all other variables in the training set are inputs to Model 1. Then, we can use the formula SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial) to build our model. Notice that the “.” is used in place of enumerating all the independent variables. (Also, keep in mind that you can choose to put quotes around binomial, or leave out the quotes. R can understand this argument either way.) However, in our case, we want to exclude some of the variables in our dataset from being used as independent variables (“year”, “songtitle”, “artistname”, “songID”, and “artistID”).&lt;/p&gt;
&lt;p&gt;To do this, we can use the following trick. First define a vector of variable names called nonvars - these are the variables that we won’t use in our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonvars = c(&amp;quot;year&amp;quot;, &amp;quot;songtitle&amp;quot;, &amp;quot;artistname&amp;quot;, &amp;quot;songID&amp;quot;, &amp;quot;artistID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove these variables from our training and testing sets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. You should use SongsTrain to build the model.&lt;/p&gt;
&lt;p&gt;Looking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog1 &amp;lt;- glm(Top10 ~ ., data = SongsTrain, family=binomial)
summary(SongsLog1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = Top10 ~ ., family = binomial, data = SongsTrain)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9220  -0.5399  -0.3459  -0.1845   3.0770  

Coefficients:
                           Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)               1.470e+01  1.806e+00   8.138 4.03e-16 ***
timesignature             1.264e-01  8.674e-02   1.457 0.145050    
timesignature_confidence  7.450e-01  1.953e-01   3.815 0.000136 ***
loudness                  2.999e-01  2.917e-02  10.282  &amp;lt; 2e-16 ***
tempo                     3.634e-04  1.691e-03   0.215 0.829889    
tempo_confidence          4.732e-01  1.422e-01   3.329 0.000873 ***
key                       1.588e-02  1.039e-02   1.529 0.126349    
key_confidence            3.087e-01  1.412e-01   2.187 0.028760 *  
energy                   -1.502e+00  3.099e-01  -4.847 1.25e-06 ***
pitch                    -4.491e+01  6.835e+00  -6.570 5.02e-11 ***
timbre_0_min              2.316e-02  4.256e-03   5.441 5.29e-08 ***
timbre_0_max             -3.310e-01  2.569e-02 -12.882  &amp;lt; 2e-16 ***
timbre_1_min              5.881e-03  7.798e-04   7.542 4.64e-14 ***
timbre_1_max             -2.449e-04  7.152e-04  -0.342 0.732087    
timbre_2_min             -2.127e-03  1.126e-03  -1.889 0.058843 .  
timbre_2_max              6.586e-04  9.066e-04   0.726 0.467571    
timbre_3_min              6.920e-04  5.985e-04   1.156 0.247583    
timbre_3_max             -2.967e-03  5.815e-04  -5.103 3.34e-07 ***
timbre_4_min              1.040e-02  1.985e-03   5.237 1.63e-07 ***
timbre_4_max              6.110e-03  1.550e-03   3.942 8.10e-05 ***
timbre_5_min             -5.598e-03  1.277e-03  -4.385 1.16e-05 ***
timbre_5_max              7.736e-05  7.935e-04   0.097 0.922337    
timbre_6_min             -1.686e-02  2.264e-03  -7.445 9.66e-14 ***
timbre_6_max              3.668e-03  2.190e-03   1.675 0.093875 .  
timbre_7_min             -4.549e-03  1.781e-03  -2.554 0.010661 *  
timbre_7_max             -3.774e-03  1.832e-03  -2.060 0.039408 *  
timbre_8_min              3.911e-03  2.851e-03   1.372 0.170123    
timbre_8_max              4.011e-03  3.003e-03   1.336 0.181620    
timbre_9_min              1.367e-03  2.998e-03   0.456 0.648356    
timbre_9_max              1.603e-03  2.434e-03   0.659 0.510188    
timbre_10_min             4.126e-03  1.839e-03   2.244 0.024852 *  
timbre_10_max             5.825e-03  1.769e-03   3.292 0.000995 ***
timbre_11_min            -2.625e-02  3.693e-03  -7.108 1.18e-12 ***
timbre_11_max             1.967e-02  3.385e-03   5.811 6.21e-09 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6017.5  on 7200  degrees of freedom
Residual deviance: 4759.2  on 7167  degrees of freedom
AIC: 4827.2

Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AIC: 4827.2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;Let’s now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). What does the model suggest?
#### The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex.&lt;/p&gt;
&lt;p&gt;What does Model 1 suggest in terms of complexity?
#### Mainstream listeners tend to prefer less complex songs&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;Songs with heavier instrumentation tend to be louder (have higher values in the variable “loudness”) and more energetic (have higher values in the variable “energy”). By inspecting the coefficient of the variable “loudness”, what does Model 1 suggest?
#### Mainstream listeners prefer songs with heavy instrumentation&lt;/p&gt;
&lt;p&gt;By inspecting the coefficient of the variable “energy”, do we draw the same conclusions as above?
#### No&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;What is the correlation between the variables “loudness” and “energy” in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(SongsTrain$loudness, SongsTrain$energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7399067&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given that these two variables are highly correlated, Model 1 suffers from multicollinearity. To avoid this issue, we will omit one of these two variables and re-run the logistic regression.&lt;/p&gt;
&lt;p&gt;In the rest of this problem, we’ll build two variations of our original model: Model 2, in which we keep “energy” and omit “loudness”, and Model 3, in which we keep “loudness” and omit “energy”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;Create Model 2, which is Model 1 without the independent variable “loudness”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just subtracted the variable loudness. We couldn’t do this with the variables “songtitle” and “artistname”, because they are not numeric variables, and we might get different values in the test-set that the training set has never seen. But this approach (subtracting the variable from the model formula) will always work when you want to remove numeric variables.&lt;/p&gt;
&lt;p&gt;Look at the summary of SongsLog2, and inspect the coefficient of the variable “energy”. What do you observe?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(SongsLog2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = Top10 ~ . - loudness, family = binomial, data = SongsTrain)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0983  -0.5607  -0.3602  -0.1902   3.3107  

Coefficients:
                           Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)              -2.241e+00  7.465e-01  -3.002 0.002686 ** 
timesignature             1.625e-01  8.734e-02   1.860 0.062873 .  
timesignature_confidence  6.885e-01  1.924e-01   3.578 0.000346 ***
tempo                     5.521e-04  1.665e-03   0.332 0.740226    
tempo_confidence          5.497e-01  1.407e-01   3.906 9.40e-05 ***
key                       1.740e-02  1.026e-02   1.697 0.089740 .  
key_confidence            2.954e-01  1.394e-01   2.118 0.034163 *  
energy                    1.813e-01  2.608e-01   0.695 0.486991    
pitch                    -5.150e+01  6.857e+00  -7.511 5.87e-14 ***
timbre_0_min              2.479e-02  4.240e-03   5.847 5.01e-09 ***
timbre_0_max             -1.007e-01  1.178e-02  -8.551  &amp;lt; 2e-16 ***
timbre_1_min              7.143e-03  7.710e-04   9.265  &amp;lt; 2e-16 ***
timbre_1_max             -7.830e-04  7.064e-04  -1.108 0.267650    
timbre_2_min             -1.579e-03  1.109e-03  -1.424 0.154531    
timbre_2_max              3.889e-04  8.964e-04   0.434 0.664427    
timbre_3_min              6.500e-04  5.949e-04   1.093 0.274524    
timbre_3_max             -2.462e-03  5.674e-04  -4.339 1.43e-05 ***
timbre_4_min              9.115e-03  1.952e-03   4.670 3.02e-06 ***
timbre_4_max              6.306e-03  1.532e-03   4.115 3.87e-05 ***
timbre_5_min             -5.641e-03  1.255e-03  -4.495 6.95e-06 ***
timbre_5_max              6.937e-04  7.807e-04   0.889 0.374256    
timbre_6_min             -1.612e-02  2.235e-03  -7.214 5.45e-13 ***
timbre_6_max              3.814e-03  2.157e-03   1.768 0.076982 .  
timbre_7_min             -5.102e-03  1.755e-03  -2.907 0.003644 ** 
timbre_7_max             -3.158e-03  1.811e-03  -1.744 0.081090 .  
timbre_8_min              4.488e-03  2.810e-03   1.597 0.110254    
timbre_8_max              6.423e-03  2.950e-03   2.177 0.029497 *  
timbre_9_min             -4.282e-04  2.955e-03  -0.145 0.884792    
timbre_9_max              3.525e-03  2.377e-03   1.483 0.138017    
timbre_10_min             2.993e-03  1.804e-03   1.660 0.097004 .  
timbre_10_max             7.367e-03  1.731e-03   4.255 2.09e-05 ***
timbre_11_min            -2.837e-02  3.630e-03  -7.815 5.48e-15 ***
timbre_11_max             1.829e-02  3.341e-03   5.476 4.34e-08 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6017.5  on 7200  degrees of freedom
Residual deviance: 4871.8  on 7168  degrees of freedom
AIC: 4937.8

Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model 2 suggests that songs with high energy levels tend to be more popular. This contradicts our observation in Model 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;Now, create Model 3, which should be exactly like Model 1, but without the variable “energy”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the summary of Model 3 and inspect the coefficient of the variable “loudness”. Remembering that higher loudness and energy both occur in songs with heavier instrumentation, do we make the same observation about the popularity of heavy instrumentation as we did with Model 2?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(SongsLog3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
glm(formula = Top10 ~ . - energy, family = binomial, data = SongsTrain)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9182  -0.5417  -0.3481  -0.1874   3.4171  

Coefficients:
                           Estimate Std. Error z value Pr(&amp;gt;|z|)    
(Intercept)               1.196e+01  1.714e+00   6.977 3.01e-12 ***
timesignature             1.151e-01  8.726e-02   1.319 0.187183    
timesignature_confidence  7.143e-01  1.946e-01   3.670 0.000242 ***
loudness                  2.306e-01  2.528e-02   9.120  &amp;lt; 2e-16 ***
tempo                    -6.460e-04  1.665e-03  -0.388 0.698107    
tempo_confidence          3.841e-01  1.398e-01   2.747 0.006019 ** 
key                       1.649e-02  1.035e-02   1.593 0.111056    
key_confidence            3.394e-01  1.409e-01   2.409 0.015984 *  
pitch                    -5.328e+01  6.733e+00  -7.914 2.49e-15 ***
timbre_0_min              2.205e-02  4.239e-03   5.200 1.99e-07 ***
timbre_0_max             -3.105e-01  2.537e-02 -12.240  &amp;lt; 2e-16 ***
timbre_1_min              5.416e-03  7.643e-04   7.086 1.38e-12 ***
timbre_1_max             -5.115e-04  7.110e-04  -0.719 0.471928    
timbre_2_min             -2.254e-03  1.120e-03  -2.012 0.044190 *  
timbre_2_max              4.119e-04  9.020e-04   0.457 0.647915    
timbre_3_min              3.179e-04  5.869e-04   0.542 0.588083    
timbre_3_max             -2.964e-03  5.758e-04  -5.147 2.64e-07 ***
timbre_4_min              1.105e-02  1.978e-03   5.585 2.34e-08 ***
timbre_4_max              6.467e-03  1.541e-03   4.196 2.72e-05 ***
timbre_5_min             -5.135e-03  1.269e-03  -4.046 5.21e-05 ***
timbre_5_max              2.979e-04  7.855e-04   0.379 0.704526    
timbre_6_min             -1.784e-02  2.246e-03  -7.945 1.94e-15 ***
timbre_6_max              3.447e-03  2.182e-03   1.580 0.114203    
timbre_7_min             -5.128e-03  1.768e-03  -2.900 0.003733 ** 
timbre_7_max             -3.394e-03  1.820e-03  -1.865 0.062208 .  
timbre_8_min              3.686e-03  2.833e-03   1.301 0.193229    
timbre_8_max              4.658e-03  2.988e-03   1.559 0.119022    
timbre_9_min             -9.318e-05  2.957e-03  -0.032 0.974859    
timbre_9_max              1.342e-03  2.424e-03   0.554 0.579900    
timbre_10_min             4.050e-03  1.827e-03   2.217 0.026637 *  
timbre_10_max             5.793e-03  1.759e-03   3.294 0.000988 ***
timbre_11_min            -2.638e-02  3.683e-03  -7.162 7.96e-13 ***
timbre_11_max             1.984e-02  3.365e-03   5.896 3.74e-09 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6017.5  on 7200  degrees of freedom
Residual deviance: 4782.7  on 7168  degrees of freedom
AIC: 4848.7

Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the remainder of this problem, we’ll just use Model 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;Make predictions on the test-set using Model 3. What is the accuracy of Model 3 on the test-set, using a threshold of 0.45? (Compute the accuracy as a number between 0 and 1.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predSongsTest = predict(SongsLog3, type=&amp;quot;response&amp;quot;, newdata = SongsTest)
table(SongsTest$Top10, predSongsTest &amp;gt; 0.45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   309    5
  1    40   19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(309 + 19) / nrow(SongsTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8793566&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;Let’s check if there’s any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs.&lt;/p&gt;
&lt;p&gt;What would the accuracy of the baseline model be on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(SongsTest$Top10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
  0   1 
314  59 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;314/(314 + 59)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8418231&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;It seems that Model 3 gives us a small improvement over the baseline model. Still, does it create an edge? Let’s view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. The company’s objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular.&lt;/p&gt;
&lt;p&gt;A competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer.&lt;/p&gt;
&lt;p&gt;How many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(SongsTest$Top10, predSongsTest &amp;gt; 0.45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   
    FALSE TRUE
  0   309    5
  1    40   19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;19&lt;/p&gt;
&lt;p&gt;How many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45?
#### 5&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Validating Our Model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the sensitivity of Model 3 on the test set, using a threshold of 0.45?
19 / (40 + 19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3220339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the specificity of Model 3 on the test set, using a threshold of 0.45?
309 / (309 + 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.9840764&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model 3 favors specificity over sensitivity.&lt;/li&gt;
&lt;li&gt;Model 3 provides conservative predictions, and predicts that a song will make it to the Top 10 very rarely. So while it detects less than half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Flu Epidemics via Search Engine Query Data</title>
      <link>/project/flu_epidemics/flu_epidemics/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/flu_epidemics/flu_epidemics/</guid>
      <description>


&lt;p&gt;Flu epidemics constitute a major public health concern causing respiratory illnesses, hospitalizations, and deaths. According to the National Vital Statistics Reports published in October 2012, influenza ranked as the eighth leading cause of death in 2011 in the U.S. Each year, 250,000 to 500,000 deaths are attributed to influenza related diseases throughout the world.&lt;/p&gt;
&lt;p&gt;The U.S. Centers for Disease Control and Prevention (CDC) and the European Influenza Surveillance Scheme (EISS) detect influenza activity through virologic and clinical data, including Influenza-like Illness (ILI) physician visits. Reporting national and regional data, however, are published with a 1-2 week lag.&lt;/p&gt;
&lt;p&gt;The Google Flu Trends project was initiated to see if faster reporting can be made possible by considering &lt;strong&gt;flu-related online search queries&lt;/strong&gt; – data that is available almost immediately.&lt;/p&gt;
&lt;p&gt;I would like to estimate influenza-like illness (ILI) activity using Google web search logs. Fortunately, one can easily access this data online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ILI Data - The CDC publishes on its website the official regional and state-level percentage of patient visits to healthcare providers for ILI purposes on a weekly basis.&lt;/li&gt;
&lt;li&gt;Google Search Queries - Google Trends allows public retrieval of weekly counts for every query searched by users around the world.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each location, the counts are normalized by dividing the count for each query in a particular week by the total number of online search queries submitted in that location during the week. Then, the values are adjusted to be between 0 and 1.&lt;/p&gt;
&lt;p&gt;The csv file FluTrain.csv aggregates this data from January 1, 2004 until December 31, 2011 as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Week” - The range of dates represented by this observation, in year/month/day format.&lt;/li&gt;
&lt;li&gt;“ILI” - This column lists the percentage of ILI-related physician visits for the corresponding week.&lt;/li&gt;
&lt;li&gt;“Queries” - This column lists the fraction of queries that are ILI-related for the corresponding week, adjusted to be between 0 and 1 (higher values correspond to more ILI-related search queries).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before applying analytics tools on the training set, we first need to understand the data at hand. Looking at the time period 2004-2011, which week corresponds to the highest percentage of ILI-related physician visits?&lt;/p&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain &amp;lt;- read.csv(&amp;quot;FluTrain.csv&amp;quot;)
summary(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                      Week          ILI            Queries       
 2004-01-04 - 2004-01-10:  1   Min.   :0.5341   Min.   :0.04117  
 2004-01-11 - 2004-01-17:  1   1st Qu.:0.9025   1st Qu.:0.15671  
 2004-01-18 - 2004-01-24:  1   Median :1.2526   Median :0.28154  
 2004-01-25 - 2004-01-31:  1   Mean   :1.6769   Mean   :0.28603  
 2004-02-01 - 2004-02-07:  1   3rd Qu.:2.0587   3rd Qu.:0.37849  
 2004-02-08 - 2004-02-14:  1   Max.   :7.6189   Max.   :1.00000  
 (Other)                :411                                     &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   417 obs. of  3 variables:
 $ Week   : Factor w/ 417 levels &amp;quot;2004-01-04 - 2004-01-10&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
 $ ILI    : num  2.42 1.81 1.71 1.54 1.44 ...
 $ Queries: num  0.238 0.22 0.226 0.238 0.224 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - EDA&lt;/h3&gt;
&lt;p&gt;Select the day of the month corresponding to the start of this week?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain[which.max(FluTrain$ILI),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       Week      ILI Queries
303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which week corresponds to the highest percentage of ILI-related query fraction?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain[which.max(FluTrain$Queries),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       Week      ILI Queries
303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(FluTrain, Queries == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                       Week      ILI Queries
303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;October 18, 2009&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - EDA&lt;/h3&gt;
&lt;p&gt;Let’s now understand the data at a high level. Plot the histogram of the dependent variable, ILI.&lt;/p&gt;
&lt;p&gt;What best describes the distribution of values of ILI?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(FluTrain$ILI)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;most-of-the-ili-values-are-small-with-a-relatively-small-number-of-much-larger-values-in-statistics-this-sort-of-data-is-called-skew-right.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Most of the ILI values are small, with a relatively small number of much larger values (in statistics, this sort of data is called “skew right”).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - EDA&lt;/h3&gt;
&lt;p&gt;When handling a skewed dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself – this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models.&lt;/p&gt;
&lt;p&gt;In this problem, I’ll predict the natural log of the ILI variable, which can be computed using the log() function. Plot the natural logarithm of ILI versus Queries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(log(FluTrain$ILI), FluTrain$Queries)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(FluTrain$Queries, log(FluTrain$ILI))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What does the plot suggest?
#### There is a positive, linear relationship between log(ILI) and Queries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;Based on the plot we just made, it seems that a linear regression model could be a good modeling choice. Based on our understanding of the data from the previous subproblem, which model best describes our estimation problem?
#### log(ILI) = intercept + coefficient x Queries, where the coefficient is positive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;Let’s call the regression model from the previous problem (Problem 2.1). FluTrend1 and run it. Hint: to take the logarithm of a variable Var in a regression equation, you simply use log(Var) when specifying the formula to the lm() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrend1 &amp;lt;- lm(log(ILI) ~ Queries, data = FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the training set R-squared value for FluTrend1 model (the “Multiple R-squared”)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = log(ILI) ~ Queries, data = FluTrain)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.76003 -0.19696 -0.01657  0.18685  1.06450 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -0.49934    0.03041  -16.42   &amp;lt;2e-16 ***
Queries      2.96129    0.09312   31.80   &amp;lt;2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.2995 on 415 degrees of freedom
Multiple R-squared:  0.709, Adjusted R-squared:  0.7083 
F-statistic:  1011 on 1 and 415 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0.709&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;For a single variable linear regression model, there is a direct relationship between the R-squared and the correlation between the independent and the dependent variables.&lt;/p&gt;
&lt;p&gt;What is the relationship we infer from our problem? (Don’t forget that you can use the cor function to compute the correlation between two variables.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corILIQueries &amp;lt;- cor(log(FluTrain$ILI), FluTrain$Queries)
cor(FluTrain$ILI, FluTrain$Queries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.8142115&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corILIQueries^2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7090201&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(1/corILIQueries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.1719357&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(-0.5 * corILIQueries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6563792&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note = R-squared = Correlation^2&lt;br /&gt;
Note that the “exp” function stands for the exponential function. The exponential can be computed in R using the function exp().&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;The file provides the 2012 weekly data of the ILI-related search queries and the observed weekly percentage of ILI-related physician visits.&lt;/p&gt;
&lt;p&gt;Load this data into a dataframe called FluTest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest &amp;lt;- read.csv(&amp;quot;FluTest.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally, we would obtain test-set predictions from the model FluTrend1 using the code PredTest1 = predict(FluTrend1, newdata=FluTest) However, the dependent variable in our model is log(ILI), so PredTest1 would contain predictions of the log(ILI) value.&lt;/p&gt;
&lt;p&gt;We are instead interested in obtaining predictions of the ILI value. We can convert from predictions of log(ILI) to predictions of ILI via exponentiation, or the exp() function. The new code, which predicts the ILI value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredTest1 = exp(predict(FluTrend1, newdata=FluTest))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is our estimate for the percentage of ILI-related physician visits for the week of March 11, 2012? (HINT: You can either just output FluTest$Week to find which element corresponds to March 11, 2012, or you can use the “which” function in R. To learn more about the which function, type ?which in your R console.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$Week&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] 2012-01-01 - 2012-01-07 2012-01-08 - 2012-01-14
 [3] 2012-01-15 - 2012-01-21 2012-01-22 - 2012-01-28
 [5] 2012-01-29 - 2012-02-04 2012-02-05 - 2012-02-11
 [7] 2012-02-12 - 2012-02-18 2012-02-19 - 2012-02-25
 [9] 2012-02-26 - 2012-03-03 2012-03-04 - 2012-03-10
[11] 2012-03-11 - 2012-03-17 2012-03-18 - 2012-03-24
[13] 2012-03-25 - 2012-03-31 2012-04-01 - 2012-04-07
[15] 2012-04-08 - 2012-04-14 2012-04-15 - 2012-04-21
[17] 2012-04-22 - 2012-04-28 2012-04-29 - 2012-05-05
[19] 2012-05-06 - 2012-05-12 2012-05-13 - 2012-05-19
[21] 2012-05-20 - 2012-05-26 2012-05-27 - 2012-06-02
[23] 2012-06-03 - 2012-06-09 2012-06-10 - 2012-06-16
[25] 2012-06-17 - 2012-06-23 2012-06-24 - 2012-06-30
[27] 2012-07-01 - 2012-07-07 2012-07-08 - 2012-07-14
[29] 2012-07-15 - 2012-07-21 2012-07-22 - 2012-07-28
[31] 2012-07-29 - 2012-08-04 2012-08-05 - 2012-08-11
[33] 2012-08-12 - 2012-08-18 2012-08-19 - 2012-08-25
[35] 2012-08-26 - 2012-09-01 2012-09-02 - 2012-09-08
[37] 2012-09-09 - 2012-09-15 2012-09-16 - 2012-09-22
[39] 2012-09-23 - 2012-09-29 2012-09-30 - 2012-10-06
[41] 2012-10-07 - 2012-10-13 2012-10-14 - 2012-10-20
[43] 2012-10-21 - 2012-10-27 2012-10-28 - 2012-11-03
[45] 2012-11-04 - 2012-11-10 2012-11-11 - 2012-11-17
[47] 2012-11-18 - 2012-11-24 2012-11-25 - 2012-12-01
[49] 2012-12-02 - 2012-12-08 2012-12-09 - 2012-12-15
[51] 2012-12-16 - 2012-12-22 2012-12-23 - 2012-12-29
52 Levels: 2012-01-01 - 2012-01-07 ... 2012-12-23 - 2012-12-29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest[11, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                      Week      ILI   Queries
11 2012-03-11 - 2012-03-17 2.293422 0.4329349&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredTest1[11]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      11 
2.187378 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.293422&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;What is the relative error betweeen the estimate (our prediction) and the observed value for the week of March 11, 2012? Note that the relative error is calculated as (Observed ILI - Estimated ILI)/Observed ILI.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(FluTest[11, 2] - PredTest1[11]) / FluTest[11, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        11 
0.04623827 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;What is the Root Mean Square Error (RMSE) between our estimates and the actual observations for the percentage of ILI-related physician visits, on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTestSSE = sum((PredTest1 - FluTest$ILI)^2)
FluTestRMSE = sqrt(FluTestSSE/nrow(FluTest))
FluTestRMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7490645&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;The observations in this dataset are consecutive weekly measurements of the dependent and independent variables. This sort of dataset is called a &lt;strong&gt;“time series.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Often, statistical models can be improved by predicting the current value of the dependent variable using the value of the dependent variable from earlier weeks. In our models, this means we will predict the ILI variable in the current week using values of the ILI variable from previous weeks.&lt;/p&gt;
&lt;p&gt;First, we need to decide the amount of time to lag the observations. Because the ILI variable is reported with a 1- or 2-week lag, a decision maker cannot rely on the previous week’s ILI value to predict the current week’s value. Instead, the decision maker will only have data available from 2 or more weeks ago.&lt;/p&gt;
&lt;p&gt;We will build a variable called ILILag2 that contains the ILI value from 2 weeks before the current observation.&lt;/p&gt;
&lt;p&gt;To do so, we’ll use the &lt;strong&gt;“zoo” package,&lt;/strong&gt; which provides a number of helpful methods for time series models. While many functions are built into R, you need to add new packages to use some functions. New packages can be installed and loaded easily in R. Run the following two codes to install and load the zoo package.&lt;/p&gt;
&lt;p&gt;In the first code, you will be prompted to select a CRAN mirror to use for your download. Select a mirror near you geographically. install.packages(“zoo”)&lt;/p&gt;
&lt;p&gt;After installing and loading the zoo package, create the ILILag2 variable in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The value of -2 passed to lag means to return 2 observations before the current one; a positive value would have returned future observations. The parameter na.pad=TRUE means to add missing values for the first two weeks of our dataset, where we can’t compute the data from 2 weeks earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?lag
?coredata
ILILag2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        1         2         3         4         5         6         7 
       NA        NA 2.4183312 1.8090560 1.7120239 1.5424951 1.4378683 
        8         9        10        11        12        13        14 
1.3242740 1.3072567 1.0369770 1.0103204 1.0524925 1.0200901 0.9244187 
       15        16        17        18        19        20        21 
0.7906450 0.8026098 0.8361300 0.7924358 0.6835877 0.7574523 0.7885854 
       22        23        24        25        26        27        28 
0.8121710 0.8044629 0.8777009 0.7414530 0.6610222 0.7151092 0.5622412 
       29        30        31        32        33        34        35 
0.7868082 0.8606578 0.6899440 0.7796912 0.6281439 0.9024586 0.8064432 
       36        37        38        39        40        41        42 
0.8748878 0.9932130 0.8761408 0.9480916 0.9269426 0.9716430 0.8971591 
       43        44        45        46        47        48        49 
1.0224828 1.0629632 1.1469570 1.2049501 1.3051655 1.2869916 1.5946756 
       50        51        52        53        54        55        56 
1.3971432 1.4499567 1.6174545 2.1911192 2.5664893 2.1764491 2.2017121 
       57        58        59        60        61        62        63 
2.5301211 3.0652381 3.9806083 4.5956803 4.7519706 4.1796206 3.4535851 
       64        65        66        67        68        69        70 
3.1585224 2.6732010 2.3516104 1.8924285 1.5249048 1.4113441 1.2506826 
       71        72        73        74        75        76        77 
1.2070250 1.0789550 1.1452080 1.0612426 1.0567977 1.2519310 1.0141893 
       78        79        80        81        82        83        84 
1.0419693 0.9540274 0.8482299 0.8418715 0.7308936 0.7134316 0.6706772 
       85        86        87        88        89        90        91 
0.6892776 0.7049290 0.6159033 0.6094256 0.6802587 0.7754884 0.6834214 
       92        93        94        95        96        97        98 
0.7810748 0.8069435 1.0763468 1.0586890 1.1152326 1.1238125 1.2548892 
       99       100       101       102       103       104       105 
1.3366090 1.3786364 1.6082900 1.4831056 1.6537399 2.0067892 2.5685716 
      106       107       108       109       110       111       112 
3.0527762 2.4250373 2.0019506 2.0586902 2.2127697 2.3222001 2.4927920 
      113       114       115       116       117       118       119 
2.7948942 2.9691114 2.8395905 2.7779902 2.4728693 2.1806146 2.0167951 
      120       121       122       123       124       125       126 
1.6410133 1.3582865 1.1427983 1.0403125 0.9643469 0.9379817 0.9474493 
      127       128       129       130       131       132       133 
0.8919182 0.8646427 0.9703199 0.8443901 0.7748704 0.8213725 0.8727445 
      134       135       136       137       138       139       140 
0.9226345 0.8994868 0.8430824 0.8818244 0.8171452 0.8715001 0.7386205 
      141       142       143       144       145       146       147 
0.7979660 1.0139373 0.8809358 0.9433663 0.8915462 1.2032228 1.0578822 
      148       149       150       151       152       153       154 
1.1305354 1.1255230 1.2080820 1.3495244 1.4689004 1.8276716 1.6656012 
      155       156       157       158       159       160       161 
1.8596834 2.3889130 2.7897759 3.1154858 2.2694245 1.8635464 1.9998635 
      162       163       164       165       166       167       168 
2.4406044 2.8301821 3.1234256 3.2701949 3.1775688 2.7236366 2.5020140 
      169       170       171       172       173       174       175 
2.4271992 1.9604132 1.5913980 1.3697835 1.3631668 1.1736951 1.0635756 
      176       177       178       179       180       181       182 
0.9697111 0.9653617 0.8567489 0.8633465 0.9353695 0.7455694 0.7404281 
      183       184       185       186       187       188       189 
0.6728965 0.6662820 0.6627473 0.5456190 0.5862306 0.6606867 0.5340928 
      190       191       192       193       194       195       196 
0.5855491 0.6180750 0.6874647 0.7156961 0.8293131 0.8009115 0.9184839 
      197       198       199       200       201       202       203 
0.8142590 1.0719708 1.2178574 1.2457554 1.3598449 1.4467085 1.5328638 
      204       205       206       207       208       209       210 
1.6665324 1.9748773 1.6730547 1.6340509 1.7459475 1.9364319 2.4890534 
      211       212       213       214       215       216       217 
2.2540484 2.0914715 2.3593428 3.3233143 4.4338100 5.3454714 5.4225751 
      218       219       220       221       222       223       224 
5.3030330 4.2445550 3.6280001 3.0346275 2.5359536 2.0573015 1.7415035 
      225       226       227       228       229       230       231 
1.4065217 1.2686070 1.0771887 0.9934452 0.9112119 0.9721091 0.9932575 
      232       233       234       235       236       237       238 
1.0913202 0.8884460 0.8876915 0.8831874 0.8267564 0.7832014 0.7806103 
      239       240       241       242       243       244       245 
0.7690726 0.7212979 0.7525273 0.7527210 0.7927660 0.7438962 0.8141663 
      246       247       248       249       250       251       252 
0.8384009 0.8511236 1.1097575 1.0311436 1.0228436 1.0301739 1.0124478 
      253       254       255       256       257       258       259 
1.0835911 1.1657765 1.1912964 1.2807470 1.2705251 1.5957825 1.4584994 
      260       261       262       263       264       265       266 
1.4992072 1.6298157 2.1556121 2.0205270 1.5456623 1.6422367 1.9652378 
      267       268       269       270       271       272       273 
2.3436784 2.8605744 3.3421049 3.2056588 3.1004908 2.9581850 2.4638058 
      274       275       276       277       278       279       280 
2.1927224 1.8739459 1.6481690 1.4987776 1.2923267 1.2716411 2.9815890 
      281       282       283       284       285       286       287 
2.4370224 2.2813011 3.8157199 4.2131523 3.1783224 2.5097162 2.0663177 
      288       289       290       291       292       293       294 
1.7180460 1.5596467 1.3085629 1.1869460 1.1379623 1.1500523 1.1126189 
      295       296       297       298       299       300       301 
1.1614188 1.6410714 2.4716598 3.7196936 3.9497480 4.0875636 4.0189724 
      302       303       304       305       306       307       308 
4.6036164 5.6608671 6.8152222 7.6188921 7.3883586 6.3392723 4.9434950 
      309       310       311       312       313       314       315 
3.8099612 3.4410588 2.6677306 2.4718250 2.3449995 2.7143498 2.6766718 
      316       317       318       319       320       321       322 
1.9828382 1.8274862 1.9260563 1.9249472 2.0887684 2.0343408 1.9764946 
      323       324       325       326       327       328       329 
1.9936177 1.8538260 1.8673036 1.6998677 1.4974082 1.4511188 1.2071478 
      330       331       332       333       334       335       336 
1.1741508 1.1620668 1.1721343 1.1216765 1.1498116 1.1332758 1.0817133 
      337       338       339       340       341       342       343 
1.1995860 0.9528083 0.9160321 0.9265822 0.8696197 0.9031331 0.7737757 
      344       345       346       347       348       349       350 
0.7427744 0.7309345 0.7868818 0.7630507 0.8410432 0.7915728 0.9127318 
      351       352       353       354       355       356       357 
1.0339765 0.9340091 1.0818888 1.0656260 1.1350529 1.2525629 1.2456956 
      358       359       360       361       362       363       364 
1.2677380 1.4372295 1.5334125 1.6944544 1.9915024 1.8130453 2.0142579 
      365       366       367       368       369       370       371 
2.5565913 3.3818486 3.4317231 2.6915111 2.9106289 3.4923189 4.0036963 
      372       373       374       375       376       377       378 
4.4353368 4.2421482 4.3971861 3.9025565 3.1507275 2.7242234 2.3333563 
      379       380       381       382       383       384       385 
1.9250003 1.7524260 1.5770365 1.3576558 1.3122310 1.1493747 1.1145057 
      386       387       388       389       390       391       392 
1.1098449 1.0524026 1.0353647 1.1177658 0.9829495 0.9251944 0.8355311 
      393       394       395       396       397       398       399 
0.8323927 0.8555910 0.7069494 0.6943868 0.6879762 0.6447430 0.6753299 
      400       401       402       403       404       405       406 
0.7282297 0.8065263 0.8604084 0.9360754 0.9666827 0.9960071 1.1084635 
      407       408       409       410       411       412       413 
1.2030858 1.2369566 1.2525865 1.3054612 1.4528432 1.4408922 1.4622115 
      414       415       416       417 
1.6554147 1.4657230 1.5181061 1.6639544 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many values are missing in the new ILILag2 variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(FluTrain$ILILag2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;Use the plot() function to plot the log of ILILag2 against the log of ILI.&lt;/p&gt;
&lt;p&gt;Which best describes the relationship between these two variables?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(log(FluTrain$ILILag2), log(FluTrain$ILI))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a strong positive relationship between log(ILILag2) and log(ILI).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;Train a linear regression model on the FluTrain dataset to predict the log of the ILI variable using the Queries variable as well as the log of the ILILag2 variable. Call this model FluTrend2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrend2 &amp;lt;- lm(log(ILI) ~ Queries + log(ILILag2), data = FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which coefficients are significant at the p=0.05 level in this regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52209 -0.11082 -0.01819  0.08143  0.76785 

Coefficients:
             Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  -0.24064    0.01953  -12.32   &amp;lt;2e-16 ***
Queries       1.25578    0.07910   15.88   &amp;lt;2e-16 ***
log(ILILag2)  0.65569    0.02251   29.14   &amp;lt;2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.1703 on 412 degrees of freedom
  (2 observations deleted due to missingness)
Multiple R-squared:  0.9063,    Adjusted R-squared:  0.9059 
F-statistic:  1993 on 2 and 412 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All are significant at p&amp;lt;0.05&lt;/p&gt;
&lt;p&gt;What is the R^2 value of the FluTrend2 model?
#### 0.9063&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;On the basis of R-squared value and significance of coefficients, which statement is the most accurate?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = log(ILI) ~ Queries, data = FluTrain)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.76003 -0.19696 -0.01657  0.18685  1.06450 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -0.49934    0.03041  -16.42   &amp;lt;2e-16 ***
Queries      2.96129    0.09312   31.80   &amp;lt;2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.2995 on 415 degrees of freedom
Multiple R-squared:  0.709, Adjusted R-squared:  0.7083 
F-statistic:  1011 on 1 and 415 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52209 -0.11082 -0.01819  0.08143  0.76785 

Coefficients:
             Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)  -0.24064    0.01953  -12.32   &amp;lt;2e-16 ***
Queries       1.25578    0.07910   15.88   &amp;lt;2e-16 ***
log(ILILag2)  0.65569    0.02251   29.14   &amp;lt;2e-16 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.1703 on 412 degrees of freedom
  (2 observations deleted due to missingness)
Multiple R-squared:  0.9063,    Adjusted R-squared:  0.9059 
F-statistic:  1993 on 2 and 412 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;flutrend2-is-a-stronger-model-than-flutrend1-on-the-training-set-due-to-its-higher-r2-value.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;FluTrend2 is a stronger model than FluTrend1 on the training set, due to it’s higher R^2 value.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;So far, we have only added the ILILag2 variable to the FluTrain dataframe. To make predictions with our FluTrend2 model, we’ll also need to add ILILag2 to the FluTest dataframe (note that adding variables before splitting into a training and testing set can prevent this duplication of effort).&lt;/p&gt;
&lt;p&gt;Modifying the code from the previous subproblem to add an ILILag2 variable to the FluTest dataframe.&lt;/p&gt;
&lt;p&gt;How many missing values are there in this new variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Test_ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE)
FluTest$ILILag2 = coredata(Test_ILILag2)
sum(is.na(FluTest$ILILag2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.2---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.2 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;In this problem, the training and testing sets are split sequentially – the training set contains all observations from 2004-2011 and the testing set contains all observations from 2012.&lt;/p&gt;
&lt;p&gt;There is no time gap between the two datasets, meaning the first observation in FluTest was recorded one week after the last observation in FluTrain. From this, we can identify how to fill in the missing values for the ILILag2 variable in FluTest. Which value should be used to fill in the ILILag2 variable for the first observation in FluTest?&lt;/p&gt;
&lt;p&gt;The ILI value of the second-to-last observation in the FluTrain dataframe. Which value should be used to fill in the ILILag2 variable for the second observation in FluTest?
#### The ILI value of the last observation in the FluTrain dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.3---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.3 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Fill in the missing values for ILILag2 in FluTest. In terms of syntax, you could set the value of ILILag2 in row “x” of the FluTest dataframe to the value of ILI in row “y” of the FluTrain dataframe with “FluTest&lt;span class=&#34;math inline&#34;&gt;\(ILILag2[x] = FluTrain\)&lt;/span&gt;ILI[y]”.&lt;/p&gt;
&lt;p&gt;Use the answer to the previous questions to determine the appropriate values of “x” and “y”. It may be helpful to check the total number of rows in FluTrain using str(FluTrain) or nrow(FluTrain).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 417&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[1] = FluTrain$ILI[416]
FluTest$ILILag2[2] = FluTrain$ILI[417]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new value of the ILILag2 variable in the first row of FluTest?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain$ILI[416]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.852736&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.852736&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new value of the ILILag2 variable in the second row of FluTest?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain$ILI[417]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2.12413&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2.12413&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.4---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.4 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Obtain test-set predictions of the ILI variable from the FluTrend2 model, again remembering to call the exp() function on the result of the predict() function to obtain predictions for ILI instead of log(ILI).&lt;/p&gt;
&lt;p&gt;What is the test-set RMSE of the FluTrend2 model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
FluTestSSE2 = sum((PredTest2 - FluTest$ILI)^2)
FluTestRMSE2 = sqrt(FluTestSSE2/nrow(FluTest))
FluTestRMSE2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2942029&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.5---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.5 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Which model obtained the best test-set RMSE?
#### FluTrend2 (less RMSE is better)&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this analysis, I’ve used a simple time series model with a single lag term. ARIMA models are a more general form of the model we built, which can include multiple lag terms as well as more complicated combinations of previous values of the dependent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading Test Scores</title>
      <link>/project/pisa2009/pisa/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/pisa2009/pisa/</guid>
      <description>


&lt;p&gt;The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in &lt;strong&gt;mathematics, reading, and science.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test provides a quantitative way to compare the performance of students from different parts of the world.&lt;/p&gt;
&lt;p&gt;In this analysis, I’ll predict the reading scores of students from the USA on the 2009 PISA exam.&lt;/p&gt;
&lt;p&gt;The datasets contain information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, &lt;strong&gt;by using the data we are bound by the NCES data use agreement, which prohibits any attempt to determine the identity of any student in the datasets.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each row in the datasets represents one student taking the exam. The datasets have the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grade: The grade in school of the student (most 15-year-olds in America are in 10th grade)&lt;/li&gt;
&lt;li&gt;male: Whether the student is male (1/0)&lt;/li&gt;
&lt;li&gt;raceeth: The race/ethnicity composite of the student&lt;/li&gt;
&lt;li&gt;preschool: Whether the student attended preschool (1/0)&lt;/li&gt;
&lt;li&gt;expectBachelors: Whether the student expects to obtain a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;motherHS: Whether the student’s mother completed high school (1/0)&lt;/li&gt;
&lt;li&gt;motherBachelors: Whether the student’s mother obtained a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;motherWork: Whether the student’s mother has part-time or full-time work (1/0)&lt;/li&gt;
&lt;li&gt;fatherHS: Whether the student’s father completed high school (1/0)&lt;/li&gt;
&lt;li&gt;fatherBachelors: Whether the student’s father obtained a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;fatherWork: Whether the student’s father has part-time or full-time work (1/0)&lt;/li&gt;
&lt;li&gt;selfBornUS: Whether the student was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;motherBornUS: Whether the student’s mother was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;fatherBornUS: Whether the student’s father was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;englishAtHome: Whether the student speaks English at home (1/0)&lt;/li&gt;
&lt;li&gt;computerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0)&lt;/li&gt;
&lt;li&gt;read30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0)&lt;/li&gt;
&lt;li&gt;minutesPerWeekEnglish: The number of minutes per week the student spend in English class&lt;/li&gt;
&lt;li&gt;studentsInEnglish: The number of students in this student’s English class at school&lt;/li&gt;
&lt;li&gt;schoolHasLibrary: Whether this student’s school has a library (1/0)&lt;/li&gt;
&lt;li&gt;publicSchool: Whether this student attends a public school (1/0)&lt;/li&gt;
&lt;li&gt;urban: Whether this student’s school is in an urban area (1/0)&lt;/li&gt;
&lt;li&gt;schoolSize: The number of students in this student’s school&lt;/li&gt;
&lt;li&gt;readingScore: The student’s reading score, on a 1000-point scale&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---dataset-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Dataset size&lt;/h3&gt;
&lt;p&gt;Load the training and testing sets using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaTrain &amp;lt;- read.csv(&amp;quot;pisa2009train.csv&amp;quot;)
pisaTest &amp;lt;- read.csv(&amp;quot;pisa2009test.csv&amp;quot;)
str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3663 obs. of  24 variables:
 $ grade                : int  11 11 9 10 10 10 10 10 9 10 ...
 $ male                 : int  1 1 1 0 1 1 0 0 0 1 ...
 $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: NA 7 7 3 4 3 2 7 7 5 ...
 $ preschool            : int  NA 0 1 1 1 1 0 1 1 1 ...
 $ expectBachelors      : int  0 0 1 1 0 1 1 1 0 1 ...
 $ motherHS             : int  NA 1 1 0 1 NA 1 1 1 1 ...
 $ motherBachelors      : int  NA 1 1 0 0 NA 0 0 NA 1 ...
 $ motherWork           : int  1 1 1 1 1 1 1 0 1 1 ...
 $ fatherHS             : int  NA 1 1 1 1 1 NA 1 0 0 ...
 $ fatherBachelors      : int  NA 0 NA 0 0 0 NA 0 NA 0 ...
 $ fatherWork           : int  1 1 1 1 0 1 NA 1 1 1 ...
 $ selfBornUS           : int  1 1 1 1 1 1 0 1 1 1 ...
 $ motherBornUS         : int  0 1 1 1 1 1 1 1 1 1 ...
 $ fatherBornUS         : int  0 1 1 1 0 1 NA 1 1 1 ...
 $ englishAtHome        : int  0 1 1 1 1 1 1 1 1 1 ...
 $ computerForSchoolwork: int  1 1 1 1 1 1 1 1 1 1 ...
 $ read30MinsADay       : int  0 1 0 1 1 0 0 1 0 0 ...
 $ minutesPerWeekEnglish: int  225 450 250 200 250 300 250 300 378 294 ...
 $ studentsInEnglish    : int  NA 25 28 23 35 20 28 30 20 24 ...
 $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 0 1 ...
 $ publicSchool         : int  1 1 1 1 1 1 1 1 1 1 ...
 $ urban                : int  1 0 0 1 1 0 1 0 1 0 ...
 $ schoolSize           : int  673 1173 1233 2640 1095 227 2080 1913 502 899 ...
 $ readingScore         : num  476 575 555 458 614 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     grade            male                      raceeth    
 Min.   : 8.00   Min.   :0.0000   White             :2015  
 1st Qu.:10.00   1st Qu.:0.0000   Hispanic          : 834  
 Median :10.00   Median :1.0000   Black             : 444  
 Mean   :10.09   Mean   :0.5111   Asian             : 143  
 3rd Qu.:10.00   3rd Qu.:1.0000   More than one race: 124  
 Max.   :12.00   Max.   :1.0000   (Other)           :  68  
                                  NA&amp;#39;s              :  35  
   preschool      expectBachelors     motherHS    motherBachelors 
 Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  
 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  
 Median :1.0000   Median :1.0000   Median :1.00   Median :0.0000  
 Mean   :0.7228   Mean   :0.7859   Mean   :0.88   Mean   :0.3481  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  
 NA&amp;#39;s   :56       NA&amp;#39;s   :62       NA&amp;#39;s   :97     NA&amp;#39;s   :397     
   motherWork        fatherHS      fatherBachelors    fatherWork    
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  
 Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  
 Mean   :0.7345   Mean   :0.8593   Mean   :0.3319   Mean   :0.8531  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :93       NA&amp;#39;s   :245      NA&amp;#39;s   :569      NA&amp;#39;s   :233     
   selfBornUS      motherBornUS     fatherBornUS    englishAtHome   
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  
 Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  
 Mean   :0.9313   Mean   :0.7725   Mean   :0.7668   Mean   :0.8717  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :69       NA&amp;#39;s   :71       NA&amp;#39;s   :113      NA&amp;#39;s   :71      
 computerForSchoolwork read30MinsADay   minutesPerWeekEnglish
 Min.   :0.0000        Min.   :0.0000   Min.   :   0.0       
 1st Qu.:1.0000        1st Qu.:0.0000   1st Qu.: 225.0       
 Median :1.0000        Median :0.0000   Median : 250.0       
 Mean   :0.8994        Mean   :0.2899   Mean   : 266.2       
 3rd Qu.:1.0000        3rd Qu.:1.0000   3rd Qu.: 300.0       
 Max.   :1.0000        Max.   :1.0000   Max.   :2400.0       
 NA&amp;#39;s   :65            NA&amp;#39;s   :34       NA&amp;#39;s   :186          
 studentsInEnglish schoolHasLibrary  publicSchool        urban       
 Min.   : 1.0      Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:20.0      1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  
 Median :25.0      Median :1.0000   Median :1.0000   Median :0.0000  
 Mean   :24.5      Mean   :0.9676   Mean   :0.9339   Mean   :0.3849  
 3rd Qu.:30.0      3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :75.0      Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :249       NA&amp;#39;s   :143                                       
   schoolSize    readingScore  
 Min.   : 100   Min.   :168.6  
 1st Qu.: 712   1st Qu.:431.7  
 Median :1212   Median :499.7  
 Mean   :1369   Mean   :497.9  
 3rd Qu.:1900   3rd Qu.:566.2  
 Max.   :6694   Max.   :746.0  
 NA&amp;#39;s   :162                   &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Number of students in the training set is 3663&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---summarizing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Summarizing the dataset&lt;/h3&gt;
&lt;p&gt;Using tapply() on pisaTrain, what is the average reading test score of males?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(pisaTrain$readingScore, pisaTrain$male, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       0        1 
512.9406 483.5325 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Males reading score, 483.5325 and Females reading score is 512.9406&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---locating-missing-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Locating missing values&lt;/h3&gt;
&lt;p&gt;Which variables are missing data in at least one observation in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     grade            male                      raceeth    
 Min.   : 8.00   Min.   :0.0000   White             :2015  
 1st Qu.:10.00   1st Qu.:0.0000   Hispanic          : 834  
 Median :10.00   Median :1.0000   Black             : 444  
 Mean   :10.09   Mean   :0.5111   Asian             : 143  
 3rd Qu.:10.00   3rd Qu.:1.0000   More than one race: 124  
 Max.   :12.00   Max.   :1.0000   (Other)           :  68  
                                  NA&amp;#39;s              :  35  
   preschool      expectBachelors     motherHS    motherBachelors 
 Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  
 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  
 Median :1.0000   Median :1.0000   Median :1.00   Median :0.0000  
 Mean   :0.7228   Mean   :0.7859   Mean   :0.88   Mean   :0.3481  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  
 NA&amp;#39;s   :56       NA&amp;#39;s   :62       NA&amp;#39;s   :97     NA&amp;#39;s   :397     
   motherWork        fatherHS      fatherBachelors    fatherWork    
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  
 Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  
 Mean   :0.7345   Mean   :0.8593   Mean   :0.3319   Mean   :0.8531  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :93       NA&amp;#39;s   :245      NA&amp;#39;s   :569      NA&amp;#39;s   :233     
   selfBornUS      motherBornUS     fatherBornUS    englishAtHome   
 Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  
 Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  
 Mean   :0.9313   Mean   :0.7725   Mean   :0.7668   Mean   :0.8717  
 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :69       NA&amp;#39;s   :71       NA&amp;#39;s   :113      NA&amp;#39;s   :71      
 computerForSchoolwork read30MinsADay   minutesPerWeekEnglish
 Min.   :0.0000        Min.   :0.0000   Min.   :   0.0       
 1st Qu.:1.0000        1st Qu.:0.0000   1st Qu.: 225.0       
 Median :1.0000        Median :0.0000   Median : 250.0       
 Mean   :0.8994        Mean   :0.2899   Mean   : 266.2       
 3rd Qu.:1.0000        3rd Qu.:1.0000   3rd Qu.: 300.0       
 Max.   :1.0000        Max.   :1.0000   Max.   :2400.0       
 NA&amp;#39;s   :65            NA&amp;#39;s   :34       NA&amp;#39;s   :186          
 studentsInEnglish schoolHasLibrary  publicSchool        urban       
 Min.   : 1.0      Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
 1st Qu.:20.0      1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  
 Median :25.0      Median :1.0000   Median :1.0000   Median :0.0000  
 Mean   :24.5      Mean   :0.9676   Mean   :0.9339   Mean   :0.3849  
 3rd Qu.:30.0      3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
 Max.   :75.0      Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
 NA&amp;#39;s   :249       NA&amp;#39;s   :143                                       
   schoolSize    readingScore  
 Min.   : 100   Min.   :168.6  
 1st Qu.: 712   1st Qu.:431.7  
 Median :1212   Median :499.7  
 Mean   :1369   Mean   :497.9  
 3rd Qu.:1900   3rd Qu.:566.2  
 Max.   :6694   Max.   :746.0  
 NA&amp;#39;s   :162                   &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;raceeth-preschool-expectbachelors-motherhs-motherbachelors-motherwork-fatherhs-fatherbachelors-fatherwork-selfbornus-motherbornus-fatherbornus-englishathome-computerforschoolwork-read30minsaday-minutesperweekenglish-studentsinenglish-schoolhaslibrary-schoolsize&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;raceeth, preschool, expectBachelors, motherHS, motherBachelors, motherWork, fatherHS, fatherBachelors, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, computerForSchoolWork, read30MinsADay, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, schoolSize&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---removing-missing-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Removing missing values&lt;/h3&gt;
&lt;p&gt;Linear regression discards observations with missing data, so I’ll remove all such observations from the training and testing sets. Later, we’ll learn about imputation, which deals with missing data by filling in missing values with plausible information.&lt;/p&gt;
&lt;p&gt;Removing observations with missing value from pisaTrain and pisaTest:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are now in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2414 obs. of  24 variables:
 $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
 $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
 $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
 $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
 $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
 $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
 $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
 $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
 $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
 $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
 $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
 $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
 $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
 $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
 $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
 $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
 $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
 $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
 $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
 $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
 $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
 $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
 $ readingScore         : num  575 458 614 439 466 ...
 - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
  ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2414&lt;/p&gt;
&lt;p&gt;How many observations are now in the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   990 obs. of  24 variables:
 $ grade                : int  10 10 10 10 11 10 10 10 10 10 ...
 $ male                 : int  0 0 0 0 0 1 0 1 1 0 ...
 $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 7 1 7 7 4 7 4 7 4 ...
 $ preschool            : int  1 1 1 1 0 1 0 1 1 1 ...
 $ expectBachelors      : int  0 1 0 0 0 1 1 0 1 1 ...
 $ motherHS             : int  1 1 1 1 1 1 1 1 1 1 ...
 $ motherBachelors      : int  1 0 0 0 1 1 0 0 1 0 ...
 $ motherWork           : int  1 0 0 1 1 1 0 1 1 1 ...
 $ fatherHS             : int  1 1 1 1 1 1 1 1 1 1 ...
 $ fatherBachelors      : int  0 1 0 0 1 0 0 0 1 1 ...
 $ fatherWork           : int  0 1 0 1 1 1 1 0 1 1 ...
 $ selfBornUS           : int  1 1 1 1 1 1 1 1 1 1 ...
 $ motherBornUS         : int  1 1 1 1 1 1 1 1 1 1 ...
 $ fatherBornUS         : int  1 1 1 1 1 1 1 1 1 1 ...
 $ englishAtHome        : int  1 1 1 1 1 1 1 1 1 1 ...
 $ computerForSchoolwork: int  1 1 1 1 1 1 1 1 1 1 ...
 $ read30MinsADay       : int  0 0 1 1 1 1 0 0 0 1 ...
 $ minutesPerWeekEnglish: int  240 240 240 270 270 350 350 360 350 360 ...
 $ studentsInEnglish    : int  30 30 30 35 30 25 27 28 25 27 ...
 $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ publicSchool         : int  1 1 1 1 1 1 1 1 1 1 ...
 $ urban                : int  0 0 0 0 0 0 0 0 0 0 ...
 $ schoolSize           : int  808 808 808 808 808 899 899 899 899 899 ...
 $ readingScore         : num  355 454 405 665 605 ...
 - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  2 3 4 6 12 16 17 19 22 23 ...
  ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;6&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;990&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---factor-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Factor variables&lt;/h3&gt;
&lt;p&gt;Factor variables are variables that take on a discrete set of values. This is an unordered factor because there isn’t any natural ordering between the levels.&lt;/p&gt;
&lt;p&gt;An ordered factor has a natural ordering between the levels (an example would be the classifications “large,” “medium,” and “small”).&lt;/p&gt;
&lt;p&gt;Which of the following variables is an unordered factor with at least 3 levels?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2414 obs. of  24 variables:
 $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
 $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
 $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
 $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
 $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
 $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
 $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
 $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
 $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
 $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
 $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
 $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
 $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
 $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
 $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
 $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
 $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
 $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
 $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
 $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
 $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
 $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
 $ readingScore         : num  575 458 614 439 466 ...
 - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
  ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;raceeth&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;raceeth&lt;/h4&gt;
&lt;p&gt;Which of the following variables is an ordered factor with at least 3 levels?
#### grade&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---unordered-factors-in-regression-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Unordered factors in regression models&lt;/h3&gt;
&lt;p&gt;To include unordered factors in a linear regression model, we define one level as the “reference level” and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.&lt;/p&gt;
&lt;p&gt;As an example, consider the unordered factor variable “color”, with levels “red”, “green”, and “blue”. If “green” were the reference level, then we would add binary variables “colored” and “colorblue” to a linear regression problem. All red examples would have colored=1 and colorblue=0. All blue examples would have colored=0 and colorblue=1. All green examples would have colored=0 and colorblue=0.&lt;/p&gt;
&lt;p&gt;Now, consider the variable “raceeth” in our problem, which has levels &lt;strong&gt;“American Indian/Alaska Native”, “Asian”, “Black”, “Hispanic”,&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;“More than one race”, &lt;strong&gt;“Native Hawaiian/Other Pacific Islander”, and “White”.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Because it’s the most common in our population, we will select White as the reference level.&lt;/p&gt;
&lt;p&gt;Which binary variables will be included in the regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   2414 obs. of  24 variables:
 $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
 $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
 $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
 $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
 $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
 $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
 $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
 $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
 $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
 $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
 $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
 $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
 $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
 $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
 $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
 $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
 $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
 $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
 $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
 $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
 $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
 $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
 $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
 $ readingScore         : num  575 458 614 439 466 ...
 - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
  ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;raceethAmerican Indian/Alaska Native&lt;/li&gt;
&lt;li&gt;raceethAsian&lt;/li&gt;
&lt;li&gt;raceethBlack&lt;/li&gt;
&lt;li&gt;raceethHispanic&lt;/li&gt;
&lt;li&gt;raceethMore than one race&lt;/li&gt;
&lt;li&gt;raceethNative Hawaiian/Other Pacific Islander&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---example-unordered-factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Example unordered factors&lt;/h3&gt;
&lt;p&gt;Consider again adding our unordered factor race to the regression model with reference level “White”. For a student who is Asian, which binary variables would be set to 0. All remaining variables will be set to 1. (all except raceethAsian)
#### all&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Building a model&lt;/h3&gt;
&lt;p&gt;Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() – you can see this when you run str(pisaTrain) or str(pisaTest).&lt;/p&gt;
&lt;p&gt;However, by default R selects the first level alphabetically (“American Indian/Alaska Native”) as the reference level of our factor instead of the most common level (“White”).&lt;/p&gt;
&lt;p&gt;Let’s Set the reference level of the factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaTrain$raceeth = relevel(pisaTrain$raceeth, &amp;quot;White&amp;quot;)
pisaTest$raceeth = relevel(pisaTest$raceeth, &amp;quot;White&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, building a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables. It would be time-consuming to type all the variables, but R provides the shorthand notation “readingScore ~ .” to mean “predict readingScore using all the other variables in the dataframe.” The period is used to replace listing out all of the independent variables.&lt;/p&gt;
&lt;p&gt;As an example, if our dependent variable is called “Y”, our independent variables are called “X1”, “X2”, and “X3”, and our training dataset is called “Train”, instead of the regular notation: LinReg = lm(Y ~ X1 + X2 + X3, data = Train)&lt;/p&gt;
&lt;p&gt;You would use the following code to build our model:
LinReg = lm(Y ~ ., data = Train)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmScore &amp;lt;- lm(readingScore ~ ., data = pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the Multiple R-squared value of lmScore on the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lmScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = readingScore ~ ., data = pisaTrain)

Residuals:
    Min      1Q  Median      3Q     Max 
-247.44  -48.86    1.86   49.77  217.18 

Coefficients:
                                                Estimate Std. Error
(Intercept)                                   143.766333  33.841226
grade                                          29.542707   2.937399
male                                          -14.521653   3.155926
raceethAmerican Indian/Alaska Native          -67.277327  16.786935
raceethAsian                                   -4.110325   9.220071
raceethBlack                                  -67.012347   5.460883
raceethHispanic                               -38.975486   5.177743
raceethMore than one race                     -16.922522   8.496268
raceethNative Hawaiian/Other Pacific Islander  -5.101601  17.005696
preschool                                      -4.463670   3.486055
expectBachelors                                55.267080   4.293893
motherHS                                        6.058774   6.091423
motherBachelors                                12.638068   3.861457
motherWork                                     -2.809101   3.521827
fatherHS                                        4.018214   5.579269
fatherBachelors                                16.929755   3.995253
fatherWork                                      5.842798   4.395978
selfBornUS                                     -3.806278   7.323718
motherBornUS                                   -8.798153   6.587621
fatherBornUS                                    4.306994   6.263875
englishAtHome                                   8.035685   6.859492
computerForSchoolwork                          22.500232   5.702562
read30MinsADay                                 34.871924   3.408447
minutesPerWeekEnglish                           0.012788   0.010712
studentsInEnglish                              -0.286631   0.227819
schoolHasLibrary                               12.215085   9.264884
publicSchool                                  -16.857475   6.725614
urban                                          -0.110132   3.962724
schoolSize                                      0.006540   0.002197
                                              t value Pr(&amp;gt;|t|)    
(Intercept)                                     4.248 2.24e-05 ***
grade                                          10.057  &amp;lt; 2e-16 ***
male                                           -4.601 4.42e-06 ***
raceethAmerican Indian/Alaska Native           -4.008 6.32e-05 ***
raceethAsian                                   -0.446  0.65578    
raceethBlack                                  -12.271  &amp;lt; 2e-16 ***
raceethHispanic                                -7.528 7.29e-14 ***
raceethMore than one race                      -1.992  0.04651 *  
raceethNative Hawaiian/Other Pacific Islander  -0.300  0.76421    
preschool                                      -1.280  0.20052    
expectBachelors                                12.871  &amp;lt; 2e-16 ***
motherHS                                        0.995  0.32001    
motherBachelors                                 3.273  0.00108 ** 
motherWork                                     -0.798  0.42517    
fatherHS                                        0.720  0.47147    
fatherBachelors                                 4.237 2.35e-05 ***
fatherWork                                      1.329  0.18393    
selfBornUS                                     -0.520  0.60331    
motherBornUS                                   -1.336  0.18182    
fatherBornUS                                    0.688  0.49178    
englishAtHome                                   1.171  0.24153    
computerForSchoolwork                           3.946 8.19e-05 ***
read30MinsADay                                 10.231  &amp;lt; 2e-16 ***
minutesPerWeekEnglish                           1.194  0.23264    
studentsInEnglish                              -1.258  0.20846    
schoolHasLibrary                                1.318  0.18749    
publicSchool                                   -2.506  0.01226 *  
urban                                          -0.028  0.97783    
schoolSize                                      2.977  0.00294 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 73.81 on 2385 degrees of freedom
Multiple R-squared:  0.3251,    Adjusted R-squared:  0.3172 
F-statistic: 41.04 on 28 and 2385 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0.3251&lt;/p&gt;
&lt;p&gt;Note, that this R-squared is lower than the ones prevously observed. This does not necessarily imply that the model is of poor quality. More often than not, it simply means that the prediction problem at hand (predicting a student’s test score based on demographic and school-related variables) is more difficult than other prediction problems (like predicting a team’s number of wins from their runs scored and allowed, or predicting the quality of wine from weather conditions).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---computing-the-root-mean-squared-error-of-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Computing the root-mean squared error of the model&lt;/h3&gt;
&lt;p&gt;What is the training-set root mean squared error (RMSE) of lmScore?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmScoreSSE &amp;lt;- sum(lmScore$residuals^2)
lmScoreSSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 12993365&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(lmScoreSSE/nrow(pisaTrain))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 73.36555&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---comparing-predictions-for-similar-students&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Comparing predictions for similar students&lt;/h3&gt;
&lt;p&gt;Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9.&lt;/p&gt;
&lt;p&gt;What is the predicted reading score of student A minus the predicted reading score of student B?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaPred &amp;lt;- pisaTest[1,]
pisaPred &amp;lt;- rbind(pisaPred, pisaTest[1,])
pisaPred[1,1] &amp;lt;- 11 ## grade 11 for student A
pisaPred[2,1] &amp;lt;- 9  ## grade 9 for student B
pisaPred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  grade male raceeth preschool expectBachelors motherHS motherBachelors
1    11    0   White         1               0        1               1
2     9    0   White         1               0        1               1
  motherWork fatherHS fatherBachelors fatherWork selfBornUS motherBornUS
1          1        1               0          0          1            1
2          1        1               0          0          1            1
  fatherBornUS englishAtHome computerForSchoolwork read30MinsADay
1            1             1                     1              0
2            1             1                     1              0
  minutesPerWeekEnglish studentsInEnglish schoolHasLibrary publicSchool
1                   240                30                1            1
2                   240                30                1            1
  urban schoolSize readingScore
1     0        808       355.24
2     0        808       355.24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictedScores &amp;lt;- predict(lmScore, pisaPred)
predictedScores&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       1        2 
501.5294 442.4440 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictedScores[1] - predictedScores[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       1 
59.08541 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;59.08541 ~ 59.09&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---interpreting-model-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Interpreting model coefficients&lt;/h3&gt;
&lt;p&gt;What is the meaning of the coefficient associated with variable raceethAsian?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lmScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = readingScore ~ ., data = pisaTrain)

Residuals:
    Min      1Q  Median      3Q     Max 
-247.44  -48.86    1.86   49.77  217.18 

Coefficients:
                                                Estimate Std. Error
(Intercept)                                   143.766333  33.841226
grade                                          29.542707   2.937399
male                                          -14.521653   3.155926
raceethAmerican Indian/Alaska Native          -67.277327  16.786935
raceethAsian                                   -4.110325   9.220071
raceethBlack                                  -67.012347   5.460883
raceethHispanic                               -38.975486   5.177743
raceethMore than one race                     -16.922522   8.496268
raceethNative Hawaiian/Other Pacific Islander  -5.101601  17.005696
preschool                                      -4.463670   3.486055
expectBachelors                                55.267080   4.293893
motherHS                                        6.058774   6.091423
motherBachelors                                12.638068   3.861457
motherWork                                     -2.809101   3.521827
fatherHS                                        4.018214   5.579269
fatherBachelors                                16.929755   3.995253
fatherWork                                      5.842798   4.395978
selfBornUS                                     -3.806278   7.323718
motherBornUS                                   -8.798153   6.587621
fatherBornUS                                    4.306994   6.263875
englishAtHome                                   8.035685   6.859492
computerForSchoolwork                          22.500232   5.702562
read30MinsADay                                 34.871924   3.408447
minutesPerWeekEnglish                           0.012788   0.010712
studentsInEnglish                              -0.286631   0.227819
schoolHasLibrary                               12.215085   9.264884
publicSchool                                  -16.857475   6.725614
urban                                          -0.110132   3.962724
schoolSize                                      0.006540   0.002197
                                              t value Pr(&amp;gt;|t|)    
(Intercept)                                     4.248 2.24e-05 ***
grade                                          10.057  &amp;lt; 2e-16 ***
male                                           -4.601 4.42e-06 ***
raceethAmerican Indian/Alaska Native           -4.008 6.32e-05 ***
raceethAsian                                   -0.446  0.65578    
raceethBlack                                  -12.271  &amp;lt; 2e-16 ***
raceethHispanic                                -7.528 7.29e-14 ***
raceethMore than one race                      -1.992  0.04651 *  
raceethNative Hawaiian/Other Pacific Islander  -0.300  0.76421    
preschool                                      -1.280  0.20052    
expectBachelors                                12.871  &amp;lt; 2e-16 ***
motherHS                                        0.995  0.32001    
motherBachelors                                 3.273  0.00108 ** 
motherWork                                     -0.798  0.42517    
fatherHS                                        0.720  0.47147    
fatherBachelors                                 4.237 2.35e-05 ***
fatherWork                                      1.329  0.18393    
selfBornUS                                     -0.520  0.60331    
motherBornUS                                   -1.336  0.18182    
fatherBornUS                                    0.688  0.49178    
englishAtHome                                   1.171  0.24153    
computerForSchoolwork                           3.946 8.19e-05 ***
read30MinsADay                                 10.231  &amp;lt; 2e-16 ***
minutesPerWeekEnglish                           1.194  0.23264    
studentsInEnglish                              -1.258  0.20846    
schoolHasLibrary                                1.318  0.18749    
publicSchool                                   -2.506  0.01226 *  
urban                                          -0.028  0.97783    
schoolSize                                      2.977  0.00294 ** 
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 73.81 on 2385 degrees of freedom
Multiple R-squared:  0.3251,    Adjusted R-squared:  0.3172 
F-statistic: 41.04 on 28 and 2385 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;predicted-difference-in-the-reading-score-between-an-asian-student-and-a-white-student-who-is-otherwise-identical.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Predicted difference in the reading score between an Asian student and a white student who is otherwise identical.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---identifying-variables-lacking-statistical-significance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Identifying variables lacking statistical significance&lt;/h3&gt;
&lt;p&gt;Based on the significance codes, which variables are candidates for removal from the model? (We’ll assume that the factor variable raceeth should only be removed if none of its levels are significant.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;preschool, motherHS, motherWork, fatherHS, fatherWork, selfBornUS,&lt;/li&gt;
&lt;li&gt;motherBornUS, fatherBornUS, englishAtHome, minutesPerWeekEnglish,&lt;/li&gt;
&lt;li&gt;studentsInEnglish, schoolHasLibrary, urban&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---predicting-on-unseen-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Predicting on unseen data&lt;/h3&gt;
&lt;p&gt;Using the “predict” function and supplying the “newdata” argument, use the lmScore model to predict the reading scores of students in pisaTest. Call this vector of predictions “predTest”. Do not change the variables in the model (for example, do not remove variables that we found were not significant in the previous part of this problem). Use the summary function to describe the test-set predictions.&lt;/p&gt;
&lt;p&gt;What is the range between the max and min predicted reading score on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest &amp;lt;- predict(lmScore, newdata = pisaTest)
summary(predTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  353.2   482.0   524.0   516.7   555.7   637.7 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;637.7 - 353.2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---test-set-sse-and-rmse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Test set SSE and RMSE&lt;/h3&gt;
&lt;p&gt;What is the sum of squared errors (SSE) of lmScore on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_SSE = sum((predTest - pisaTest$readingScore)^2)
test_set_SSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 5762082&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the root mean squared error (RMSE) of lmScore on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_RMSE = sqrt(test_set_SSE/nrow(pisaTest))
test_set_RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 76.29079&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---baseline-prediction-and-test-set-sse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Baseline prediction and test-set SSE&lt;/h3&gt;
&lt;p&gt;What is the predicted test score used in the baseline model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pisaTrain$readingScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 517.9629&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the sum of squared errors of the baseline model on the testing set? HINT: We call the sum of squared errors for the baseline model the total sum of squares (SST).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
test_set_SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 7802354&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---test-set-r-squared&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Test-set R-squared&lt;/h3&gt;
&lt;p&gt;What is the test-set R-squared value of lmScore?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - test_set_SSE/test_set_SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.2614944&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Climate Change</title>
      <link>/project/climate_change/climate_change/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/climate_change/climate_change/</guid>
      <description>


&lt;p&gt;There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.&lt;/p&gt;
&lt;p&gt;In this analysis, I’ll attempt to study the relationship between average global temperature and several other factors.&lt;/p&gt;
&lt;p&gt;The file climate_change.csv contains climate data from May 1983 to December 2008. The available variables include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Year: the observation year.&lt;/li&gt;
&lt;li&gt;Month: the observation month.&lt;/li&gt;
&lt;li&gt;Temp: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia.&lt;/li&gt;
&lt;li&gt;CO2, N2O, CH4, CFC.11, CFC.12: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.
&lt;ul&gt;
&lt;li&gt;CO2, N2O and CH4 are expressed in ppmv (parts per million by volume – i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere)&lt;/li&gt;
&lt;li&gt;CFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Aerosols: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun’s energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA.&lt;/li&gt;
&lt;li&gt;TSI: the total solar irradiance (TSI) in W/m2 (the rate at which the sun’s energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website.&lt;/li&gt;
&lt;li&gt;MEI: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate &amp;lt;- read.csv(&amp;quot;climate_change.csv&amp;quot;)
str(climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   308 obs. of  11 variables:
 $ Year    : int  1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ...
 $ Month   : int  5 6 7 8 9 10 11 12 1 2 ...
 $ MEI     : num  2.556 2.167 1.741 1.13 0.428 ...
 $ CO2     : num  346 346 344 342 340 ...
 $ CH4     : num  1639 1634 1633 1631 1648 ...
 $ N2O     : num  304 304 304 304 304 ...
 $ CFC.11  : num  191 192 193 194 194 ...
 $ CFC.12  : num  350 352 354 356 357 ...
 $ TSI     : num  1366 1366 1366 1366 1366 ...
 $ Aerosols: num  0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ...
 $ Temp    : num  0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      Year          Month             MEI               CO2       
 Min.   :1983   Min.   : 1.000   Min.   :-1.6350   Min.   :340.2  
 1st Qu.:1989   1st Qu.: 4.000   1st Qu.:-0.3987   1st Qu.:353.0  
 Median :1996   Median : 7.000   Median : 0.2375   Median :361.7  
 Mean   :1996   Mean   : 6.552   Mean   : 0.2756   Mean   :363.2  
 3rd Qu.:2002   3rd Qu.:10.000   3rd Qu.: 0.8305   3rd Qu.:373.5  
 Max.   :2008   Max.   :12.000   Max.   : 3.0010   Max.   :388.5  
      CH4            N2O            CFC.11          CFC.12     
 Min.   :1630   Min.   :303.7   Min.   :191.3   Min.   :350.1  
 1st Qu.:1722   1st Qu.:308.1   1st Qu.:246.3   1st Qu.:472.4  
 Median :1764   Median :311.5   Median :258.3   Median :528.4  
 Mean   :1750   Mean   :312.4   Mean   :252.0   Mean   :497.5  
 3rd Qu.:1787   3rd Qu.:317.0   3rd Qu.:267.0   3rd Qu.:540.5  
 Max.   :1814   Max.   :322.2   Max.   :271.5   Max.   :543.8  
      TSI          Aerosols            Temp        
 Min.   :1365   Min.   :0.00160   Min.   :-0.2820  
 1st Qu.:1366   1st Qu.:0.00280   1st Qu.: 0.1217  
 Median :1366   Median :0.00575   Median : 0.2480  
 Mean   :1366   Mean   :0.01666   Mean   : 0.2568  
 3rd Qu.:1366   3rd Qu.:0.01260   3rd Qu.: 0.4073  
 Max.   :1367   Max.   :0.14940   Max.   : 0.7390  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;ml-workflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ML Workflow&lt;/h3&gt;
&lt;p&gt;Then, split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate_train &amp;lt;- subset(climate, Year &amp;lt;= 2006)
climate_test &amp;lt;- subset(climate, Year &amp;gt; 2006)
str(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   284 obs. of  11 variables:
 $ Year    : int  1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ...
 $ Month   : int  5 6 7 8 9 10 11 12 1 2 ...
 $ MEI     : num  2.556 2.167 1.741 1.13 0.428 ...
 $ CO2     : num  346 346 344 342 340 ...
 $ CH4     : num  1639 1634 1633 1631 1648 ...
 $ N2O     : num  304 304 304 304 304 ...
 $ CFC.11  : num  191 192 193 194 194 ...
 $ CFC.12  : num  350 352 354 356 357 ...
 $ TSI     : num  1366 1366 1366 1366 1366 ...
 $ Aerosols: num  0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ...
 $ Temp    : num  0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      Year          Month             MEI               CO2       
 Min.   :1983   Min.   : 1.000   Min.   :-1.5860   Min.   :340.2  
 1st Qu.:1989   1st Qu.: 4.000   1st Qu.:-0.3230   1st Qu.:352.3  
 Median :1995   Median : 7.000   Median : 0.3085   Median :359.9  
 Mean   :1995   Mean   : 6.556   Mean   : 0.3419   Mean   :361.4  
 3rd Qu.:2001   3rd Qu.:10.000   3rd Qu.: 0.8980   3rd Qu.:370.6  
 Max.   :2006   Max.   :12.000   Max.   : 3.0010   Max.   :385.0  
      CH4            N2O            CFC.11          CFC.12     
 Min.   :1630   Min.   :303.7   Min.   :191.3   Min.   :350.1  
 1st Qu.:1716   1st Qu.:307.7   1st Qu.:249.6   1st Qu.:462.5  
 Median :1759   Median :310.8   Median :260.4   Median :522.1  
 Mean   :1746   Mean   :311.7   Mean   :252.5   Mean   :494.2  
 3rd Qu.:1782   3rd Qu.:316.1   3rd Qu.:267.4   3rd Qu.:541.0  
 Max.   :1808   Max.   :320.5   Max.   :271.5   Max.   :543.8  
      TSI          Aerosols            Temp        
 Min.   :1365   Min.   :0.00160   Min.   :-0.2820  
 1st Qu.:1366   1st Qu.:0.00270   1st Qu.: 0.1180  
 Median :1366   Median :0.00620   Median : 0.2325  
 Mean   :1366   Mean   :0.01772   Mean   : 0.2478  
 3rd Qu.:1366   3rd Qu.:0.01400   3rd Qu.: 0.4065  
 Max.   :1367   Max.   :0.14940   Max.   : 0.7390  &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   24 obs. of  11 variables:
 $ Year    : int  2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...
 $ Month   : int  1 2 3 4 5 6 7 8 9 10 ...
 $ MEI     : num  0.974 0.51 0.074 -0.049 0.183 ...
 $ CO2     : num  383 384 385 386 387 ...
 $ CH4     : num  1800 1803 1803 1802 1796 ...
 $ N2O     : num  321 321 321 321 320 ...
 $ CFC.11  : num  248 248 248 248 247 ...
 $ CFC.12  : num  539 539 539 539 538 ...
 $ TSI     : num  1366 1366 1366 1366 1366 ...
 $ Aerosols: num  0.0054 0.0051 0.0045 0.0045 0.0041 0.004 0.004 0.0041 0.0042 0.0041 ...
 $ Temp    : num  0.601 0.498 0.435 0.466 0.372 0.382 0.394 0.358 0.402 0.362 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      Year          Month            MEI               CO2       
 Min.   :2007   Min.   : 1.00   Min.   :-1.6350   Min.   :380.9  
 1st Qu.:2007   1st Qu.: 3.75   1st Qu.:-1.0437   1st Qu.:383.1  
 Median :2008   Median : 6.50   Median :-0.5305   Median :384.5  
 Mean   :2008   Mean   : 6.50   Mean   :-0.5098   Mean   :384.7  
 3rd Qu.:2008   3rd Qu.: 9.25   3rd Qu.:-0.0360   3rd Qu.:386.1  
 Max.   :2008   Max.   :12.00   Max.   : 0.9740   Max.   :388.5  
      CH4            N2O            CFC.11          CFC.12     
 Min.   :1772   Min.   :320.3   Min.   :244.1   Min.   :534.9  
 1st Qu.:1792   1st Qu.:320.6   1st Qu.:244.6   1st Qu.:535.1  
 Median :1798   Median :321.3   Median :246.2   Median :537.0  
 Mean   :1797   Mean   :321.1   Mean   :245.9   Mean   :536.7  
 3rd Qu.:1804   3rd Qu.:321.4   3rd Qu.:246.6   3rd Qu.:537.4  
 Max.   :1814   Max.   :322.2   Max.   :248.4   Max.   :539.2  
      TSI          Aerosols             Temp      
 Min.   :1366   Min.   :0.003100   Min.   :0.074  
 1st Qu.:1366   1st Qu.:0.003600   1st Qu.:0.307  
 Median :1366   Median :0.004100   Median :0.380  
 Mean   :1366   Mean   :0.004071   Mean   :0.363  
 3rd Qu.:1366   3rd Qu.:0.004500   3rd Qu.:0.414  
 Max.   :1366   Max.   :0.005400   Max.   :0.601  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate &amp;lt;- 
  lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, 
     data = climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + 
    TSI + Aerosols, data = climate_train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25888 -0.05913 -0.00082  0.05649  0.32433 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -1.246e+02  1.989e+01  -6.265 1.43e-09 ***
MEI          6.421e-02  6.470e-03   9.923  &amp;lt; 2e-16 ***
CO2          6.457e-03  2.285e-03   2.826  0.00505 ** 
CH4          1.240e-04  5.158e-04   0.240  0.81015    
N2O         -1.653e-02  8.565e-03  -1.930  0.05467 .  
CFC.11      -6.631e-03  1.626e-03  -4.078 5.96e-05 ***
CFC.12       3.808e-03  1.014e-03   3.757  0.00021 ***
TSI          9.314e-02  1.475e-02   6.313 1.10e-09 ***
Aerosols    -1.538e+00  2.133e-01  -7.210 5.41e-12 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.09171 on 275 degrees of freedom
Multiple R-squared:  0.7509,    Adjusted R-squared:  0.7436 
F-statistic: 103.6 on 8 and 275 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model R2 (the “Multiple R-squared” value) is &lt;strong&gt;0.7509&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-our-first-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating Our First Model&lt;/h3&gt;
&lt;p&gt;Which variables are significant in the model? We will consider a variable signficant only if the p-value is below 0.05.
#### MEI, CO2, CFC.11, CFC.12, TSI, Aerosols&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;understanding-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Understanding the Model&lt;/h3&gt;
&lt;p&gt;Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are negative, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.&lt;/p&gt;
&lt;p&gt;Which of the following is the simplest correct explanation for this contradiction?
#### All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the dataset.&lt;/p&gt;
&lt;p&gt;Compute the correlations between all the variables in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                Year         Month           MEI         CO2         CH4
Year      1.00000000 -0.0279419602 -0.0369876842  0.98274939  0.91565945
Month    -0.02794196  1.0000000000  0.0008846905 -0.10673246  0.01856866
MEI      -0.03698768  0.0008846905  1.0000000000 -0.04114717 -0.03341930
CO2       0.98274939 -0.1067324607 -0.0411471651  1.00000000  0.87727963
CH4       0.91565945  0.0185686624 -0.0334193014  0.87727963  1.00000000
N2O       0.99384523  0.0136315303 -0.0508197755  0.97671982  0.89983864
CFC.11    0.56910643 -0.0131112236  0.0690004387  0.51405975  0.77990402
CFC.12    0.89701166  0.0006751102  0.0082855443  0.85268963  0.96361625
TSI       0.17030201 -0.0346061935 -0.1544919227  0.17742893  0.24552844
Aerosols -0.34524670  0.0148895406  0.3402377871 -0.35615480 -0.26780919
Temp      0.78679714 -0.0998567411  0.1724707512  0.78852921  0.70325502
                 N2O      CFC.11        CFC.12         TSI    Aerosols
Year      0.99384523  0.56910643  0.8970116635  0.17030201 -0.34524670
Month     0.01363153 -0.01311122  0.0006751102 -0.03460619  0.01488954
MEI      -0.05081978  0.06900044  0.0082855443 -0.15449192  0.34023779
CO2       0.97671982  0.51405975  0.8526896272  0.17742893 -0.35615480
CH4       0.89983864  0.77990402  0.9636162478  0.24552844 -0.26780919
N2O       1.00000000  0.52247732  0.8679307757  0.19975668 -0.33705457
CFC.11    0.52247732  1.00000000  0.8689851828  0.27204596 -0.04392120
CFC.12    0.86793078  0.86898518  1.0000000000  0.25530281 -0.22513124
TSI       0.19975668  0.27204596  0.2553028138  1.00000000  0.05211651
Aerosols -0.33705457 -0.04392120 -0.2251312440  0.05211651  1.00000000
Temp      0.77863893  0.40771029  0.6875575483  0.24338269 -0.38491375
                Temp
Year      0.78679714
Month    -0.09985674
MEI       0.17247075
CO2       0.78852921
CH4       0.70325502
N2O       0.77863893
CFC.11    0.40771029
CFC.12    0.68755755
TSI       0.24338269
Aerosols -0.38491375
Temp      1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following independent variables is N2O, highly correlated with (absolute correlation greater than 0.7)?
#### CO2, CH4, CFC.12&lt;/p&gt;
&lt;p&gt;The following independent variables is CFC.11, highly correlated with?
#### CH4, CFC.12&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simplifying-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simplifying the Model&lt;/h3&gt;
&lt;p&gt;Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Note, using the training set to build the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate.2 &amp;lt;- 
  lm(Temp ~ MEI + N2O + TSI + Aerosols, 
     data = climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.27916 -0.05975 -0.00595  0.05672  0.34195 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -1.162e+02  2.022e+01  -5.747 2.37e-08 ***
MEI          6.419e-02  6.652e-03   9.649  &amp;lt; 2e-16 ***
N2O          2.532e-02  1.311e-03  19.307  &amp;lt; 2e-16 ***
TSI          7.949e-02  1.487e-02   5.344 1.89e-07 ***
Aerosols    -1.702e+00  2.180e-01  -7.806 1.19e-13 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.09547 on 279 degrees of freedom
Multiple R-squared:  0.7261,    Adjusted R-squared:  0.7222 
F-statistic: 184.9 on 4 and 279 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of N2O in this reduced model is &lt;strong&gt;2.532e-02&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(How does this compare to the coefficient in the previous model with all of the variables?) The model R2 is &lt;strong&gt;0.7261&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automatically-building-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Automatically Building the Model&lt;/h3&gt;
&lt;p&gt;We have many variables in this analysis, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2.&lt;/p&gt;
&lt;p&gt;This trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.&lt;/p&gt;
&lt;p&gt;The step function has one argument - the name of the initial model. It returns a simplified model.
Using the step function in R to derive a new model, with the full model as the initial model (HINT: If your initial full model was called “climateLM”, you could create a new model with the step function by typing step(climateLM). Be sure to save your new model to a variable name so that you can look at the summary. For more information about the step function, type? step in your R console.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate.step &amp;lt;- step(fit.climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Start:  AIC=-1348.16
Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols

           Df Sum of Sq    RSS     AIC
- CH4       1   0.00049 2.3135 -1350.1
&amp;lt;none&amp;gt;                  2.3130 -1348.2
- N2O       1   0.03132 2.3443 -1346.3
- CO2       1   0.06719 2.3802 -1342.0
- CFC.12    1   0.11874 2.4318 -1335.9
- CFC.11    1   0.13986 2.4529 -1333.5
- TSI       1   0.33516 2.6482 -1311.7
- Aerosols  1   0.43727 2.7503 -1301.0
- MEI       1   0.82823 3.1412 -1263.2

Step:  AIC=-1350.1
Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols

           Df Sum of Sq    RSS     AIC
&amp;lt;none&amp;gt;                  2.3135 -1350.1
- N2O       1   0.03133 2.3448 -1348.3
- CO2       1   0.06672 2.3802 -1344.0
- CFC.12    1   0.13023 2.4437 -1336.5
- CFC.11    1   0.13938 2.4529 -1335.5
- TSI       1   0.33500 2.6485 -1313.7
- Aerosols  1   0.43987 2.7534 -1302.7
- MEI       1   0.83118 3.1447 -1264.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate.step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
Call:
lm(formula = Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + 
    Aerosols, data = climate_train)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25770 -0.05994 -0.00104  0.05588  0.32203 

Coefficients:
              Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept) -1.245e+02  1.985e+01  -6.273 1.37e-09 ***
MEI          6.407e-02  6.434e-03   9.958  &amp;lt; 2e-16 ***
CO2          6.402e-03  2.269e-03   2.821 0.005129 ** 
N2O         -1.602e-02  8.287e-03  -1.933 0.054234 .  
CFC.11      -6.609e-03  1.621e-03  -4.078 5.95e-05 ***
CFC.12       3.868e-03  9.812e-04   3.942 0.000103 ***
TSI          9.312e-02  1.473e-02   6.322 1.04e-09 ***
Aerosols    -1.540e+00  2.126e-01  -7.244 4.36e-12 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 0.09155 on 276 degrees of freedom
Multiple R-squared:  0.7508,    Adjusted R-squared:  0.7445 
F-statistic: 118.8 on 7 and 276 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R2 value of the model produced by the step function is &lt;strong&gt;0.7508&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which of the following variable(s) were eliminated from the full model by the step function?
#### It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-on-unseen-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing on Unseen Data&lt;/h3&gt;
&lt;p&gt;We have developed an understanding of how well we can fit a linear regression to the training data, but does the model quality hold when applied to unseen data?&lt;/p&gt;
&lt;p&gt;Using the model produced from the step function, calculate temperature predictions for the testing dataset, using the predict function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TempPredictions &amp;lt;- predict(fit.climate.step, newdata = climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate.SSE = sum((TempPredictions - climate_test$Temp)^2)
climate.SST = sum((climate_test$Temp - mean(climate_train$Temp))^2)
1 - climate.SSE/climate.SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.6286051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Testing set R2 is &lt;strong&gt;0.6286&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sales Analysis</title>
      <link>/post/sales_analysis/sales-analysis/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sales_analysis/sales-analysis/</guid>
      <description>


&lt;div id=&#34;sample-sales-data-order-info-sales-customer-shipping-etc.-used-for-segmentation-customer-analytics-clustering-and-more.-inspired-for-retail-analytics.-this-was-originally-used-for-pentaho-di-kettle-but-i-found-the-set-could-be-useful-for-sales-simulation-training.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;originally-written-by-maria-carina-roldan-pentaho-community-member-bi-consultant-assert-solutions-argentina.-this-work-is-licensed-under-the-creative-commons-attribution-noncommercial-share-alike-3.0-unported-license.-modified-by-gus-segura-june-2014.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Originally Written by María Carina Roldán, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;taken-from-the-link&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Taken from the &lt;a href=&#34;https://www.kaggle.com/kyanyoga/sample-sales-data/version/1#_=_&#34;&gt;link&lt;/a&gt;&lt;/h3&gt;
&lt;div id=&#34;reading-in-a-csv-file&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reading in a csv file&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sales_data &amp;lt;- read.csv(&amp;#39;sales_data_sample.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sales_data is a ‘data.frame’. It is the main way that R deals with tables of data.
Click on the arrow next to sales_data in the Environment pane to see the data types of each column
Click on sales_data in the Environment pane to see the table. You can also type View(sales_data) to do this.&lt;/p&gt;
&lt;p&gt;Run summary to see a summary of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  ORDERNUMBER    QUANTITYORDERED   PRICEEACH      ORDERLINENUMBER 
 Min.   :10100   Min.   : 6.00   Min.   : 26.88   Min.   : 1.000  
 1st Qu.:10180   1st Qu.:27.00   1st Qu.: 68.86   1st Qu.: 3.000  
 Median :10262   Median :35.00   Median : 95.70   Median : 6.000  
 Mean   :10259   Mean   :35.09   Mean   : 83.66   Mean   : 6.466  
 3rd Qu.:10334   3rd Qu.:43.00   3rd Qu.:100.00   3rd Qu.: 9.000  
 Max.   :10425   Max.   :97.00   Max.   :100.00   Max.   :18.000  
                                                                  
     SALES                   ORDERDATE           STATUS    
 Min.   :  482.1   11/14/2003 0:00:  38   Cancelled :  60  
 1st Qu.: 2203.4   11/24/2004 0:00:  35   Disputed  :  14  
 Median : 3184.8   11/12/2003 0:00:  34   In Process:  41  
 Mean   : 3553.9   11/17/2004 0:00:  32   On Hold   :  44  
 3rd Qu.: 4508.0   11/4/2004 0:00 :  29   Resolved  :  47  
 Max.   :14082.8   10/16/2004 0:00:  28   Shipped   :2617  
                   (Other)        :2627                    
     QTR_ID         MONTH_ID         YEAR_ID               PRODUCTLINE 
 Min.   :1.000   Min.   : 1.000   Min.   :2003   Classic Cars    :967  
 1st Qu.:2.000   1st Qu.: 4.000   1st Qu.:2003   Motorcycles     :331  
 Median :3.000   Median : 8.000   Median :2004   Planes          :306  
 Mean   :2.718   Mean   : 7.092   Mean   :2004   Ships           :234  
 3rd Qu.:4.000   3rd Qu.:11.000   3rd Qu.:2004   Trains          : 77  
 Max.   :4.000   Max.   :12.000   Max.   :2005   Trucks and Buses:301  
                                                 Vintage Cars    :607  
      MSRP         PRODUCTCODE                         CUSTOMERNAME 
 Min.   : 33.0   S18_3232:  52   Euro Shopping Channel       : 259  
 1st Qu.: 68.0   S10_1949:  28   Mini Gifts Distributors Ltd.: 180  
 Median : 99.0   S10_4962:  28   Australian Collectors, Co.  :  55  
 Mean   :100.7   S12_1666:  28   La Rochelle Gifts           :  53  
 3rd Qu.:124.0   S18_1097:  28   AV Stores, Co.              :  51  
 Max.   :214.0   S18_2432:  28   Land of Toys Inc.           :  49  
                 (Other) :2631   (Other)                     :2176  
            PHONE                            ADDRESSLINE1 
 (91) 555 94 44: 259   C/ Moralzarzal, 86          : 259  
 4155551450    : 180   5677 Strong St.             : 180  
 03 9520 4555  :  55   636 St Kilda Road           :  55  
 40.67.8555    :  53   67, rue des Cinquante Otages:  53  
 (171) 555-1555:  51   Fauntleroy Circus           :  51  
 6175558555    :  51   897 Long Airport Avenue     :  49  
 (Other)       :2174   (Other)                     :2176  
    ADDRESSLINE2             CITY           STATE        POSTALCODE  
          :2521   Madrid       : 304           :1486   28034  : 259  
 Level 3  :  55   San Rafael   : 180   CA      : 416   97562  : 205  
 Suite 400:  48   NYC          : 152   MA      : 190   10022  : 152  
 Level 15 :  46   Singapore    :  79   NY      : 178   94217  :  89  
 Level 6  :  46   Paris        :  70   NSW     :  92          :  76  
 2nd Floor:  36   San Francisco:  62   Victoria:  78   50553  :  61  
 (Other)  :  71   (Other)      :1976   (Other) : 383   (Other):1981  
      COUNTRY     TERRITORY    CONTACTLASTNAME CONTACTFIRSTNAME
 USA      :1004   APAC : 221   Freyre : 259    Diego  : 259    
 Spain    : 342   EMEA :1407   Nelson : 204    Valarie: 257    
 France   : 314   Japan: 121   Young  : 115    Julie  : 117    
 Australia: 185   NA&amp;#39;s :1074   Frick  :  91    Michael:  84    
 UK       : 144                Brown  :  88    Sue    :  84    
 Italy    : 113                Yu     :  80    Juri   :  60    
 (Other)  : 721                (Other):1986    (Other):1962    
   DEALSIZE   
 Large : 157  
 Medium:1384  
 Small :1282  
              
              
              
              &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or can run summary on individual columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  26.88   68.86   95.70   83.66  100.00  100.00 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$PRODUCTLINE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    Classic Cars      Motorcycles           Planes            Ships 
             967              331              306              234 
          Trains Trucks and Buses     Vintage Cars 
              77              301              607 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 14082.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 3553.889&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 482.13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1841.865&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now go through select(), arrange(), filter(), mutate(), group_by(), summarise()&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;select() function
select specific columns
first argument is always the dataset, and each argument after is the fields you want&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(sales_data, QUANTITYORDERED, PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;     QUANTITYORDERED PRICEEACH
1                 30     95.70
2                 34     81.35
3                 41     94.74
4                 45     83.26
5                 49    100.00
6                 36     96.66
7                 29     86.13
8                 48    100.00
9                 22     98.57
10                41    100.00
11                37    100.00
12                23    100.00
13                28    100.00
14                34    100.00
15                45     92.83
16                36    100.00
17                23    100.00
18                41    100.00
19                46     94.74
20                42    100.00
21                41    100.00
22                20     72.55
23                21     34.91
24                42     76.36
25                24    100.00
26                66    100.00
27                26    100.00
28                29    100.00
29                38    100.00
30                37    100.00
31                45    100.00
32                21    100.00
33                34    100.00
34                23    100.00
35                42    100.00
36                47    100.00
37                35    100.00
38                29    100.00
39                34    100.00
40                32    100.00
41                21    100.00
42                34    100.00
43                37    100.00
44                47    100.00
45                48    100.00
46                40    100.00
47                26    100.00
48                30    100.00
49                32    100.00
50                41    100.00
51                36    100.00
52                24    100.00
53                23    100.00
54                50    100.00
55                39     99.91
56                29     96.34
57                27    100.00
58                37    100.00
59                37    100.00
60                27    100.00
61                42    100.00
62                38     96.34
63                24    100.00
64                23    100.00
65                47    100.00
66                22    100.00
67                44    100.00
68                40    100.00
69                22    100.00
70                47    100.00
71                39     96.34
72                34    100.00
73                45    100.00
74                20    100.00
75                40     68.92
76                26     51.15
77                39    100.00
78                50     44.51
79                45    100.00
80                45    100.00
81                27    100.00
82                46    100.00
83                31    100.00
84                33    100.00
85                22    100.00
86                20    100.00
87                41    100.00
88                45    100.00
89                49    100.00
90                34    100.00
91                49    100.00
92                39    100.00
93                43    100.00
94                41    100.00
95                36    100.00
96                27    100.00
97                29    100.00
98                20    100.00
99                37    100.00
100               26    100.00
101               39     76.67
102               22    100.00
103               22    100.00
104               21     86.77
105               66    100.00
106               56    100.00
107               50    100.00
108               46    100.00
109               33    100.00
110               49    100.00
111               32    100.00
112               44    100.00
113               24    100.00
114               26    100.00
115               45    100.00
116               39    100.00
117               49    100.00
118               20    100.00
119               27    100.00
120               30    100.00
121               25    100.00
122               24    100.00
123               22    100.00
124               33    100.00
125               47     64.93
126               25     48.05
127               26     75.47
128               48     54.68
129               39    100.00
130               34    100.00
131               32    100.00
132               64    100.00
133               19    100.00
134               42    100.00
135               31    100.00
136               22    100.00
137               26    100.00
138               20    100.00
139               21    100.00
140               33    100.00
141               28    100.00
142               26    100.00
143               31    100.00
144               48    100.00
145               50    100.00
146               28    100.00
147               26    100.00
148               32    100.00
149               44    100.00
150               30    100.00
151               38    100.00
152               40    100.00
153               46     61.99
154               26    100.00
155               27    100.00
156               43    100.00
157               35     65.63
158               37    100.00
159               37     46.90
160               27    100.00
161               38    100.00
162               33    100.00
163               42    100.00
164               42    100.00
165               48    100.00
166               41    100.00
167               30    100.00
168               27    100.00
169               21    100.00
170               20    100.00
171               41    100.00
172               27    100.00
173               28    100.00
174               24    100.00
175               44    100.00
176               50    100.00
177               21    100.00
178               33    100.00
179               33    100.00
180               31    100.00
181               41     71.47
182               45     79.65
183               33     85.39
184               45     76.00
185               26     99.04
186               12    100.00
187               41    100.00
188               33    100.00
189               46    100.00
190               33    100.00
191               20    100.00
192               44    100.00
193               33    100.00
194               21    100.00
195               47    100.00
196               46    100.00
197               32    100.00
198               42    100.00
199               44    100.00
200               35    100.00
201               41    100.00
202               46    100.00
203               31    100.00
204               38    100.00
205               42     64.00
206               33     57.22
207               48     52.36
208               42    100.00
209               32    100.00
210               34    100.00
211               33     69.12
212               36    100.00
213               27    100.00
214               21    100.00
215               21    100.00
216               38    100.00
217               30    100.00
218               49    100.00
219               43    100.00
220               41    100.00
221               38    100.00
222               28    100.00
223               43    100.00
224               25    100.00
225               38    100.00
226               41    100.00
227               28    100.00
228               25    100.00
229               41    100.00
230               39    100.00
231               21    100.00
232               27    100.00
233               33     99.21
234               29    100.00
235               49    100.00
236               49    100.00
237               20    100.00
238               39     63.20
239               40    100.00
240               49    100.00
241               21    100.00
242               50    100.00
243               20    100.00
244               49    100.00
245               38    100.00
246               35    100.00
247               40    100.00
248               28    100.00
249               25    100.00
250               36    100.00
251               43    100.00
252               32    100.00
253               46    100.00
254               48    100.00
255               43    100.00
256               49    100.00
257               24    100.00
258               26    100.00
259               30    100.00
260               24    100.00
261               55    100.00
262               22    100.00
263               49     78.92
264               44    100.00
265               66    100.00
266               21    100.00
267               34    100.00
268               43    100.00
269               46    100.00
270               33    100.00
271               42    100.00
272               34    100.00
273               47    100.00
274               33    100.00
275               24    100.00
276               26    100.00
277               30    100.00
278               43    100.00
279               25    100.00
280               27    100.00
281               27    100.00
282               24    100.00
283               34    100.00
284               46    100.00
285               27     54.33
286               33    100.00
287               47    100.00
288               49     55.34
289               40    100.00
290               37    100.00
291               47    100.00
292               45    100.00
293               37     99.82
294               48    100.00
295               31    100.00
296               46    100.00
297               47    100.00
298               28    100.00
299               40    100.00
300               20    100.00
301               39    100.00
302               25     99.82
303               29    100.00
304               22    100.00
305               22    100.00
306               47    100.00
307               45    100.00
308               29    100.00
309               24    100.00
310               35    100.00
311               46     83.63
312               44     95.93
313               34     96.73
314               35    100.00
315               25     72.38
316               10    100.00
317               29    100.00
318               39    100.00
319               42    100.00
320               46    100.00
321               49    100.00
322               27    100.00
323               50    100.00
324               43    100.00
325               38    100.00
326               20    100.00
327               27    100.00
328               49    100.00
329               27    100.00
330               39    100.00
331               24    100.00
332               45    100.00
333               20    100.00
334               36    100.00
335               24    100.00
336               49     63.38
337               26    100.00
338               49     62.09
339               34    100.00
340               34     95.35
341               33    100.00
342               22    100.00
343               39     89.38
344               32     63.84
345               24     75.01
346               21     63.84
347               24     73.42
348               36     63.84
349               20     81.40
350               30     64.64
351               44     82.99
352               28     92.57
353               37     77.41
354               20     74.21
355               25     90.17
356               35     76.61
357               38     83.79
358               41     69.43
359               22     76.61
360               49     81.40
361               38     73.42
362               33    100.00
363               36     93.56
364               34     81.62
365               24     67.83
366               36     70.26
367               34     90.17
368               41    100.00
369               46    100.00
370               24    100.00
371               21    100.00
372               24    100.00
373               48    100.00
374               26    100.00
375               37    100.00
376               49    100.00
377               34     99.54
378               48    100.00
379               36    100.00
380               46    100.00
381               46    100.00
382               31     97.17
383               41    100.00
384               21    100.00
385               38    100.00
386               45    100.00
387               26     58.38
388               38    100.00
389               48    100.00
390               42     64.16
391               49     35.71
392               32     66.58
393               54    100.00
394               33    100.00
395               36    100.00
396               20    100.00
397               29     97.89
398               33     97.89
399               50    100.00
400               41    100.00
401               36    100.00
402               27    100.00
403               47    100.00
404               33    100.00
405               21    100.00
406               21     93.28
407               41    100.00
408               40    100.00
409               28    100.00
410               23    100.00
411               23    100.00
412               25    100.00
413               24    100.00
414               39     64.74
415               55     75.20
416               46     88.45
417               50    100.00
418               47    100.00
419               97     93.28
420               32    100.00
421               35    100.00
422               49    100.00
423               38    100.00
424               32    100.00
425               34    100.00
426               36     99.17
427               48     93.34
428               21     96.84
429               21     93.34
430               34    100.00
431               46    100.00
432               32    100.00
433               29    100.00
434               41    100.00
435               43     96.84
436               24    100.00
437               41    100.00
438               46     98.00
439               32    100.00
440               22    100.00
441               29     40.25
442               42     49.60
443               39     98.00
444               27    100.00
445               48     98.00
446               29     85.10
447               27    100.00
448               54    100.00
449               26    100.00
450               34    100.00
451               25    100.00
452               23    100.00
453               28    100.00
454               35    100.00
455               44    100.00
456               22    100.00
457               42    100.00
458               29    100.00
459               32    100.00
460               41    100.00
461               26    100.00
462               21    100.00
463               34    100.00
464               41    100.00
465               37    100.00
466               37    100.00
467               41    100.00
468               46    100.00
469               40    100.00
470               43     97.60
471               30     87.06
472               35    100.00
473               36     93.77
474               61    100.00
475               38    100.00
476               39    100.00
477               33     99.66
478               32    100.00
479               31    100.00
480               50    100.00
481               48     91.44
482               43    100.00
483               25     87.33
484               28    100.00
485               36    100.00
486               27     89.38
487               25    100.00
488               40    100.00
489               34     95.55
490               50    100.00
491               38    100.00
492               37     95.55
493               43     89.38
494               43     86.30
495               46     95.13
496               42     36.11
497               50     50.18
498               44    100.00
499               27     93.16
500               35    100.00
501               51     95.55
502               41     50.14
503               48     49.06
504               42     54.99
505               49     43.13
506               30     58.22
507               45     51.21
508               48     44.21
509               32     54.45
510               46     53.37
511               48     63.61
512               33     43.13
513               31     48.52
514               20     58.22
515               29     51.75
516               27     57.68
517               24     56.07
518               37     48.52
519               25     44.21
520               41     57.68
521               27     89.89
522               21     58.95
523               22     72.41
524               32     98.63
525               25     52.83
526               42    100.00
527               25     51.75
528               37    100.00
529               26    100.00
530               44     99.55
531               47    100.00
532               43    100.00
533               42    100.00
534               42    100.00
535               29    100.00
536               40    100.00
537               38    100.00
538               38    100.00
539               21    100.00
540               24    100.00
541               36    100.00
542               23    100.00
543               20    100.00
544               32    100.00
545               29    100.00
546               44    100.00
547               44    100.00
548               36    100.00
549               49     56.30
550               34     42.64
551               59    100.00
552               37    100.00
553               36    100.00
554               43    100.00
555               21    100.00
556               32    100.00
557               38    100.00
558               43    100.00
559               42    100.00
560               32    100.00
561               42    100.00
562               31    100.00
563               49    100.00
564               45    100.00
565               49    100.00
566               41    100.00
567               45    100.00
568               36    100.00
569               39    100.00
570               27    100.00
571               25    100.00
572               41    100.00
573               39     99.52
574               28     57.55
575               25     54.57
576               33    100.00
577               34    100.00
578               24    100.00
579               30    100.00
580               42    100.00
581               21    100.00
582               34    100.00
583               29    100.00
584               24    100.00
585               44    100.00
586               21    100.00
587               33    100.00
588               30    100.00
589               26    100.00
590               41    100.00
591               26    100.00
592               32    100.00
593               43    100.00
594               48    100.00
595               44     74.04
596               45    100.00
597               37    100.00
598               39    100.00
599               76    100.00
600               37    100.00
601               38     82.39
602               43     72.38
603               48     79.31
604               26     82.39
605               38     88.55
606               20     63.14
607               22     73.92
608               45     90.86
609               45     85.47
610               20     66.99
611               47     64.68
612               46     73.92
613               23     83.93
614               33     74.69
615               29     90.86
616               44     82.39
617               41     92.40
618               20     91.63
619               37     78.54
620               29    100.00
621               55     65.45
622               22    100.00
623               31     67.76
624               49     79.22
625               61     73.92
626               39     83.93
627               38    100.00
628               31    100.00
629               36    100.00
630               25    100.00
631               48    100.00
632               35    100.00
633               21    100.00
634               47    100.00
635               38    100.00
636               41    100.00
637               24    100.00
638               37    100.00
639               33    100.00
640               49    100.00
641               29    100.00
642               24    100.00
643               47    100.00
644               24    100.00
645               25    100.00
646               30     32.47
647               22    100.00
648               27     64.69
649               34    100.00
650               36    100.00
651               34     43.05
652               48    100.00
653               34    100.00
654               24    100.00
655               46    100.00
656               45    100.00
657               39    100.00
658               43    100.00
659               29    100.00
660               20    100.00
661               46    100.00
662               27    100.00
663               44    100.00
664               43    100.00
665               49    100.00
666               40    100.00
667               30    100.00
668               50    100.00
669               23    100.00
670               26    100.00
671               27    100.00
672               42    100.00
673               47    100.00
674               49    100.00
675               38    100.00
676               20    100.00
677               25    100.00
678               25     88.00
679               41    100.00
680               28    100.00
681               50     67.80
682               32     50.25
683               42     53.88
684               24     62.36
685               27     69.62
686               26     57.51
687               38     61.15
688               42     59.33
689               23     71.44
690               21     62.96
691               28     50.85
692               33     72.65
693               25     62.96
694               28     61.75
695               46     49.04
696               30     61.15
697               38     84.25
698               40     56.91
699               45    100.00
700               27     49.30
701               42     72.65
702               36     63.57
703               29    100.00
704               39    100.00
705               45    100.00
706               47    100.00
707               49    100.00
708               46    100.00
709               48    100.00
710               46    100.00
711               35    100.00
712               43    100.00
713               26    100.00
714               22     98.18
715               34     99.41
716               50    100.00
717               48    100.00
718               41    100.00
719               36    100.00
720               29    100.00
721               33     37.48
722               46    100.00
723               38    100.00
724               20     36.42
725               22    100.00
726               27    100.00
727               56     98.18
728               38     99.41
729               25    100.00
730               33    100.00
731               42    100.00
732               33    100.00
733               38    100.00
734               31    100.00
735               20    100.00
736               44    100.00
737               26    100.00
738               27    100.00
739               46    100.00
740               47    100.00
741               37    100.00
742               31    100.00
743               24    100.00
744               31    100.00
745               50    100.00
746               35     64.69
747               30    100.00
748               29    100.00
749               27    100.00
750               40    100.00
751               31     98.99
752                6    100.00
753               45    100.00
754               22     54.09
755               45     68.67
756               43     65.02
757               46     61.99
758               39     69.28
759               31     71.10
760               41     69.28
761               44     60.16
762               45     70.49
763               37     69.89
764               35     61.38
765               28     59.55
766               30     61.99
767               30     49.22
768               25     69.28
769               29     57.73
770               26     57.73
771               41     53.48
772               34     52.87
773               35     61.21
774               34     61.38
775               50    100.00
776               41     61.99
777               22     96.86
778               35     48.62
779               44     38.50
780               47     61.99
781               19     49.22
782               34     90.39
783               29     71.81
784               49     69.27
785               30     85.32
786               21     70.96
787               50     76.88
788               47    100.00
789               24     76.03
790               27     98.84
791               33     86.17
792               35     90.39
793               31     71.81
794               25     82.79
795               27     82.79
796               31    100.00
797               45    100.00
798               27    100.00
799               27    100.00
800               42     69.27
801               21     74.77
802               34     76.88
803               42     76.03
804               15     98.84
805               29     70.87
806               46     58.15
807               30     61.78
808               30     49.67
809               42     51.48
810               46     61.18
811               25     64.20
812               32     65.42
813               30     64.81
814               40     49.67
815               28     60.57
816               23     55.72
817               29     61.18
818               34     58.75
819               37     63.60
820               20     49.06
821               32     48.46
822               34     52.09
823               42     52.70
824               38    100.00
825               30     62.16
826               23     49.67
827               22     53.30
828               39    100.00
829               55     55.72
830               36     61.18
831               26    100.00
832               31    100.00
833               34    100.00
834               41    100.00
835               23    100.00
836               48    100.00
837               22    100.00
838               21    100.00
839               22    100.00
840               40    100.00
841               50    100.00
842               29    100.00
843               43    100.00
844               24    100.00
845               22    100.00
846               43    100.00
847               20    100.00
848               25    100.00
849               36    100.00
850               24     52.67
851               21    100.00
852               30    100.00
853               32     94.79
854               21     47.18
855               26     78.11
856               35    100.00
857               26    100.00
858               46    100.00
859               37    100.00
860               27    100.00
861               23    100.00
862               39    100.00
863               27    100.00
864               38    100.00
865               27    100.00
866               40    100.00
867               24    100.00
868               44    100.00
869               37    100.00
870               20    100.00
871               39    100.00
872               44    100.00
873               22    100.00
874               43    100.00
875               27    100.00
876               26     64.90
877               25     52.32
878               49    100.00
879               29    100.00
880               41    100.00
881               55    100.00
882               27     83.07
883               23    100.00
884               31     90.17
885               46    100.00
886               47     91.18
887               31    100.00
888               46    100.00
889               37     89.15
890               28     93.21
891               37     90.17
892               49    100.00
893               24    100.00
894               30    100.00
895               50     88.14
896               31     96.24
897               46    100.00
898               47    100.00
899               46    100.00
900               37    100.00
901               33    100.00
902               31     90.17
903               48    100.00
904               41     87.13
905               42    100.00
906               41    100.00
907               32     45.25
908               10     88.14
909               35     57.46
910               28     64.33
911               46     73.70
912               20     71.20
913               30     49.97
914               48     69.96
915               28     53.72
916               39     68.08
917               24     51.84
918               28     67.46
919               31     58.71
920               45     63.71
921               24     58.09
922               49     53.72
923               32     63.08
924               43     68.71
925               37     50.59
926               24     64.96
927               35     53.72
928               41     29.87
929               26    100.00
930               34     64.96
931               49     70.58
932               28     44.21
933               40     68.08
934               37     59.96
935               31     53.72
936               41     83.44
937               21     89.46
938               40     96.34
939               46     74.84
940               44     79.14
941               46     73.12
942               41     81.72
943               32     89.46
944               46     87.74
945               28    100.00
946               49     94.62
947               21     73.98
948               32     84.30
949               34     98.06
950               21     98.06
951               21     96.34
952               31     83.44
953               21     94.62
954               25     45.86
955               28     82.58
956               43     64.97
957               22     86.74
958               37     93.01
959               28     72.26
960               30     74.84
961               44     73.98
962               25    100.00
963               43    100.00
964               30     97.39
965               20     90.06
966               26    100.00
967               40    100.00
968               31     89.01
969               22    100.00
970               23    100.00
971               30    100.00
972               49    100.00
973               31    100.00
974               29    100.00
975               37     84.82
976               38    100.00
977               29    100.00
978               23    100.00
979               26     85.87
980               38    100.00
981               48     47.04
982               40     39.80
983               45    100.00
984               44    100.00
985               21     94.22
986               35    100.00
987               29     86.92
988               21     84.82
989               22    100.00
990               26    100.00
991               41    100.00
992               47    100.00
993               31    100.00
994               43    100.00
995               23    100.00
996               28    100.00
997               49    100.00
998               24    100.00
999               33    100.00
1000              22    100.00
1001              32    100.00
1002              40    100.00
1003              43    100.00
1004              24    100.00
1005              32    100.00
1006              20    100.00
1007              24     69.12
1008              48    100.00
1009              44    100.00
1010              28    100.00
1011              24     61.52
1012              33    100.00
1013              41    100.00
1014              23    100.00
1015              46    100.00
1016              48    100.00
1017              25    100.00
1018              22    100.00
1019              41    100.00
1020              34    100.00
1021              32    100.00
1022              21    100.00
1023              20    100.00
1024              47    100.00
1025              39    100.00
1026              29    100.00
1027              45    100.00
1028              28    100.00
1029              26    100.00
1030              50    100.00
1031              48    100.00
1032              25    100.00
1033              40    100.00
1034              43    100.00
1035              22    100.00
1036              47    100.00
1037              36    100.00
1038              40    100.00
1039              27    100.00
1040              29    100.00
1041              20    100.00
1042              42    100.00
1043              25    100.00
1044              36    100.00
1045              21    100.00
1046              23    100.00
1047              37    100.00
1048              48    100.00
1049              25    100.00
1050              33    100.00
1051              27    100.00
1052              27    100.00
1053              20    100.00
1054              30    100.00
1055              48    100.00
1056              32     93.49
1057              34    100.00
1058              27     56.85
1059              39    100.00
1060              47    100.00
1061              22    100.00
1062              55    100.00
1063              60    100.00
1064              35    100.00
1065              28    100.00
1066              38    100.00
1067              21     95.80
1068              41    100.00
1069              22     97.81
1070              29     88.74
1071              50    100.00
1072              29    100.00
1073              49     80.67
1074              35    100.00
1075              48    100.00
1076              23     80.67
1077              48     95.80
1078              42    100.00
1079              47    100.00
1080              36    100.00
1081              22    100.00
1082              40     91.76
1083              23    100.00
1084              32    100.00
1085              21    100.00
1086              41     93.04
1087              25     84.71
1088              26    100.00
1089              24     89.75
1090              48    100.00
1091              26     68.35
1092              21     73.17
1093              45     78.00
1094              36     86.04
1095              21     81.21
1096              32     70.76
1097              30     82.82
1098              36     94.88
1099              33     86.04
1100              35     78.00
1101              37     95.69
1102              41     73.17
1103              20     76.39
1104              45     86.84
1105              38     69.96
1106              43     70.76
1107              49     78.80
1108              27     80.41
1109              46     73.98
1110              38     59.10
1111              25     66.74
1112              46     60.30
1113              22    100.00
1114              40    100.00
1115              46    100.00
1116              39    100.00
1117              38     82.34
1118              30    100.00
1119              42     94.25
1120              43    100.00
1121              29     95.24
1122              33     86.31
1123              32     79.37
1124              28     87.30
1125              41    100.00
1126              33    100.00
1127              36     84.33
1128              26     89.29
1129              34    100.00
1130              26     96.23
1131              38    100.00
1132              33    100.00
1133              33     91.27
1134              46    100.00
1135              26    100.00
1136              25    100.00
1137              45     73.08
1138              50    100.00
1139              36    100.00
1140              21     89.29
1141              29    100.00
1142              21    100.00
1143              42    100.00
1144              37    100.00
1145              25    100.00
1146              36    100.00
1147              22    100.00
1148              23    100.00
1149              32    100.00
1150              28    100.00
1151              27    100.00
1152              49    100.00
1153              41    100.00
1154              49    100.00
1155              30    100.00
1156              40    100.00
1157              23    100.00
1158              49    100.00
1159              25    100.00
1160              37    100.00
1161              55    100.00
1162              23    100.00
1163              24    100.00
1164              43     96.49
1165              50    100.00
1166              47    100.00
1167              34    100.00
1168              31    100.00
1169              28    100.00
1170              36    100.00
1171              48    100.00
1172              39    100.00
1173              45    100.00
1174              35    100.00
1175              45    100.00
1176              46    100.00
1177              37    100.00
1178              31    100.00
1179              33    100.00
1180              31    100.00
1181              27    100.00
1182              39    100.00
1183              32    100.00
1184              28    100.00
1185              26     67.91
1186              44     84.88
1187              46    100.00
1188              32     70.83
1189              65    100.00
1190              43    100.00
1191              43     67.77
1192              35     49.74
1193              45     50.36
1194              47     67.14
1195              21     64.66
1196              38     68.39
1197              21     50.36
1198              43     72.74
1199              46     54.09
1200              38     58.44
1201              26     52.22
1202              31     52.84
1203              48     54.71
1204              33     50.36
1205              38     57.20
1206              39     55.95
1207              42     67.14
1208              44     59.06
1209              29     69.63
1210              26     55.95
1211              31     53.47
1212              32     89.12
1213              28    100.00
1214              36    100.00
1215              36     52.22
1216              41    100.00
1217              27     99.52
1218              33    100.00
1219              34    100.00
1220              29    100.00
1221              34    100.00
1222              48    100.00
1223              46    100.00
1224              22    100.00
1225              20    100.00
1226              45     85.75
1227              46    100.00
1228              34    100.00
1229              50     85.75
1230              46    100.00
1231              22     84.70
1232              48     86.81
1233              47     86.81
1234              34    100.00
1235              45    100.00
1236              20    100.00
1237              50     60.49
1238              22     57.55
1239              45    100.00
1240              58    100.00
1241              51    100.00
1242              38    100.00
1243              22    100.00
1244              25    100.00
1245              24    100.00
1246              35    100.00
1247              28    100.00
1248              36    100.00
1249              39    100.00
1250              27    100.00
1251              40    100.00
1252              50    100.00
1253              42    100.00
1254              48    100.00
1255              25    100.00
1256              31    100.00
1257              44    100.00
1258              23    100.00
1259              29    100.00
1260              49    100.00
1261              36    100.00
1262              34    100.00
1263              25    100.00
1264              48    100.00
1265              38    100.00
1266              37    100.00
1267              49    100.00
1268              22     86.51
1269              28     89.27
1270              36     85.59
1271              34    100.00
1272              39    100.00
1273              21     75.46
1274              36    100.00
1275              24     97.55
1276              29     85.59
1277              38     94.79
1278              34    100.00
1279              42     90.19
1280              35    100.00
1281              35     80.99
1282              38     89.27
1283              41     81.91
1284              50    100.00
1285              21    100.00
1286              43     62.72
1287              32    100.00
1288               6     90.19
1289              66     92.95
1290              41     82.50
1291              23     97.42
1292              43     92.16
1293              24     70.22
1294              22     83.38
1295              26     73.73
1296              35     74.60
1297              47     77.24
1298              50    100.00
1299              45     87.77
1300              39     89.53
1301              23     89.53
1302              42     75.48
1303              20     89.53
1304              33     71.09
1305              34    100.00
1306              49    100.00
1307              39     90.40
1308              36    100.00
1309              50     86.01
1310              29    100.00
1311              30    100.00
1312              41     86.89
1313              28     58.58
1314              45    100.00
1315              16     75.48
1316              36    100.00
1317              41    100.00
1318              50    100.00
1319              40    100.00
1320              49    100.00
1321              45    100.00
1322              47    100.00
1323              21    100.00
1324              32    100.00
1325              47    100.00
1326              38    100.00
1327              41    100.00
1328              21    100.00
1329              41    100.00
1330              38    100.00
1331              25     99.29
1332              48    100.00
1333              22     99.29
1334              28    100.00
1335              47    100.00
1336              49    100.00
1337              45    100.00
1338              28    100.00
1339              29     57.53
1340              39    100.00
1341              46    100.00
1342              38    100.00
1343              41     47.29
1344              50     49.81
1345              43     53.83
1346              29     43.27
1347              30     42.76
1348              25     53.83
1349              49     44.78
1350              40     49.30
1351              41     44.78
1352              21     53.33
1353              46     45.28
1354              39     40.25
1355              45     59.87
1356              21     59.87
1357              44     58.36
1358              44     59.87
1359              29     51.82
1360              34     49.30
1361              39     56.85
1362              38    100.00
1363              24     79.86
1364              29    100.00
1365              30    100.00
1366              20    100.00
1367              39    100.00
1368              35     59.87
1369              26     59.87
1370              44    100.00
1371              28    100.00
1372              31    100.00
1373              29    100.00
1374              32    100.00
1375              33    100.00
1376              44    100.00
1377              32    100.00
1378              41    100.00
1379              35    100.00
1380              44    100.00
1381              26    100.00
1382              20    100.00
1383              48    100.00
1384              34    100.00
1385              49    100.00
1386              40    100.00
1387              45    100.00
1388              50    100.00
1389              38    100.00
1390              25    100.00
1391              28     58.18
1392              49     67.14
1393              49    100.00
1394              42     61.29
1395              23     57.73
1396              29     81.25
1397              25     80.54
1398              39     71.98
1399              44     69.84
1400              25     76.26
1401              45     76.26
1402              25     83.39
1403              37     57.73
1404              30     66.99
1405              36     75.55
1406              26     60.58
1407              23     73.41
1408              23     72.70
1409              25     66.99
1410              21    100.00
1411              26     63.43
1412              44     85.25
1413              24    100.00
1414              66     66.99
1415              36     57.73
1416              36     85.25
1417              22     77.90
1418              25     60.26
1419              37     72.76
1420              32     75.69
1421              47     74.22
1422              37     69.82
1423              20     62.47
1424              41     82.31
1425              21     60.26
1426              22     76.43
1427              40     80.10
1428              32     74.96
1429              36     66.14
1430              27     72.02
1431              26     87.45
1432              30     70.55
1433              23     56.84
1434              29     59.53
1435              21     60.37
1436              34    100.00
1437              26     76.43
1438              60     64.67
1439              35     55.49
1440              47     69.36
1441              20     60.69
1442              20     54.33
1443              25     65.31
1444              25     69.36
1445              27     68.78
1446              31     60.11
1447              44     66.47
1448              49     46.82
1449              26     56.07
1450              36     54.33
1451              44     52.60
1452              28     46.82
1453              45     64.74
1454              29     46.82
1455              40     53.75
1456              45     61.85
1457              44     53.18
1458              25     69.16
1459              45    100.00
1460              48     47.40
1461              44     60.76
1462              25     97.27
1463              22     91.76
1464              31     50.29
1465              21     52.60
1466              55     46.82
1467              25    100.00
1468              35     98.05
1469              35     93.54
1470              43     95.80
1471              44    100.00
1472              50    100.00
1473              48    100.00
1474              25    100.00
1475              39    100.00
1476              25     90.16
1477              32     91.29
1478              20    100.00
1479              26    100.00
1480              42    100.00
1481              21    100.00
1482              34    100.00
1483              47    100.00
1484              21    100.00
1485              48    100.00
1486              30     87.78
1487              27     84.39
1488              50     96.92
1489              38    100.00
1490              45    100.00
1491              46    100.00
1492              35    100.00
1493              29     59.37
1494              50     59.87
1495              26     49.81
1496              47     56.85
1497              23     53.33
1498              34     42.76
1499              34     53.83
1500              47     53.83
1501              45     49.81
1502              45     53.33
1503              36     43.27
1504              21     40.25
1505              28     48.30
1506              35     45.28
1507              50     52.32
1508              22     51.32
1509              45     49.30
1510              48     42.26
1511              20     87.96
1512              27     36.21
1513              38     38.50
1514              32    100.00
1515              64     40.25
1516              37     60.37
1517              28     88.63
1518              39    100.00
1519              41     94.10
1520              40     87.54
1521              49    100.00
1522              27     98.48
1523              34    100.00
1524              23     96.29
1525              31     88.63
1526              34     97.38
1527              25     95.20
1528              22    100.00
1529              32    100.00
1530              31    100.00
1531              25    100.00
1532              47     87.54
1533              21     50.65
1534              28     71.73
1535              46     94.10
1536              33     41.71
1537              43    100.00
1538              38     96.29
1539              47     88.63
1540              45     31.20
1541              20     35.51
1542              45     37.84
1543              36     33.19
1544              37     27.22
1545              31     31.53
1546              39     36.84
1547              26     29.21
1548              32     37.17
1549              20     34.19
1550              42     29.21
1551              33     29.54
1552              20     28.88
1553              29     38.17
1554              23     30.20
1555              39     29.54
1556              20    100.00
1557              45     81.91
1558              20     35.18
1559              48    100.00
1560              23     36.29
1561              32     70.56
1562              33    100.00
1563              61     29.54
1564              45     26.88
1565              38     83.03
1566              34     83.79
1567              43     83.03
1568              47     83.03
1569              22     67.03
1570              29     75.41
1571              28     68.55
1572              40     91.40
1573              25     73.88
1574              30     61.70
1575              38     69.31
1576              36     87.60
1577              32     87.60
1578              37     62.46
1579              30     79.98
1580              39     70.08
1581              32     65.51
1582              47     63.22
1583              26     86.83
1584              37     94.43
1585              55     79.98
1586              21    100.00
1587              23    100.00
1588              49     81.40
1589              59     87.60
1590              32     87.60
1591              43    100.00
1592              41    100.00
1593              45    100.00
1594              33    100.00
1595              40    100.00
1596              33    100.00
1597              50    100.00
1598              30    100.00
1599              41    100.00
1600              35    100.00
1601              49    100.00
1602              46    100.00
1603              48    100.00
1604              36    100.00
1605              22    100.00
1606              42    100.00
1607              21    100.00
1608              29    100.00
1609              35    100.00
1610              41    100.00
1611              29     71.97
1612              34     50.33
1613              37    100.00
1614              28     80.54
1615              49    100.00
1616              23    100.00
1617              46     53.76
1618              39     44.35
1619              22     45.25
1620              49     49.28
1621              43     36.29
1622              27     41.22
1623              31     36.74
1624              20     50.62
1625              24     38.08
1626              49     47.94
1627              24     48.38
1628              39     45.25
1629              37     45.70
1630              45     47.49
1631              45     48.38
1632              44     39.42
1633              23     37.63
1634              30    100.00
1635              26     85.52
1636              43     53.76
1637              26     31.86
1638              28     30.59
1639              27     68.35
1640              24    100.00
1641              40     45.70
1642              36    100.00
1643              21    100.00
1644              27    100.00
1645              47    100.00
1646              42    100.00
1647              32    100.00
1648              28    100.00
1649              24    100.00
1650              49    100.00
1651              46    100.00
1652              28    100.00
1653              48    100.00
1654              29    100.00
1655              47    100.00
1656              43    100.00
1657              25    100.00
1658              48    100.00
1659              24    100.00
1660              42    100.00
1661              31    100.00
1662              42    100.00
1663              37    100.00
1664              41    100.00
1665              20    100.00
1666              20    100.00
1667              70    100.00
1668              49    100.00
1669              35     58.87
1670              32     76.88
1671              29     61.64
1672              27     60.95
1673              27     80.34
1674              38     74.11
1675              35     72.03
1676              42     76.19
1677              21     63.72
1678              37     80.34
1679              26     79.65
1680              47     65.80
1681              37     65.10
1682              46     75.49
1683              38     59.56
1684              33     66.49
1685              24     56.10
1686              31     81.73
1687              42     81.03
1688              32    100.00
1689              41     70.65
1690              43     61.23
1691              20    100.00
1692              35     65.13
1693              27     79.65
1694              43     78.15
1695              32     72.70
1696              21     73.60
1697              20    100.00
1698              22     74.51
1699              36     73.60
1700              46     83.60
1701              47     96.32
1702              45     88.14
1703              47     88.14
1704              47     94.50
1705              38     87.24
1706              49     79.97
1707              35     80.87
1708              49    100.00
1709              28     93.60
1710              30     72.70
1711              39     86.72
1712              25    100.00
1713              40    100.00
1714              36     37.50
1715              76     94.50
1716              39    100.00
1717              44     39.60
1718              24     30.06
1719              39     38.19
1720              21     42.43
1721              30     40.31
1722              27     31.82
1723              37     31.12
1724              42     31.82
1725              32     28.29
1726              42     29.70
1727              21     40.31
1728              33     32.88
1729              49     36.07
1730              31     33.24
1731              38     41.72
1732              20     40.66
1733              39     30.06
1734              48     31.47
1735              39     37.13
1736              30    100.00
1737              33     37.13
1738              36     37.13
1739              36     82.94
1740              45    100.00
1741              40    100.00
1742              46     38.90
1743              30     36.07
1744              31     33.24
1745              49     74.68
1746              41     59.60
1747              35     67.14
1748              27     60.97
1749              23     72.62
1750              21     69.88
1751              34     80.84
1752              22     69.20
1753              48     67.82
1754              43     82.21
1755              32     81.53
1756              20     67.82
1757              24     67.14
1758              40     65.08
1759              30     73.99
1760              21     71.25
1761              25     75.36
1762              34     63.71
1763              48     58.92
1764              55    100.00
1765              25     74.68
1766              38     70.44
1767              39     55.96
1768              28     57.55
1769              24     61.66
1770              21     67.82
1771              46    100.00
1772              25     93.95
1773              34    100.00
1774              25    100.00
1775              23    100.00
1776              20    100.00
1777              23    100.00
1778              42    100.00
1779              27    100.00
1780              33    100.00
1781              28     98.65
1782              43    100.00
1783              48    100.00
1784              48    100.00
1785              45    100.00
1786              43    100.00
1787              44     42.26
1788              24     87.24
1789              31    100.00
1790              44     36.29
1791              59     98.65
1792              55     96.30
1793              29     32.10
1794              39     30.96
1795              20     35.87
1796              25     42.67
1797              42     37.00
1798              36     35.49
1799              37     42.67
1800              30     30.59
1801              21     37.00
1802              34     43.42
1803              42     36.63
1804              20     44.56
1805              40     42.67
1806              34     40.40
1807              31     38.89
1808              36     39.65
1809              48     34.36
1810              33     41.91
1811              37     33.23
1812              27     42.24
1813              39     40.40
1814              36     38.52
1815              36    100.00
1816              41    100.00
1817              37    100.00
1818              47     44.56
1819              15     42.67
1820              44     72.58
1821              35     87.62
1822              41     94.71
1823              49     98.25
1824              31     91.17
1825              20     79.66
1826              45     72.58
1827              33     74.35
1828              47     83.20
1829              20     89.40
1830              47     70.81
1831              40     94.71
1832              30    100.00
1833              22     91.17
1834              27    100.00
1835              34     92.94
1836              46     84.97
1837              31     84.08
1838              24     86.74
1839              41     85.85
1840              55    100.00
1841              30    100.00
1842              33     57.32
1843              43     97.87
1844              27     83.20
1845              60    100.00
1846              27     73.62
1847              49     83.04
1848              31     73.62
1849              20     77.05
1850              24     81.33
1851              33     94.17
1852              32     72.77
1853              40     79.62
1854              27     79.62
1855              40     79.62
1856              26     81.33
1857              44     96.74
1858              33     71.06
1859              34     68.49
1860              48     74.48
1861              25     83.04
1862              39     84.75
1863              45     34.19
1864              24    100.00
1865              46     79.62
1866              44     79.06
1867              13     81.33
1868              35     96.74
1869              30     63.07
1870              34     50.21
1871              27     66.13
1872              30     68.58
1873              50     69.80
1874              34     50.21
1875              23     65.52
1876              48     60.01
1877              34     64.90
1878              48     48.98
1879              24     50.21
1880              47     62.45
1881              24     52.66
1882              47     62.45
1883              20     61.23
1884              20     67.97
1885              31     58.78
1886              38     56.94
1887              26     61.23
1888              25    100.00
1889              48     62.45
1890              44     62.45
1891              21    100.00
1892              46     41.54
1893              46     52.84
1894              55     52.66
1895              31     52.60
1896              20     72.98
1897              29     59.18
1898              33     77.59
1899              34     55.89
1900              32     63.12
1901              27     73.64
1902              21     69.04
1903              27     71.67
1904              36     77.59
1905              43     70.35
1906              25     69.70
1907              46     70.35
1908              24     72.33
1909              39     71.67
1910              31     53.92
1911              22     71.67
1912              47     76.93
1913              20     72.98
1914              29     99.69
1915              38     68.38
1916              34    100.00
1917              46     66.00
1918              35     63.76
1919              34     71.67
1920              38     57.20
1921              18     69.70
1922              37    100.00
1923              43    100.00
1924              27    100.00
1925              30    100.00
1926              22     98.51
1927              49    100.00
1928              46    100.00
1929              48     91.02
1930              46     87.81
1931              48     92.09
1932              27     86.73
1933              43    100.00
1934              48    100.00
1935              41    100.00
1936              22     96.37
1937              46     92.09
1938              21     89.95
1939              31     37.18
1940              26     95.88
1941              20     99.58
1942              34    100.00
1943              43     86.73
1944              26    100.00
1945              50     79.67
1946              35     90.57
1947              50     77.99
1948              23     80.51
1949              37     67.93
1950              29     83.86
1951              21     72.12
1952              36     85.54
1953              22     86.38
1954              22     89.73
1955              46     80.51
1956              23     76.31
1957              49     87.21
1958              48     83.02
1959              33     72.96
1960              22     77.15
1961              22     91.41
1962              25     92.25
1963              20     92.25
1964              42     59.36
1965              25     60.34
1966              23    100.00
1967              37     85.54
1968              37     90.57
1969              42     72.96
1970              51     76.31
1971              40    100.00
1972              43    100.00
1973              47    100.00
1974              23    100.00
1975              35    100.00
1976              34    100.00
1977              25    100.00
1978              45    100.00
1979              47    100.00
1980              49    100.00
1981              40    100.00
1982              29    100.00
1983              39    100.00
1984              24    100.00
1985              25    100.00
1986              36    100.00
1987              50    100.00
1988              45    100.00
1989              26    100.00
1990              21    100.00
1991              42    100.00
1992              32    100.00
1993              31     94.58
1994              33     53.27
1995              45    100.00
1996              76    100.00
1997              70    100.00
1998              50     64.83
1999              28     70.29
2000              50     81.89
2001              28     66.19
2002              44     77.11
2003              27     73.02
2004              30     72.33
2005              43     66.19
2006              29     69.60
2007              48     56.64
2008              33     60.05
2009              40     75.06
2010              48     61.42
2011              41     81.89
2012              21     55.96
2013              32     71.65
2014              43     76.43
2015              30     77.79
2016              35     76.43
2017              45     96.92
2018              34     59.37
2019              26    100.00
2020              39     73.00
2021              41     73.32
2022              41     68.24
2023              64     60.05
2024              18     75.06
2025              49     34.47
2026              48     34.47
2027              46     33.23
2028              26     38.98
2029              37     38.98
2030              35     33.23
2031              23     42.26
2032              22     41.03
2033              39     33.23
2034              44     34.88
2035              27     43.90
2036              46     36.93
2037              33     41.85
2038              33     40.62
2039              24     40.21
2040              31     35.29
2041              41     77.24
2042              22     97.44
2043              46     37.34
2044              43     95.03
2045              15     36.93
2046              15     43.49
2047              26    100.00
2048              44    100.00
2049              20     96.99
2050              40     94.62
2051              23    100.00
2052              24     99.36
2053              29    100.00
2054              49    100.00
2055              34    100.00
2056              28    100.00
2057              37    100.00
2058              45    100.00
2059              46    100.00
2060              22    100.00
2061              39    100.00
2062              27    100.00
2063              36    100.00
2064              38    100.00
2065              44    100.00
2066              31    100.00
2067              23    100.00
2068              22    100.00
2069              28     50.32
2070              21     93.91
2071              37    100.00
2072              31    100.00
2073              25    100.00
2074              26     86.68
2075              34    100.00
2076              29    100.00
2077              20     90.57
2078              42     91.55
2079              22    100.00
2080              47    100.00
2081              20    100.00
2082              33     97.39
2083              39     90.57
2084              33    100.00
2085              40     86.68
2086              46     78.89
2087              48     97.39
2088              21     78.89
2089              45    100.00
2090              33    100.00
2091              44    100.00
2092              33    100.00
2093              39    100.00
2094              39     50.31
2095              41    100.00
2096              40     86.92
2097              33    100.00
2098              28     78.89
2099              26     63.76
2100              29     85.49
2101              46     77.52
2102              33     84.77
2103              48     78.25
2104              40     71.00
2105              23     74.62
2106              40     81.14
2107              37     74.62
2108              24     75.35
2109              27     62.31
2110              21     71.00
2111              23     72.45
2112              44     83.32
2113              35     83.32
2114              43     60.86
2115              40     84.77
2116              35     89.90
2117              25     62.46
2118              43    100.00
2119              50     63.34
2120              45     78.25
2121              52     81.14
2122              48     74.62
2123              31     68.71
2124              29     71.14
2125              23     87.31
2126              31     64.67
2127              23     67.10
2128              24     94.58
2129              28     71.14
2130              44     66.29
2131              22     92.16
2132              46     70.33
2133              22     93.77
2134              38     87.31
2135              47     83.27
2136              48     75.18
2137              40     88.12
2138              32     80.84
2139              49     97.01
2140              43     85.69
2141              41    100.00
2142              30    100.00
2143              28     95.39
2144              43    100.00
2145              41    100.00
2146              30     82.42
2147              31    100.00
2148              32    100.00
2149              43     96.31
2150              26    100.00
2151              27    100.00
2152              24    100.00
2153              22    100.00
2154              46    100.00
2155              37     97.27
2156              49     80.90
2157              21    100.00
2158              25    100.00
2159              37    100.00
2160              45     86.68
2161              32     85.72
2162              29     82.83
2163              26     83.79
2164              28    100.00
2165              27     87.64
2166              20     98.18
2167              44    100.00
2168              42    100.00
2169              41    100.00
2170              26    100.00
2171              26    100.00
2172              26    100.00
2173              41     86.68
2174              20     92.90
2175              22    100.00
2176              23    100.00
2177              33     93.90
2178              28    100.00
2179              44     98.89
2180              46     79.91
2181              21    100.00
2182              41    100.00
2183              31    100.00
2184              31     79.91
2185              23     81.91
2186              37     98.89
2187              26    100.00
2188              24     79.91
2189              47    100.00
2190              45     63.91
2191              55    100.00
2192              46     81.17
2193              50    100.00
2194              37    100.00
2195              44     94.90
2196              49    100.00
2197              45    100.00
2198              27     43.45
2199              31     44.66
2200              33     40.23
2201              31     35.80
2202              35     35.40
2203              26     39.83
2204              34     45.46
2205              46     32.99
2206              41     42.24
2207              43     39.43
2208              26     40.23
2209              36     48.28
2210              20     32.59
2211              27     36.61
2212              37     41.03
2213              24     42.24
2214              36     43.05
2215              29     38.22
2216              28    100.00
2217              29    100.00
2218              38     39.83
2219              48     48.28
2220              40     82.46
2221              41     44.56
2222              30     40.23
2223              35     47.62
2224              28     55.73
2225              45     51.95
2226              24     45.99
2227              41     63.85
2228              48     45.99
2229              50     63.31
2230              33     62.77
2231              32     43.29
2232              27     60.06
2233              35     55.19
2234              23     54.11
2235              35     48.70
2236              40     43.83
2237              35     47.62
2238              31     55.19
2239              50     46.53
2240              40     57.90
2241              38     45.45
2242              38    100.00
2243              40     60.60
2244              33     46.53
2245              36    100.00
2246              20     66.47
2247              32     53.18
2248              36     62.77
2249              19     48.70
2250              11     43.83
2251              49     65.87
2252              27     63.38
2253              29     70.84
2254              42     74.57
2255              33     50.95
2256              44     53.44
2257              22     64.00
2258              48     50.95
2259              33     54.68
2260              45     56.55
2261              20     52.82
2262              46     60.90
2263              40     49.71
2264              45     64.63
2265              36     59.65
2266              31     67.73
2267              46     50.33
2268              35     66.49
2269              28    100.00
2270              31     84.71
2271              27    100.00
2272              22    100.00
2273              30     99.55
2274              44     36.07
2275              30     60.28
2276              24     49.71
2277              45     75.63
2278              23     68.52
2279              26     62.70
2280              28     60.76
2281              49     58.18
2282              49     54.94
2283              29     74.98
2284              49     64.64
2285              39     54.94
2286              36     58.82
2287              39     62.05
2288              30     73.04
2289              44     69.16
2290              20     61.41
2291              21     63.35
2292              36     77.57
2293              32     71.75
2294              36     73.04
2295              34     56.24
2296              48    100.00
2297              33     73.69
2298              31    100.00
2299              36    100.00
2300              25    100.00
2301              48    100.00
2302              27     69.16
2303              44     61.41
2304              33     72.92
2305              29     72.23
2306              49     57.10
2307              20     81.86
2308              31     73.61
2309              39     59.16
2310              20     66.04
2311              34     77.73
2312              50     61.22
2313              40     79.11
2314              28     63.97
2315              50     81.86
2316              28     79.80
2317              46     66.04
2318              24     59.16
2319              24     81.17
2320              39     59.16
2321              40     44.51
2322              49     72.33
2323              44     82.26
2324              35    100.00
2325              22     67.41
2326              62     77.73
2327              26     61.22
2328              31    100.00
2329              25     86.74
2330              30     89.80
2331              27    100.00
2332              23    100.00
2333              34    100.00
2334              22    100.00
2335              42     85.72
2336              37    100.00
2337              30    100.00
2338              27    100.00
2339              25    100.00
2340              34     97.97
2341              38    100.00
2342              26    100.00
2343              38    100.00
2344              50     84.70
2345              22    100.00
2346              32    100.00
2347              31     71.02
2348              40    100.00
2349              22    100.00
2350              41    100.00
2351              45     48.98
2352              45    100.00
2353              39     40.15
2354              49     50.62
2355              27     50.19
2356              34     36.66
2357              20     41.02
2358              48     51.93
2359              29     38.40
2360              43     41.02
2361              41     46.26
2362              41     35.35
2363              36     51.93
2364              49     37.97
2365              38     45.39
2366              33     51.93
2367              26     48.44
2368              47     43.64
2369              34     47.57
2370              34     51.93
2371              40     50.62
2372              40     82.21
2373              33     82.59
2374              49     65.80
2375              27    100.00
2376              49     36.66
2377              56     35.35
2378              37     51.93
2379              33    100.00
2380              27    100.00
2381              46    100.00
2382              44    100.00
2383              26    100.00
2384              48     94.92
2385              23    100.00
2386              45    100.00
2387              49    100.00
2388              28     94.92
2389              37    100.00
2390              34    100.00
2391              22    100.00
2392              29    100.00
2393              34     98.39
2394              38    100.00
2395              41    100.00
2396              42    100.00
2397              28    100.00
2398              38    100.00
2399              23    100.00
2400              31     71.40
2401              46    100.00
2402              48     56.55
2403              29    100.00
2404              46    100.00
2405              26    100.00
2406              18    100.00
2407              32     53.31
2408              21     49.21
2409              46     69.12
2410              42     49.79
2411              31     57.41
2412              38     66.78
2413              38     64.44
2414              20     48.62
2415              46     62.09
2416              30     65.61
2417              30     68.54
2418              43     52.14
2419              49     63.85
2420              43     56.82
2421              37     66.78
2422              35     55.07
2423              34     60.34
2424              38     61.51
2425              44    100.00
2426              21    100.00
2427              44    100.00
2428              25     64.93
2429              24     58.58
2430              38     60.06
2431              45    100.00
2432              51     63.85
2433              34     82.99
2434              44     74.85
2435              44     96.00
2436              38     81.36
2437              31     71.60
2438              48     80.55
2439              21     93.56
2440              40     66.72
2441              40     80.55
2442              50     77.29
2443              20     68.34
2444              48     72.41
2445              47     89.50
2446              21     70.78
2447              39     78.92
2448              44     80.55
2449              28     88.68
2450              45     77.29
2451              20    100.00
2452              38    100.00
2453              26    100.00
2454              44    100.00
2455              49    100.00
2456              22    100.00
2457              31     68.34
2458              41     70.67
2459              25     76.67
2460              31     60.00
2461              41     64.00
2462              43     64.67
2463              43     75.34
2464              24     76.00
2465              21     54.00
2466              23     64.67
2467              38     74.67
2468              31     62.67
2469              36     70.67
2470              36     71.34
2471              34     62.00
2472              21     65.34
2473              45     78.67
2474              26     75.34
2475              50     54.00
2476              41     62.00
2477              39     60.00
2478              22    100.00
2479              46     76.67
2480              44    100.00
2481              25     77.34
2482              39     66.67
2483              37     71.34
2484              31    100.00
2485              47     82.21
2486              24     77.64
2487              36    100.00
2488              48    100.00
2489              28     98.65
2490              48     83.12
2491              21     78.55
2492              25    100.00
2493              25    100.00
2494              31     91.34
2495              40     84.03
2496              32     89.51
2497              24     83.12
2498              42    100.00
2499              21    100.00
2500              34     82.21
2501              27    100.00
2502              30     88.60
2503              39    100.00
2504              20     60.54
2505              37     81.87
2506              46    100.00
2507              47     87.69
2508              11    100.00
2509              23     91.34
2510              29     70.15
2511              38     79.68
2512              32     97.00
2513              43     84.01
2514              31     87.48
2515              29    100.00
2516              31     88.34
2517              30     94.40
2518              50     94.40
2519              40     80.55
2520              23     97.00
2521              26     88.34
2522              40    100.00
2523              21    100.00
2524              43     86.61
2525              29     71.89
2526              38     91.81
2527              23     76.22
2528              20    100.00
2529              36     70.30
2530              28    100.00
2531              44    100.00
2532              49    100.00
2533              32     80.55
2534              34    100.00
2535              30    100.00
2536              29     94.14
2537              22     85.99
2538              26    100.00
2539              32     91.43
2540              29    100.00
2541              34     96.86
2542              24     99.57
2543              24     90.52
2544              33     88.71
2545              26    100.00
2546              40     95.95
2547              44     94.14
2548              24     90.52
2549              20     94.14
2550              34    100.00
2551              34     97.76
2552              45     93.24
2553              41    100.00
2554              55     71.25
2555              23    100.00
2556              24     45.39
2557              32     84.41
2558              29     85.76
2559              36    100.00
2560              46     87.80
2561              32     95.95
2562              34    100.00
2563              24    100.00
2564              40    100.00
2565              26     82.77
2566              20    100.00
2567              31    100.00
2568              22     87.75
2569              42    100.00
2570              26     99.72
2571              37     87.75
2572              38     88.75
2573              35    100.00
2574              33     90.75
2575              39    100.00
2576              45    100.00
2577              24    100.00
2578              35     88.75
2579              23    100.00
2580              37    100.00
2581              55     87.75
2582              49    100.00
2583              26    100.00
2584              33    100.00
2585              37     83.84
2586              22     86.76
2587              85     88.75
2588              22    100.00
2589              31     65.77
2590              38     65.77
2591              45     85.29
2592              31     85.29
2593              36     64.33
2594              46     70.11
2595              32     76.62
2596              39     57.82
2597              50     78.79
2598              46     74.45
2599              36     80.95
2600              29     82.40
2601              32     75.89
2602              44     68.67
2603              42     62.16
2604              47     65.77
2605              44     58.55
2606              43     75.17
2607              48     74.45
2608              21     96.31
2609              50     74.35
2610              29     75.35
2611              41     70.33
2612              37    100.00
2613              22     66.50
2614              31     75.89
2615              42    100.00
2616              42    100.00
2617              45    100.00
2618              36    100.00
2619              20    100.00
2620              39     81.93
2621              42     85.98
2622              23     86.99
2623              26     89.01
2624              33    100.00
2625              31     88.00
2626              50    100.00
2627              44    100.00
2628              45     80.92
2629              46     88.00
2630              27     85.98
2631              28    100.00
2632              40    100.00
2633              30     99.13
2634              34    100.00
2635              46    100.00
2636              32     82.83
2637              27    100.00
2638              34    100.00
2639              34     54.84
2640              34    100.00
2641              46     80.92
2642              32    100.00
2643              24    100.00
2644              27     99.67
2645              20    100.00
2646              36    100.00
2647              29    100.00
2648              25    100.00
2649              29    100.00
2650              25     96.11
2651              44    100.00
2652              47    100.00
2653              48    100.00
2654              45    100.00
2655              35    100.00
2656              31    100.00
2657              50    100.00
2658              33    100.00
2659              29    100.00
2660              48     68.80
2661              44     72.42
2662              25     66.73
2663              50    100.00
2664              23    100.00
2665              21     96.11
2666              41    100.00
2667              44     74.40
2668              43     76.00
2669              28     96.00
2670              43     86.40
2671              48     96.00
2672              38     82.40
2673              31     86.40
2674              26     67.20
2675              32     92.00
2676              44     67.20
2677              27     76.00
2678              43     73.60
2679              25     69.60
2680              22     80.80
2681              21     87.20
2682              48     75.20
2683              33     64.00
2684              34    100.00
2685              43     81.95
2686              44    100.00
2687              44    100.00
2688              32     94.34
2689              29     65.60
2690              77     92.00
2691              39     67.20
2692              39     81.14
2693              36    100.00
2694              42     91.15
2695              21    100.00
2696              50     88.15
2697              24    100.00
2698              44     92.16
2699              37    100.00
2700              27     92.16
2701              37    100.00
2702              38    100.00
2703              48     96.16
2704              30    100.00
2705              25     88.15
2706              40     86.15
2707              22     88.15
2708              34    100.00
2709              32     90.15
2710              31     86.15
2711              43     80.00
2712              31     89.38
2713              31     77.34
2714              34     96.34
2715              45     92.08
2716              48    100.00
2717              28    100.00
2718              22    100.00
2719              45     83.42
2720              30     85.41
2721              38     85.41
2722              20    100.00
2723              28    100.00
2724              24    100.00
2725              22     79.45
2726              35     93.35
2727              33     85.41
2728              31     95.34
2729              35     82.43
2730              35     90.37
2731              50     81.43
2732              26    100.00
2733              38     89.38
2734              45    100.00
2735              30    100.00
2736              37     91.37
2737              37     86.61
2738              36     71.89
2739              25    100.00
2740              37    100.00
2741              30     95.48
2742              36    100.00
2743              27     90.37
2744              48     61.44
2745              26     59.22
2746              26     85.13
2747              34     85.87
2748              44     85.87
2749              39     82.91
2750              45     76.25
2751              40     63.67
2752              42     70.33
2753              43     74.03
2754              34     72.55
2755              38     62.19
2756              35     71.07
2757              31     72.55
2758              32     64.41
2759              47     86.62
2760              39     68.11
2761              44     62.19
2762              39     85.87
2763              50     57.86
2764              22     75.51
2765              35    100.00
2766              45     55.62
2767              44     86.40
2768              36     87.36
2769              28     72.55
2770              43     62.19
2771              48     52.64
2772              28     48.17
2773              21     41.71
2774              37     50.65
2775              34     49.16
2776              40     41.71
2777              45     51.15
2778              28     52.14
2779              29     41.71
2780              48     44.69
2781              31     45.69
2782              32     57.61
2783              21     57.11
2784              32     58.60
2785              43     57.61
2786              21     45.19
2787              34     53.63
2788              34     43.70
2789              44     86.13
2790              27     76.31
2791              49     52.64
2792              23     95.20
2793              25     64.97
2794              50     87.15
2795              34     40.22
2796              20     56.12
2797              42     57.61
2798              25     56.78
2799              50     43.68
2800              32     64.97
2801              39     44.23
2802              50     60.06
2803              38     48.59
2804              40     50.23
2805              28     64.43
2806              42     50.23
2807              42     63.88
2808              36     63.34
2809              24     49.69
2810              23     65.52
2811              29     50.78
2812              37     45.86
2813              33     51.32
2814              32     60.06
2815              35     59.51
2816              40     55.69
2817              37     86.74
2818              42     97.16
2819              20    100.00
2820              29    100.00
2821              43    100.00
2822              34     62.24
2823              47     65.52&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;don’t forget to assign it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_data &amp;lt;- select(sales_data, QUANTITYORDERED, PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;arrange() function
order your data&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arranged_asc_data &amp;lt;- arrange(select_data, PRICEEACH)
arranged_desc_data &amp;lt;- arrange(select_data, -PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;filter() function
filter out data
first argument is the dataset, second argument is the filter conditions&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39;)
filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;if-i-want-to-filter-and-select-data-i-have-to-run-the-command-twice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;If I want to ‘filter’ and ‘select’ data, I have to run the command twice&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;)

# notice the first argument is filtered_data
select_data &amp;lt;- select(filtered_data, QUANTITYORDERED, PRICEEACH)

# notice the first argument is select_data
arrange_data &amp;lt;- arrange(select_data, PRICEEACH) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s kind of tedious to have to run the command twice, so we will use a concept called piping (%&amp;gt;%)
Piping is sending the output of one function into the input of another. The output will be the first argument of the next function
The same command above can be written like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;piped_data &amp;lt;- sales_data %&amp;gt;%
  filter(STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;) %&amp;gt;%
  select(QUANTITYORDERED, PRICEEACH) %&amp;gt;%
  arrange(PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;mutate() function
create your own columns using mutate
Same as above but using piping&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutated_data &amp;lt;- mutate(sales_data, discounted = 0.95 * SALES)

# same as above but using piping
mutated_data &amp;lt;- sales_data %&amp;gt;%
  mutate(discounted = 0.95 * SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;YOUR TURN!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sales_data &amp;lt;- read.csv(&amp;#39;sales_data_sample.csv&amp;#39;) # Reread the data in case you made any changes to it&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Q1: what is the most common deal size (column name: DEALSIZE)?
summary(sales_data$DEALSIZE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Large Medium  Small 
   157   1384   1282 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Q2: what is the average quantity ordered? (HINT: Can also use mean function)
mean(sales_data$QUANTITYORDERED)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 35.09281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$QUANTITYORDERED)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   6.00   27.00   35.00   35.09   43.00   97.00 &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Q3: create a new dataset called q3_data with 
#       - a new column called MSRP_REV which is equal to the MSRP * QUANTITYORDERED
#       - filtered to only have &amp;#39;Large&amp;#39; sized deals 
#       - with only the selected columns ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV
#       - ordered in descending order by SALES
q3_data &amp;lt;- sales_data %&amp;gt;% 
  mutate(MSRP_REV = MSRP * QUANTITYORDERED) %&amp;gt;% 
  filter(DEALSIZE == &amp;#39;Large&amp;#39;) %&amp;gt;% 
  select(ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV) %&amp;gt;% 
  arrange(-SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5. group_by() and summarise() function
#    group_by() and summarise() will help us solve questions such as, what are the total sales by country?
grouped_data &amp;lt;- group_by(sales_data, COUNTRY)
summarised_data &amp;lt;- summarise(grouped_data, total_sales = sum(SALES))

# same as above, But using piping
summarised_data &amp;lt;- sales_data %&amp;gt;%
  group_by(COUNTRY) %&amp;gt;%
  summarise(total_sales = sum(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Instead of sum(), can also do max(), min(), mean(), n() for count, and others

# Q4: what is the average SALE by PRODUCTLINE?
summarised_data &amp;lt;- sales_data %&amp;gt;% 
  group_by(PRODUCTLINE) %&amp;gt;% 
  summarise(average_sales = mean(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a simple dot plot
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point() +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# change color
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add labels and remove legend
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  labs(title = &amp;#39;Sales and Quantity Ordered&amp;#39;,
       y = &amp;#39;Unit Price ($)&amp;#39;,
       x = &amp;#39;Quantity Ordered (Units)&amp;#39;) + 
  theme(legend.position=&amp;quot;none&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add regression line
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  labs(title = &amp;#39;Sales and Quantity Ordered&amp;#39;,
       y = &amp;#39;Unit Price ($)&amp;#39;,
       x = &amp;#39;Quantity Ordered (Units)&amp;#39;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)+
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bar charts
# will first create a grouped by and summarised dataset
status_data &amp;lt;- sales_data %&amp;gt;%
  group_by(STATUS) %&amp;gt;%
  summarise(total_count = n()) %&amp;gt;%
  select(STATUS, total_count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now we will create a bar chart
ggplot(status_data, aes(x = STATUS, y = total_count)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;blue&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Q5: create a bar chart of the total sales by country with the following properties:
#     - x axis label: Product Line
#     - y axis label: Total Sales ($)
#     - title: Sales by Product Line
#     - Outline of bars: red
#     - Fill of bars: pink

# Will first create a grouped by and summarised dataset
status_data &amp;lt;- sales_data %&amp;gt;%
  group_by(PRODUCTLINE) %&amp;gt;%
  summarise(total_sales = sum(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bar plot
ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;pink&amp;#39;) +
  labs(title = &amp;#39;Sales by Product Line&amp;#39;) +
  theme_classic() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Q6: using the ggplot2 cheat sheet, try constructing your own plot of choice!
# bar plot
ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;yellow&amp;#39;) +
  labs(title = &amp;#39;Sales by Product Line&amp;#39;) +
  theme_classic() +
  coord_polar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Credit Card Fraud</title>
      <link>/publication/credit_card_fraud/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/credit_card_fraud/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lending Club Loan Defaulter Prediction</title>
      <link>/publication/loan.defaulters/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/loan.defaulters/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bank ATM Cash Machine Forecast w/ Time Series</title>
      <link>/post/atm/bank/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/atm/bank/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. The variable ‘Cash’ is provided in hundreds of dollars.&lt;/p&gt;
&lt;p&gt;This is a time series spanning daily transactions from May 1, 2009 to April 30, 2010 from four ATMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;forecast how much cash is taken out of 4 different ATM machines for May 2010&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ATM624Data.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
atm &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview first 5 rows
head(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;        DATE  ATM Cash
1 2009-05-01 ATM1   96
2 2009-05-01 ATM2  107
3 2009-05-02 ATM1   82
4 2009-05-02 ATM2   89
5 2009-05-03 ATM1   85
6 2009-05-03 ATM2   90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   1474 obs. of  3 variables:
 $ DATE: Date, format: &amp;quot;2009-05-01&amp;quot; &amp;quot;2009-05-01&amp;quot; ...
 $ ATM : Factor w/ 4 levels &amp;quot;ATM1&amp;quot;,&amp;quot;ATM2&amp;quot;,..: 1 2 1 2 1 2 1 2 1 2 ...
 $ Cash: num  96 107 82 89 85 90 90 55 99 79 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      DATE              ATM           Cash        
 Min.   :2009-05-01   ATM1:365   Min.   :    0.0  
 1st Qu.:2009-08-01   ATM2:365   1st Qu.:    0.5  
 Median :2009-11-01   ATM3:365   Median :   73.0  
 Mean   :2009-10-31   ATM4:365   Mean   :  155.6  
 3rd Qu.:2010-02-01   NA&amp;#39;s: 14   3rd Qu.:  114.0  
 Max.   :2010-05-14              Max.   :10919.8  
                                 NA&amp;#39;s   :19       &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Skewed distribution since the mean is higher than the third quartile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview periods between dates in the time series
xts::periodicity(unique(atm$DATE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Daily periodicity from 2009-05-01 to 2010-05-14 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans daily transactions from May 1, 2009 to May 14, 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations that have no missing values
atm[!complete.cases(atm), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;          DATE  ATM Cash
87  2009-06-13 ATM1   NA
93  2009-06-16 ATM1   NA
98  2009-06-18 ATM2   NA
105 2009-06-22 ATM1   NA
110 2009-06-24 ATM2   NA
731 2010-05-01 &amp;lt;NA&amp;gt;   NA
732 2010-05-02 &amp;lt;NA&amp;gt;   NA
733 2010-05-03 &amp;lt;NA&amp;gt;   NA
734 2010-05-04 &amp;lt;NA&amp;gt;   NA
735 2010-05-05 &amp;lt;NA&amp;gt;   NA
736 2010-05-06 &amp;lt;NA&amp;gt;   NA
737 2010-05-07 &amp;lt;NA&amp;gt;   NA
738 2010-05-08 &amp;lt;NA&amp;gt;   NA
739 2010-05-09 &amp;lt;NA&amp;gt;   NA
740 2010-05-10 &amp;lt;NA&amp;gt;   NA
741 2010-05-11 &amp;lt;NA&amp;gt;   NA
742 2010-05-12 &amp;lt;NA&amp;gt;   NA
743 2010-05-13 &amp;lt;NA&amp;gt;   NA
744 2010-05-14 &amp;lt;NA&amp;gt;   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ATM transactions have missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(factor(atm$ATM)[!is.na(atm$Cash) &amp;amp; atm$Cash %% 1 != 0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;ATM1 ATM2 ATM3 ATM4 
   0    0    0  365 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are non-integer transactions at ATM 4 implying that these data are likely debit card purchase transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# time plot represents a line graph that plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  n &amp;lt;- nrow(atm_ts); l &amp;lt;- rep(1, n); m &amp;lt;- rep(20, n); h &amp;lt;- rep(100, n)
  print(plot(cbind(atm_ts, l, m,h), main=paste0(&amp;quot;ATM&amp;quot;, i)))
  
# histogram displays the frequency at which values in a vector occur.
  hist(atm_ts, col=&amp;quot;green&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time Plots and Histograms for ATM1 and ATM2 are unremarkable.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM3 shows the data consists mostly of zero values with a handful of transactions occurring at the end of the series.&lt;/li&gt;
&lt;li&gt;ATM3 will not be modeled due to these degenerative properties.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM4 shows an extreme outlier around the three-quarter mark of the series. The horizontal lines in the Time Plots delineate $1, $20, and $100 in red, green, and blue; respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation The xts::xts()function converts data to a time series object which displays better in visualizations than time series objects created using other packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  acf(na.omit(atm_ts), ylab=paste0(&amp;quot;ACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;) 
  pacf(na.omit(atm_ts), ylab=paste0(&amp;quot;PACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots for ATM1, ATM2, and ATM3 show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations. The ACF and PACF plots for ATM3 however, are not reliable due to the death of observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;p&gt;Data are cleaned using forecast::tsclean() and then converted to a time series object using the ts() function. The tsclean() function imputes nulls and removes outliers. The ts()function converts data to a time series object which is compatible with the forecast package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:length(levels(atm$ATM))) {
  atm_num &amp;lt;- paste0(&amp;quot;ATM&amp;quot;, i)
  atm_sub &amp;lt;- subset(atm, ATM == atm_num, select=-2)
  atm_sub$Cash &amp;lt;- forecast::tsclean(atm_sub$Cash, replace.missing=T)
  assign(atm_num, ts(atm_sub$Cash, frequency = 7, start=start(atm_sub$DATE)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-examine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Examine&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 1), mar = c(0, 4, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(ATM1, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(ATM1, order=7), col=2)  # weekly
lines(forecast::ma(ATM1, order=30), col=4) # monthly
plot(ATM2, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM3&amp;quot;)
lines(forecast::ma(ATM2, order=7), col=2)  # weekly
lines(forecast::ma(ATM2, order=30), col=4) # monthly
plot(ATM4, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM4&amp;quot;)
lines(forecast::ma(ATM4, order=7), col=2)  # weekly
lines(forecast::ma(ATM4, order=30), col=4) # monthly&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 7-day (weekly) and 30-day (monthly) moving average smoother line shows that the data for the ATMs have no apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposition Plot decomposes and plots the observed values, the underlying trend, seasonality, and randomness of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM1), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM2), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM4), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have no apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey-fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey-Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tseries::adf.test(ATM1): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Augmented Dickey-Fuller Test

data:  ATM1
Dickey-Fuller = -4.5329, Lag order = 7, p-value = 0.01
alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tseries::adf.test(ATM2): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Augmented Dickey-Fuller Test

data:  ATM2
Dickey-Fuller = -6.046, Lag order = 7, p-value = 0.01
alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tseries::adf.test(ATM4): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Augmented Dickey-Fuller Test

data:  ATM4
Dickey-Fuller = -5.6304, Lag order = 7, p-value = 0.01
alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-values are below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows by index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train/test split
index_train &amp;lt;- 1:(length(ATM1) - 30)
ATM1_train &amp;lt;- ts(ATM1[index_train], frequency=7)
ATM1_test &amp;lt;- ts(ATM1[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM2) - 30)
ATM2_train &amp;lt;- ts(ATM2[index_train], frequency=7)
ATM2_test &amp;lt;- ts(ATM2[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM3) - 30)
ATM3_train &amp;lt;- ts(ATM3[index_train], frequency=7)
ATM3_test &amp;lt;- ts(ATM3[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM4) - 30)
ATM4_train &amp;lt;- ts(ATM4[index_train], frequency=7)
ATM4_test &amp;lt;- ts(ATM4[-index_train], frequency=7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The indexed rows for the test set are a window at the end of the times series. The window sized for the testing set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted.Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda1 &amp;lt;- forecast::BoxCox.lambda(ATM1_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.4355901&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda2 &amp;lt;- forecast::BoxCox.lambda(ATM2_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.7156895&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda4 &amp;lt;- forecast::BoxCox.lambda(ATM4_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 0.3945256&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameters suggested are around λ=0.5. This rounded (more interpretable) value is suggestive of a 1/yt‾‾√ transformation. These Box-Cox transformations stabilize the variance and make each series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit1 &amp;lt;- forecast::auto.arima(ATM1_train, stepwise=F, approximation=F, d=0, lambda=lambda1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Series: ATM1_train 
ARIMA(0,0,2)(1,1,1)[7] 
Box Cox transformation: lambda= 0.4355901 

Coefficients:
         ma1      ma2    sar1     sma1
      0.1449  -0.1116  0.1320  -0.7243
s.e.  0.0547   0.0537  0.0893   0.0669

sigma^2 estimated as 6.441:  log likelihood=-770.86
AIC=1551.73   AICc=1551.92   BIC=1570.69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit2 &amp;lt;- forecast::auto.arima(ATM2_train, stepwise=F, approximation=F, d=0, lambda=lambda2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Series: ATM2_train 
ARIMA(2,0,2)(0,1,1)[7] with drift 
Box Cox transformation: lambda= 0.7156895 

Coefficients:
          ar1      ar2     ma1     ma2     sma1    drift
      -0.4282  -0.9254  0.4761  0.8044  -0.7672  -0.0246
s.e.   0.0464   0.0413  0.0764  0.0555   0.0483   0.0155

sigma^2 estimated as 66.34:  log likelihood=-1152.83
AIC=2319.66   AICc=2320.01   BIC=2346.21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit4 &amp;lt;- forecast::auto.arima(ATM4_train, stepwise=F, approximation=F, d=0, lambda=lambda4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Series: ATM4_train 
ARIMA(1,0,0)(2,0,0)[7] with non-zero mean 
Box Cox transformation: lambda= 0.3945256 

Coefficients:
         ar1    sar1    sar2     mean
      0.0814  0.2060  0.1911  22.7977
s.e.  0.0548  0.0537  0.0547   0.9477

sigma^2 estimated as 97.25:  log likelihood=-1240.53
AIC=2491.06   AICc=2491.24   BIC=2510.13&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluate&lt;/h2&gt;
&lt;p&gt;ACF and PACF&lt;/p&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit1), ylab=&amp;quot;ACF ATM1&amp;quot;); pacf(residuals(fit1), ylab=&amp;quot;PACF ATM1&amp;quot;)
acf(residuals(fit2), ylab=&amp;quot;ACF ATM2&amp;quot;); pacf(residuals(fit2), ylab=&amp;quot;PACF ATM2&amp;quot;)
acf(residuals(fit4), ylab=&amp;quot;ACF ATM4&amp;quot;); pacf(residuals(fit4), ylab=&amp;quot;PACF ATM4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the models appear to display the characteristics of White Noise in the ACF and PACF plots with only one of the twenty residuals (or 0.05%) being significant. At a 95% confidence interval this is within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The arma attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,period,d,and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit1), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Box-Ljung test

data:  residuals(fit1)
X-squared = 5.7195, df = 5, p-value = 0.3345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit2), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Box-Ljung test

data:  residuals(fit2)
X-squared = 7.9286, df = 5, p-value = 0.1602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit4), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Box-Ljung test

data:  residuals(fit4)
X-squared = 4.6833, df = 5, p-value = 0.4557&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the models are not significantly different from zero at α=0.05. The residuals of the models display the characteristics of White Noise. The models pass the required checks and are therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;ATM3 was not modeled due to its degenerative properties. To forecast values for ATM3, the model for an ATM with a similar mean will be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(mean(ATM1), mean(ATM2), mean(ATM3[ATM3!=0]), mean(ATM4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]  84.15479  62.59178  87.66667 444.75681&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of ATM1 is very close to the mean of the few values in ATM3. Therefore, the ARIMA(0,0,1)(2,0,0)7 ARIMA model for ATM1 will be used to make predictions for ATM3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- forecast::Arima(ATM3_train, model=fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data were not seasonally adjusted, they need not be reseasonalized prior to forecast. Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a red line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast1 &amp;lt;- forecast::forecast(fit1, h=30)
fcast2 &amp;lt;- forecast::forecast(fit2, h=30)
fcast3 &amp;lt;- forecast::forecast(fit3, h=30)
fcast4 &amp;lt;- forecast::forecast(fit4, h=30)
par(mfrow=c(4, 1), mar = c(0, 4, 0, 0), oma = c(4, 4, 2, 0.5))
plot(fcast1, ylab=&amp;quot;Cash ATM1&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM1_test, -length(ATM1_train)), col=&amp;quot;red&amp;quot;)
plot(fcast2, ylab=&amp;quot;Cash ATM2&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM2_test, -length(ATM2_train)), col=&amp;quot;red&amp;quot;)
plot(fcast3, ylab=&amp;quot;Cash ATM3&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM3_test, -length(ATM3_train)), col=&amp;quot;red&amp;quot;)
plot(fcast4, ylab=&amp;quot;Cash ATM4&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM4_test, -length(ATM4_train)), col=&amp;quot;red&amp;quot;)
title(&amp;quot;ATM Predictions&amp;quot;, outer=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/atm/atm_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The predictions appear to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast1, length(ATM1_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  ME   RMSE    MAE      MPE    MAPE  MASE  ACF1
Training set   2.038 25.007 16.039  -96.186 114.754 0.427 0.011
Test set     -53.187 53.187 53.187 -177.290 177.290 1.416    NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast2, length(ATM2_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                  ME   RMSE    MAE      MPE    MAPE MASE   ACF1
Training set   1.456 24.795 17.275     -Inf     Inf 0.40 -0.013
Test set     -44.485 44.485 44.485 -148.284 148.284 1.03     NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast4, length(ATM4_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                   ME    RMSE     MAE       MPE     MAPE MASE  ACF1
Training set   96.204 360.567 280.446  -342.343  388.847 0.72 0.017
Test set     -385.878 385.878 385.878 -1286.261 1286.261 0.99    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications vary. ATM1 and ATM2 predictions are more accurate than ATM4 predictions. The closer the original data are to being White Noise, the less accurate the predictions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Forecasting w/ Time Series Analysis</title>
      <link>/post/residential_energy/residential-energy-usage/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/residential_energy/residential-energy-usage/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. A simple dataset of residential power usage from January 1998 to December 2013.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through an analysis, model this data and monthly forecast for 2014&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ResidentialCustomerForecastLoad-624.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
energy &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)

# the “YYYY-MMM” format dates are interpreted as factors. They must be converted to dates
energy$YYYY.MMM &amp;lt;- as.Date(paste0(energy$YYYY.MMM,&amp;quot;-01&amp;quot;), format = &amp;quot;%Y-%b-%d&amp;quot;)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  CaseSequence   YYYY.MMM     KWH
1          733 1998-01-01 6862583
2          734 1998-02-01 5838198
3          735 1998-03-01 5420658
4          736 1998-04-01 5010364
5          737 1998-05-01 4665377
6          738 1998-06-01 6467147&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the class of the dataset
class(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   192 obs. of  3 variables:
 $ CaseSequence: num  733 734 735 736 737 738 739 740 741 742 ...
 $ YYYY.MMM    : Date, format: &amp;quot;1998-01-01&amp;quot; &amp;quot;1998-02-01&amp;quot; ...
 $ KWH         : num  6862583 5838198 5420658 5010364 4665377 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  CaseSequence      YYYY.MMM               KWH          
 Min.   :733.0   Min.   :1998-01-01   Min.   :  770523  
 1st Qu.:780.8   1st Qu.:2001-12-24   1st Qu.: 5429912  
 Median :828.5   Median :2005-12-16   Median : 6283324  
 Mean   :828.5   Mean   :2005-12-15   Mean   : 6502475  
 3rd Qu.:876.2   3rd Qu.:2009-12-08   3rd Qu.: 7620524  
 Max.   :924.0   Max.   :2013-12-01   Max.   :10655730  
                                      NA&amp;#39;s   :1         &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the periods between dates in dataset
xts::periodicity(unique(energy$YYYY.MMM))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Monthly periodicity from 1998-01-01 to 2013-12-01 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans monthly from January 1, 1998 to December 1, 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations in the dataframe that have no missing values
energy[!complete.cases(energy), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    CaseSequence   YYYY.MMM KWH
129          861 2008-09-01  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe contains one missing value in kWh usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
kWh &amp;lt;- xts::xts(energy$KWH, order.by=energy$YYYY.MMM)
par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(kWh, main=&amp;quot;kWh&amp;quot;)

# display frequency at which values in a vector occur
hist(kWh, col=&amp;quot;yellow&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obervations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Line plot and Histogram shows an outlier around the three-quarter mark of the series.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
# ACF autocorrelations between each observation and its immediate predecessor (lagged observation)
acf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;) 

# PACF autocorrelations between the current observation and each individual lagged observation
pacf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data cleaning w/ forecast::tsclean() and converted to a time series object using the ts(). 
# tsclean() function imputes nulls and removes outliers.
# ts() function converts data to a time series object which is compatible with the forecast package.
kWh &amp;lt;- ts(forecast::tsclean(energy$KWH, replace.missing=T), 
          frequency = 12, start=start(energy$YYYY.MMM)) # data sampled monthly = 12
kWh[kWh==min(kWh)] &amp;lt;- mean(kWh[kWh!=min(kWh)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-preview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Preview&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(kWh, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(kWh, order=6), col=6)  # pink line biannual period
lines(forecast::ma(kWh, order=12), col=4) # blue line annual period&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 6-month and 12-month moving average smoother line shows that the data has a slight apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposes and plots the &lt;strong&gt;observed&lt;/strong&gt; values, the underlying &lt;strong&gt;trend,&lt;/strong&gt; &lt;strong&gt;seasonality,&lt;/strong&gt; and &lt;strong&gt;randomness&lt;/strong&gt; of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(kWh), col=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obseravations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have a slight apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey_fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey_Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(kWh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Warning in tseries::adf.test(kWh): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Augmented Dickey-Fuller Test

data:  kWh
Dickey-Fuller = -4.5454, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-value is below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows w/ index. The indexed rows for the testing set are a window at the end of the times series. The window sized for the test set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index_train &amp;lt;- 1:(length(kWh) - 12)
kWh_train &amp;lt;- ts(kWh[index_train], frequency=12)
kWh_test &amp;lt;- ts(kWh[index_train], frequency=12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted. Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda &amp;lt;- forecast::BoxCox.lambda(kWh_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] -0.1733063&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameter suggested is about λ=−0.25. This rounded (slightly more interpretable) value is suggestive of an inverse quartic root. This Box-Cox transformation stabilizes the variance and makes the series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- forecast::auto.arima(kWh_train, stepwise=F, approximation=F, d=0, lambda=lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Series: kWh_train 
ARIMA(0,0,3)(2,1,0)[12] with drift 
Box Cox transformation: lambda= -0.1733063 

Coefficients:
         ma1     ma2     ma3     sar1     sar2  drift
      0.2807  0.0855  0.2232  -0.7724  -0.4408  1e-04
s.e.  0.0757  0.0823  0.0687   0.0742   0.0812  1e-04

sigma^2 estimated as 3.707e-05:  log likelihood=621.98
AIC=-1229.95   AICc=-1229.25   BIC=-1208.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The auto.arima() function suggests an ARIMA(0,0,3)(2,1,0)12 model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit), ylab=&amp;quot;ACF kWh&amp;quot;); pacf(residuals(fit), ylab=&amp;quot;PACF kWh&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the model appear to display the characteristics of White Noise in both the ACF and PACF plots. None of the residuals are significant. At a 95% confidence interval this is well within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The ARIMA attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,periods,d and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit), lag=7, fitdf=sum(fit$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
    Box-Ljung test

data:  residuals(fit)
X-squared = 7.2523, df = 4, p-value = 0.1231&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the model are not significantly different from zero at α=0.05. The residuals of the model displays the characteristics of White Noise. The model passes the required checks and is therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data was not seasonally adjusted, they need not be reseasonalized prior to forecast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast &amp;lt;- forecast::forecast(fit, h=15)
plot(fcast, ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;kWh Predictions&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(kWh_test, -length(kWh_train)), col=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The prediction appears to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;li&gt;Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a pink line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast, length(kWh_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;                      ME      RMSE       MAE          MPE        MAPE
Training set    39449.18  581186.1  456353.6        0.056       7.067
Test set     -9046871.23 9046871.2 9046871.2 -5026039.573 5026039.573
              MASE  ACF1
Training set 0.413 0.115
Test set     8.185    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications is fair. The large metrics are representative of the large values found in the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Display Jupyter Notebooks with Academic</title>
      <link>/post/jupyter/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/jupyter/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.core.display import Image
Image(&#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./academic_0_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Welcome to Academic!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Welcome to Academic!
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;install-python-and-jupyter&#34;&gt;Install Python and Jupyter&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.anaconda.com/distribution/#download-section&#34; target=&#34;_blank&#34;&gt;Install Anaconda&lt;/a&gt; which includes Python 3 and Jupyter notebook.&lt;/p&gt;

&lt;p&gt;Otherwise, for advanced users, install Jupyter notebook with &lt;code&gt;pip3 install jupyter&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-a-new-blog-post-as-usual-https-sourcethemes-com-academic-docs-managing-content-create-a-blog-post&#34;&gt;Create a new blog post &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-a-blog-post&#34; target=&#34;_blank&#34;&gt;as usual&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Run the following commands in your Terminal, substituting &lt;code&gt;&amp;lt;MY_WEBSITE_FOLDER&amp;gt;&lt;/code&gt; and &lt;code&gt;my-post&lt;/code&gt; with the file path to your Academic website folder and a name for your blog post (without spaces), respectively:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;
hugo new  --kind post post/my-post
cd &amp;lt;MY_WEBSITE_FOLDER&amp;gt;/content/post/my-post/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;create-or-upload-a-jupyter-notebook&#34;&gt;Create or upload a Jupyter notebook&lt;/h2&gt;

&lt;p&gt;Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (&lt;em&gt;New &amp;gt; Python Notebook&lt;/em&gt;) or upload a notebook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;convert-notebook-to-markdown&#34;&gt;Convert notebook to Markdown&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=.

# Copy the contents of Untitled.md and append it to index.md:
cat Untitled.md | tee -a index.md

# Remove the temporary file:
rm Untitled.md
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;edit-your-post-metadata&#34;&gt;Edit your post metadata&lt;/h2&gt;

&lt;p&gt;Open &lt;code&gt;index.md&lt;/code&gt; in your text editor and edit the title etc. in the &lt;a href=&#34;https://sourcethemes.com/academic/docs/front-matter/&#34; target=&#34;_blank&#34;&gt;front matter&lt;/a&gt; according to your preference.&lt;/p&gt;

&lt;p&gt;To set a &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#featured-image&#34; target=&#34;_blank&#34;&gt;featured image&lt;/a&gt;, place an image named &lt;code&gt;featured&lt;/code&gt; into your post&amp;rsquo;s folder.&lt;/p&gt;

&lt;p&gt;For other tips, such as using math, see the guide on &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;writing content with Academic&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Energy Efficiency on Buildings</title>
      <link>/post/energye/energye/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energye/energye/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;The dataset is available at [&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;to explore three data points, and visualize how they influence the energy load.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following variables &lt;strong&gt;Wall.Area&lt;/strong&gt;, &lt;strong&gt;Roof.Area&lt;/strong&gt;, &lt;strong&gt;Glazing.Area&lt;/strong&gt; are identified as key indicators that can influence the energy load efficiency for both (Heating and Cooling spaces).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Plot Load Distribution Using Scatter Plot&lt;/li&gt;
&lt;li&gt;Plot Heating Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Cooling Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Energy Efficiency&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())

sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/StephenElston/DataScience350/master/Lecture1/EnergyEfficiencyData.csv&amp;quot;

df &amp;lt;- read.csv( sourceURL, header = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  Relative.Compactness Surface.Area Wall.Area Roof.Area Overall.Height
1                 0.98        514.5     294.0    110.25              7
2                 0.98        514.5     294.0    110.25              7
3                 0.98        514.5     294.0    110.25              7
4                 0.98        514.5     294.0    110.25              7
5                 0.90        563.5     318.5    122.50              7
6                 0.90        563.5     318.5    122.50              7
  Orientation Glazing.Area Glazing.Area.Distribution Heating.Load
1           2            0                         0        15.55
2           3            0                         0        15.55
3           4            0                         0        15.55
4           5            0                         0        15.55
5           2            0                         0        20.84
6           3            0                         0        21.46
  Cooling.Load
1        21.33
2        21.33
3        21.33
4        21.33
5        28.28
6        25.38&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   768 obs. of  10 variables:
 $ Relative.Compactness     : num  0.98 0.98 0.98 0.98 0.9 0.9 0.9 0.9 0.86 0.86 ...
 $ Surface.Area             : num  514 514 514 514 564 ...
 $ Wall.Area                : num  294 294 294 294 318 ...
 $ Roof.Area                : num  110 110 110 110 122 ...
 $ Overall.Height           : num  7 7 7 7 7 7 7 7 7 7 ...
 $ Orientation              : int  2 3 4 5 2 3 4 5 2 3 ...
 $ Glazing.Area             : num  0 0 0 0 0 0 0 0 0 0 ...
 $ Glazing.Area.Distribution: int  0 0 0 0 0 0 0 0 0 0 ...
 $ Heating.Load             : num  15.6 15.6 15.6 15.6 20.8 ...
 $ Cooling.Load             : num  21.3 21.3 21.3 21.3 28.3 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Categorize useful variables and convert them to a categorical variables, namely &lt;strong&gt;Orientation&lt;/strong&gt;, &lt;strong&gt;Glazing.Area.Distribution&lt;/strong&gt;, and &lt;strong&gt;Glazing.Area&lt;/strong&gt; (variance) variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# change vector values to factor values
df$Orientation &amp;lt;- as.factor(df$Orientation) 

# attributes of variable 
levels(df$Orientation) &amp;lt;- c(&amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

# change vector values to factor values
df$Glazing.Area.Distribution &amp;lt;- as.factor(df$Glazing.Area.Distribution)

# attributes of variable
levels(df$Glazing.Area.Distribution) &amp;lt;- c(&amp;quot;UnKnown&amp;quot;, &amp;quot;Uniform&amp;quot;, &amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

# change vector values to factor values
df$Glazing.Area &amp;lt;- as.factor(df$Glazing.Area) 

# attributes of variable
levels(df$Glazing.Area) &amp;lt;- c(&amp;quot;0%&amp;quot;, &amp;quot;10%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;40%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; Relative.Compactness  Surface.Area     Wall.Area       Roof.Area    
 Min.   :0.6200       Min.   :514.5   Min.   :245.0   Min.   :110.2  
 1st Qu.:0.6825       1st Qu.:606.4   1st Qu.:294.0   1st Qu.:140.9  
 Median :0.7500       Median :673.8   Median :318.5   Median :183.8  
 Mean   :0.7642       Mean   :671.7   Mean   :318.5   Mean   :176.6  
 3rd Qu.:0.8300       3rd Qu.:741.1   3rd Qu.:343.0   3rd Qu.:220.5  
 Max.   :0.9800       Max.   :808.5   Max.   :416.5   Max.   :220.5  
 Overall.Height Orientation Glazing.Area Glazing.Area.Distribution
 Min.   :3.50   North:192   0% : 48      UnKnown: 48              
 1st Qu.:3.50   East :192   10%:240      Uniform:144              
 Median :5.25   South:192   25%:240      North  :144              
 Mean   :5.25   West :192   40%:240      East   :144              
 3rd Qu.:7.00                            South  :144              
 Max.   :7.00                            West   :144              
  Heating.Load    Cooling.Load  
 Min.   : 6.01   Min.   :10.90  
 1st Qu.:12.99   1st Qu.:15.62  
 Median :18.95   Median :22.08  
 Mean   :22.31   Mean   :24.59  
 3rd Qu.:31.67   3rd Qu.:33.13  
 Max.   :43.10   Max.   :48.03  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-load-distribution-using-scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Load Distribution Using Scatter Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# visualize if there is any relation between &amp;#39;Roof.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; and how load is distributed using scatter plot.
ggplot(df, aes(x = Cooling.Load, y = Heating.Load), alpha = 0.5)+
  geom_point(aes(colour = Roof.Area ))+
  facet_grid(Overall.Height + Glazing.Area ~ Surface.Area,  space = &amp;quot;free&amp;quot;) +
  ggtitle(&amp;quot;Load distribuiton of energy by Roof Area and Surface Area \n by Glazing Area and Overall Height&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Roof area and Surface area range is high for minimum/ lowest (3.5) over-all height and&lt;/li&gt;
&lt;li&gt;Roof area and Surface area range is low for maximum/ highest (7.0) over-all height.&lt;/li&gt;
&lt;li&gt;There are no data points when the overall height is 7 and highest surface area range and also for low overall height 3.5, we have no data points with the low surface area range.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-heating-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Heating Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot how &amp;#39;Wall.Area&amp;#39; influence heating load using raster plot.
ggplot(df, aes( Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Heating.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;steelblue&amp;quot;, high = &amp;quot;red&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Heating Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By looking at the figures, we can conclude that &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in heating, irrespective of Surface Area and Roof Area. (Higher the wall area, higher the heating load).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-cooling-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Cooling Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot how &amp;#39;Wall.Area&amp;#39; influence cooling load using raster plot.
ggplot(df, aes(Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Cooling.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;grey&amp;quot;, high = &amp;quot;steelblue&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Cooling Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So, &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in both Heating and Cooling Load efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-energy-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Energy Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we have seen more variation in load data when the overall height is (7.0). So lets create a subset named(energy.eff.sub7.0) which contains the filtered data with overall height = 7.0. Lets visualize, if the &amp;#39;Roof.Area&amp;#39;, &amp;#39;Wall.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; are influencing the load efficiency.
energy.eff.sub7.0 &amp;lt;- df[ df$Overall.Height ==7.0,]
ggplot(energy.eff.sub7.0,
       aes(x = Cooling.Load, y = Heating.Load, group = factor(round(Wall.Area)), 
           size = Glazing.Area,
           shape = factor(round(Wall.Area))))+
  geom_point(aes(colour= factor(round(Surface.Area))), alpha = 0.3)+
  geom_smooth(method = &amp;quot;lm&amp;quot;,se = TRUE )+
  facet_wrap(~ Roof.Area) +
  ggtitle(&amp;#39;Load efficiency by Roof Area, by Wall Area by Surface Area and by Glazing Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is clearly evident that the Load efficiency is influenced by the Roof Area, Wall Area, Surface Area, and Glazing Area.&lt;/li&gt;
&lt;li&gt;When the Glazing Area is high, Roof Area is high and Wall Area is high, Load will be high and viceversa.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Demand Analysis w/ Time Series Forecasting</title>
      <link>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</guid>
      <description>


&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;R. H. Shumway, D. S. Stoffer. &lt;em&gt;Time Series Analysis and Its Applications&lt;/em&gt;. 2010.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman. &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;. 2013.&lt;/li&gt;
&lt;li&gt;P. S. P. Cowpertwait, A. V. Metcalfe. &lt;em&gt;Introductory Time Series with R&lt;/em&gt;. 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on an analysis of the energy demands of a European country.&lt;/p&gt;
&lt;p&gt;The dataset of the daily energy needs (in GWh) between 2004 and 2010.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;build a model for energy demand forecasting using time series analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;seasonal ARIMA model&lt;/li&gt;
&lt;li&gt;Forecast model&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;#39;https://gist.githubusercontent.com/Peque/715e91350f0e68e3342f/raw/d28312ac0e49888a5079fcea188770acaf3aa4a2/mme.csv&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download and load data into memory
tmp &amp;lt;- tempfile()
download.file(sourceURL, tmp, method = &amp;#39;curl&amp;#39;)
df &amp;lt;- read.csv(tmp)
unlink(tmp)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;      date demand
1 01-01-04 488.07
2 02-01-04 582.02
3 03-01-04 575.58
4 04-01-04 542.39
5 05-01-04 600.26
6 06-01-04 544.76&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert date strings to POSIX dates
df$date &amp;lt;- strptime(df$date, format = &amp;#39;%d-%m-%y&amp;#39;)
# day of week
df$day &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%A&amp;#39;))
# day of year
df$yearday &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%m%d&amp;#39;))
# structure for analysis
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;data.frame&amp;#39;:   3288 obs. of  4 variables:
 $ date   : POSIXlt, format: &amp;quot;2004-01-01&amp;quot; &amp;quot;2004-01-02&amp;quot; ...
 $ demand : num  488 582 576 542 600 ...
 $ day    : Factor w/ 7 levels &amp;quot;Friday&amp;quot;,&amp;quot;Monday&amp;quot;,..: 5 1 3 4 2 6 7 5 1 3 ...
 $ yearday: Factor w/ 366 levels &amp;quot;0101&amp;quot;,&amp;quot;0102&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df split to create test set
df_test &amp;lt;- subset(df, date &amp;gt;= strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
df &amp;lt;- subset(df, date &amp;lt; strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
ts &amp;lt;- ts(df$demand, frequency = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df and time series objects
demandts &amp;lt;- xts(df$demand, df$date)
plot(demandts, main = &amp;#39;Energy Demand Preview&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A seasonal dependency of demand can be easily spotted in the graphics, although there are other factors that may affect the results, such as the temperature, holidays, weekends, etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand by day of the week
ggplot(df, aes(day, demand)) +
  geom_boxplot(fill=&amp;#39;slateblue&amp;#39;, alpha=0.2) + xlab(&amp;#39;Time&amp;#39;) + ylab(&amp;#39;Demand (GWh)&amp;#39;) + ggtitle(&amp;#39;Demand per day of the week&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During weekends, the demand decreases considerably compared to the rest of the week days.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_per_yearday &amp;lt;- aggregate(demand ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand, f = 1 / 45)
l &amp;lt;- length(avg_demand_per_yearday$demand)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting results
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_per_yearday$demand, type = &amp;#39;l&amp;#39;, main = &amp;#39;Average Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During the winter &amp;amp; summer seasons the demand is clearly higher exept for, vacation periods. Holydays are also easily spotted in the graphics, being the lowest peaks of demand.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_per_yearday$demand - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The graphics show the errors. Notice how the biggest errors are all negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(strftime(dates[order(-abs_diff)], format = &amp;#39;%B %d&amp;#39;), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;January 01&amp;quot;  &amp;quot;December 25&amp;quot; &amp;quot;May 01&amp;quot;      &amp;quot;January 06&amp;quot;  &amp;quot;August 15&amp;quot;  
 [6] &amp;quot;December 08&amp;quot; &amp;quot;December 31&amp;quot; &amp;quot;October 12&amp;quot;  &amp;quot;November 01&amp;quot; &amp;quot;December 26&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact dates which are generating these errors are indeed, holidays or the day just before holidays (as is the case for the 25th November and 31th Devember).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))
acf(df$demand, 100, main = &amp;#39;Autocorrelation&amp;#39;)
acf(df$demand, 1500, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)
pacf(df$demand, 1500, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The autocorrelation function shows a highly autocorrelated seasonal non-stationary process with, as expected, yearly and weekly cicles. The ACF alone, however, tells us little about the orders of dependence for ARMIA or AR processes. The PACF is better for AR models, and also shows the weekly and yearly seasons, although the correlation is lost faster with the lag.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data decomposition&lt;/h2&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of weeks at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of weekly seasonal time series
wts &amp;lt;- ts(ts, frequency = 7)
dec_wts &amp;lt;- decompose(wts)
plot(dec_wts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand minus week seasonal
df$demand_mws &amp;lt;- df$demand - as.numeric(dec_wts$season)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of yearly time series
yts &amp;lt;- ts(subset(df, yearday != &amp;#39;0229&amp;#39;)$demand_mws, frequency = 365)
dec_yts &amp;lt;- decompose(yts)
plot(dec_yts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decomposition of the yearly seasonal time series. 29th February days are excluded for frequency matching. The time series is formed out of the original observation minus the weekly seasonal data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;days365 &amp;lt;- which(df$yearday != &amp;#39;0229&amp;#39;)
february29ths &amp;lt;- which(df$yearday == &amp;#39;0229&amp;#39;)
df$demand_mwys[days365] &amp;lt;- df$demand_mws[days365] - as.numeric(dec_yts$season)
# Fill values on February 29th
df$demand_mwys[february29ths] &amp;lt;- df$demand_mws[february29ths]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# form new ts from original observations less the weekly and yearly seasonal data
par(mfrow = c(1, 1))
ts_mwys &amp;lt;- ts(df$demand_mwys, frequency = 1)
demandts_mwys &amp;lt;- xts(df$demand_mwys, df$date)
plot(demandts_mwys, main = &amp;#39;Energy Demand Less Seasonal Data&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_mwys_per_yearday &amp;lt;- aggregate(demand_mwys ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand_mwys, f = 1 / 45)
l &amp;lt;- length(avg_demand_mwys_per_yearday$demand_mwys)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting the result
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_mwys_per_yearday$demand_mwys, type = &amp;#39;l&amp;#39;, main = &amp;#39;Mean Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_mwys_per_yearday$demand_mwys - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the average daily demand of the demand less the seasonal data shows a new error rate much lower than the one seen before.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new acf and pacf created
par(mfrow = c(1, 2))
acf(df$demand_mwys, 100, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand_mwys, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonal-arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;seasonal ARIMA model&lt;/h2&gt;
&lt;p&gt;The initial ARIMA parameters have been found using the R &lt;span class=&#34;math inline&#34;&gt;\(auto.arima()\)&lt;/span&gt; function. The differencing parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is selected using the KPSS test. If the null hypothesis of stationarity is accepted when the KPSS is applied to the original time series, then &lt;span class=&#34;math inline&#34;&gt;\(d = 0\)&lt;/span&gt;. Otherwise, the series is differenced until the KPSS accepts the null hypothesis. After that, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; are selected using either AIC or BIC. The SARIMA model has been created using those ARIMA parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# forecast the error w/ test dataframe
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  pred &amp;lt;- c(pred, p)
  errs &amp;lt;- c(errs, p - df_test$demand[i])
  perc &amp;lt;- c(perc, (p - df_test$demand[i]) / df_test$demand[i])
  auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 2.299037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test datadrame &lt;strong&gt;(2.3%)&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# special days present less demand than others. Those days may be taken into account in order to reduce the error
specialday &amp;lt;- function(day) {
  correction = 0
  if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0101&amp;#39;, &amp;#39;0501&amp;#39;, &amp;#39;0106&amp;#39;, &amp;#39;0815&amp;#39;, &amp;#39;1012&amp;#39;, &amp;#39;1101&amp;#39;, &amp;#39;1206&amp;#39;, &amp;#39;1208&amp;#39;, &amp;#39;1224&amp;#39;, &amp;#39;1225&amp;#39;, &amp;#39;1226&amp;#39;, &amp;#39;1231&amp;#39;))
      correction = -100
  else if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0319&amp;#39;))
    correction = -50

# on Sunday, do not apply correction
  if (as.factor(strftime(day, format = &amp;#39;%A&amp;#39;)) == &amp;#39;Sunday&amp;#39;)
    return(0)
  return(correction)
}

model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  correction = specialday(df_test$date[i])
  pred &amp;lt;- c(pred, p + correction)
  errs &amp;lt;- c(errs, p + correction - df_test$demand[i])
  perc &amp;lt;- c(perc, (p + correction - df_test$demand[i]) / df_test$demand[i])
  if (!correction)
    auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  else
    auxts &amp;lt;- ts(c(auxts, p), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1] 1.956568&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test dataframe &lt;strong&gt;(1,96%)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecast-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecast Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(forecast(Arima(tail(ts, 200), model = model))) +
  labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Energy Demand (GWh)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Analysis</title>
      <link>/post/time_series_ap/time-series-analysis/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time_series_ap/time-series-analysis/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the airpassengers dataframe.&lt;/p&gt;
&lt;p&gt;The AirPassenger dataset in R provides monthly totals of US airline passengers, from 1949 to 1960.&lt;/p&gt;
&lt;p&gt;Description of dataframe airpassengers can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through analysis and modelling, preview a time series forecast&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess whether a linear regression or arima model is a best fit for the time series forecast as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;Stationarity test&lt;/li&gt;
&lt;li&gt;Fit a model using an algorithm&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(AirPassengers)
AP &amp;lt;- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is already of a time series class.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview of data
AP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Passenger numbers in (’000) per month for the relevant years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for missing values
sum(is.na(AP))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zero missing values GREAT!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test frequency
frequency(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;12 calendar months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test cycle
cycle(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949   1   2   3   4   5   6   7   8   9  10  11  12
## 1950   1   2   3   4   5   6   7   8   9  10  11  12
## 1951   1   2   3   4   5   6   7   8   9  10  11  12
## 1952   1   2   3   4   5   6   7   8   9  10  11  12
## 1953   1   2   3   4   5   6   7   8   9  10  11  12
## 1954   1   2   3   4   5   6   7   8   9  10  11  12
## 1955   1   2   3   4   5   6   7   8   9  10  11  12
## 1956   1   2   3   4   5   6   7   8   9  10  11  12
## 1957   1   2   3   4   5   6   7   8   9  10  11  12
## 1958   1   2   3   4   5   6   7   8   9  10  11  12
## 1959   1   2   3   4   5   6   7   8   9  10  11  12
## 1960   1   2   3   4   5   6   7   8   9  10  11  12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dataset summary
summary(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   104.0   180.0   265.5   280.3   360.5   622.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statistical values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the raw data using the base plot function
autoplot(AP) + labs(x=&amp;quot;Time&amp;quot;, y =&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(AP~cycle(AP), xlab=&amp;quot;Passenger Numbers (&amp;#39;000)&amp;quot;, ylab=&amp;quot;Months&amp;quot;, col=rgb(0.1,0.9,0.3,0.4), main=&amp;quot;Monthly Air Passengers Boxplot from 1949 to 1961&amp;quot;, horizontal=TRUE, notch=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The passenger numbers increase over time with each year which may be indicative of an increasing linear trend. Possible due to an increase in demand for flights and commercialisation of airlines in that time period.&lt;/li&gt;
&lt;li&gt;The boxplot shows more passengers travelling in months 6 to 9 with higher averages and higher variances than the other months, indicating seasonality within an apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US.&lt;/li&gt;
&lt;li&gt;The dataset appears to be a multiplicative time series, since passenger numbers increase, with a pattern of seasonality.&lt;/li&gt;
&lt;li&gt;There do not appear to be any outliers and there are no missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data decomposition&lt;/h3&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The multiplicative model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of passengers at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;decomposeAP &amp;lt;- decompose(AP,&amp;quot;multiplicative&amp;quot;)
autoplot(decomposeAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the “remainder”.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stationarity test&lt;/h3&gt;
&lt;p&gt;A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. I’ll use two methods to test the stationarity.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (ADF)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to test the stationarity of the time series, let’s run the Augmented Dickey-Fuller (ADF) Test. using the adf.test function from the tseries R package.&lt;/p&gt;
&lt;p&gt;First set the hypothesis test:&lt;/p&gt;
&lt;p&gt;The null hypothesis: that the time series is non stationary
The alternative hypothesis: that the time series is stationary&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in adf.test(AP): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  AP
## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a rule of thumb, where the p-value is less than 5%, we reject the null hypothesis. As the p-value is 0.01 which is less than 0.05 we reject the null in favour of the alternative hypothesis that the time series is stationary.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (Autocorrelation)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way to test for stationarity is to use autocorrelation. I’ll use autocorrelation function (acf). This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(acf(AP, plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# review random time series for any missing values
decomposeAP$random &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Jan       Feb       Mar       Apr       May       Jun       Jul
## 1949        NA        NA        NA        NA        NA        NA 0.9516643
## 1950 0.9626030 1.0714668 1.0374474 1.0140476 0.9269030 0.9650406 0.9835566
## 1951 1.0138446 1.0640180 1.0918541 1.0176651 1.0515825 0.9460444 0.9474041
## 1952 1.0258814 1.0939696 1.0134734 0.9695596 0.9632673 1.0003735 0.9468562
## 1953 0.9976684 1.0151646 1.0604644 1.0802327 1.0413329 0.9718056 0.9551933
## 1954 0.9829785 0.9232032 1.0044417 0.9943899 1.0119479 0.9978740 1.0237753
## 1955 1.0154046 0.9888241 0.9775844 1.0015732 0.9878755 1.0039635 1.0385512
## 1956 1.0066157 0.9970250 0.9876248 0.9968224 0.9985644 1.0275560 1.0217685
## 1957 0.9937293 0.9649918 0.9881769 0.9867637 0.9924177 1.0328601 1.0261250
## 1958 0.9954212 0.9522762 0.9469115 0.9383993 0.9715785 1.0261340 1.0483841
## 1959 0.9825176 0.9505736 0.9785278 0.9746440 1.0177637 0.9968613 1.0373136
## 1960 1.0039279 0.9590794 0.8940857 1.0064948 1.0173588 1.0120790        NA
##            Aug       Sep       Oct       Nov       Dec
## 1949 0.9534014 1.0022198 1.0040278 1.0062701 1.0118119
## 1950 0.9733720 1.0225047 0.9721928 0.9389527 1.0067914
## 1951 0.9397599 0.9888637 0.9938809 1.0235337 1.0250824
## 1952 0.9931171 0.9746302 1.0046687 1.0202797 1.0115407
## 1953 0.9894989 0.9934337 1.0192680 1.0009392 0.9915039
## 1954 0.9845184 0.9881036 0.9927613 0.9995143 0.9908692
## 1955 0.9831117 1.0032501 1.0003084 0.9827720 1.0125535
## 1956 1.0004765 1.0008730 0.9835071 0.9932761 0.9894251
## 1957 1.0312668 1.0236147 1.0108432 1.0212995 1.0005263
## 1958 1.0789695 0.9856540 0.9977971 0.9802940 0.9405687
## 1959 1.0531001 0.9974447 1.0013371 1.0134608 0.9999192
## 1960        NA        NA        NA        NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138], plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers Random Component from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acf of the residuals are centered around zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-model-using-an-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit a model using an algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Linear regression Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given there is an upwards trend we’ll look at a linear model first for comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(AP) + geom_smooth(method=&amp;quot;lm&amp;quot;) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This may not be the best model to fit as it doesn’t capture the seasonality and multiplicative effects over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. ARIMA Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arimaAP &amp;lt;- auto.arima(AP)
arimaAP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: AP 
## ARIMA(2,1,1)(0,1,0)[12] 
## 
## Coefficients:
##          ar1     ar2      ma1
##       0.5960  0.2143  -0.9819
## s.e.  0.0888  0.0880   0.0292
## 
## sigma^2 estimated as 132.3:  log likelihood=-504.92
## AIC=1017.85   AICc=1018.17   BIC=1029.35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d), an autoregressive term of second lag (p) and a moving average model of order 1 (q). Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units, in this case months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggtsdiag(arimaAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residual plots appear to be centered around 0 as noise, with no pattern. The arima model is a fairly good fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;forcasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forcasting&lt;/h3&gt;
&lt;p&gt;Plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecastAP &amp;lt;- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Home Loan Default Risk</title>
      <link>/publication/homeloan.default/</link>
      <pubDate>Sat, 19 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/homeloan.default/</guid>
      <description></description>
    </item>
    
    <item>
      <title>mtcars Data Analysis</title>
      <link>/post/mtcars/2019-01-03-r-rmarkdown/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/mtcars/2019-01-03-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preamble:&lt;/h3&gt;
&lt;p&gt;This document focuses on the analysis of the mtcars dataframe.&lt;/p&gt;
&lt;p&gt;Description of dataframe mtcars can be found at the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Research questions:&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;is a vehicle with auto or manual transmission better in terms of miles p/gallons(mpg)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;quantify the (mpg) difference between auto &amp;amp; manual transmission.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Structure of analysis:&lt;/h3&gt;
&lt;p&gt;I will asssess both queries from different perspectives employing a set of methodologies that can be broadly grouped as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Univariate Analysis on target varibale (mpg).&lt;/li&gt;
&lt;li&gt;Bivariate Analysis on target varibale &amp;amp; relevant covariates.&lt;/li&gt;
&lt;li&gt;Multivariate Analysis by estimating a set of regresssion models for the conditional mean of mpg. For model selection, I compare the best fit and forward stepwise selection process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Univariate Analysis&lt;/h3&gt;
&lt;p&gt;Analysing the target variable alone by splitting the observations into two groups, i.e. vehicles with auto or manual transmission. I shall deploy 3 analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute sample means by group ie auto VS manual.&lt;/li&gt;
&lt;li&gt;Validate if the difference of the group means are statistically significant by computing a 95% confidence interval for means’ difference.&lt;/li&gt;
&lt;li&gt;Verify the robustness of this result by executing a permutation test with Monte Carlo trials that shuffle the allocation group &amp;gt; mpg.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get to know the data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)
&amp;#39;data.frame&amp;#39;:   32 obs. of  11 variables:
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
 $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
 $ disp: num  160 160 108 258 360 ...
 $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
 $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
 $ qsec: num  16.5 17 18.6 19.4 17 ...
 $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
 $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We notice that the set is relatively small! We’ll look at the desriptive statistics for each field - (min, 1st Q, Median, Mean, 3rd Q, max)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mtcars)
      mpg             cyl             disp             hp       
 Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  
 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  
 Median :19.20   Median :6.000   Median :196.3   Median :123.0  
 Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  
 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  
 Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  
      drat             wt             qsec             vs        
 Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  
 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  
 Median :3.695   Median :3.325   Median :17.71   Median :0.0000  
 Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  
 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  
 Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  
       am              gear            carb      
 Min.   :0.0000   Min.   :3.000   Min.   :1.000  
 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  
 Median :0.0000   Median :4.000   Median :2.000  
 Mean   :0.4062   Mean   :3.688   Mean   :2.812  
 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  
 Max.   :1.0000   Max.   :5.000   Max.   :8.000  &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-means-by-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample means by group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

# dimensions
dim(mtcars)
[1] 32 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(cars_auto); dim(cars_manu)
[1] 19 11
[1] 13 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample means mpg by group
mean(cars_auto$mpg); mean(cars_manu$mpg)
[1] 17.14737
[1] 24.39231&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(cars_auto$mpg); sd(cars_manu$mpg)
[1] 3.833966
[1] 6.166504&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# % increase in mpg based on the sample mean
(mean(cars_manu$mpg) - mean(cars_auto$mpg))/mean(cars_auto$mpg)
[1] 0.4225103&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Including plots&lt;/h3&gt;
&lt;p&gt;To get a feel for the distribution of some of the data to be analyzed, we plot some histograms, the first against mpg - auto transmission, the second against mpg - manual transission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(mpg ~ am, data = mtcars, col=rgb(0.3,0.2,0.5,0.6), ylab = &amp;quot;mpg&amp;quot;, xlab = &amp;quot;am&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg empirical mean of vehicles with manual transmission is greater than cars with auto transmission, however this also has a higher variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-interval-for-the-difference-of-the-group-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;95% confidence interval for the difference of the group means&lt;/h3&gt;
&lt;p&gt;The analysis on sample means concludes that sample mean of mpg for vehicles with manual trasmission is greater than automatic:&lt;/p&gt;
&lt;p&gt;Now I test if this difference (i.e. in the sample means) is statistically significant (from zero).&lt;/p&gt;
&lt;p&gt;I execute a t.test for unpaired samples: I assume inequality in variances for the two groups for the computation of the pooled variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 95% confidence interval for mean difference ####

# Question: is the sample mean difference significant?
t.test(cars_manu$mpg, cars_auto$mpg, paired = F, var.equal = F)

    Welch Two Sample t-test

data:  cars_manu$mpg and cars_auto$mpg
t = 3.7671, df = 18.332, p-value = 0.001374
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.209684 11.280194
sample estimates:
mean of x mean of y 
 24.39231  17.14737 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% interval does not contain 0&lt;/li&gt;
&lt;li&gt;sample mean difference is significant at 95% (p-value 0.1%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-test-on-groups-association&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Permutation test on groups association&lt;/h3&gt;
&lt;p&gt;I test the robustness of results obtained in the previous step.&lt;/p&gt;
&lt;p&gt;I execute a permutation test by shuffling the allocation mean &amp;gt; groups with 100,000 trials of Montecarlo simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### Permutation test ####
# what if I shuffle the am groups and calculate the mean?

# get target variable and group vectors
y = mtcars$mpg
group = mtcars$am
y; group
 [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2
[15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4
[29] 15.8 19.7 15.0 21.4
 [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1

# baseline group means and difference
baselineMeans = tapply(mtcars$mpg, mtcars$am, mean)
baselineMeansDiff = baselineMeans[2] - baselineMeans[1]

tStat = function(w, g) mean(w[g == 1]) - mean(w[g == 0])
observedDiff = tStat(y, group)

# check if function works - should be 0:
baselineMeansDiff - observedDiff
1 
0 

# execute shuffle:
permutations = sapply(1:100000, function(i) tStat(y, sample(group)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot the analysis:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# shuffle experiment results plots:
par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))
hist(permutations, main = &amp;quot;Distribution of shuffled group mean differences&amp;quot;) # distribution of difference of averages of permuted groups
plot(permutations, type = &amp;quot;b&amp;quot;, main = &amp;quot;Shuffled group mean trials&amp;quot;, xlab = &amp;quot;trial&amp;quot;, ylab = &amp;quot;shuffled group mean differences&amp;quot;, ylim = c(-14, 14))
abline(h = observedDiff, col = &amp;quot;red&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there is not even 1 case where by chance I get a difference greater than the observed!
mean(permutations &amp;gt; observedDiff)
[1] 0.00019&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out of 100,000 trails only 0.002% has breached the observed value for the diffs in the group empirical means.&lt;/li&gt;
&lt;li&gt;concluding that empirical means diffs of groups is robust with regards to random reshuffling and is not likely to be generated by pure chance. &lt;em&gt;is this correct?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bivariate Analysis&lt;/h3&gt;
&lt;p&gt;Analyse the behaviour of target variable (mpg) conditional upon a set of explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

#### Visual inspection of all covariates ####
pairs(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 4 bivariate analysis: hp / wt / drat / disp ####
par(mfrow = c(2, 2), mar = c(2, 3, 2, 3))

# plot1
with(mtcars, plot(hp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs hp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(hp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(hp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model1_auto = lm(mpg ~ hp, data = cars_auto)
model1_manu = lm(mpg ~ hp, data = cars_manu)
abline(model1_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model1_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot2
with(mtcars, plot(wt, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs weight - by transmission type&amp;quot;)) # no data
with(cars_auto, points(wt, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(wt, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
abline(v = 3.2, lty = 2)

# plot 3
with(mtcars, plot(drat, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs drat - by transmission type&amp;quot;)) # no data
with(cars_auto, points(drat, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(drat, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model2_auto = lm(mpg ~ drat, data = cars_auto)
model2_manu = lm(mpg ~ drat, data = cars_manu)
abline(model2_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model2_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot 4
with(mtcars, plot(disp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs disp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(disp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(disp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
labels = with(mtcars, paste(as.character(disp), as.character(mpg), sep = &amp;quot;,&amp;quot;)) # generate point labels
with(mtcars, text(disp, mpg, labels = labels, cex = 0.7, pos = 2))
abline(v = 167.6, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg vs hp: linear negative relation: as horse power of the engine (hp) increases, the mileage (mpg) reduces. Vehicles with manual transmission seems however to be more efficient: the group restricted regression (blue) has a higher intercept. It has to be highlighted however, that the parameters of blue regression might be influenced by two extreme values with high hp - the regression should be re-estimated by removing the two datapoints.&lt;/li&gt;
&lt;li&gt;mpg vs weight: negative relation, the functional form might be non-linear (hyperbolic ?), as weight of the vehicle increases, the mileage decreases. The weight variable seems to provide perfect separation between manual and auto transmission vehilces, i.e. all vehicles that are heavier than 3.2 ton (circa) are auto and vice-versa.&lt;/li&gt;
&lt;li&gt;mpg vs drat: the functional form is not clear: it appears also to be an increase in the variance as the rear axel ratio (drat) increases. To verify this a regression model using all observations has to be estimated and analyse the residuals for verifying if the model is heteroskedastic.&lt;/li&gt;
&lt;li&gt;mpg vs disp: seems to have a negative (hyperbolic ?) relation: as the displacement (disp) of the engine increases, the mileage decreases. Also, in this case it seems that disp accounts for perfect separation in the transmission type: almost all vehilces with disp &amp;gt; 180 are auto.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate analysis&lt;/h3&gt;
&lt;p&gt;Run a set of regression models for estimating the impact of some predictions on mpg.&lt;/p&gt;
&lt;p&gt;For model selection, I employ the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manual selection of regressors: I hand pick regressors for:&lt;/li&gt;
&lt;li&gt;Best fit procedure&lt;/li&gt;
&lt;li&gt;Forward stepwise procedure&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Manual selection&lt;/h3&gt;
&lt;p&gt;Analysis of covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### analyse covariance matrix for regressor selection:
z &amp;lt;- cor(mtcars)
require(lattice)
Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levelplot(z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A model with only transmission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# only am
data = mtcars
data$am = as.factor(data$am)
model2 = lm(mpg ~ am, data = data)

# get results
summary(model2)

Call:
lm(formula = mpg ~ am, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.3923 -3.0923 -0.2974  3.2439  9.5077 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)    
(Intercept)   17.147      1.125  15.247 1.13e-15 ***
am1            7.245      1.764   4.106 0.000285 ***
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 4.902 on 30 degrees of freedom
Multiple R-squared:  0.3598,    Adjusted R-squared:  0.3385 
F-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is 17.15: exactly the same mean of mpg for vehicles with auto transmission.&lt;/li&gt;
&lt;li&gt;the coefficient of am is 7.24: exactly the difference of mpg means for vehicles with manual / auto transmission.&lt;/li&gt;
&lt;li&gt;the sum of intercept and am coefficient gives the mpg unconditional mean for vehicles with manual transmission.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;best-fit-procedure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Best Fit Procedure&lt;/h3&gt;
&lt;p&gt;Run the best fit procedure for identifying the optimal number of regressors that minimises the cp, which is (…)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### model selection using leaps ####
data = mtcars
data$log_mpg = log(data$mpg) # add log of y

#### method 1. best fit ####
regfit.full = regsubsets(log_mpg ~. , data = data, nvmax = 10)
reg.summary = summary(regfit.full)
reg.summary
Subset selection object
Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10)
11 Variables  (and intercept)
     Forced in Forced out
mpg      FALSE      FALSE
cyl      FALSE      FALSE
disp     FALSE      FALSE
hp       FALSE      FALSE
drat     FALSE      FALSE
wt       FALSE      FALSE
qsec     FALSE      FALSE
vs       FALSE      FALSE
am       FALSE      FALSE
gear     FALSE      FALSE
carb     FALSE      FALSE
1 subsets of each size up to 10
Selection Algorithm: exhaustive
          mpg cyl disp hp  drat wt  qsec vs  am  gear carb
1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot the analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how I selected the optimal number of variables?
plot(reg.summary$cp, xlab = &amp;quot;Number of variables&amp;quot;, ylab = &amp;quot;cp&amp;quot;, type = &amp;quot;b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forward-stepwise-procedure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forward Stepwise Procedure&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regfit.fwd = regsubsets(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
summary(regfit.fwd)
Subset selection object
Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
11 Variables  (and intercept)
     Forced in Forced out
mpg      FALSE      FALSE
cyl      FALSE      FALSE
disp     FALSE      FALSE
hp       FALSE      FALSE
drat     FALSE      FALSE
wt       FALSE      FALSE
qsec     FALSE      FALSE
vs       FALSE      FALSE
am       FALSE      FALSE
gear     FALSE      FALSE
carb     FALSE      FALSE
1 subsets of each size up to 10
Selection Algorithm: forward
          mpg cyl disp hp  drat wt  qsec vs  am  gear carb
1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; 
7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot the analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regfit.fwd, scale = &amp;quot;Cp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Appendix&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A model including all regressors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### lm with all variables / no split ####
# prepare data
data = mtcars
data$am = as.factor(data$am)

model1 = lm(mpg ~ ., data = data)

# get results
summary(model1)

Call:
lm(formula = mpg ~ ., data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4506 -1.6044 -0.1196  1.2193  4.6271 

Coefficients:
            Estimate Std. Error t value Pr(&amp;gt;|t|)  
(Intercept) 12.30337   18.71788   0.657   0.5181  
cyl         -0.11144    1.04502  -0.107   0.9161  
disp         0.01334    0.01786   0.747   0.4635  
hp          -0.02148    0.02177  -0.987   0.3350  
drat         0.78711    1.63537   0.481   0.6353  
wt          -3.71530    1.89441  -1.961   0.0633 .
qsec         0.82104    0.73084   1.123   0.2739  
vs           0.31776    2.10451   0.151   0.8814  
am1          2.52023    2.05665   1.225   0.2340  
gear         0.65541    1.49326   0.439   0.6652  
carb        -0.19942    0.82875  -0.241   0.8122  
---
Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1

Residual standard error: 2.65 on 21 degrees of freedom
Multiple R-squared:  0.869, Adjusted R-squared:  0.8066 
F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot the analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot residual analysis
par(mfrow = c(2, 2))
plot(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot hist
par(mfrow = c(1, 1))
hist(model1$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# normality test on residuals
shapiro.test(model1$residuals)

    Shapiro-Wilk normality test

data:  model1$residuals
W = 0.95694, p-value = 0.2261&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Earthquake Prediction</title>
      <link>/publication/earthquake.challenge/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/earthquake.challenge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic: the website builder for Hugo</title>
      <link>/post/getting-started/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/getting-started/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 &lt;em&gt;widgets&lt;/em&gt;, &lt;em&gt;themes&lt;/em&gt;, and &lt;em&gt;language packs&lt;/em&gt; included!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Check out the latest &lt;strong&gt;demo&lt;/strong&gt;&lt;/a&gt; of what you&amp;rsquo;ll get in less than 10 minutes, or &lt;a href=&#34;https://sourcethemes.com/academic/#expo&#34; target=&#34;_blank&#34;&gt;view the &lt;strong&gt;showcase&lt;/strong&gt;&lt;/a&gt; of personal, project, and business sites.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#install&#34;&gt;&lt;strong&gt;Setup Academic&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;View the documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://discuss.gohugo.io/&#34; target=&#34;_blank&#34;&gt;Ask a question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/issues&#34; target=&#34;_blank&#34;&gt;Request a feature or report a bug&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Updating? View the &lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;Update Guide&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/updates/&#34; target=&#34;_blank&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support development of Academic:

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://paypal.me/cushen&#34; target=&#34;_blank&#34;&gt;Donate a coffee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.patreon.com/cushen&#34; target=&#34;_blank&#34;&gt;Become a backer on Patreon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redbubble.com/people/neutreno/works/34387919-academic&#34; target=&#34;_blank&#34;&gt;Decorate your laptop or journal with an Academic sticker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://academic.threadless.com/&#34; target=&#34;_blank&#34;&gt;Wear the T-shirt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png&#34; alt=&#34;Screenshot&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with &lt;a href=&#34;https://sourcethemes.com/academic/docs/page-builder/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;widgets&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;elements&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://sourcethemes.com/academic/docs/jupyter/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jupyter&lt;/strong&gt;&lt;/a&gt;, or &lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable &lt;a href=&#34;https://sourcethemes.com/academic/themes/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;color&lt;/strong&gt; and &lt;strong&gt;font themes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code highlighting and &lt;a href=&#34;https://en.wikibooks.org/wiki/LaTeX/Mathematics&#34; target=&#34;_blank&#34;&gt;LaTeX math&lt;/a&gt; supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - &lt;a href=&#34;https://analytics.google.com&#34; target=&#34;_blank&#34;&gt;Google Analytics&lt;/a&gt;, &lt;a href=&#34;https://disqus.com&#34; target=&#34;_blank&#34;&gt;Disqus commenting&lt;/a&gt;, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 15+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;color-themes&#34;&gt;Color Themes&lt;/h2&gt;

&lt;p&gt;Academic comes with &lt;strong&gt;day (light) and night (dark) mode&lt;/strong&gt; built-in. Click the sun/moon icon in the top right of the &lt;a href=&#34;https://academic-demo.netlify.com/&#34; target=&#34;_blank&#34;&gt;Demo&lt;/a&gt; to see it in action!&lt;/p&gt;

&lt;p&gt;Choose a stunning color and font theme for your site. Themes are fully customizable and include:&lt;/p&gt;









  
  


&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-1950s.png&#34; data-caption=&#34;1950s&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-1950s_huaf5482f8cea0c5a703a328640e3b7509_21614_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-apogee.png&#34; data-caption=&#34;Apogee&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-apogee_hu4b45d99db97150df01464c393bfd17d4_24119_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-coffee-playfair.png&#34; data-caption=&#34;Coffee theme with Playfair font&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-coffee-playfair_hu446a8f670cc5622adcc77b97ba95f6c5_22462_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-cupcake.png&#34; data-caption=&#34;Cupcake&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-cupcake_hueba8cfa8cfbc7543924fcbf387a99e92_23986_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-dark.png&#34; data-caption=&#34;Dark&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-dark_hu1e8601ecc47f58eada7743fdcd709d3d_21456_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-default.png&#34; data-caption=&#34;Default&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-default_huba6228b7bdf30e2f03f12ea91b2cba0d_21751_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-forest.png&#34; data-caption=&#34;Forest&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-forest_hu4f093a1c683134431456584193ea41ee_21797_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;./post/getting-started/gallery/theme-ocean.png&#34; data-caption=&#34;Ocean&#34;&gt;
  &lt;img src=&#34;./post/getting-started/gallery/theme-ocean_hu14831ccafc2219f30a7a096fa7617e01_21760_0x190_resize_lanczos_2.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;

&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-admin&#34; target=&#34;_blank&#34;&gt;Academic Admin&lt;/a&gt;:&lt;/strong&gt; An admin tool to import publications from BibTeX or import assets for an offline site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/sourcethemes/academic-scripts&#34; target=&#34;_blank&#34;&gt;Academic Scripts&lt;/a&gt;:&lt;/strong&gt; Scripts to help migrate content to new versions of Academic&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;You can choose from one of the following four methods to install:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-web-browser&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;one-click install using your web browser (recommended)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-git&#34; target=&#34;_blank&#34;&gt;install on your computer using &lt;strong&gt;Git&lt;/strong&gt; with the Command Prompt/Terminal app&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-zip&#34; target=&#34;_blank&#34;&gt;install on your computer by downloading the &lt;strong&gt;ZIP files&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/install/#install-with-rstudio&#34; target=&#34;_blank&#34;&gt;install on your computer with &lt;strong&gt;RStudio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then &lt;a href=&#34;https://sourcethemes.com/academic/docs/get-started/&#34; target=&#34;_blank&#34;&gt;personalize and deploy your new site&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;updating&#34;&gt;Updating&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/update/&#34; target=&#34;_blank&#34;&gt;View the Update Guide&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Feel free to &lt;em&gt;star&lt;/em&gt; the project on &lt;a href=&#34;https://github.com/gcushen/hugo-academic/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; to help keep track of &lt;a href=&#34;https://sourcethemes.com/academic/updates&#34; target=&#34;_blank&#34;&gt;updates&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;

&lt;p&gt;Copyright 2016-present &lt;a href=&#34;https://georgecushen.com&#34; target=&#34;_blank&#34;&gt;George Cushen&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Released under the &lt;a href=&#34;https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md&#34; target=&#34;_blank&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/example/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
   One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;
&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
