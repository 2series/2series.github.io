[{"authors":["admin"],"categories":null,"content":" Hey, I\u0026rsquo;m Rihad! Focused on gaining interpretable insights from data. Experienced in working with large datasets and using advanced data analysis to answer complex questions with accuracy, applying sophisticated tools and techniques. Proficient in mastering new knowledge and techniques quickly. Able to understand and articulate what questions can and can’t be answered given certain data\nExperienced in using statistical techniques and modeling with real-world, messy data. Able to apply sophisticated mathematics to understanding data. Proficient in the modern machine learning toolkit, including supervised and unsupervised learning techniques, and practically how to build predictive models\nArticulate, insightful, and able to communicate technical procedures and results to expert and non-expert collaborators. Capable of turning a complex analysis into a compelling story so that decision-makers can move forward with an appropriate strategy. Flexible, creative, resourceful, and effective in both independent and collaborative environments\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Hey, I\u0026rsquo;m Rihad! Focused on gaining interpretable insights from data. Experienced in working with large datasets and using advanced data analysis to answer complex questions with accuracy, applying sophisticated tools and techniques. Proficient in mastering new knowledge and techniques quickly. Able to understand and articulate what questions can and can’t be answered given certain data\nExperienced in using statistical techniques and modeling with real-world, messy data. Able to apply sophisticated mathematics to understanding data.","tags":null,"title":"Rihad Variawa","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Look out of your window today and, while you might not realise it, the world is in the midst of a revolution.\nThings might appear totally normal. Humans are still commuting to and from work, shops remain open for business, and communication lines are functioning as they should. But we are actually living through the early stages of the 4IR (Fourth Industrial Revolution) which is sometimes also known as Industry 4.0.\nCoined in 2016 by the founder and executive chairman of the World Economic Forum Klaus Schwab, the term refers to new technologies becoming intertwined with our day-to-day lives and how they will connect us, our bodies and our buildings like never before. While it is the sci-fi nature of Ai and robotics that grabs the headlines, the key to their success is the data that they rely on to work.\nAll 3 previous industrial revolutions have been characterised by massive changes to the way the world works. Whether it was the Agrarian Age changing the way we farm and eat, the Industrial Age transforming manufacturing, or the Information Age spawning rapid advances in computing and digital systems, each 1 has led to an increase in how much data we generate as a species.\nNow, as we enter the Analytics Age, we finally have the tools to make sense of it all and potentially solve any problem the world has ever faced. But there are reasons to be cautious too.\nNot every change brought about by the previous revolutions has been a benefit to all, with the new developments initially limited to those with the necessary money to invest, leading to a polarisation of wealth and power.\nTech has moved so quickly in recent years that a digital divide has opened up, and with unprecedented automation in particular set to alter the landscape like never before, there are similar fears that the 4IR will actually increase inequality in a world already plagued by it.\nOf course, it doesn’t need to be that way. These new technologies have the potential to kickstart economies and improve lives worldwide – so how do we stop people and businesses from getting left behind?\nData now informs all kinds of areas of the modern world, with the good use of it leading to better decision-making and more profitable businesses. Data literacy should therefore be treated as a crucial skill for pretty much everyone. That does not mean everyone needs to become a qualified Data Scientist. But for companies to successfully implement digital transformation initiatives, they must first focus on building a culture of data literacy within their company. Only by empowering data workers at all levels of the company, regardless of technical acumen, to become more data literate as well as improve their analytic knowledge, will organisations succeed.\nTraining employees to use analytics tools can help companies to capitalise on the information that is at their fingertips. Forums such as the Malastare Community are full of data science and analytics experts, keen to share new ways of working with data. After all, new technologies offer many exciting possibilities, but there is no point in having all this extra data if nobody knows what to do with it. According to a 2019 study by NewVantage Partners, 92.5% of respondents blamed people or processes for an inability to adopt a data-driven approach to their business.\nWe should, however, pay particular attention to those traditionally left behind by technological progress. The 4IR is defined by its focus on science and tech – a world dominated by men, and particularly white men. This diversity imbalance puts women at an immediate disadvantage as the world and workplaces are changed by emerging technologies.\nAlthough gender, ethnic and cultural diversity in technology and analytics is no longer a rarity. For organisations to benefit from this recent increase in diversity, a collaborative and supportive infrastructure must be created to enhance the industry, culture, and workspaces with the missing half of the human experience.\nThe analytics space is particularly attractive for women – almost half of analytics professionals are women. With a diverse group of analysts around the table working through insights to solve for key business insights, the approach is richer when women and men work together to deliver answers.\nThe most successful firms over the next decade or so will therefore be the ones that understand the need to transform their workforces in line with their data management practices to ensure nobody gets left behind.\nMalastare AI embodies this approach with its for Good program and Women of Analytics initiative, which use events, discussions and community activities to share knowledge and encourage diversity at every level. These help to ensure that projects are completed collectively rather than in cultural silos, making any challenges easier to overcome.\nConclusion The potential of Industry 4.0 is huge - but revolutions don’t take place in a vacuum. The key component to success—data literacy—comes from within, and companies will only realise that full potential if they foster data-driven cultures fuelled by collaboration and diversity, presenting an opportunity for everyone to accelerate their careers by embracing analytic roles.\nYou Rock! ","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585180800,"objectID":"065a6135c962cdb27644574d23a94f17","permalink":"/post/dataliteracy/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/post/dataliteracy/","section":"post","summary":"Look out of your window today and, while you might not realise it, the world is in the midst of a revolution.\nThings might appear totally normal. Humans are still commuting to and from work, shops remain open for business, and communication lines are functioning as they should. But we are actually living through the early stages of the 4IR (Fourth Industrial Revolution) which is sometimes also known as Industry 4.","tags":null,"title":"Data Literacy. THE SKILL ALL SHOULD HAVE","type":"post"},{"authors":null,"categories":null,"content":" Bitcoin-mania peaked with the price of the digital currency back in late 2017 but behind the scenes, the mystical and confusing blockchain industry has been quietly gathering momentum. Unlike bitcoin and other digital currencies, blockchain has plenty of real-life usages to make all our lives more productive.\nFew people understand how blockchain works which is a shame as it represents a fast-emerging technology. But no one should be faulted for not understanding blockchain. Here is a simple explanation of blockchain.\nWhat Is Blockchain\nBefore understanding what blockchain tech is it would be wise to discuss real-world financial transactions that we\u0026rsquo;re all used to. Suppose you are selling your home to a young couple and everyone agreed on a price and date. Congratulations!\nThe transaction cannot be completed with an intermediary, say a notary. The notary is a trusted individual who will approve the transaction and arrange all the regulatory and legal paperwork that needs to go on behind the scenes. Essentially, all the boring stuff few care to understand.\nNow, mistakes can happen during the process – as rare as it is. According to the US National Notary Association, even the smallest of mistakes can bring “dire consequences.”\nInstead, a whole new process can exist in the digital world which replaces the outdated processes, called a blockchain. Blockchain in simple terms refers to a digital ledger that contains transaction details. The records of transactions are kept in “containers” referred to as blocks. And these blocks are joined together for all to see and secured using cryptography.\nThe main characteristic of blockchain is that data can only be added. It can never be altered or changed for any reason. The reason this works is everyone has a copy of the ledger so it would be very easy to identify one piece of information that is not consistent with the rest of public records.\nOther key characteristics and blockchain basics include:\n Every user obtains their own unique public key that serves as an identification tool  Near real-time transactions and settlements. No more waiting for a notary to \u0026laquo;call you back,\u0026raquo; only never to do so  The ledger keeps a record of every transaction that occurs  Each transaction is verified and confirmed which eliminates any possible manipulation OR fraud  Incentives are offered to anyone who validates the blocks which lower the likelihood of external parties modifying prior transactions\n Real-Life Case\nIn our real estate example, it would be understandable for a newcomer to blockchain and cryptocurrency to laugh it off. But it is important to understand the world is moving in this direction whether we like it or not.\nAccording to “Big Four” accounting and consulting firm Deloitte, there are several reasons why real estate transactions should be backed by blockchain technology:\nYes, This Actually Happens\nA California-based blockchain start-up company called Propy announced in early 2018 it entered into an agreement with the city of South Burlington, Vermont to use its blockchain technology to close real estate deals.\nA homeowner in the city wanted to purchase land that was for sale directly behind their home. But the owner was located in North Carolina which adds another layer of complexity to the transaction.\nSo, here is how this real-case example of the blockchain technology was deployed: the owner of the land first put their property for sale on Propy’s online platform. Then, the prospective buyer’s lawyer reached out to the owner and was able to leverage Propy’s platform to perform a title search and due diligence and then complete the transaction using a cryptocurrency.\nThe deal was closed and confirmed into the blockchain as a smart contract. While this type of transaction is the first of its kind in the state, it required physical signatures and confirmation for government records. But it appears it is only a matter of time, be it years or decades, before the process is 100% digitised and signatures are a thing of the past.\nSo Can Anyone Build A Blockchain\nUnderstanding the theory behind blockchain is half the battle. The other half is understanding how a blockchain works. Perhaps the more pressing question fresh on everyone’s mind is the most obvious one: if there are in theory infinite uses for blockchain, can anyone create one?\nThe answer is a simple YES! In fact, skilled programmers can build a blockchain from scratch in under 15 minutes. A simple search on YouTube shows dozens of videos on how anyone with advanced knowledge of programming can become a blockchain developer.\nSome sort of currency needs to be involved for a transaction to finalise, the most common of which is a digital or cryptocurrency. Similar to creating a blockchain, anyone can create their own cryptocurrency. It should be noted that advanced knowledge of programming is required.\nFor those who do not aspire to become a blockchain developer, there are more common ways to benefit from the blockchain technologies. One of them is crypto trading\nThere are a wide range of crypto exchanges out there with different performance and level of credibility. Check out numerous available options and focus on an award-winning and regulated crypto exchange, where you can buy, sell and hold your crypto assets safely.\nA crypto exchange operates similar to platforms that traders use to trade stocks. The exchange connects you directly to the crypto marketplace, where you can exchange traditional fiat currencies for Bitcoin or altcoins. Somehow, crypto exchanges resemble FX markets, where you can exchange various crypto-to-fiat pairs like BTC to USD, BTC to EUR, or ETH to EUR.\nConclusion\nAccording to research firm Gartner, blockchain technology will create more than USD176 billion worth of business value within five years and by the end of the decade, this figure will soar to UDS3.1 trillion.\nFor the time being, the blockchain universe is still stuck in a “hype phase,” according to Gartner. This means exactly how it sounds: blockchain’s real-life usage is extremely limited. But those who follow the industry are certainly excited and as the data suggests, cryptocurrency blockchain will be the standard for the future of commerce.\n You Rock! ","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"606c930500795aa566c82ead3fa9aafe","permalink":"/post/blockchain/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/post/blockchain/","section":"post","summary":"Bitcoin-mania peaked with the price of the digital currency back in late 2017 but behind the scenes, the mystical and confusing blockchain industry has been quietly gathering momentum. Unlike bitcoin and other digital currencies, blockchain has plenty of real-life usages to make all our lives more productive.\nFew people understand how blockchain works which is a shame as it represents a fast-emerging technology. But no one should be faulted for not understanding blockchain.","tags":null,"title":"Blockchain In Action","type":"post"},{"authors":null,"categories":null,"content":" The key to success is consistently making good decisions, and the key to making good decisions is having good information. This belief is the main impetus behind the explosive interest in Big Data. We all know intuitively that access to more data presents the potential to obtain better data and therefore better decisions, yet more data in-and-of itself does not necessarily result in better decisions. We must also sift through the data and discover the good information. Doing so effectively is especially important in capital intensive industries.\nThe oil and gas industry is an asset-intensive business with capital assets ranging from drilling rigs, offshore platforms and wells to pipelines, LNG terminals, and refineries. These assets are costly to design, build, operate, and maintain. Analysis of the financial statements of the five super-majors (BP, ConocoPhillips, ExxonMobil, Shell, Total) shows that plant, property and equipment on average accounts for 51% of total assets. Effectively managing these assets requires oil and gas industry to leverage advanced machine learning and analytics on extreme large volumes of data, in batch and real-time. Apache Spark is ideal for handling this type of workload and Databricks is the ideal platform for building Apache Spark solutions.\nIn this blog we will solve a typical problem in the oil and gas industry – asset optimization. We will demonstrate a solution with three components:\n AWS Kinesis to stream the real time data; AWS RDS to store our historical data; Malastare AI to process the data from RDS and Kinesis to determine the optimal asset levels.  Background To Asset Optimization Asset refers to tangible goods used by the business to generate revenue – raw material, equipment, etc. Business operations consume assets (i.e., wearing down a piece of equipment), and must replenish them to continue revenue generation. The estimation of timing and quantity of the replenishment is the heart of asset optimization because errors are costly: revenue stops flowing if the business runs out of raw materials, while excess stockpiles incur holding costs. Ideally, asset optimization accurately determines the correct asset levels based on analytics of near real-time consumption data. The goal is to precisely estimate how much stock will be used in the time it takes for an order to arrive with pinpoint accuracy.\nAsset Optimization Example In the capital intensive oil and gas industry, every single hour of inefficient asset operation or unscheduled downtime cost millions. In the current Internet-of-Things (IoT) Big Data era, asset optimization focuses on continuously monitoring key operating characteristics of assets and applying advanced machine learning to maximize asset performance and minimize unplanned outages. That is where Big Data and advance analytics come in. The remainder of the blog we will look at a power generation plant example, where we monitor asset meters in real-time and model key measurements to determine whether assets are functioning optimally.\nWe model this by fitting a distribution to the limited lead time data we have and then sampling from that distribution. Fitting the distribution is the slowest part as it must be done numerically using Markov chain Monte Carlo (MCMC), for our asset this requires a loop of 100,000 iterations which cannot be done in parallel. This whole process must be done for each material in the data set, depending on the plant this can be 3,000+. Each material can be analyzed independently and in parallel.\nStreaming Sensor Reading With AWS Kinesis Step 1: Import the Kinesis Libaries This example assumes a Spark 2.0.1 (Scala 2.11). In this particular notebook, make sure you have attached Maven dependencies spark-streaming-kinesis for same version of Spark as your cluster and corresponding kinesis-client library.\nStep 2: Configure Kinesis Stream // === Configuration to control the flow of the application === val stopActiveContext = true // \u0026quot;true\u0026quot; = stop if any existing StreamingContext is running; // \u0026quot;false\u0026quot; = don't stop, and let it run undisturbed, but your latest code may not be used // === Configurations for Spark Streaming === val batchIntervalSeconds = 10 val eventsPerSecond = 1000 // For the dummy source // Verify that the attached Spark cluster is 1.4.0+ require(sc.version.replace(\u0026quot;.\u0026quot;, \u0026quot;\u0026quot;).toInt \u0026gt;= 140)  Step 3: Defining the function that consumes the Stream This function consumes a dummy stream that we have created for the sake of demonstrating Kinesis. The data that we use latter is staged as JSON files.\nimport scala.util.Random import org.apache.spark.streaming.receiver._ class DummySource(ratePerSec: Int) extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) { def onStart() { // Start the thread that receives data over a connection new Thread(\u0026quot;Dummy Source\u0026quot;) { override def run() { receive() } }.start() } def onStop() { // There is nothing much to do as the thread calling receive() // is designed to stop by itself isStopped() returns false } /** Create a socket connection and receive data until receiver is stopped */ private def receive() { while(!isStopped()) { store(\u0026quot;I am a dummy source \u0026quot; + Random.nextInt(10)) Thread.sleep((1000.toDouble / ratePerSec).toInt) } } }  Storing Historical Data In AWS RDS Let’s connect to a relational database to look at our master data and choose the Power Plant we want to create our first model for. We will be using Redshift as our database but the steps are essentially the same for connecting to any database. For our simulation, Redshift is where master data regarding the assets is stored. In the real world, this data could be stored in any relational database.\nStep 1: Create a DataFrame from an entire Redshift table val mstr_plant_from_redshift = sqlContext.read .format(\u0026quot;com.malastareAI.spark.redshift\u0026quot;) .option(\u0026quot;url\u0026quot;, jdbcUrl) // JDBC URL that we configured earlier .option(\u0026quot;tempdir\u0026quot;, tempDir) // temporary bucket that we created earlier .option(\u0026quot;dbtable\u0026quot;, \u0026quot;mstr_plant_rsi\u0026quot;) // name of the table in Redshift .load()  Step 2: Create a Temporary View mstr_plant_from_redshift.createOrReplaceTempView(\u0026quot;tmp_mstr_plant1\u0026quot;)  Step 3: Select and View list of Power Plants sql select * from tmp_mstr_plant1  We can use ANSI SQL to explore our master data and decide what asset we would like to use for our initial analysis.\nMonitoring And Anomaly Detection Step 1: Let\u0026rsquo;s load our data Source measurement data from staged JSON data. In the real world, this would be sourced directly from Kinesis or another streaming technology as I showed with the dummy example above.\nLoad staged data from JSON files:\nmounts_list = [ {'bucket':'databricks-corp-training/structured_streaming/devices', 'mount_folder':'/mnt/sdevices'} ] for mount_point in mounts_list: bucket = mount_point['bucket'] mount_folder = mount_point['mount_folder'] try: dbutils.fs.ls(mount_folder) dbutils.fs.unmount(mount_folder) except: pass finally: #If MOUNT_FOLDER does not exist dbutils.fs.mount(\u0026quot;s3a://\u0026quot;+ ACCESSY_KEY_ID + \u0026quot;:\u0026quot; + SECRET_ACCESS_KEY + \u0026quot;@\u0026quot; + bucket,mount_folder)  Define a schema for the JSON Device data so that Spark doesn’t have to infer it:\nimport org.apache.spark.sql.types._ //fetch the JSON device information uploaded into the Filestore val jsonFile = \u0026quot;dbfs:/mnt/sdevices/\u0026quot; val jsonSchema = new StructType() .add(\u0026quot;battery_level\u0026quot;, LongType) .add(\u0026quot;c02_level\u0026quot;, LongType) .add(\u0026quot;cca3\u0026quot;,StringType) .add(\u0026quot;cn\u0026quot;, StringType) .add(\u0026quot;device_id\u0026quot;, LongType) .add(\u0026quot;device_type\u0026quot;, StringType) .add(\u0026quot;signal\u0026quot;, LongType) .add(\u0026quot;ip\u0026quot;, StringType) .add(\u0026quot;temp\u0026quot;, LongType) .add(\u0026quot;timestamp\u0026quot;, TimestampType)  Read the JSON files from the mounted directory using the specified schema. Providing the schema avoids Spark to infer Schema, hence making the read operation faster:\nval devicesDF = spark .read .schema(jsonSchema) .json(jsonFile)  Step 2: Explore our data disolay(devicesDF)  Step 3: Visualize our data // import some SQL aggregate and windowing function import org.apache.spark.sql.functions._ val staticCountsDF = devicesDF .select(\u0026quot;device_type\u0026quot;, \u0026quot;battery_level\u0026quot;) .where (\u0026quot;signal \u0026lt;= 15\u0026quot;) .groupBy($\u0026quot;device_type\u0026quot;, $\u0026quot;battery_level\u0026quot;) .count() // Let's register the DataFrame as table 'static_device_counts' staticCountsDF.createOrReplaceTempView(\u0026quot;static_device_counts\u0026quot;) display(staticCountsDF)  Step 4: Stream Processing Read the stream\nval streamingSignalsCountsDF = streamingDevicesDF .select(\u0026quot;device_type\u0026quot;, \u0026quot;battery_level\u0026quot;) .where (\u0026quot;signal \u0026lt;= 15\u0026quot;) .groupBy($\u0026quot;device_type\u0026quot;, $\u0026quot;battery_level\u0026quot;) .count() // Is this DF actually a streaming DF? streamingSignalsCountsDF.isStreaming  Step 5: Monitor the Stream in real time display(streamingSignalsCountsDF)  Step 6: Model the data and optimize the asset We have staged some sensor data as a csv. In the real world, you would read this off the stream as I have shown above. Let\u0026rsquo;s create a temporary table we will use in our analysis.\nsqlContext.read.format(\u0026quot;csv\u0026quot;) .option(\u0026quot;header\u0026quot;, \u0026quot;true\u0026quot;) .option(\u0026quot;delimiter\u0026quot;, \u0026quot;\\t\u0026quot;) .option(\u0026quot;inferSchema\u0026quot;, \u0026quot;true\u0026quot;) .load(\u0026quot;dbfs:/databricks-datasets/power-plant/data/\u0026quot;) .createOrReplaceTempView(\u0026quot;power_plant_sf\u0026quot;)  The next step is to prepare the data. Since all of this data is numeric and consistent this is a simple task for us today. We will need to convert the predictor features from columns to Feature Vectors using the org.apache.spark.ml.feature.VectorAssembler. The VectorAssembler will be the first step in building our ML pipeline.\nimport org.apache.spark.ml.feature.VectorAssembler val dataset = sqlContext.table(\u0026quot;power_plant_sf\u0026quot;) val vectorizer = new VectorAssembler() .setInputCols(Array(\u0026quot;AT\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;AP\u0026quot;, \u0026quot;RH\u0026quot;)) .setOutputCol(\u0026quot;features\u0026quot;)  The linear correlation is not as strong between Exhaust Vacuum Speed and Power Output but there is some resemblance of a pattern. Now let’s model our data to predict what the power output will be given a set of sensor readings.\n// First let's hold out 20% of our data for testing and leave 80% for training var Array(split20, split80) = dataset.randomSplit(Array(0.20, 0.80), 1800009193L) // Let's cache these datasets for performance val testSet = split20.cache() testSet.count() val trainingSet = split80.cache() trainingSet.count() // ***** LINEAR REGRESSION MODEL **** import org.apache.spark.ml.regression.LinearRegression import org.apache.spark.ml.regression.LinearRegressionModel import org.apache.spark.ml.Pipeline // Let's initialize our linear regression learner val lr = new LinearRegression() // Now we set the parameters for the method lr.setPredictionCol(\u0026quot;Predicted_PE\u0026quot;) .setLabelCol(\u0026quot;PE\u0026quot;) .setMaxIter(100) .setRegParam(0.1) // We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar. val lrPipeline = new Pipeline() lrPipeline.setStages(Array(vectorizer, lr)) // Let's first train on the entire dataset to see what we get val lrModel = lrPipeline.fit(trainingSet) val predictionsAndLabels = lrModel.transform(testSet) display(predictionsAndLabels.select(\u0026quot;AT\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;AP\u0026quot;, \u0026quot;RH\u0026quot;, \u0026quot;PE\u0026quot;, \u0026quot;Predicted_PE\u0026quot;))  Now that we have real predictions we can use an evaluation metric such as RMSE (Root Mean Squared Error) to validate our regression model. The lower the RMSE, the better our model.\n//Now let's compute some evaluation metrics against our test dataset import org.apache.spark.mllib.evaluation.RegressionMetrics val metrics = new RegressionMetrics(predictionsAndLabels.select(\u0026quot;Predicted_PE\u0026quot;, \u0026quot;PE\u0026quot;).rdd.map(r =\u0026gt; (r(0).asInstanceOf[Double], r(1).asInstanceOf[Double]))) val rmse = metrics.rootMeanSquaredError val explainedVariance = metrics.explainedVariance val r2 = metrics.r2 // First we calculate the residual error and divide it by the RMSE predictionsAndLabels.selectExpr(\u0026quot;PE\u0026quot;, \u0026quot;Predicted_PE\u0026quot;, \u0026quot;PE - Predicted_PE Residual_Error\u0026quot;, s\u0026quot;\u0026quot;\u0026quot; (PE - Predicted_PE) / $rmse Within_RSME\u0026quot;\u0026quot;\u0026quot;).createOrReplaceTempView(\u0026quot;Power_Plant_RMSE_Evaluation\u0026quot;)  Now we can display the RMSE as a Histogram. Clearly this shows that the RMSE is centered around 0 with the vast majority of the error within 2 RMSEs.\nSELECT Within_RSME from Power_Plant_RMSE_Evaluation  As you can see the Predictions are very close to the real data points. Now we can predict the optimal operating parameters for this plant and apply this model to other plants in real-time.\nThis just one of many examples of how Malastare AI can seamlessly work with other AWS components to deliver advanced solutions. ","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"15eb4559dd9e038372306edd150fca66","permalink":"/post/oilandgas/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/post/oilandgas/","section":"post","summary":"The key to success is consistently making good decisions, and the key to making good decisions is having good information. This belief is the main impetus behind the explosive interest in Big Data. We all know intuitively that access to more data presents the potential to obtain better data and therefore better decisions, yet more data in-and-of itself does not necessarily result in better decisions. We must also sift through the data and discover the good information.","tags":null,"title":"Oil and Gas Asset Optimization","type":"post"},{"authors":null,"categories":null,"content":" IoT stands for Internet of Things, an IoT is nothing but it is a system of interrelated computing devices, digital machines, and objects, humans or animals that all provided with particular identifiers and it is able to transfer data over the web without demanding human-to-human or human-to-computer communication. It indicates to the ever-growing structure of physical phenomenon that features an IP location for the web connectivity.\nWhat is IoT Platform? How the IoT platform is useful for my Business and why it needed? Actually, IoT platform is a collection of components that enables deployment of apps that monitor manage and control all the connected devices to the server, The suite also allows the connected device to transmit and collect data from one another.\nAn IoT platform is a multiplied technology that empowers straightforward provisioning, management, and automation of linked devices within the IoT universe.\nAdvanced IoT Platforms We have few fundamentals that differentiate IoT platforms among each other, such as scalability, customizability, combination with 3rd party software, deployment options, and the input conservancy level.\nScalable – The advanced IoT platforms assure elastic scalability across any number of endpoints that the client may need.\nCustomizable – This decides the factor of the speed of delivery. It nearly relates to the flexibility of integration APIs, API is more enough to fly small scale undemanding IoT solutions\nSecure – These are the basics of how to avoid potentially negotiable breaches in your IoT solution.such as data security involves encryption, comprehensive identity management, and flexible deployment. End-to-end data flow encryption, including data at rest, device authentication, user access rights management, and private cloud infrastructure for sensitive data\nWhy IOT is important?\nIOT is so important nowadays because it finds solutions for every problem. The further Industrial Revolution is going to change our lives in ways never thought before, Fast changes in IOT technology makes it a demanding task for the most Professional experts to anticipate the future of standardization in the field. For humanity, which is moderately clutter by nature, the IoT is a phenomenal advancement.\nStrategic Trends For IOT Platforms\nThe future generation of business ecosystems will be increasing more digital, intelligent, and connected.\nDigital twins: Digital twins contribute a comprehensive digital representation of real-world devices and systems, This provides thus improving their state monitoring and enabling faster output to internal and events.\nThe operations of digital twins very extensively asset from inventory and predictive maintenance to event simulation and usage analytics. In the future, they were expected to grow into a keystone of every efficient IoT ecosystem.\nWhat is Artificial Intelligence in IOT?\nArtificial Intelligence For Cloud-based IoT, The Internet of Things is a language that has been introduced in the latest years to determine objects that are capable to connect and transmit data via the Internet. IoT ecosystems generally require developers to have a greater degree of control all over the system, its source code, integration interfaces, deployment options, data schemas, connectivity and security mechanisms, etc.\nAlready we can talk to virtual assistants like Siri or Alexa to search for a movie or order new stuff with delivery at door. Why can’t we do the same thing in other things?\nThe best example of AI and IoT is successfully working together is self-driving cars by Tesla Motors. Cars act as “things” and use the capability of Artificial Intelligence to predict the behavior of cars and pedestrians in different circumstances.\nHow is Artificial Intelligence Helping IOT Grow\nThere is a lot of interesting thing around artificial intelligence Experts told that AI is expected to change our lives in extraordinary ways.\nThere are numbers of thing written about artificial intelligence. AI is expected to execute a number of intelligent function like speech recognition, decision-making, language understanding, etc.\n5 Key Wireless Technologies for IoT :\nWiFi\nBluetooth\nZ-Wave\nZigbee\nLoRaWAN\nArtificial Intelligence for Cloud-based Internet of Things (IoT) The cloud-based IoT is used to relate a wide range of things such as vehicles, mobile devices, sensors, industrial types of equipment and manufacturing machines to promote different smart systems it consists of smart city and smart home, smart grid, smart industry, smart vehicles, smart health, and smart environmental monitoring.\nIn the IoT, cloud computing environment has built the job of handling the few amounts of data produced by coupling devices easily and provides the IoT devices with resources on-demand.\nWHY ARE AI AND IOT PARTNERS FOR GROWTH The recent survey of top IT executives suggested not only are IoT and AI the most popular technologies currently in use. The top list of future investment for businesses searching for increased capability and competitive advantage.\nBut why are IoT and AI so far in front of other popular technologies such as Edge Computing or Blockchain?\nThe logic is very simple, Combining IOT with fast-growing IOT technologies can create all ‘smart machines’ that replicate brilliant behavior to make well-informed decisions with little or no human interference. A small miracle in the IOT and AI is rapidly developing in the organization\nExamples of AI in IOT?\nLet’s have a look at business and industries that have earlier managed to cut charges. Open up create a new business idea using AI in IOT. These industries will make you consider achieving AI and IoT in your business.\nIndustrial Internet of Things (IIoT) Healthcare Smart Home\nYou Rock! ","date":1581897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581897600,"objectID":"275c16bde65d61988cac103258f72a12","permalink":"/post/iot/","publishdate":"2020-02-17T00:00:00Z","relpermalink":"/post/iot/","section":"post","summary":"IoT stands for Internet of Things, an IoT is nothing but it is a system of interrelated computing devices, digital machines, and objects, humans or animals that all provided with particular identifiers and it is able to transfer data over the web without demanding human-to-human or human-to-computer communication. It indicates to the ever-growing structure of physical phenomenon that features an IP location for the web connectivity.\nWhat is IoT Platform?","tags":null,"title":"AI and IoT","type":"post"},{"authors":null,"categories":null,"content":" ETL with Python ETL is the process of fetching data from one or many systems and loading it into a target data warehouse after doing some intermediate transformations. There are various ETL tools that can carry out this process.\nSome tools offer a complete end-to-end ETL implementation out-the-box and some tools aid you to create a custom ETL process from scratch while there are a few options that fall somewhere in between. In this post, we\u0026rsquo;ll see some commonly used Python ETL tools and understand in which situations they may be a good fit for your project.\nBefore going through the list of Python ETL tools, let’s first understand some essential features that any ETL tool should have.\nFeatures of ETL Tools ETL stands for Extract, Transform, and Load and so any ETL tool should at least have the following features:\nExtract  This is the process of extracting data from various sources. A good ETL tool supports various types of data sources. This should include most databases (both NoSQL and SQL-based) and file formats such as: csv, xls, XML, and json.\n Transform  The extracted data is usually kept in a staging area where raw data is cleansed and transformed into a meaningful form for storing it in a data warehouse. A standard ETL tool supports all the basic data transformation features like row operations, joins, sorting, aggregations, etc.\n Load  In the load process, the transformed data is loaded into the target warehouse database. The standard ETL tools support connectors for various databases such as: Snowflake, MSSQL, and Oracle.\n Other Add-On Features  Apart from basic ETL functionality, some tools support additional features like dashboards for visualizing and tracking various ETL pipelines. In fact, besides ETL, some tools also provide the ability to carry out parallel or distributed processing, and in some cases even basic analytics, that can be good add-ons depending on ones project needs.\n Python ETL Tools Python as a programming language is relatively easy to learn and use. Python has an impressively active open-source community on GitHub that is churning out new Python libraries and enhancement frequently. Due to this active community and Python’s low difficulty/functionality ratio, Python now sports an impressive presence in many diverse fields such as:\n Gaming developments Web developments Application developments NLP Computer vision  just to name a few.\nIn recent times, Python has become a popular programming language choice for data processing, data analytics, and data science (especially with the powerful Pandas library). So it should not come as a surprise that there are plenty of Python ETL tools out there to choose from.\nLet’s take a look at the most common ones.\nPetl  Petl (stands for Python ETL), a basic tool that offers the standard ETL functionality of importing data from different sources (csv, XML, json, text, xls) into your database. It is trivial in terms of features and does not offer data analytics capabilities like some other tools in our list. However, it does support all the standard transformations like row operation, sorting, joining, and aggregation.  Petl isn’t bad for a simple tool, but it can suffer from performance issues; especially compared to some of the other options out there. So, if you just need to build a simple ETL pipeline and performance is not a big factor, then this lightweight tool should do the job. But for anything more complex or if you expect the project to grow in scope, you may want to keep looking.\n Pandas  Pandas is one of the most popular Python libraries nowadays and is a personal favorite of mine. I’ve used it to process hydrology data, astrophysics data, and drone data. Its rise in popularity is largely due to its use in data science, which is a fast-growing field in itself, and is how I first encountered it.  Pandas use dataframes as the data structure to hold the data in memory (similar to how data is handled in the R programming language) Apart from regular ETL functionalities, Pandas supports loads of data analytics and visualization features.  Pandas is relatively easy to use and has many rich features, which is why it is a commonly used tool for simple ETL and EDA by data scientists. If you are already using Pandas it may be a good solution for deploying a proof-of-concept ETL pipeline.\n Mara  Mara is a Python ETL tool that is lightweight but still offers the standard features for creating an ETL pipeline. It also offers other built-in features like web-based UI and command line integration. Web UI helps to visualize the ETL pipeline execution, which can also be integrated into a Flask based app. It uses PostgreSQL as the data processing engine.  If you are looking for an ETL tool that is simple but still has a touch of sophisticated features then Mara can be a good choice.\n Airflow  Apache Airflow was created by Airbnb and is an open-source workflow management tool. It can be used to create data ETL pipelines. Strictly speaking, it is not an ETL tool itself, instead, it is more of an orchestration tool that can be used to create, schedule, and monitor workflows. This means you can use Airflow to create a pipeline by consolidating various independently written modules of your ETL process.  Airflow workflow follows the concept of DAG (Directed Acyclic Graph). Airflow, like other tools in our list, also has a browser-based dashboard to visualize workflow and track execution of multiple workflows. Airflow is a good choice if you want to create a complex ETL workflow by chaining independent and existing modules together\n Pyspark  Pyspark is the version of Spark which runs on Python and hence the name. As per their website, “Spark is a unified analytics engine for large-scale data processing.”  Spark core not only provides robust features for creating ETL pipelines but also has support for data streaming (Spark Streaming), SQL (Spark SQL), machine learning (MLib) and graph processing (Graph X).  The main advantage of using Pyspark is the fast processing of huge amounts data. So if you are looking to create an ETL pipeline to process big data very fast or process streams of data, then you should definitely consider Pyspark. That said, it’s not an ETL solution out-of-the-box, but rather would be one part of your ETL pipeline deployment.\n Bonobo  Bonobo is a lightweight ETL tool built using Python. It is simple and relatively easy to learn. It uses the graph concept to create pipelines and also supports the parallel processing of multiple elements in the pipeline. It also has a visual UI where the user can track the progress of the ETL pipeline.  All in all, it is just another easy-to-use ETL Python tool, that may be a good option for simple use-cases, but doesn’t have a lot of stand out features that separate it from the pack.\n Luigi  Luigi is a Python-based ETL tool that was created by Spotify but now is available as an open-source tool. It is a more sophisticated tool than many on this list and has powerful features for creating complex ETL pipelines. As per their Github page, “It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more”.  It also comes with a web dashboard to track all the ETL jobs. If you are looking to build an enterprise solution then Luigi may be a good choice.\n Odo  Odo is a Python tool that can convert data from one format to another. But its main noteworthy feature is the performance it gives when loading huge csv datasets into various databases.  As they describe it on their website: “Odo uses the native csv loading capabilities of the databases it supports. These loaders are extremely fast. Odo will beat any other pure Python approach when loading large datasets.”  We haven’t done a performance test to verify these claims, but if anyone has, please share in the comments. But regardless, it’s use-case seems clear: if you are looking to create a simple pipeline where the focus is just to load huge csv datasets into your data warehouse, then you should give Odo a try.\n Conclusion As seem in this post, there are plenty of Python ETL tools to choose from and each brings its own set of features and drawbacks. Whether you are looking for just standard ETL functionality or if you are looking for more add-on features and sophistication, Python may be a good choice.\nYou Rock! ","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581724800,"objectID":"c02de0367e5cb63306441d42acb5b259","permalink":"/post/pythonetltools/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/post/pythonetltools/","section":"post","summary":"ETL with Python ETL is the process of fetching data from one or many systems and loading it into a target data warehouse after doing some intermediate transformations. There are various ETL tools that can carry out this process.\nSome tools offer a complete end-to-end ETL implementation out-the-box and some tools aid you to create a custom ETL process from scratch while there are a few options that fall somewhere in between.","tags":null,"title":"Python Libraries For ETL","type":"post"},{"authors":null,"categories":null,"content":" Analytics is probably the most important tool a company has today to gain customer insights. This is why the Big Data space is set to reach over USD273 Billion by 2023 and companies like Google, Amazon and Microsoft among so many others are so heavily invested in not only collecting data, but enabling data for the enterprise.\nAs Artificial Intelligence and machine learning continue to develop, the way we use analytics also continues to grow and change. While in the past, businesses focused on harvesting descriptive data about their customers and products, more and more, they’re about pulling both predictive and prescriptive learnings from the information they gather. So—what is the difference between descriptive, predictive analytics and prescriptive analytics? And do you need the latter in your organization?\nIf you’re new to the data analytics field, let’s do a quick overview: ● Descriptive analytics: data that provides information about what has happened in your company. Think about a monthly sales report, web hit numbers, marketing campaign rates, etc. They give you insights on how a project performed. This is the most basic form of analytics. (Think “analysis” vs. “analytics.”)\n● Predictive analytics: data that provides information about what will happen in your company. Pulling on more complex Ai and ML processes and algorithms, predictive analytics help you determine what will happen — how well a product will sell, who is likely to buy it, which marketing to use for the greatest impact.\n● Prescriptive analytics: data that provides information on not just what will happen in your company, but how it could happen better if you did X, Y, or Z. Beyond providing information, prescriptive analytics goes even one step further to recommend actions you should take to optimize a process, campaign, or service to the highest degree.\nTo best honest, there is still a lot of confusion between what constitutes predictive and prescriptive analytics, and you may see them used interchangeably in some circles. Regardless, descriptive, predictive, and prescriptive analytics all play important roles in our organizations today. We don’t always need complex algorithms running on our data. Sometimes we just want to know where our financials stand or how much traffic our social media pages are getting. However, in those instances where we do want to improve efficiencies and optimize performance, prescriptive analytics are playing an increasingly important role.\nPrescriptive Analytics Makes Marketing Easier Let’s take a for instance. In the past, marketing teams would draft campaigns and use descriptive analytics to target who they felt would be most open to receiving it. Customers in the 20-30 range might get a “younger” message than those in the 45-60 age group. They might be pitched different products or services. This would generally lead to better overall performance of the campaign. And honestly: many companies still market this way. But this type of marketing still isn’t optimally efficient. There are still many assumptions going into it, and even the results — a high or low purchase rate — won’t necessarily provide insights on why the campaign did or didn’t perform well.\nWhen we move into predictive analytics, things get a bit clearer. Ai and ML can tell us more specifically which groups of customers to target, and which products or discounts to offer to maximize impact. They can even tell us what time of day and what medium to use to reach them. But the results of those campaigns are still descriptive. They won’t tell you what you should be doing to improve your results even further.\nEnter, prescriptive analytics. Prescriptive analytics takes 3 main forms—guided marketing, guided selling and guided pricing. It uses AI to guide buyers with less human interaction—prescribing the right buyer, at the right time, with the right content—telling sales people which product to offer using what words—informing you what price to use at what time in which situation. This information allows you to maximize not just sales but price and margin overall.\nIndeed, the benefits of predictive and prescriptive analytics go far beyond sales conversions. They bleed down into time savings, efficiencies, human capital, transaction costs. Predictive analytic, when automated, can allow you to make real-time decisions—something gasoline and chemical companies do, for instance, changing prices throughout the day to maximize profit. Achieving the benefits of data and more specifically prescriptive analytics comes down to having the technology, systems and processes to maximize available data. In one of my recent pieces I spoke a lot about the importance of having the right infrastructure and software to power your data. Those thoughts remain true here if you want to move up the food chain to leverage the power of prescriptive analytics. This is because prescriptive analytics are about trusting that the Ai will do the work to maximize sales on your behalf, based on the calculations it’s performing in the background (which is driven by your systems of record, tools and infrastructure). It also requires relinquishing control. But the data it creates from these exchanges is also incredibly insightful, proving that often Ai can optimize sales and marketing like humans never could.\nConclusion To know which type of analytics your company should be investing in, you need to start with the big question: what do you want to accomplish? As I noted above, prescriptive analytics are powerful, but they won’t be necessary for every company, or every campaign you push out to customers. They also will require a lot of tweaking. No algorithm was crafted perfectly the first time. It takes time, effort, and focus to make prescriptive analytics work effectively. But if you are in a competitive marketplace—managing anything from products to people — prescriptive analytics could mean a huge boost to profit, productivity, and the bottom line. And honestly: it’s still early in the prescriptive analytics game. I’m guessing we’re only seeing the tip of the iceberg in terms of what prescriptive analytics can accomplish. (And for small and medium-sized businesses out there, don’t worry: My guess is Prescriptive Analytics as a Service isn’t far behind.)\nYou Rock! ","date":1580947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580947200,"objectID":"16433e176ad43367786bd1da16227b2b","permalink":"/post/dataanalytics/","publishdate":"2020-02-06T00:00:00Z","relpermalink":"/post/dataanalytics/","section":"post","summary":"Analytics is probably the most important tool a company has today to gain customer insights. This is why the Big Data space is set to reach over USD273 Billion by 2023 and companies like Google, Amazon and Microsoft among so many others are so heavily invested in not only collecting data, but enabling data for the enterprise.\nAs Artificial Intelligence and machine learning continue to develop, the way we use analytics also continues to grow and change.","tags":null,"title":"Prescriptive Analytics Is The Future","type":"post"},{"authors":null,"categories":null,"content":" There is a little-noticed talent that’s critical for success in a tech-centric world; it’s up there with being a great programmer, a master strategist, or even an innovative entrepreneur.\nIt’s being good at explaining stuff. Explaining why and how something functions has always been a high-value pursuit, essential for leadership. How you explain things frames how you see the world, and the ability to clearly convey your intentions, goals and methods is the stuff of clear mission statements, great speeches, and effective selling. Defining something effectively, in this sense, establishes a kind of ownership of it, and can stir thousands to action.\nIt’s why Steve Jobs, among many other leaders, would spend months on a mere product presentation. He wasn’t just explaining things; he was giving context to a new way of encountering the world, through his product.\nSomething like that level of patience and skill is now needed in the engine rooms of business, where cloud computing, artificial intelligence (Ai), and an explosion of data are reshaping how we live, work, and play, even as the rest of the world struggles to understand what’s going on. These new technologies are incredibly powerful: they can deliver us new insights, they make things happen at an accelerated rate, and they touch an increasing number of areas in life.\nPutting these technologies into rapid use, then telling people how the technologies worked and why they did what they did, is critical. In fact, it’s already a big part of information technology. Providing fast and accurate answers to questions, easy navigation, and clean and organized web pages, all inherently show an understanding of both user needs and product capabilities.\nMore important is what practitioners of Ai call “explainability.” That means sorting out what an algorithm did, what data was used, and why certain conclusions were reached. If, say, an algorithm also made business decisions, these decisions need to be annotated and presented effectively.\nExplainability helps business leaders understand why a company is doing what they’re doing with Ai. This kind of thing will be even more important as Ai becomes commonplace in legal matters or in regulated activities.\nIn these cases, it will be incumbent on Ai specialists to show that their data is free of bias and that the outcomes their programs reach are consistent — an interesting challenge for things like deep learning, where there are many, many layers of analysis and different approaches that can affect the outcome. In a conversation with Ai researcher and professor Sir Nigel Shadbolt, he talked of a future need for algorithmic accountants and data accountants: people who worry about the nature and origin of the datasets.\nElsewhere in the corporation, the increased level of business made possible by cloud-based systems means that, both within different departments and with external partners, there will be a growing emphasis on developing well-defined roles and identities so people can move swiftly and with certainty.\nWhatever the challenges, there is much to like about the explainability revolution. The potential for Ai to improve our lives is vast; but the more we understand it, the more valuable it will be. For one thing, Ai that is well examined and understood often surfaces data biases that arose among the humans the algorithm was aping (this story of an Ai hiring program that ruled out female engineering candidates is a good example). Departments that can explain themselves to other parts of the company will likely have better outcomes, since they’ll be better understood. In turn, they can help the company explain itself to customers, and vice versa.\nFulfilling that need—to be better understood, on all sides—is a high-value activity, whatever technology is at hand.\nYou Rock! ","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"a038ca99c111d5d42abded6719df48bc","permalink":"/post/explainability/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/post/explainability/","section":"post","summary":"There is a little-noticed talent that’s critical for success in a tech-centric world; it’s up there with being a great programmer, a master strategist, or even an innovative entrepreneur.\nIt’s being good at explaining stuff. Explaining why and how something functions has always been a high-value pursuit, essential for leadership. How you explain things frames how you see the world, and the ability to clearly convey your intentions, goals and methods is the stuff of clear mission statements, great speeches, and effective selling.","tags":null,"title":"Explaining Stuff \u0026 Being Great At It","type":"post"},{"authors":null,"categories":null,"content":" With ever more data being generated across modern organizations, management are looking for actionable intelligence to drive optimization, increase margins and avoid supply chain distributions. The sheer volume of data can make it difficult to see trends; this is where machine learning (ML) is a revolution for business intelligence.\nML is a type of Artificial Intelligence (Ai) that powers machines with the ability to learn without being explicitly programmed. It excels at finding (anomalies, patterns and predictive insights in large datasets) — the data lakes — by reporting on historical data as well as deploying models built to forecast likely outcomes. In particular, ML automates \u0026laquo;what if\u0026raquo; analysis by modeling a range of scenarios and prescribing actions that can help the organization achieve optimal results.\nHow machine learning empowers better decision-making with prescriptive analytics Traditionally, management have made decisions based on historical data. Increasingly, the availability of real-time data about every aspect of operations is allowing increased agility not just to see issues as they arise but to see trends as they\u0026rsquo;re developing.\nPredictive analytics empowers management to be proactive; it improves decision-making and forecasting based on both historical and real-time data/trends. Companies are leveraging ML and predictive analytics to better forecast demand, minimize program launch delays, discover opportunities for cost reductions or preemptively anticipate cost increases and drive accurate, on-time shipments.\nAs organizations face increased cost pressure in a rapidly growing and fiercely competitive global marketplace, and as just-in-time models demand precision while raising the stakes, predicting what\u0026rsquo;s coming is key to maintaining a healthy business. However, ML can provide an edge beyond predictive analytics: prescriptive analytics.\nPrescriptive analytics can drive even better results because it integrates a decision support system to perform \u0026laquo;what if\u0026raquo; analyses, evaluate options under constraints and make adjustments in real time. In the analysis, more weight is given to factors that have more impact on desired outcomes, such as detecting and acting on inconsistent quality or delivery performance.\nWhere is the data? Leveraging the data lake Certainly, a wealth of data lives in a company\u0026rsquo;s enterprise resource planning, product lifecycle management and other enterprise systems, but there is a universe of data outside of them. Stored in spreadsheets, emails or messages, much of that other data is unstructured and incompatible with traditional data warehouses driven by relational databases.\nThis is why organizations are increasingly turning to the data lake approach. Amazon defines a data lake (as a centralized, secure and durable cloud-based storage platform that allows you to ingest and store structured and unstructured data, and transform these raw data assets as needed). This presents a challenge to many business analytics systems. With the vast amounts of data collected across these disparate systems and formats, being able to harness that data to drive operational performance can provide a major advantage.\nAccording to University of St. Andrews researcher Andreas François Vermeulen, everything from the (data lake) is available for analysis such as \u0026laquo;SQL queries, big data analytics, full-text search, real-time analytics and ML.\u0026raquo; Machine learning enables a sort of \u0026laquo;social listening\u0026raquo; to mine the unstructured data in other systems, such as email and spreadsheets. As a result, ML will allow management to leverage the vast and varied data they\u0026rsquo;re collecting to not only see and respond to trends but also to run scenarios involving any possible influence on operations.\nThis will be even more critical as IoT and advanced robotics become more common, as communication between organizations and their vendors and partners occurs across an ever-evolving variety of channels and as new technologies enter the market.\nHow you can use machine learning to make smarter business decisions Discover advantageous relationships. With ML, companies can discover quickly — even preemptively — who their best and worst suppliers are and flag potential threats for disruption. Historical data relating to every interaction with suppliers can be tracked and analyzed, and this data can be used to determine if a supplier meets or exceeds expectations, if there are opportunities for improvement or if another supplier needs to be selected.\nIdentify suppliers/partners whose performance is trending in the wrong direction and take action. For instance, if a supplier\u0026rsquo;s defect level or missed shipments has increased recently, this could foreshadow a bigger problem that could cause a major disruption. ML automates the detection of this, and when flagged, a company can identify a supplier that may be in good standing but is trending in a concerning direction so it can proactively award the business to an alternate supplier, mitigating future disruption.\nLeverage machine learning to identify timing issues that may delay the launch of a program. A data practitioner can determine suppliers that historically take longer than scheduled to complete a product launch task. This will allow one to select another supplier for the process or adjust the launch schedule based on the supplier\u0026rsquo;s demonstrated performance.\nModel scenarios that project of supplier capacity issues. In particular, an organization can gain insight into which suppliers would be best suited to respond well to a 20% increase in orders — and which would be unlikely to meet demand — by analyzing contracted capacity measured against demonstrated capacity. Here, ML not only drives decision-making but helps increase transparency while surfacing a significant issue in the supply chain.\nWhat to consider when introducing machine learning into your organization  Engage a vendor who will partner with you, as many organizations do not have in-house data scientists and will need some guidance to make the most of the technology as it evolves.\n The knowledge and recommendations that ML uncovers can lead to a paradigm shift in your organization. Be prepared to examine and optimize established business practices.\n ML tools require some teaching; they must be trained to learn your strategic goals. Before initiating the project, your team should define what success looks like and recognize that the system must be provided with several data points and feedback that will lead it to anticipate the best course of action in each situation.\n  Conclusion Machine learning uncovers opportunities for business optimization hidden in the (data lake) by supercharging analysis of ever-more-complex data. As organizations deploy the next generation of analytics, they\u0026rsquo;ll have better insight into operations and potential threats for disruption. They realize the benefits through improved program launch, cost avoidance and cost reductions, and they can ensure on-time deliveries.\nYou Rock! ","date":1579305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579305600,"objectID":"e53cea0cae67829ad968684f9a58df2a","permalink":"/post/optimisingorg/","publishdate":"2020-01-18T00:00:00Z","relpermalink":"/post/optimisingorg/","section":"post","summary":"With ever more data being generated across modern organizations, management are looking for actionable intelligence to drive optimization, increase margins and avoid supply chain distributions. The sheer volume of data can make it difficult to see trends; this is where machine learning (ML) is a revolution for business intelligence.\nML is a type of Artificial Intelligence (Ai) that powers machines with the ability to learn without being explicitly programmed. It excels at finding (anomalies, patterns and predictive insights in large datasets) — the data lakes — by reporting on historical data as well as deploying models built to forecast likely outcomes.","tags":null,"title":"Optimizing Business With Machine Learning","type":"post"},{"authors":null,"categories":null,"content":" The code\nLending Club is the world\u0026rsquo;s largest peer-to-peer lending company, offering a platform for borrowers and lenders to work directly with one another, eliminating the need for a financial intermediary like a bank.\nRemoving the middle-man generally allows both borrowers and lenders to benefit from better interest rates than they otherwise would, which makes peer-to-peer lending an attractive proposition.\nThis post will be the first in a series of posts analyzing the probability of default and expected return of Lending Club notes. In this first post, I\u0026rsquo;ll cover some of the background on Lending Club, talk about getting and cleaning the loan data, and perform some exploratory analysis on the available variables and outcomes. In subsequent posts, I\u0026rsquo;ll work on developing a predictive model for determining the loan default probabilities. Before investing, it is always important to fully understand the risks, and this post does not constitute investment advice in either Lending Club or in Lending Club notes.\nBackground and Gathering Data Lending Club makes all past borrower data freely available on their website for review, and I will be referencing the 2012-2013 data throughout this post.\nTo download the 2012-2013 data from Lending Club:\n# Download and extract data from Lending Club if (!file.exists(\u0026quot;LoanStats3b.csv\u0026quot;)) { fileUrl \u0026lt;- \u0026quot;https://resources.lendingclub.com/LoanStats3b.csv.zip\u0026quot; download.file(fileUrl, destfile = \u0026quot;LoanStats3b.csv.zip\u0026quot;, method=\u0026quot;curl\u0026quot;) dateDownloaded \u0026lt;- date() unzip(\u0026quot;LoanStats3b.csv.zip\u0026quot;) } # Read in Lending Club Data if (!exists(\u0026quot;full_dataset\u0026quot;)) { full_dataset \u0026lt;- read.csv(file=\u0026quot;LoanStats3b.csv\u0026quot;, header=TRUE, skip = 1) }  For each loan in the file, Lending Club provides an indication of the current loan status. Because many of the loan statuses represent similar outcomes, I\u0026rsquo;ve mapped them from Lending Club\u0026rsquo;s 7 down to only 2, simplifying the problem of classifying loan outcomes without much loss of information useful for investment decisions. My two outcomes \u0026laquo;Performing\u0026raquo; and \u0026laquo;NonPerforming\u0026raquo; seek to separate those loans likely to pay in full from those likely to default. Below I include a table summarizing the mappings:\nNow that we\u0026rsquo;ve loaded the data, let\u0026rsquo;s extract the fields we need and do some cleaning. We can eliminate any fields that would not have been known at the time of issuance, as we\u0026rsquo;ll be trying to make decisions on loan investments using available pre-issuance data. We can also eliminate a few indicative data fields that are repetitive or too granular to be analyzed, and make some formatting changes to get the data ready for analysis. Finally, we\u0026rsquo;ll map the loan statuses to the binary \u0026laquo;Performing\u0026raquo; and \u0026laquo;NonPerforming\u0026raquo; classifiers as discussed above.\n# Select variables to keep and subset the data variables \u0026lt;- c(\u0026quot;id\u0026quot;, \u0026quot;loan_amnt\u0026quot;, \u0026quot;term\u0026quot;, \u0026quot;int_rate\u0026quot;, \u0026quot;installment\u0026quot;, \u0026quot;grade\u0026quot;, \u0026quot;sub_grade\u0026quot;, \u0026quot;emp_length\u0026quot;, \u0026quot;home_ownership\u0026quot;, \u0026quot;annual_inc\u0026quot;, \u0026quot;is_inc_v\u0026quot;, \u0026quot;loan_status\u0026quot;, \u0026quot;purpose\u0026quot;, \u0026quot;addr_state\u0026quot;, \u0026quot;dti\u0026quot;, \u0026quot;delinq_2yrs\u0026quot;, \u0026quot;earliest_cr_line\u0026quot;, \u0026quot;inq_last_6mths\u0026quot;, \u0026quot;mths_since_last_delinq\u0026quot;, \u0026quot;mths_since_last_record\u0026quot;, \u0026quot;open_acc\u0026quot;, \u0026quot;pub_rec\u0026quot;, \u0026quot;revol_bal\u0026quot;, \u0026quot;revol_util\u0026quot;, \u0026quot;total_acc\u0026quot;, \u0026quot;initial_list_status\u0026quot;, \u0026quot;collections_12_mths_ex_med\u0026quot;, \u0026quot;mths_since_last_major_derog\u0026quot;) train \u0026lt;- full_dataset[variables] # Reduce loan status to binary \u0026quot;Performing\u0026quot; and \u0026quot;NonPerforming\u0026quot; Measures: train$new_status \u0026lt;- factor(ifelse(train$loan_status %in% c(\u0026quot;Current\u0026quot;, \u0026quot;Fully Paid\u0026quot;), \u0026quot;Performing\u0026quot;, \u0026quot;NonPerforming\u0026quot;)) # Convert a subset of the numeric variables to factors train$delinq_2yrs \u0026lt;- factor(train$delinq_2yrs) train$inq_last_6mths \u0026lt;- factor(train$inq_last_6mths) train$open_acc \u0026lt;- factor(train$open_acc) train$pub_rec \u0026lt;- factor(train$pub_rec) train$total_acc \u0026lt;- factor(train$total_acc) # Convert interest rate numbers to numeric (strip percent signs) train$int_rate \u0026lt;- as.numeric(sub(\u0026quot;%\u0026quot;, \u0026quot;\u0026quot;, train$int_rate)) train$revol_util \u0026lt;- as.numeric(sub(\u0026quot;%\u0026quot;, \u0026quot;\u0026quot;, train$revol_util))  Analyzing Predictive Power of Variables Lending Club Grades and Subgrades All types of borrowers are using peer-to-peer lending for a variety of purposes. This raises the question of how to determine appropriate interest rates given the varying levels of risk across borrowers. Luckily for us, Lending Club handles this for us. They use an algorithm to determine a borrower\u0026rsquo;s level of risk, and then set the interest rates according to the level of risk. Specifically, Lending Club maps borrowers to a series of grades [A-F] and subgrades [A-F][1-5] based on their risk profile. Loans in each subgrade are then given appropriate interest rates. The specific rates will change over time according to market conditions, but generally they will fall within a tight range for each subgrade.\nLet\u0026rsquo;s take a look at the proportions of performing and non-performing loans by Lending Club\u0026rsquo;s provided grades:\nby_grade \u0026lt;- table(train$new_status, train$grade, exclude=\u0026quot;\u0026quot;) prop_grade \u0026lt;- prop.table(by_grade,2) barplot(prop_grade, main = \u0026quot;Loan Performance by Grade\u0026quot;, xlab = \u0026quot;Grade\u0026quot;, col=c(\u0026quot;darkblue\u0026quot;,\u0026quot;red\u0026quot;), legend = rownames(prop_grade)) by_subgrade \u0026lt;- table(train$new_status, train$sub_grade, exclude=\u0026quot;\u0026quot;) prop_subgrade \u0026lt;- prop.table(by_subgrade,2) barplot(prop_subgrade, main = \u0026quot;Loan Performance by Sub Grade\u0026quot;, xlab = \u0026quot;SubGrade\u0026quot;, col=c(\u0026quot;darkblue\u0026quot;,\u0026quot;red\u0026quot;),legend = rownames(prop_subgrade))  We can see from the chart below that rates of default steadily increase as the loan grades worsen from A to G, as expected.\nWe see a similar pattern for the subgrades, although the trend begins to weaken across the G1-G5 subgrades. On further investigation, I found that there are only a few hundred data points for each of these subgrades, in contrast to thousands of data points for the A-F subgrades, and these differences are not large enough to be significant.\nOverall, it looks like the Lending Club grading system does a pretty great job of predicting ultimate loan performance, but let\u0026rsquo;s check out some of the other available data to see what other trends we might be able to find in the data.\nHome Ownership The Lending Club data has 3 main classifications for home ownership: mortgage (outstanding mortgage payment), own (home is owned outright), and rent. I would expect those with mortgages to default less frequently than those who rent, both because there are credit requirements to get a mortgage and because those with mortgages might tend to be more established. Let\u0026rsquo;s see whether this is actually the case:\nownership_status \u0026lt;- table(train$new_status,train$home_ownership, exclude=c(\u0026quot;OTHER\u0026quot;,\u0026quot;NONE\u0026quot;,\u0026quot;\u0026quot;)) prop_ownership \u0026lt;- round(prop.table(ownership_status, 2) * 100, 2)  So those with mortgages default the least, followed by those who own their homes outright and finally those who rent. The differences here are much smaller than when comparing different grades, but they are still notable. Let\u0026rsquo;s verify whether these are statistically significant:\n# Calculate the counts of mortgage, owners, and renters: count_m \u0026lt;- sum(train$home_ownership == \u0026quot;MORTGAGE\u0026quot;) count_o \u0026lt;- sum(train$home_ownership == \u0026quot;OWN\u0026quot;) count_r \u0026lt;- sum(train$home_ownership == \u0026quot;RENT\u0026quot;) # Calculate the counts of default for mortgages, owners, and renters: dflt_m \u0026lt;- sum(train$home_ownership == \u0026quot;MORTGAGE\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) dflt_o \u0026lt;- sum(train$home_ownership == \u0026quot;OWN\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) dflt_r \u0026lt;- sum(train$home_ownership == \u0026quot;RENT\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) # 1-sided proportion test for mortgage vs owners prop.test(c(dflt_m,dflt_o), c(count_m,count_o), alternative = \u0026quot;less\u0026quot;) # 1-sided proportion test for owners vs renters prop.test(c(dflt_o,dflt_r), c(count_o,count_r), alternative = \u0026quot;less\u0026quot;)  The p-value of the first test was $$6.377*10^-12$$ and the p-value for the second test was $$3.787*10^-8$$, indicating that the differences in both of these proportions are very statistically significant. Although the differences in the default probabilities are somewhat small, on the order of 1.5%, the number of data points is in the high tens of thousands, which contributes to the significance. Given this result, we can conclude that similar differences in default probabilities for other factors should also be significant, so long as a similar quantity of data points is available.\nNote: for the remaining analysis, the code for each variable becomes a bit repetitive, so in the interest of minimizing the length of this post I will present only the results. If you are interested to see the actual code, you will find it in the appendix at the bottom of this post. You can also read the complete code on Github.\nDebt to Income Ratio Debt to income ratio indicates the ratio between a borrowers monthly debt payment and monthly income. This was originally formatted as a continuous numerical variable, but I bucketed it into 5% increments to better visualize the effect on loan performance. As we might expect, there is a steady increase in the percentage of non-performing loans as DTI increases, reflecting the constraints that increased debt put onto borrower ability to repay:\nRevolving Utilization Percent Revolving utilization percent is the portion of a borrower\u0026rsquo;s revolving credit limit (i.e. credit card limit) that they actually are using at any given point. For example, if a borrower\u0026rsquo;s total credit limit is UDS15,000 and their outstanding balance is USD1,500 their utilization rate would be 10%. We can see below that the percentage of non-performing loans steadily increases with utilization rate.\nBorrowers with high utilization rates are more likely to have high fixed credit card payments which might affect their ability to repay their loans. Also, a high utilization rate often reflects a lack of other financing options, with borrowers turning to peer-to-peer lending as a last resort. This is in contrast to those borrowers with low utilization rates, who may be using peer-to-peer lending opportunistically to pursue lower interest payments.\nLoan Purpose Loan purpose refers to the borrower\u0026rsquo;s stated reason for taking out the loan. We see below that credit card and debt consolidation tend to have better performance, along with home improvement, cars, and other major purchases. Luxury spending on vacations and weddings and unexpected medical and moving expenses generally have worse performance. Small business loans perform very poorly, perhaps reflecting the fact that those borrowers unable to get bank financing for their small business may have poor credit or business plans that aren\u0026rsquo;t fully developed.\nInquiries in the Past 6 Months Number of inquiries refers to the number of times a borrower\u0026rsquo;s credit report is accessed by financial institutions, which generally happens when the borrower is seeking a loan or credit line. More inquiries leads to higher rates of nonperformance, perhaps indicating that increased borrower desperation to access credit might highlight poor financial health. Interestingly, we see an increase in loan performance in the 4+ inquiries bucket. These high levels of inquiries may reflect financially savvy borrowers shopping around for mortgage loans or credit cards.\nNumber of Total Accounts A larger number of total accounts indicates a longer credit history and a high level of trust between the borrower and financial institutions, both of which point to financial health and lower rates of default. We see steady increases in the rates of performing loans as the number of accounts increases from 7 to around 20, but diminishing effects after that.\nAnnual Income As we might expect, the higher a borrower\u0026rsquo;s annual income the more likely they are to be able to repay their loans. Below I\u0026rsquo;ve broken the income data into quintiles, and we can see that those in the top 20% of annual incomes (USD95000 +) are approximately 6% more likely to be performing borrowers than those in the bottom 20% (less than USD42000).\nLoan Amount As the amount borrowed increases, we see increasing rates of nonperforming loans. The difference between the first two buckets is only around 1% (and the intra-bucket differences are very small), but we see a larger decrease in loan quality in the USD30,000 - USD35,000 bucket. Noting that the Lending Club maximum loan is USD35,000, this may indicate particularly desperate borrowers who are maximizing their possible borrowing.\nEmployment Length We\u0026rsquo;d expect those who have been employed longer to be more stable, and thus less likely to default. Looking into the data, 3 key groups emerged: the unemployed, those employed less than 10 years, and those employed for 10+ years:\nDelinquencies in the Past 2 Years The number of delinquencies in the past 2 years indicates the number of times a borrower has been behind on payments. I combined all values 3 or larger into a single bucket for analysis, as this was a long right-tailed distribution. Interestingly, those with a single delinquency seem to perform more often than those with none. In general however, the differences between 0, 1, and 2 delinquencies are relatively small, while those with greater than 3 show a significant decrease in performance.\nNumber of Open Accounts Unlike the number of total accounts above, which we saw to be quite significant, the number of open accounts variable was not a particularly strong indicator:\nVerified Income Status Lending Club categorizes income verification into three statuses: not verified, source verified, and verified. Verified income means that Lending Club independently verified both the source and size of reported income, source verified means that they verified only the source of the income, and not verified means there was no independent verification of the reported values. Interestingly, we see that as income verification increases, the loan performance actually worsens. During the mortgage crisis, non-verified \u0026laquo;no-doc\u0026raquo; loans were among the worst performing, so the reversal here is interesting. This likely reflects the fact that Lending Club only verifies those borrowers who seem to be of worse credit quality, so there may be confounding variables present here.\nNumber of Public Records Public records generally refer to bankruptcies, so we would expect those with more public records to show worse performance. Actually, performance increases as we move from 0 to 1 to 2 public records. This possibly indicates stricter lending standards from Lending Club on those borrowers with public records:\nVariables that were not significant:  Months since last delinquency Months since last major derogatory note Collections previous 12 months (too few data points on which to make any conclusions or form predictions)  Summary  Lending club grade and subgrade variables provide the most predictive power for determining expected loan performance. A large number of the other variables also provide strong indications of expected performance. Among the most telling are debt-to-income ratio, credit utilization rate, home ownership status, loan purpose, annual income, inquiries in the past 6 months, and number of total accounts. Verified income status and number of public records show results opposite from what we would expect. This is likely due to increased standards on borrowers with poorer credit history, so all else equal we see outperformance in these loans.  We\u0026rsquo;ve gotten a good understanding of the available borrower data, and we\u0026rsquo;ve seen which variables give the best indiciations of future loan performance. In the next post, We\u0026rsquo;ll work on developing a predictive model for projecting the probability of default for newly issued loans.\nAppendix Below I\u0026rsquo;ve included the code used to generate the numbers in the tables above. You can also find the complete code available on Github\n### Explore the relationships between default rates and factor levels ### I take a few different approaches, but the key idea is the same # Home Ownership (exclude status \u0026quot;OTHER\u0026quot; and \u0026quot;NONE\u0026quot; because of few data points) home_ownership \u0026lt;- table(train$new_status,train$home_ownership, exclude=c(\u0026quot;OTHER\u0026quot;,\u0026quot;NONE\u0026quot;,\u0026quot;\u0026quot;)) prop_home_ownership \u0026lt;- round(prop.table(home_ownership, 2) * 100, 2) # Test for significance of the difference in proportions for home ownership factors # Calculate the counts of mortgage, owners, and renters: count_m \u0026lt;- sum(train$home_ownership == \u0026quot;MORTGAGE\u0026quot;) count_o \u0026lt;- sum(train$home_ownership == \u0026quot;OWN\u0026quot;) count_r \u0026lt;- sum(train$home_ownership == \u0026quot;RENT\u0026quot;) # Calculate the counts of default for mortgages, owners, and renters: dflt_m \u0026lt;- sum(train$home_ownership == \u0026quot;MORTGAGE\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) dflt_o \u0026lt;- sum(train$home_ownership == \u0026quot;OWN\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) dflt_r \u0026lt;- sum(train$home_ownership == \u0026quot;RENT\u0026quot; \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) # 1-sided proportion test for mortgage vs owners prop.test(c(dflt_m,dflt_o), c(count_m,count_o), alternative = \u0026quot;less\u0026quot;) # 1-sided proportion test for owners vs renters prop.test(c(dflt_o,dflt_r), c(count_o,count_r), alternative = \u0026quot;less\u0026quot;) # Debt to Income Ratio (break into factors at 5% levels) train$new_dti \u0026lt;- cut(train$dti, breaks = c(0, 5, 10, 15, 20, 25, 30, 35)) dti \u0026lt;- table(train$new_status, train$new_dti) prop_dti \u0026lt;- round(prop.table(dti, 2) * 100, 2) # Revolving Utilization (break into 0 - 20, then factors of 10, then 80+) train$new_revol_util \u0026lt;- cut(train$revol_util, breaks = c(0, 20, 30, 40, 50, 60, 70, 80, 141)) revol_util \u0026lt;- table(train$new_status, train$new_revol_util) prop_revol_util \u0026lt;- round(prop.table(revol_util, 2) * 100, 2) # Loan Purpose (exclude renewable energy because so few data points) purpose \u0026lt;- table(train$new_status,train$purpose, exclude = c(\u0026quot;renewable_energy\u0026quot;,\u0026quot;\u0026quot;)) prop_purpose \u0026lt;- round(prop.table(purpose, 2) * 100, 2) # Inquiries in the last 6 months (combine factor levels for any \u0026gt; 4) levels(train$inq_last_6mths) \u0026lt;- c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;3\u0026quot;, rep(\u0026quot;4+\u0026quot;, 5)) inq_last_6mths \u0026lt;- table(train$new_status, train$inq_last_6mths) prop_inq_last_6mths \u0026lt;- round(prop.table(inq_last_6mths, 2) * 100, 2) # Number of total accounts (combine factor levels into groups of 5, then 23+) levels(train$total_acc) \u0026lt;- c(rep(\u0026quot;\u0026lt;= 7\u0026quot;, 5), rep(\u0026quot;8 - 12\u0026quot;, 5), rep(\u0026quot;13 - 17\u0026quot;, 5), rep(\u0026quot;18 - 22\u0026quot;, 5), rep(\u0026quot;23+\u0026quot;, 68)) total_acc \u0026lt;- table(train$new_status, train$total_acc) prop_total_acc \u0026lt;- round(prop.table(total_acc, 2) * 100, 2) # Annual Income (factor into quantiles of 20%) train$new_annual_inc \u0026lt;- cut(train$annual_inc, quantile(train$annual_inc, na.rm = TRUE, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))) annual_inc \u0026lt;- table(train$new_status, train$new_annual_inc) prop_annual_inc \u0026lt;- round(prop.table(annual_inc, 2) * 100, 2) # Loan Amount (break into \u0026lt; 15k, 15k - 30k, 30k - 35k) train$new_loan_amnt \u0026lt;- cut(train$loan_amnt,c(0, 15000, 30000, 35000)) loan_amnt \u0026lt;- table(train$new_status, train$new_loan_amnt) prop_loan_amnt \u0026lt;- round(prop.table(loan_amnt, 2) * 100, 2) # Employment Length (combine factor levels for better comparison) levels(train$emp_length) \u0026lt;- c(\u0026quot;None\u0026quot;, \u0026quot;\u0026lt; 10 years\u0026quot;, \u0026quot;\u0026lt; 10 years\u0026quot;, \u0026quot;10+ years\u0026quot;, rep(\u0026quot;\u0026lt; 10 years\u0026quot;, 8), \u0026quot;None\u0026quot;) emp_length \u0026lt;- table(train$new_status, train$emp_length) prop_emp_length \u0026lt;- round(prop.table(emp_length, 2) * 100, 2) # Delinquencies in the past 2 Years (combine factors levels for any \u0026gt; 3) levels(train$delinq_2yrs) \u0026lt;- c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, rep(\u0026quot;3+\u0026quot;, 17)) delinq_2yrs \u0026lt;- table(train$new_status, train$delinq_2yrs) prop_delinq_2yrs \u0026lt;- round(prop.table(delinq_2yrs, 2) * 100, 2) # Number of Open Accounts (combine factor levels into groups of 5) levels(train$open_acc) \u0026lt;- c(rep(\u0026quot;\u0026lt;= 5\u0026quot;, 6), rep(\u0026quot;6 - 10\u0026quot;, 5), rep(\u0026quot;11 - 15\u0026quot;, 5), rep(\u0026quot;16+\u0026quot;, 38)) open_acc \u0026lt;- table(train$new_status, train$open_acc) prop_open_acc \u0026lt;- round(prop.table(open_acc, 2) * 100, 2) # Verified income status is_inc_v \u0026lt;- table(train$new_status, train$is_inc_v, exclude = \u0026quot;\u0026quot;) prop_is_inc_v \u0026lt;- round(prop.table(is_inc_v, 2) * 100, 2) # Number of Public Records (break factor levels into 0, 1, 2+) levels(train$pub_rec) \u0026lt;- c(\u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, rep(\u0026quot;2+\u0026quot;, 12)) pub_rec \u0026lt;- table(train$new_status, train$pub_rec) prop_pub_rec \u0026lt;- round(prop.table(pub_rec, 2) * 100, 2) # Months Since Last Record (compare blank vs. non-blank) na_last_record \u0026lt;- sum(is.na(train$mths_since_last_record)) not_na_last_record \u0026lt;- sum(!is.na(train$mths_since_last_record)) na_last_rec_dflt \u0026lt;- sum(is.na(train$mths_since_last_record) \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) not_na_last_rec_dflt \u0026lt;- sum(!is.na(train$mths_since_last_record) \u0026amp; train$new_status == \u0026quot;NonPerforming\u0026quot;) not_na_last_rec_pct_dflt \u0026lt;- not_na_last_rec_dflt / not_na_last_record na_last_rec_pct_dflt \u0026lt;- na_last_rec_dflt/na_last_record # Months since last delinquency (break factor levels in increments of 10) train$mths_since_last_delinq \u0026lt;- cut(train$mths_since_last_delinq, breaks = c(0, 10, 20, 30, 40, 50, 60, 156)) mths_since_last_delinq \u0026lt;- table(train$new_status, train$mths_since_last_delinq) prop_mths_since_last_delinq \u0026lt;- round(prop.table(mths_since_last_delinq, 2) * 100, 2) # Collections last 12 months collections \u0026lt;- table(train$new_status, train$collections_12_mths_ex_med) prop_collections \u0026lt;- round(prop.table(collections, 2) * 100, 2)  You Rock! ","date":1575936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575936000,"objectID":"a3e16dc4e47ab782bcbc3ec1db48ff9e","permalink":"/post/lendingclub/","publishdate":"2019-12-10T00:00:00Z","relpermalink":"/post/lendingclub/","section":"post","summary":"The code\nLending Club is the world\u0026rsquo;s largest peer-to-peer lending company, offering a platform for borrowers and lenders to work directly with one another, eliminating the need for a financial intermediary like a bank.\nRemoving the middle-man generally allows both borrowers and lenders to benefit from better interest rates than they otherwise would, which makes peer-to-peer lending an attractive proposition.\nThis post will be the first in a series of posts analyzing the probability of default and expected return of Lending Club notes.","tags":null,"title":"Analyzing Historical Default Rates of Lending Club Notes","type":"post"},{"authors":null,"categories":null,"content":" Forming a significant theme in several organisational technology strategies, AI can augment a gamut of business practices, including compliance Compliance is a must-do activity, not a nice-to-have. It is essential that companies extract maximum value from compliance processes, reducing the possibility of it being considered a cost centre.\nTechnological innovation can help to lift some of the compliance burden. The level of technology you can realistically implement depends on how advanced the organisation is to start with. One company’s moonshot could be another’s business as usual. Assessing the starting point is just as important as considering the benefits and end goal.\nRegTech, AI and the future of compliance This is the question that the burgeoning RegTech (regulatory technology) industry is seeking to answer. Ai is typically at the forefront. RegTech partly focuses on improving the efficiency and effectiveness of existing processes. As part of that improvement, organizations are using Ai, machine learning and RPA (robotic process automation) to smooth the integration and processes between new RegTech solutions, existing legacy compliance solutions and legacy platforms.\nWhy look to Ai for help? Recent regulations, such as GDPR or PSD2, are handed down in the form of large and extremely dense documentation (the UK government’s guidance document for GDPR alone is 201 pages). Identifying the appropriate actions mandated by these lengthy documents requires a great deal of cross-referencing, prior knowledge of historical organisational actions, and knowledge of the relevant organisational systems and processes. What’s more, several regulations attract fines or corrective actions if not applied properly (like the infamous “4% of company turnover” penalty attached to GDPR).\nIn short, the practical application of regulations currently relies on human interpretation and subsequent deployment of a solution, with heavy penalties for noncompliance. This is where Ai can help, reducing the workload involved and improving accuracy. Here are three key examples of how Ai can help companies turn compliance into a value-added activity.\n1.Reducing the risk of nonconformity\nFollowing the deployment of compliance processes, there is often residual risk. This can be as a result of unforseen gaps in compliance processes, or unexpected occurrences that become apparent when operating at scale.\nThat’s partly because there are usually a lot of steps and processes to be carried out during the data collation stage of compliance programmes. RPA can help reduce administrative load associated with these processes that include a high degree of repetition – for example, copying data from one system to another. Ai can then help process cross-organisational documentation, combining internal and external sources and appropriately matching where necessary.\nAi can also help to reduce companies’ risk of noncompliance with, for example, privacy regulations. Furthermore, using Ai techniques, organisations can automate transforming and enhancing data. Intelligent automation allows companies to carry out processes with a higher degree of accuracy.\n2.Improving process efficiency\nInefficient processes can also hinder compliance. For example, automated systems that detect suspicious transactions for AML (anti-money laundering) processes are sometimes not always as accurate as they could be. A report highlighted that 95% of flagged transactions are closed in the first stage of review. Effectively, investigators spend most of their day looking at poor quality cases.\nUse of an Ai hybrid approach to detection ensures there are fewer, higher quality alerts produced. Furthermore, it is possible to risk-rank cases which are flagged for investigation, speeding up the interaction and relegating lower-risk transactions. Although Ai forms an underlying principle across most modern detection systems, maintenance is key to managing effective performance.\nAI can also be used to bolster AML and fraud measures more widely. For example, applying AI to techniques such as text mining, anomaly detection and advanced analytics can improve trade finance monitoring. This, in turn, can improve the regularity for document review and consignment checking, improving the validation rates of materials as they cross borders.\n3.Keeping up with regulatory changes\nCompliance never stands still. Businesses have to contend with a constantly evolving landscape, potentially across several regions. Ai can help to optimise the processing of these regulations and the actions they require, helping companies keep up to date. Companies that need to effectively comply with several differing regulations require a wide range of understanding across all parts of the business. The size, complexity and legacy systems of the business can be significant obstacles.\nTo mitigate this risk, companies can use NLP (natural language processing) to automate aspects of regulatory review, identifying appropriate changes contained in the regulation and then relaying potential impacts to the appropriate departments. For example, Ai could help geographically diverse companies determine whether changes in the US have an impact on their Singapore office.\nHumans still needed It’s important to note at this point that Ai and RegTech are not expected to widely replace humans. We are seeing early Ai entries in the RegTech space, but they’re primarily helping with lower-hanging fruit and repetitive tasks. Ai is primarily enhancing the work humans do, making them more effective in their roles.\nAi does not come without some considerations, however. There is a great deal of focus and scrutiny on associated possible bias in Ai deployments. Other discussions are exploring the transparency and governance of applications and questions around who owns generated IP. As a result, it’s essential that Ai works closely with humans, enhancing activities and balancing an appropriate level of manual oversight.\nConclusion Ai is augmenting compliance practices by providing faster document review, deeper fraud prevention measures and greater contextual insight. It is also reducing noise in high-transaction environments and lightening the documentary burden on staff. From the start of the regulatory review to the end of the compliance process, Ai holds part of the overall solution to a more efficient and valuable compliance function.\nYou Rock! ","date":1575072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575072000,"objectID":"abd94a4491d39850030ac59325d85bd3","permalink":"/post/augmentingcompliance/","publishdate":"2019-11-30T00:00:00Z","relpermalink":"/post/augmentingcompliance/","section":"post","summary":"Forming a significant theme in several organisational technology strategies, AI can augment a gamut of business practices, including compliance Compliance is a must-do activity, not a nice-to-have. It is essential that companies extract maximum value from compliance processes, reducing the possibility of it being considered a cost centre.\nTechnological innovation can help to lift some of the compliance burden. The level of technology you can realistically implement depends on how advanced the organisation is to start with.","tags":null,"title":"Ai Augmenting Compliance","type":"post"},{"authors":null,"categories":null,"content":" Can we adequately protect the privacy and security of data used in machine learning? As the adoption of machine learning (ML) increases, it is becoming clear that ML poses new privacy and security challenges that are difficult to prevent in practice. This leaves the data involved in ML exposed to risks in ways that are frequently misunderstood.\nWhile traditional software systems already have standard best practices such as the FIPPs (Fair Information Practice Principles) to guide privacy efforts, or the Confidentiality, Integrity and Availability triad to guide security activities, there exists no widely accepted best practices for the data involved in ML.\nAdapting existing standards or creating new ones is critical to the successful, widespread adoption of ML. Without such standards, neither privacy professionals, security practitioners, nor data scientists will be able to deploy ML with confidence that the data they steward is adequately protected. And without such protection, ML will face significant barriers to adoption.\nThis post aims to create the beginnings of a framework for such standards by focusing on specific privacy and security vulnerabilities within ML systems. At present, viewing these vulnerabilities as warning signs, either of a future in which the benefits of ML are not fully embraced, or a future in which ML\u0026rsquo;s liabilities are insufficiently protected.\nThe ultimate goal is to raise awareness of new privacy and security issues confronting ML based systems for everyone from the most technically proficient data scientists to the most legally knowledgeable privacy personnel, along with the many in between. Ultimately, aiming to suggest practical methods to mitigate these potential harms, thereby contributing to the privacy-protective and secure use of ML.\nWhy machine learning is exposed to New Privacy and Security Risks Experience has already proven that security and privacy as applied to ML differ from the data protection frameworks applied to traditional software systems. The scale of the volume of data collected, the range of uses for existing models (beyond simply those envisioned by their creators), and the power of the inferences such models generate are unlike those seen in traditional use cases.\nPast frameworks for data protection, for example, were largely premised on harms derived from the point of access, either to the collected data or to software systems themselves. In information security, harms began with unauthorized access to datasets or to networks. In privacy, overly broad or insufficiently enforced access to data again served as the starting point for all subsequent harms, such as unauthorized use, sharing, or sale. Preventing or managing access was, as a result, a relatively intuitive task that privacy and security teams could prioritize as the basis of their efforts.\nHarms from ML, however, do not always require the same type of direct access to underlying data to infringe upon that data’s confidentiality or to create privacy violations. This exposes ML systems to privacy and security risks in novel ways, as we will see below.\nBoth privacy and security harms can occur, for example, absent direct access to underlying training data because ML models themselves may subtly represent that data long after training. Similarly, the behavior of models can be manipulated without needing direct access to their source code. The types of activities that once required hacking under a traditional computing paradigm can now be carried out through other methods.\nInformational vs. Behavioral: Two types of harms in Machine Learning The types of security and privacy harms enabled by ML fall into roughly two categories: informational and behavioral.\n Informational harms relate to the unintended or unanticipated leakage of information. Behavioral harms, on the other hand, relate to manipulating the behavior of the model itself, impacting the predictions or outcomes of the model.\n By describing the specific “attacks” that constitute these types of harms below, viewing each such attack as a warning sign of future, more widely known and exploited vulnerabilities associated with ML.\nINFORMATIONAL HARMS Membership Inference: This attack involves inferring whether or not an individual’s data was contained in the data used to train the model, based on a sample of the model’s output. While seemingly complex, this analysis requires much less technical sophistication than is frequently assumed. A group of researchers from Cornell University, for example, recently released an auditing technique meant to help the general public learn if their data was used to train ML models, hoping to enable compliance with privacy regulations such as the EU’s GDPR. If used by malicious third parties, such analysis could compromise the confidentiality of the model and violate the privacy of affected individuals by revealing whether they are members of sensitive classes.\nModel Inversion: Model inversion uses ML outputs to recreate the actual data the model was originally trained upon. In one well-known example of model inversion, researchers were able to reconstruct an image of an individual’s face that was used to train a facial recognition model. Another study, focused on ML systems that used genetic information to recommend dosing of specific medications, was able to directly predict individual patients\u0026rsquo; genetic markers.\nModel Extraction: This type of attack uses model outputs to recreate the model itself. Such attacks have been piblicly demonstrated against ML as a service providers like BigML and Amazon Machine Learning, and can have implications for privacy and security as well as the intellectual property or proprietary business logic of the underlying model. While there exist myriad types of harms that can arise from this type of attack, the very fact that models retain representations of their training data, as described above, makes the threat of extraction an inherent vulnerability from the privacy perspective.\nCollective harms posed by ML-Enabled inferences ML exacerbates one particularly thorny informational harm in the world of data analytics: creating dangers for individuals with no relation to the underlying training data or the model itself. That is, if ML models are able to make increasingly powerful predictions, the ability to apply those predictions to new individuals raises serious privacy concerns on its own. In that sense, a narrow focus on protecting the privacy and security of only the individuals whose data is used to train models is mistaken; all individuals may be affected by significantly powerful ML.\nOne such example is the recent creation of a model that can detect anxiety and depression in children simply based on statistical patterns in each child’s voice. The model can take ordinary input data (voice recordings) and make decisions that constitute sensitive diagnostic data (the presence of anxiety or depression in a specific child). As a result, the very act of any child speaking—beyond the children involved in this study—now contains new privacy implications.\nBEHAVIORAL HARMS Poisoning: Model poisoning occurs when an adversary is able to insert malicious data into training data in order to alter the behavior of the model at a later point in time. This technique may be used in practice for a variety of malicious activities, such as creating an artificially low insurance premium for particular individuals, or otherwise training a model to intentionally discriminate against a group of people. Altering the behavior of models can have both security and privacy implications, and does not necessarily require that the malicious actor have direct access to a model once deployed.\nEvasion: Evasion occurs when input data is fed into an ML system that intentionally causes the system to misclassify that data. Such attacks may occur in a range of scenarios, and the input data may not be noticeable by humans. In one such example, researchers were able to cause a road sign classifier to misidentify road signs by placing small black and white stickers on each sign. This type of attack could cause traffic violations in systems such as those in autonomous vehicles. Similar evasion attacks have been demonstrated in a variety of other sensitive contexts as well.\nA layered approach to data protection in ML What can we do to guard against these harms in practice? While there are no easy answers, there are a series of actions that can make such harms less likely to occure or minimize their impact. Below are a handful of approaches. Noise Injection: From a technical perspective, one of the most promising techniques involves adding tailored amounts of noise into the data used to train the model. Rather than training directly on the raw data, models can train on data with slight perturbations, which increases the difficulty of gaining insight into the original data or manipulating the model itself. One such method, known as differential privacy, is among the most widely accepted (and promising) methods of randomized noise injection.\nIntermediaries: Another approach relies on inserting intermediaries or additional layers between the raw training data and the model, which can be implemented in a variety of ways. Federated learning, for example, trains models against data that is separated in silos, which can make the attacks discussed above more difficult to implement. Another method involves what is known as a student teacher approach, in which a variety of student models are trained on different aspects of the underlying data, which are then used to train the parent model or models that are actually deployed.\nTransparent ML Mechanisms: A motivated attacker may be able to learn more about a black-box ML model than is known by its original creators, creating the possibility for privacy and security harms that they might not have envisioned. While traditionally dominated by black-box modeling routines, the field of ML has experienced a renaissance of research and techniques for training transparent models, which can help to address such concerns. Examples of such techniques, with accompanying open source code, include explainable boosting machines and scalable Bayesian rule lists.\nexplainable boosting machines\nscalable Bayesian rule lists\nAccess Controls: While it is broadly true that attacks against ML do not require the type of direct access needed to cause harms in traditional software systems, access to the model output is still required in many cases. For this reason, attempts to limit access to model output, along with methods to detect when such access is being abused, are among the most simple and effective ways to protect against the attacks described above.\nModel Monitoring: It can be difficult to predict how ML systems will respond to new inputs, making their behavior difficult to manage over time. Detecting when such models are misbehaving is therefore critical to managing security and privacy risks. Key components of monitoring include outlining major risks and failure modes, devising a plan for how to detect complications or anomalies that occur, along with mechanisms for responding quickly if problems are detected.\nModel Documentation: A long-standing best practice in the high-stakes world of credit scoring, model documentation formally records information about modeling systems, including but not limited to: business justifications; business and IT stakeholders; data scientists involved in model training; names, locations, and properties of training data; assumptions and details of ML methodologies; test and out-of-time sample performance; and model monitoring plans. A good model report should allow future model owners or validators to determine whether a model is behaving as intended.\nWhite Hat or Red Team Hacking: As many ML attacks are described in technical detail in peer-reviewed publications, organizations can use these details to test public-facing or mission-critical ML end points against known attacks. White hat or red teams, either internally or provided by third parties, may therefore be able to identify and potentially remediate discovered vulnerabilities.\nOpen Source Software Privacy \u0026amp; Security Resources: Nascent open source tools for private learning, accurate and transparent models, and debugging of potential security vulnerabilities are currently being released and are often associated with credible research or software organizations.\nNo team is an Island: The importance of cross-functional expertise Ongoing, cross-functional communication is required to help ensure the privacy and security of ML systems. Data scientists and software developers need access to legal expertise to identify privacy risks at the beginning of the ML lifecycle. Similarly, lawyers and privacy personnel need access to those with design responsibilities and security proficiencies to understand technical limitations and to identify potential security harms. Processes for ongoing communication, for risk identification and management, and for clear setting of objectives should be established early and followed scrupulously to ensure that no team operates in isolation.\nThere is no point in time in the process of creating, testing, deploying, and auditing production ML where a model can be ‘certified’ as being free from risk. There are, however, a host of methods to thoroughly document and monitor ML throughout its lifecycle to keep risk manageable, and to enable organizations to respond to fluctuations in the factors that affect this risk. Identifying, preventing, minimizing and responding to such risks must be an ongoing and thorough process.\nConclusion This blog aimed to outline a framework for understanding and addressing privacy and security risks in ML, and welcomes suggestions or comments to improve our analysis. Please reach out with feedback.\nYou Rock! ","date":1574985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574985600,"objectID":"863eeea6ff1f9440b13df1ed8846798e","permalink":"/post/privacysecurity/","publishdate":"2019-11-29T00:00:00Z","relpermalink":"/post/privacysecurity/","section":"post","summary":"Can we adequately protect the privacy and security of data used in machine learning? As the adoption of machine learning (ML) increases, it is becoming clear that ML poses new privacy and security challenges that are difficult to prevent in practice. This leaves the data involved in ML exposed to risks in ways that are frequently misunderstood.\nWhile traditional software systems already have standard best practices such as the FIPPs (Fair Information Practice Principles) to guide privacy efforts, or the Confidentiality, Integrity and Availability triad to guide security activities, there exists no widely accepted best practices for the data involved in ML.","tags":null,"title":"Privacy \u0026 Security in the Age of Machine Learning","type":"post"},{"authors":null,"categories":null,"content":" Deep Learning applied to natural language processing  Language is the medium that humans use for conversing. Giving machines the ability to learn human language with natural language processing has given rise to several new products and possibilities that were not previously imaginable.\n NLP (Natural language processing) is one of the most important technologies present in the information age. Understanding complex language utterances is a crucial part of Ai. Applications of NLP can be found across several industry domains such as web search, advertisement, emails, customer service, language translation, radiology reports, etc.\nNLP techniques are designed in the same manner that the human brain learns language processing. One can understand language at varying granularities. When a person learns a new language, they start with words: understanding their meaning, identifying similar and dissimilar words, and then developing a sense of contextual appropriateness of a word. In the beginning, a person usually starts with a small dictionary of words, later (s)he builds up her dictionary over time; mentally mapping each newly learned word close to similar words in her dictionary. After this, a person eventually combines sentences in a sensible way, to write paragraphs and pages. Once a person is at this stage, (s)he is comfortable expressing complicated thoughts in a given language.\nDeep Learning in NLP An ANN (artificial neural network) has several stacked layers of neurons, usually accelerated in computation using GPUs. They are used in speech recognition and natural language processing, beating the previous domains such as language modelling, translation, speech recognition.\nWithin the broad domain of neural networks, there are certain kinds of neural networks that are more popular and well suited than others to address a variety of problems in NLP. While applying Deep Learning in NLP, each word in the sentence is translated into a set of numbers before being fed into the neural network. These numbers can change over a period of time while the neural net trains itself, encoding unique properties such as semantics and contextual information for each word. Thus, Deep Learning methodologies provide a better understanding of emotions of all demographics and text analytics.\nUse cases in NLP Deep Learning approaches have generated high-level performance results across many different NLP tasks. Take the example of search engines that have the ability of putting the world\u0026rsquo;s information at our fingertips. But, these search engines are still quite primitive when it comes to actually answering specific questions that are posted by people. Google has focused its efforts on implementing Deep Learning techniques in NLP systems. This has boosted the ability of NLP systems to easily recognize natural language questions, extract their meaning, and thereby provide correct answers. Even spam filters are using statistical Deep Learning techniques that help NLP systems in measuring occurrence of typical words that are repeated in a corpus of spam and non-spam emails.\nBenefits of this approach As Deep Learning models can be easily trained with a single end-to-end mode, they do not require traditional, task-specific feature engineering, thereby providing businesses with a number of advantages. Take a look at the three broad benefits of Deep Learning in NLP.\nDeep Learning can enhance several features of NLP, such as sentiment analysis that can help the company gain insight about the feelings of customers when they are interacting with the company. These insights, when included with those gained from behavior prediction, can help enterprises in providing enhanced services to their customers.\nThus, implementation of Deep Learning techniques in NLP can definitely increase customer loyalty for the company, automatically increasing the value of the business.\nYou Rock! ","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"a079f1a8872f7ca77737abb24af9cf03","permalink":"/post/nlp/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/post/nlp/","section":"post","summary":"Deep Learning applied to natural language processing  Language is the medium that humans use for conversing. Giving machines the ability to learn human language with natural language processing has given rise to several new products and possibilities that were not previously imaginable.\n NLP (Natural language processing) is one of the most important technologies present in the information age. Understanding complex language utterances is a crucial part of Ai.","tags":null,"title":"NLP applied in Deep Learning","type":"post"},{"authors":null,"categories":null,"content":" Here\u0026rsquo;s the app accompanying this blog\nThe code\nAs the Internet gets more and more popular, information overload poses an important challenge for a lot of online services. With all of the information pouring out from the web, users can be overwhelmed and confused as to what, exactly, they should be paying attention. A recommendation system provides a solution when a lot of useful content becomes too much of a good thing. A recommendation engine can help users discover information of interest by analyzing historical behaviors. More and more online companies — including Netflix, Google, Facebook, and many others — are integrating a recommendation system into their services to help users discover and select information that may be of particular interest to them.\nWith literally tens of thousands of hours of premium video content, users are also prone to content overload. Given the wide variety of content available on the service at any one time, it may be difficult for users to discover new video that best matches their historic interests. So the first goal of a Recommendation System is to help users find content which will be of interest to them.\nIn addition to users, recommendation system should also help content owners promote their video. Part of the mission is to deliver a service that users, advertisers, and content owners all unabashedly love. Having many different content partners, and understanding the content, these partners want users to watch their videos — especially when new videos are released. By using personal recommendation instead of more traditional recommendation systems, one promotes video content more effectively to users who are likely to enjoy the content being recommended.\nData Characteristics Before explaining the design of a recommendation system, its crucial to explain some characteristics of the data.\nSince a lot of content is comprised of episodes or clips within a show, by deciding to recommend shows to users instead of individual videos. Shows are a good method of organization, and videos in the same show are usually very closely related.\nContent can be mainly divided into two parts: on-air shows and library shows. On-air shows are highly important since more than half of streaming comes from them.\nAlthough on-air shows occupy a large part of the content, they are touched by a seasonal effect. During summer months, most of on-air shows do not air, causing on-air show streaming to decrease. Furthermore, there are fewer shows aired during weekends, thus the streaming of library shows will increase. Keeping this information in mind we can design a recommendation system to recommend more library shows to users during the weekend or summer months, as an example.\nThe key data that drives most recommendation systems is user behavior data. There are two main types of user behavior data: implicit user feedback data and explicit user feedback data.\n Explicit user feedback data primarily includes user voting data. Explicit feedback data can show a user\u0026rsquo;s preference on a show explicitly,\nImplicit feedback data includes information on users watching, browsing, searching, etc. Implicit feedback data does not show users preference of a show explicitly. For example, if a user gives a 5-star rating to a show, we know that this user likes the show very much. But if a user only watches a video from a show page or searches for a show, we don\u0026rsquo;t know whether this user likes the show.\n As the quantity of implicit data at far outweighs the amount of explicit feedback, the system should be designed primarily to work with implicit feedback data.\nArchitecture There are many different types of recommendation algorithms, and perhaps the most famous algorithm is collaborative filtering (CF). CF relies on user behavior data, and its main idea is to predict user preferences by analyzing their behaviors. There are two types of CF methods: user-based CF (UserCF) and item-based CF (ItemCF).\n UserCF assumes that a user will prefer items which are liked by other users who have similar preferences to that user.\nItemCF assumes that a user will prefer items similar to the assets (s)he preferred previously. ItemCF is widely used by many others (for example, Amazon and Netflix), as it has two main advantages. Firstly, it is suitable for sites where there are a lot more users than items. Secondly, ItemCF could easily explain recommendations given users’ historical behaviors. For example, if you have watched “Family Guy” it will recommend “American Dad” to you and tell you that it recommend this because you have watched “Family Guy”. Hence, ItemCF as our basic recommendation algorithm for our Movies Recommender System Application.\n Online Architecture Fig 1 shows an on-line architecture of the recommendation system. This system contains 5 main modules:\n User profile builder: When a user first comes into the recommendation system, it will first build a profile of them. The profile includes the user\u0026rsquo;s historical behaviors and topics, and these are generated from their old behaviors. Users can have many different types of behaviors. For example, they can watch videos, add shows to favorites, search for videos and vote on videos and shows. All these behaviors are all considered by the system and, after extracting all these behaviors, it uses a topic model which is trained offline to generate users\u0026rsquo; preference on topics. Recommendation Core: After generating the list of user\u0026rsquo;s historical preferences on shows and topics, it put all of those similar shows into raw recommendations. Filtering: For some pretty obvious reasons, raw recommendation results cannot be presented to users directly. The need to filter out shows the user has already seen or engaged with, thus increases the recommendations shows a little more precise. Ranking: The ranking module will re-rank raw recommendations to make them better fit users preferences. First, it will make recommendation more diverse. Then increase novelty of recommendations so that users will find shows they like, but have never seen before. Explanation: Explanation is one of the most crucial components of every recommendation system. The explanation module generates some reasoning for every recommendation result using the user\u0026rsquo;s historical behaviors. For example, it will recommend \u0026laquo;American Dad\u0026raquo; to a user who had previously watched \u0026laquo;Family Guy.\u0026raquo; The explanation will say, \u0026laquo;We recommend \u0026lsquo;American Dad\u0026rsquo; to you because you have watched \u0026lsquo;Family Guy\u0026rsquo;\u0026raquo;.  Offline Architecture In the above on-line architecture, some components rely on offline resources, such as the topic model, related model, feedback model, etc. The offline system is also an important part of the recommendation system. The offline system has these main components:\n Data Center: The data center contains all user behavior data. Some of them are stored in Hadoop clusters and some of them are stored in a relational database. Related Table Generator: The related table is an important resource for online recommendation. Using two main types of related table: one that\u0026rsquo;s based on collaborative filtering (which we\u0026rsquo;ll call CF), and another based on content.   In CF, show A and show B will have high similarity if users who like show A also like show B.\nWith content filtering, we use content information including title, description, channel, company, actor/actress, and tags.\n  Topic Model: A topic is represented by a group of shows that have similar content. Topics are thus larger in scope than shows, but they\u0026rsquo;re still smaller than channels. The topics are learned by LDA\u0026ndash;part of the dimension reduction family of algorithms, which is a popular topic model in machine learning. Feedback Analyzer: Feedback specifically means users\u0026rsquo; reactions to recommendation results. Using user feedback can improve recommendation quality. For example, say a show is recommended to many users, but most of them do not click this show. In that case, it will decrease the rank of this show. Users will also have different types of behavior, thus using all these behaviors in developing the recommendations. However, some users may prefer recommendations to come from their prior watch history, and some users may prefer their recommendations to come from their voting behavior. All these effects can be modeled offline by analyzing users\u0026rsquo; feedback on their recommendations. Report Generator: Evaluation is the most important part of the recommendation system. The report generator will generate a report including multiple metrics each day to show the quality of recommendations.  Algorithms So far, we\u0026rsquo;ve given a brief overview of the recommendation architecture. From previous discussion, we can see that a recommendation system is primarily based on ItemCF. By added many improvements on top of the ItemCF algorithm, too, in order to make it generate better recommendations. To test these improvements, we\u0026rsquo;ve performed many A/B tests on different algorithms. In following sections, we\u0026rsquo;ll introduce some of these algorithms and the experiment results.\nItem-based Collaborative Filtering Item-based Collaborative Filtering (ItemCF) is the basis of all our algorithms. In ItemCF, let N(u) be a set of items user u has preferred previously. User u\u0026rsquo;s preference on item i (i is not in N(u)) can then be measured by:\n$$p(u,i)=\\sum\\limits_{j\\in N(u)}^{} r(u,j)s(i,j)$$\nHere, r(u,j) is the preference weight of user u on show j, and s(i,j) is the similarity between show i and show j. In CF, the similarity between two shows is calculated by user behavior data on these two shows. Let N(i) be a set of users who watched show i and N(j) be a set of users who watched show j. Then, the similarity s(i,j) between show i and show j is calculated by following formula:\n$$s(i,j)=\\frac{\\left | N(i)\\cap N(j) \\right |}{\\sqrt{\\left | N(i) \\parallel N(j) \\right |}}$$\nIn this definition, show i will be highly relevant to show j if most users who watch show i will also watch show j. However, this definition will have the \u0026laquo;Harry Potter problem,\u0026raquo; which means that every show will have high relevance with popular shows.\nRecent Behavior The first lesson we learned from A/B testing is that recommendations should fit users\u0026rsquo; recent preference and that users\u0026rsquo; recent behavior is more important than their older, historical behaviors. So, in our engine, we will put more weight on users\u0026rsquo; recent behaviors. In our system, CTR of recommendations that originate from users\u0026rsquo; recent watch behavior is 1.8 times higher than CTR of recommendations originating from users\u0026rsquo; old watch behavior.\nNovelty Just because a recommendation system can accurately predict user behavior does not mean it produces a show that you want to recommend to an active user. For example, \u0026laquo;Family Guy\u0026raquo; is a very popular show, and thus most users have watched at least some episodes from this show. These users do not need us to recommend this show to them — the show is popular enough that users will decide whether or not to watch it by themselves.\nThus, novelty is also an important metric to evaluate recommendations. The first way we think can increase novelty is by revising ItemCF algorithm:\n First, we will decrease weight of popular shows that users have watched before. Then, we\u0026rsquo;ll put more weight on shows that are not only similar to shows the active user watched before, but also less popular than shows the active user watched before.  Explanation-Based Diversity Most users have diverse preferences, so the recommendation should also meet their diverse interests. In our system, we use explanations to diversify our recommendations. We think a diverse recommendation means most of the recommendation shows have different explanations.\nBy performing an A/B test to show the usefulness of diversification (shown in the above fig). The results of the experiment show that, for active users who had previously watched 10 or more shows, diversification can increase recommendation CTR significantly.\nTemporal Diversity A good recommendation system should not generate static recommendations. Users want to see new suggestions every time they visit the recommendation system. If a user has new behaviors, (s)he will find the recommendations have changed because due to more weight on the user\u0026rsquo;s recent behaviors. But if a user has no new behaviors, we also need to change our recommendations. We use three methods to keep temporal diversity of our system:\n Firstly, we\u0026rsquo;ll recommend recently-added shows to users. Many new shows are added each day, and it will suggest these shows to users who will like them. Thus, users will see fresh ideas for shows to watch when new ones are added. Secondly, we\u0026rsquo;ll randomize our recommendations. Randomization is the simplest way to keep recommendations fresh. Finally, we\u0026rsquo;ll decrease rank of recommendations which users have seen many times. This is called implicit feedback, and data show that CTR is increased by 10% after using this method.  Performance Of Recommendation Hub The recommendation hub is a personal recommendation page for each user. On this page users will see 6 carousels. The top carousel is \u0026laquo;top recommendations\u0026raquo;, which includes shows that we think users will prefer very much. After top recommendations, there are three carousels for three genres. These three genres are selected by analyzing users\u0026rsquo; historical preferences. The next carousel is bookmarks, which include shows that users have indicated they\u0026rsquo;d like to watch later. The last carousel is filled with shows that the user has already rated. This carousel is designed to collect more explicit feedback from users.\nBy performing an A/B test to compare our recommendation algorithms with two simple recommendation algorithms: Most Popular (which recommends the most popular shows to every user) and Highest Rated (which recommends highly-rated shows to each user). As shown in the above figure, experiment results show that the CTR of the algorithm is much higher than both simple methods.\nLessons Each user behavior can reflect user preferences. In our system, we use a slew of user behaviors to come up with our recommendations. We\u0026rsquo;ve computed the CTR of recommendations originating from different types of behaviors. As shown in Fig 3, we can see that recommendations from every type of behavior can generate recommendations that will be clicked by users.\nConclusion Explicit Feedback data is more important than implicit feedback data.\nAs shown in Fig 3, CTR of recommendations that originate from users\u0026rsquo; historically loved (voted 5 stars on shows) and liked (voted 4 stars on shows) behaviors is higher than CTR of recommendations that come from users\u0026rsquo; historical subscribe/watch/search behavior. So although the size our explicit feedback data is much smaller than implicit feedback data, they\u0026rsquo;re much more important.\n Recent behaviors are much more important than old behaviors.\n Novelty, Diversity, and offline accuracy are all important factors.\n  Most researchers focus on improving offline accuracy, such as RMSE, precision/recall. However, recommendation systems that can accurately predict user behavior alone may not be a good enough for practical use. A good recommendation system should consider multiple factors together. In our system, after considering novelty and diversity, the CTR has improved by more than 10%.\nYou Rock! ","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"8657831d585e56a257a327604939ecf1","permalink":"/post/moviesrecommender/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/post/moviesrecommender/","section":"post","summary":"Here\u0026rsquo;s the app accompanying this blog\nThe code\nAs the Internet gets more and more popular, information overload poses an important challenge for a lot of online services. With all of the information pouring out from the web, users can be overwhelmed and confused as to what, exactly, they should be paying attention. A recommendation system provides a solution when a lot of useful content becomes too much of a good thing.","tags":null,"title":"A Movies Recommender System","type":"post"},{"authors":null,"categories":null,"content":" Recently major plays in the technology industry have been talking about why the potential applications of artificial intelligence could be something we should be worried about. Their argument comes from two different places. On the one hand, they see Ai as one of the most fundamental transformative technologies that we have ever seen in the history of mankind, and on the other hand, that transformative power is something we should be scared of and be wary about. If Ai is transformative, then it has the power to be transformative both for good reasons as well as bad.\nHowever, fear of the unknown has always been the case with technology from the wheel to the internet. So, is Ai something we should be intimidated by? The fears of Ai seem to stem from a few common causes: general anxiety about machine intelligence, the fear of mass unemployment, concerns about super-intelligence, putting the power of Ai into the wrong people’s hands, and general concern and caution when it comes to new technology.\nGeneral Anxiety About Ai One of the most widespread fears of Ai is just general anxiety about it and what it’s potentially capable of. A recurring theme in movies and science fiction is Ai systems that go rogue - think HAL from 2001: A Space Odyssey or the Terminator movie series. People don’t like machines that get too smart, because we fear we can’t control it. This popular representation of Ai gone bad is causing a general wariness in the public surrounding the development of intelligent systems technologies. The fear is generally surrounding the unknown that as Ai systems are becoming more intelligent and human intelligence surrounding these systems is increasing, these two unknowns don’t really give us a clear direction for where things can go.\nHowever, just as we have examples of HAL and Terminator, we also have examples as C3PO and the computers from Star Trek. These are highly intelligent systems that are well within the control of humans. The future could be as great and benign as Star Trek if we had that perspective on the possibilities of intelligent machines. A good antidote to general anxiety is the realization that whenever human society has faced a major change or shift due to technological advances, humans have developed and adapted right along with it.\nFear of Ai: Ai is a Job Killer Another major fear of Ai is rooted in the idea of mass unemployment of human intelligence due to their replacement by Ai workers. A big concern is that in the previous wave of automation, it was mostly blue collar jobs like manufacturing oriented jobs that were automated away, but in this new wave, it will be mostly white collar service-oriented jobs that are based around knowledge workers that will bear the brunt of intelligent forms of automation. The need for trained human workers in many areas of the economy will go away as the use of Ai grows and increasingly permeates the business world. Ai also has an effect on blue collar workers such as delivery drivers, cab drivers and many more aspects of supply chain, logistics, and manufacturing. The fact is the technology is in place that 80% of any of these jobs can be done by machines that are smart enough, so the reality already exists.\nThe counterargument here is that some of these systems aren’t to a point where they can reliably replace many human jobs. While Ai systems provide a lot of capabilities, they simply can’t operate in a fully autonomous mode. In fact, most successful Ai implementations are being done such that the Ai is providing an augmented intelligence role, supporting the human at what they do best, and not fully replacing them. In general, as technology waves disrupt industries and workers, they replace job categories, but don’t take away overall jobs. In fact overall jobs continue to grow and find new niches while machines simply replace the old ways of doing things. Companies aren’t completely throwing things out that have been working for them. It’s a more general transition into the world of new technologies such as Ai. As is often said, Ai isn’t a job killer, it’s a job category killer.\nThe fact is that a lot of industries are already being disrupted by the advancement of technology and a lot of it has nothing to do with AI. Rather, it is due to automation and streamlining processes that make it easier and quicker to go about inputting work for ourselves and not relying on businesses and other organizations to be the middleman when it comes to getting things done.\nFear of Ai: Bad People Doing Bad Things Another common fear of Ai is that bad actors can do bad things when it comes to Ai. Leaders in Russia made a pronouncement that whoever leads the advancement of Ai is going to be one of the top rulers of the world. It is no surprise that countries are pouring significant amounts of investment and research into developing AI systems from everything to military advancement to intelligence systems that can influence the news. We can expect governments to continue to use Ai systems in ways that will make us increasingly uncomfortable in the ways they are applied to warfare, surveillance, law enforcement, and other purposes.\nYet, while we can expect governments and countries to compete with each other for Ai dominance, it’s not the governments we have to fear. After all, laws and governance are there to keep an eye on government behavior. We have more to fear from bad actors, criminals, and mischief makers using Ai technologies and bending them to their ill-conceived purposes. As Ai systems tend to learn from their creators, that can call into question the intention of the creator and those who are teaching the systems and what all they hope to accomplish. The fear is all stemming from the unknown. In addition, there hasn’t really been a strong counter argument as to what could be the best way to approach this particular scenario and what it means for our future.\nFear of Ai: The SuperIntelligence Probably the biggest fear of Ai making media waves is that of super intelligence that Ai will reach a point where it doesn’t care for or about the existence of humanity anymore, such as what happens with Skynet in the Terminator series of movies. The technology will get to a point where it can teach itself and improve and invent on its own, and instead of becoming a force for the betterment of humanity, humanity becomes a servant of technology. The fear is that our brains will just not be able to keep up with advancement, development and invention after a certain point because things will be moving way too fast.\nMachines could very well reach a point where they outstrip their human creators, and what will that mean for humanity when we reach that point? It makes us question what exactly intelligence is, and how we measure and define intelligence as concept for both humanity and machines, and how that new definition will fit into the world both now and moving forward. But all of this is assuming that systems can and will be able to achieve the goal of AGI (Artificial General Intelligence) and that we as a species or a society will not be able to put safeguards in place to keep the computers from reaching that point.\nThe big counterargument to all of this is that we are still much farther away from achieving AGI than we really think we are. While a lot of the technology is moving quickly to realizing goals of narrow Ai, there are parts that aren’t working particularly well. Data is still the cornerstone of Ai, and a lot of it is still messy and dirty — the Achilles Heel of Ai.\nAll of these fears boil down to the fact that we just don’t know where Ai is going and how soon it will take us to get there. Technology makes surprising and unusual leaps and bounds in ways we never think it will and things we think will take a while don’t. On the other hand things we thought would be here sooner aren’t there yet. It’s just a situation where we have to wait and see what comes.\nYou Rock! ","date":1572998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572998400,"objectID":"7c03e6f5ff2b41480f9d3923b45c4f6f","permalink":"/post/shoulditintimidateus/","publishdate":"2019-11-06T00:00:00Z","relpermalink":"/post/shoulditintimidateus/","section":"post","summary":"Recently major plays in the technology industry have been talking about why the potential applications of artificial intelligence could be something we should be worried about. Their argument comes from two different places. On the one hand, they see Ai as one of the most fundamental transformative technologies that we have ever seen in the history of mankind, and on the other hand, that transformative power is something we should be scared of and be wary about.","tags":null,"title":"Should Ai Intimidate Us?","type":"post"},{"authors":null,"categories":null,"content":" Retailers, big tech’s and fintechs are fighting back against cash-reward cards.\nAttending a recent outing with family and friends at a Yankees playoff game, I ordered a round of food and drinks and paid for it using cash back from my Apple credit card. It felt like we all got free hot dogs and beverages.\nLove of credit card rewards is as American as baseball, and in no other market do rewards hold as much sway as in the U.S., where consumers have been trained to believe that paying by plastic entitles them to a bonus either in cash or in points. The result has been that rewards spending by the top five credit card issuers grew to 31 billion in 2018, up from 11 billion dollars just three years earlier, according to new research from Accenture.\nBut the introduction of the cash-back Apple card may signal the peak of card rewards in the U.S. as the industry begins to deal with a less attractive volume-value trade-off.\nFor the moment, that trade-off is still working in favor of the payments industry, with North American bank payments revenue growing by 50 billion in the last three years to 300 billion dollars. That growth trend is expected to continue, with projected growth of 4% through 2025, creating a 405 billion dollars industry in North America.\nIf we look below the surface, North America is increasingly out of step with the rest of the global consumer payments industry, which is moving to high-volume, low-margin payments, and many of those are moving over account-to-account payment rails rather than over the card networks. This has been stimulated by the move toward real-time payments in many countries, but also by the rise of QR-based digital payments systems in Asia. Recent estimates indicate that it costs a merchant only 0.5% on average to accept an Alipay payment, while credit card payments in the U.S. can still be over 2% for many merchants.\nThe pressure to move toward lower-cost payments isn’t going to come from consumers, as they see the benefit but not the costs of the current card-centric system. Instead, pressure is going to come from the merchants, as it is their high acceptance costs that are funding the free flights and cash-back payments beloved by the North American consumer.\nWe are already seeing merchants begin to favor debit over credit as a lower-cost payment mechanism, and favoring their own loyalty schemes rather than relying on those run by the card-issuing banks. For instance, Irving, a large New England gas station retailer, takes six cents off each gallon if customers pay with debit cards linked to their loyalty account, using the delta between debit- and credit-processing fees to incentivize customers to use the lower-cost option.\nThe migration away from reward-rich credit cards will also be driven by two other factors. The first is the belated development of real-time payments in the U.S. and the opportunity it provides for merchants to incent lower-cost payments that will be even cheaper than debit transactions. In the Netherlands, the vast majority of online consumer payment transactions are now completed via Ideal, a bank-owned network that enables direct account-to-account payments that don’t use the traditional card networks. This is also the trend in Scandinavia, where account-to-account direct payments are becoming the preferred consumer payment mechanism.\nThe second major driver of change will be the continued internalization of payments by major retailers to avoid having to pay merchant acceptance fees at all. Leading this internalization effort are companies like Starbucks, Walmart, Uber and Amazon, all of whom are providing meaningful incentives for consumers to switch their payments to prepaid accounts in return for either discounts or internal reward points. Take, for example, Uber Cash, where you transfer money from your bank account directly into your Uber Cash account and use it to get a 5% discount on rides and food. Similarly, Walmart’s MoneyCard gives customers 1% to 3% back at stores, online, and at some gas stations. Amazon is taking this idea one step further by extending its Amazon Cash product to other retailers, including GameStop and 7-Eleven.\nIn the current interest-rate environment, retail deposits don’t earn much interest, and so customers don’t mind holding small sums of money across a variety of payment accounts, even if they are not insured. But even the shift of relatively small amounts of spending scaled across a large number of users will create deposit fragmentation that makes it harder for banks to make money through traditional balance-sheet spread. With tens of billions of dollars sitting in PayPal, Starbucks and Walmart accounts, this deposit fragmentation is one reason why consumer bank liabilities have not risen as quickly as would have been expected in a period of robust economic growth.\nAs consumer payments move inexorably toward being instant, invisible and free, the traditional bank-card issuers are going to lose revenue. Some of that pressure will come from retailers incenting alternative and cheaper payment methods, and depending on how politics play out over the next few years, some of it may come from European-style direct regulation of payment fees. While overall the volume-value trade-off is still positive, Accenture estimates that North American banks will lose 11% to 15% revenue market share in the next three years; over the long run, the volume-value trade-off will turn negative and the consumer payments revenue pool will begin to shrink.\nLong-term, North American banks need to start positioning themselves to take one of two paths. As fees continue to be driven lower, banks can compete in the low-margin, high-volume business of small payments and account-to-account transfers. In that case, banks need to focus on scale, and the evidence from Europe is that even as payment revenue per transaction falls, operating costs can fall even quicker, leading to higher profitability.\nAlternatively, payments players in North America can focus on adding value to payments transactions in the form of non-card-based credit options, micro-insurance services and other add-ons that customers are willing to pay for.\nAt least in the short run, the serotonin hit that comes from getting “free food” at a Yankees game will continue to support the North American rewards-rich cards ecosystem. But over time, the link between payment initiation method, line of credit and rewards will be severed as merchants find ways to channel consumer purchases to payment methods that benefit the merchants more than the banks. In that world, traditional card issuers are going to need to get creative about how they maintain their fair share of the payments revenue pool.\nYou Rock! ","date":1571788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571788800,"objectID":"94975423a2710bdaeaa55f59ad29df70","permalink":"/post/cardrewardpayments/","publishdate":"2019-10-23T00:00:00Z","relpermalink":"/post/cardrewardpayments/","section":"post","summary":"Retailers, big tech’s and fintechs are fighting back against cash-reward cards.\nAttending a recent outing with family and friends at a Yankees playoff game, I ordered a round of food and drinks and paid for it using cash back from my Apple credit card. It felt like we all got free hot dogs and beverages.\nLove of credit card rewards is as American as baseball, and in no other market do rewards hold as much sway as in the U.","tags":null,"title":"Cash Reward Payments","type":"post"},{"authors":null,"categories":null,"content":" Of the many downsides to the recent revival and popularity of Ai (Artificial Intelligence) one is that we see a lot of vendors, professional services firms, and end users jumping on the Ai bandwagon branding their technologies, products, service offerings, and projects as Ai products, projects, or offerings without it necessarily being the case. On the one side, there isn’t a well-accepted delineation between what is definitely Ai and what is definitely not Ai. This is because there isn’t a well-accepted and standard definition of what is artificial intelligence. Indeed, there isn’t a standard definition of intelligence, period.\nPerhaps it is best to start with the overall goals of what we’re trying to achieve with Ai, rather than definitions of what Ai is or isn’t. Since the origin of Ai in the 1950s, the goals of intelligent systems are those that mimic human cognitive abilities. This means the ability to perceive and understand its surroundings, learn from training and its own experiences, make decisions based on reasoning and thought processes, and the development of intuition in situations that are vague and imprecise; basically the world in which we live in. From a delineation perspective, it’s easy to classify the movements towards AGI (Artificial General Intelligence) as Ai initiatives. After all, AGI systems are attempting to create systems that have all the cognitive capabilities of humans, and then some. Therefore certainly all AGI initiatives as Ai initiatives.\nOn the flip side, simply automating things doesn’t make them intelligent. It may take time and effort to train a machine to understand the difference between an image of a cat and an image of a dog or even between different species of dogs, but that doesn’t mean that the system can understand what it is looking at, learn from its own experiences, and make decisions based on that understanding. Similarly, a voice assistant can process your speech when you ask it “What weighs more: a ton of carrots or a ton of peas?”, but that doesn’t mean that the assistant understands what you are actually talking about or the meaning of your words. So, can we really argue that these systems are intelligent?\nIn a recent interview with MIT Professor Luis Perez-Breva, he argues that while these various complicated training and data-intensive learning systems are most definitely Machine Learning capabilities, that does not make them Ai capabilities. In fact, he argues, most of what is currently being branded as Ai in the market and media is not Ai at all, but rather just different versions of machine learning where the systems are being trained to do a specific, narrow task, using different approaches, of which Deep Learning is currently the most popular. He argues that if you’re trying to get a computer to recognize an image just feed it enough data and with the magic of math, statistics and neural nets that weigh different connections more or less over time, you’ll get the results you would expect. But what you’re really doing is using the human’s intelligence of what the image is to create a large dataset that can then be mathematically matched against inputs to verify what the human understands.\nHow Machine Learning relate to AI? The view espoused by Professor Perez-Breva is not isolated or outlandish. In fact, when you dig deeper into these arguments, it’s hard to argue that the narrower the ML task, the less Ai it in fact is. However, does that mean that ML doesn’t play a role at all in Ai? Or, at what point can you say that a particular machine learning project is an Ai effort in the way we discussed above? If you read the Wikipedia description of Ai, it will tell you that, as of 2017, the industry generally accepts that “successfully understanding human speech, competing at the highest level in strategic game systems, autonomous cars, intelligent routing in content delivery network and military simulations” can be classified as Ai systems.\nThe line between intelligence and just math or automation is a tricky one. If you decompose any intelligent system, even the eventual end goal of AGI, it will look just like bits and bytes, neural networks, decision-trees, lots of data, and mathematical algorithms. Similarly, if you decompose the human brain, it’s just a bunch of neurons firing electrochemical pathways. Are humans intelligent? Are zebras intelligent? Is bacteria intelligent? Where’s the delineation between intelligence in living organisms? Perhaps intelligence is not truly a well-defined thing, but rather an observation of the characteristics of a system that exhibit certain behaviors. In this light, one of those behaviors is understanding and perceiving its surroundings, and another of those is learning from experiences and making decisions based on those experiences. In this light, ML definitely forms a part of what is necessary to make Ai work.\nOver the past 60+ years there have been many approaches and attempts to get systems to learn to understand its surroundings and learn from its experiences. These approaches have included decision trees, association rules, artificial neural networks of which Deep Learning is one such approach, inductive logic, support vector machines, clustering, similarity and metric learning including nearest-neighbor approaches, Bayesian networks, reinforcement learning, genetic algorithms and related evolutionary computing approaches, rules-based machine learning, learning classifier systems, sparse dictionary approaches, and more. For the layperson, we want to stress that Ai is not interchangeable for ML and certainly ML is not interchangeable with Deep Learning. But ML supports the goals of Ai, and Deep Learning is one way to do certain aspects of ML. Or to put it another way, doing machine learning is necessary, but not sufficient, to achieve the goals of Ai, and Deep Learning is an approach to doing ML that may not be sufficient for all ML needs.\nWhat Parts of AI are not Machine Learning? It’s an interesting exercise to think about how we, as humans, have gained the intelligence that we have now. In some instances, we learned from simply being part of our environment such as learning how gravity works, how to speak to others and understand what they are saying, and cultural norms. In other instances, we learn in a teaching environment from instructors who knew a particular abstract subject area such as math or physics. In yet other instances we learn from repeating a particular task over and over again to get better at that task, such as music or sports. From an Ai perspective, these are just different kinds of learning, and therefore, different machine learning strategies. Supervised learning for being taught how to do things. Unsupervised learning when you’re learning from observing the world. Reinforcement learning when you’re learning by trial and error. Therefore, doesn’t it make sense that all forms of machine learning should be considered Ai? What else could there be?\nSome say machine learning is a form of pattern recognition, understanding when a particular pattern occurs in nature or experience or through senses, and then acting on that pattern recognition. When you look at it from that perspective, it becomes clear that the learning part must be paired with an action part. Decisions and reasoning is not just applying the same response to the same patterns over and over again. If that was the case, then all we’re doing is using ML to simply automate better. Given the same inputs and feedback, the robot will perform the same action. But do humans really work that way? We experiment with different outcomes. We weigh alternatives. We respond differently when we’re stressed than when we’re relaxed. We prioritize. We think ahead and think about the potential outcomes of a decision. We play politics and we don’t always say what we want to say. And the big one: we have emotions. We have self-consciousness. We have awareness. All of these things move us beyond the task of learning into the world of perceiving, acting, and behaving. These are the frontiers of Ai.\nMoving Threshold of Intelligence In reading this piece, you’re actually yourself thinking and learning about ML and Ai, the relationships to each other, and whether or not specific ML activities are accomplishing the goals of what we aim to achieve in Ai. Likewise, even for those at the extremes of the Ai spectrum considering only AGI to be truly Ai or on the other polar opposite that consider any application of ML to be Ai, the truth lies somewhere in the middle. Some machine learning initiatives are more like automation and application of formulas that can’t continuously evolve or respond to change, while other machine learning efforts are closer to intelligence, which can change and adapt over time with experience, improving at their task or desired outcome.\nConclusion The tech industry continues to iterate on ML and address problems previously considered to be more complicated and difficult. As the collection of ML activities mature, while some are definitely not Ai-like at all or particularly intelligent, others are progressing the industry down the path of Ai. Eventually we’ll start to see the sort of technology evolution that has long been the goal of Ai.\nYou Rock! ","date":1571011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571011200,"objectID":"ec309c959a0afdac8d9cc3ada02cb9f9","permalink":"/post/mlai/","publishdate":"2019-10-14T00:00:00Z","relpermalink":"/post/mlai/","section":"post","summary":"Of the many downsides to the recent revival and popularity of Ai (Artificial Intelligence) one is that we see a lot of vendors, professional services firms, and end users jumping on the Ai bandwagon branding their technologies, products, service offerings, and projects as Ai products, projects, or offerings without it necessarily being the case. On the one side, there isn’t a well-accepted delineation between what is definitely Ai and what is definitely not Ai.","tags":null,"title":"Machine Learning Relationship with Ai","type":"post"},{"authors":null,"categories":null,"content":" Data Science Life Cycle Data science is quickly evolving to be one of the hottest fields in the technology industry. With rapid advancements in computational performance that now allow for the analysis of massive datasets, we can uncover patterns and insights about user behavior and world trends to an unprecedented extent.\nWith the influx of buzzwords in the field of data science and relevant fields, a common question I’ve heard from friends is “Data science sounds pretty cool — how do I get started?” And so what started out as an attempt to explain it to a friend who wanted to get started with Kaggle projects has culminated in this post. I’ll give a brief overview of the seven steps that make up a data science lifecycle — business understanding, data mining, data cleaning, data exploration, feature engineering, predictive modeling, and data visualization. For each step, I will also provide some resources that I’ve found to be useful in my experience. As a disclaimer, there are countless interpretations to the lifecycle (and to what data science even is), and this is the understanding that I have built up through my reading and experience so far. Data science is a quickly evolving field, and its terminology is rapidly evolving with it. If there’s something that you strongly disagree with, I’d love to hear about it!\n 1. Business Understanding The data scientists in the room are the people who keep asking why’s. They’re the people who want to ensure that every decision made in the company is supported by concrete data and that it is guaranteed (with a high probability) to achieve results. Before you can even start on a data science project, it is critical that you understand the problem you are trying to solve.  According to Microsoft Azure’s blog, we typically use data science to answer five types of questions:\n How much or how many? (regression) Which category? (classification) Which group? (clustering) Is this weird? (anomaly detection) Which option should be taken? (recommendation)\n In this stage, you should also be identifying the central objectives of your project by identifying the variables that need to be predicted. If it’s a regression, it could be something like a sales forecast. If it’s a clustering, it could be a customer profile. Understanding the power of data and how you can utilize it to derive results for your business by asking the right questions is more of an art than a science, and doing this well comes with a lot of experience. One shortcut to gaining this experience is to read what other people have to say about the topic, which is why I’m going to suggest a bunch of books for getting started.\n Get started: Data Science for Business Everybody Lies: Big Data, New Data, and What the Internet Can Tell Us About Who We Really Are\n  2. Data Mining Now that you’ve defined the objectives of your project, it’s time to start gathering the data. Data mining is the process of gathering data from different sources. Some people tend to group data retrieval and cleaning together, but each of these processes is such a substantial step that I’ve decided to break them apart. At this stage, some of the questions worth considering are — what data do I need for my project? Where does it live? How can I obtain it? What is the most efficient way to store and access all of it?  If all the data necessary for the project is packaged and handed to you, you’ve won the lottery. More often than not, finding the right data takes both time and effort. If the data lives in databases, your job is relatively simple — you can query the relevant data using SQL queries, or manipulate it using a dataframe tool like Pandas. However, if your data doesn’t actually exist in a dataset, you’ll need to scrape it. Beautiful Soup is a popular library used to scrape web pages for data. If you’re working with a mobile app and want to track user engagement and interactions, there are countless tools that can be integrated within the app so that you can start getting valuable data from customers. Google Analytics, for example, allows you to define custom events within the app which can help you understand how your users behave and collect the corresponding data.\n Get started: MySQL, Beautiful Soup, Google Analytics for Firebase\n  3. Data Cleaning Now that you’ve got all of your data, we move on to the most time-consuming step of all — cleaning and preparing the data. This is especially true in big data projects, which often involve terabytes of data to work with. According to interviews with data scientists, this process (also referred to as ‘data janitor work’) can often take 50 to 80 percent of their time. So what exactly does it entail, and why does it take so long?  The reason why this is such a time-consuming process is simply that there are so many possible scenarios that could necessitate cleaning. For instance, the data could also have inconsistencies within the same column, meaning that some rows could be labeled 0 or 1, and others could be labeled yes or no. The data types could also be inconsistent — some of the 0s might be integers, whereas some of them could be strings. If we’re dealing with a categorical data type with multiple categories, some of the categories could be misspelled or have different cases, such as having categories for both male and Male. This is just a subset of examples where you can see inconsistencies, and it’s important to catch and fix them in this stage. One of the steps that are often forgotten in this stage, causing a lot of problems later on, is the presence of missing data. Missing data can throw a lot of errors in the machine learning model creation and training. One option is to either ignore the instances which have any missing values. Depending on your dataset, this could be unrealistic if you have a lot of missing data. Another common approach is to use something called average imputation, which replaces missing values with the average of all the other instances. This is not always recommended because it can reduce the variability of your data, but in some cases it makes sense.\n Get started: Pandas, Dplyr, Cleaning Dirty Data\n  4. Data Exploration Now that you’ve got a sparkling clean dataset, you’re ready to finally get started in your analysis. The data exploration stage is like the brainstorming of data analysis. This is where you understand the patterns and bias in your data. It could involve pulling up and analyzing a random subset of the data using Pandas, plotting a histogram or distribution curve to see the general trend, or even creating an interactive visualization that lets you dive down into each data point and explore the story behind the outliers.  Using all of this information, you start to form hypotheses about your data and the problem you are tackling. If you were predicting student grades, for example, you could try visualizing the relationship between grades and sleep. If you were predicting real estate prices, you could perhaps plot the prices as a heat map on a spatial plot to see if you can catch any trends. There is a great summary of tools and approaches on the Wikipedia page for exploratory data analysis.\n Get started: Exploratory Data Analysis\n  5. Feature Engineering In machine learning, a feature is a measurable property or attribute of a phenomenon being observed. If we were predicting the grades of a student, a possible feature is the amount of sleep they get. In more complex prediction tasks such as character recognition, features could be histograms counting the number of black pixels. According to Andrew Ng, one of the top experts in the fields of machine learning and deep learning, “Coming up with features is difficult, time-consuming, requires expert knowledge. ‘Applied machine learning’ is basically feature engineering.” Feature engineering is the process of using domain knowledge to transform your raw data into informative features that represent the business problem you are trying to solve. This stage will directly influence the accuracy of the predictive model you construct in the next stage. We typically perform two types of tasks in feature engineering — feature selection and construction. Feature selection is the process of cutting down the features that add more noise than information. This is typically done to avoid the curse of dimensionality, which refers to the increased complexity that arises from high-dimensional spaces (i.e. way too many features). I won’t go too much into detail here because this topic can be pretty heavy, but we typically use filter methods (apply statistical measure to assign scoring to each feature), wrapper methods (frame the selection of features as a search problem and use a heuristic to perform the search) or embedded methods (use machine learning to figure out which features contribute best to the accuracy). Feature construction involves creating new features from the ones that you already have (and possibly ditching the old ones). An example of when you might want to do this is when you have a continuous variable, but your domain knowledge informs you that you only really need an indicator variable based on a known threshold. For example, if you have a feature for age, but your model only cares about if a person is an adult or minor, you could threshold it at 18, and assign different categories to instances above and below that threshold. You could also merge multiple features to make them more informative by taking their sum, difference or product. For example, if you were predicting student grades and had features for the number of hours of sleep on each night, you might want to create a feature that denoted the average sleep that the student had instead.   Get started: Introduction to Feature Selection Methods, Feature Selection with sklearn, Best Practices for Feature Engineering\n  6. Predictive Modeling Predictive modeling is where machine learning finally comes into your data science project. I use the term predictive modeling because I think a good project is not one that just trains a model and obsesses over the accuracy, but also uses comprehensive statistical methods and tests to ensure that the outcomes from the model actually make sense and are significant. Based on the questions you asked in the business understanding stage, this is where you decide which model to pick for your problem. This is never an easy decision, and there is no single right answer. The model (or models, and you should always be testing several) that you end up training will be dependent on the size, type and quality of your data, how much time and computational resources you are willing to invest, and the type of output you intend to derive. There are a couple of different cheat sheets available online which have a flowchart that helps you decide the right algorithm based on the type of classification or regression problem you are trying to solve. The two that I really like are the Microsoft Azure Cheat Sheet and SAS Cheat Sheet.  Once you’ve trained your model, it is critical that you evaluate its success. A process called k-fold cross-validation is commonly used to measure the accuracy of a model. It involves separating the dataset into k equally sized groups of instances, training on all the groups except one, and repeating the process with different groups left out. This allows the model to be trained on all the data instead of using a typical train-test split.\nFor classification models, we often test accuracy using percent correct classification (PCC), along with a confusion matrix which breaks down the errors into false positives and false negatives. Plots such as ROC curves, which is the true positive rate plotted against the false positive rate, are also used to benchmark the success of a model. For a regression model, the common metrics include the coefficient of determination (which gives information about the goodness of fit of a model), mean squared error (MSE), and average absolute error.\n Get started: Machine Learning Udacity Course, Essentials of Machine Learning Algorithms, Evaluating Machine Learning Models\n  7. Data Visualization Data visualization is a tricky field, mostly because it seems simple but it could possibly be one of the hardest things to do well. That’s because data viz combines the fields of communication, psychology, statistics, and art, with an ultimate goal of communicating the data in a simple yet effective and visually pleasing way. Once you’ve derived the intended insights from your model, you have to represent them in a way that the different key stakeholders in the project can understand.  Again, this is a topic that could be a blog post on its own, so instead of diving deeper into the field of data visualization, I will give a couple of starting points. I personally love working through the analysis and visualization pipeline on an interactive Python notebook like Jupyter, in which I can have my code and visualizations side by side, allowing for rapid iteration with libraries like Seaborn and Matplotlib. Tools like Tableau and Plotly make it really easy to drag-and-drop your data into visualization and manipulate it to get more complex visualizations. If you’re building an interactive visualization for the web, there is no better starting point than D3.js.\n Get started: An Overview of Every Data Visualization Course on the Internet, Tableau, Plotly, Seaborn, Bokeh, D3.js\n  8. Business Understanding Phew. Now that you’ve gone through the entire lifecycle, it’s time to go back to the drawing board. Remember, this is a cycle, and so it’s an iterative process. This is where you evaluate how the success of your model relates to your original business understanding. Does it tackle the problems identified? Does the analysis yield any tangible solutions? If you encountered any new insights during the first iteration of the lifecycle (and I assure you that you will), you can now infuse that knowledge into the next iteration to generate even more powerful insights and unleash the power of data to derive phenomenal results for your business or project.  What’s to do once you have your Data Science Pipeline set? The last step of the build phase is executing the build plan for the product. Most software engineers are probably familiar with the trials and tribulations of building a complicated piece of software, but they may not be familiar with the difficulty of building software that deals with data of dubious quality. Statisticians, on the other hand, know what it’s like to have dirty data but may have little experience with building higher-quality software. Likewise, individuals in different roles relating to the project, each of whom might possess various experiences and training, will expect and prepare for different things. If you’re a statistician, you know dirty data, and you know about bias and overstating the significance of results. On the other hand, you may not have much experience building software for business, particularly production software. You should consult software engineers with hands-on experience to learn how to improve your software’s robustness.\nIf you’re a software engineer, you know what a development lifecycle looks like, and you know how to test the software before deployment and delivery. But you may not know about data and no matter how good you are at software design and development, data will eventually break your application in ways that had never occurred to you. This requires new patterns of thought when building software and a new level of tolerance for errors and bugs because they’ll happen that much more often. You should consult statisticians who are well versed in foreseeing and handling problematic data such as outliers, missing values, and corrupted values.\nIf you’re starting out in data science, without much experience in statistics or software engineering, anyone with some experience can probably give you some solid advice if you can explain your project and your goals to them. As a beginner, you have a double duty at this stage of the process to make up for lack of experience.\nIf you’re merely one member of a team for the purposes of this project, communication and coordination are paramount. It isn’t necessary that you know everything that’s going on within the team, but it is necessary that goals and expectations are clear and that someone is managing the team as a whole.\nThe plan should contain multiple paths and options, all depending on the outcomes, goals, and deadlines of the project. No matter how good a plan is, there’s always a chance that it should be revised as the project progresses. Even if you thought of all uncertainties and were aware of every possible outcome, things outside the scope of the plan may change. The most common reason for a plan needing to change is that new information comes to light, from a source external to the project, and either one or more of the plan’s paths change or the goals themselves change.\nAs a project progresses, you usually see more and more results accumulate, giving you a chance to make sure they meet your expectations. Generally speaking, in a data science project involving statistics, expectations are based either on a notion of statistical significance or on some other concept of the practical usefulness or applicability of those results or both. Statistical significance and practical usefulness are often closely related and are certainly not mutually exclusive. As part of your plan for the project, you probably included a goal of achieving some accuracy or significance in the results of your statistical analyses. Meeting these goals would be considered a success for the project.\nConclusion Data science still carries the aura of a new field. Most of its components — statistics, software development, evidence-based problem solving, and so on — descend directly from well-established, even old fields, but data science seems to be a fresh assemblage of these pieces into something that is new. The core of data science doesn’t concern itself with specific database implementations or programming languages, even if these are indispensable to practitioners. The core is the interplay between data content, the goals of a given project, and the data-analytic methods used to achieve those goals.\nYou Rock! ","date":1570752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570752000,"objectID":"5ab5b5c334d884c20f59e1fb8bdfc46e","permalink":"/post/ds/","publishdate":"2019-10-11T00:00:00Z","relpermalink":"/post/ds/","section":"post","summary":"Data Science Life Cycle Data science is quickly evolving to be one of the hottest fields in the technology industry. With rapid advancements in computational performance that now allow for the analysis of massive datasets, we can uncover patterns and insights about user behavior and world trends to an unprecedented extent.\nWith the influx of buzzwords in the field of data science and relevant fields, a common question I’ve heard from friends is “Data science sounds pretty cool — how do I get started?","tags":null,"title":"Data Science Life Cycle","type":"post"},{"authors":null,"categories":null,"content":" One of the biggest challenges with artificial intelligence and data science is the integrity of our data. Even if we did all the right things in our models, and our testing, data might conform to some technical standard of “cleanliness,” there might still be biases in our data as well as “common sense” challenges. With Big Data, it is difficult to get to a certain granularity of data validity without proper real-world testing. By real-world testing, I mean that when data is being used to make decisions, as consumers, as testers, as programmers, as data scientists, we look at groups of scenarios to see if the decisions made conform to a kind of “common sense” standard. This is when we discover the most important biases in our data. It is also when we discover the real impact of the decisions made by our Ai systems.\nAI Impact On Systems With the speedy proliferation of Ai technology, it’s not hard to see where the impact of Ai systems may lie, such as in countries like China which is aggressively implementing a Social Credit System based on artificial intelligence. Ai systems’ decisions will drive the citizen’s ability to travel, receive government services, take out loans, and receive an education. It will also drive the corporation’s ability to conduct business, obtain capital, and make a profit. Needless to say, in such a system, the impact is immense. In the United States, Ai systems are implemented by corporations to gain cost savings, efficiency, and competitive advantage over the competition. These types of Ai systems tend to work alongside humans such as portfolio management systems that execute automatic trading strategies, Ai-assisted surgery, and Ai-assisted medical diagnosis, etc. When Ai systems start to make judgments about a person’s quality of life independently without checks and balances such as in the case of Ai systems that monitor student’s emotions in classrooms to gauge engagement, and Ai systems that make decisions on whether someone should be incarcerated, issues such as bias in data and privacy should involve lengthy questions from our legal, political systems, our media, and society. The reason for such scrutiny is precisely because these Ai systems can easily impact someone’s life and infringe on someone’s liberties as defined by our Constitution.\nWhy Bias Is Important For AI? When we discuss artificial intelligence and machine learning, we are mostly talking about the widely used Deep Learning algorithms that use Neural Networks to learn from data generated by humans. This type of algorithm learns from human-generated data. This information may be collected from real life through social media. In the process of collecting such data, there can be many biases such as data collection, cognitive, social, and algorithmic, etc. Algorithms are replicating our decision-making process by learning from the data. Inherently, this data is not objective. Historic data can contain biases, human emotions, interests, and perspectives. Hence, Ai’s decisions are subjective. An objective decision is one that is not influenced by one’s personal feelings, perspectives, interests, and biases.\nTypes Of Data That Impacts Ai’s Decision Process The types of data that an Ai system learns from drives the subjectivity of the decisions made by the Ai system. For instance, if an Ai system is simply a system that learns from MRI images of the human body to look for a specific tumor, then this system is likely more objective than an Ai system that learns from social media tweets to identify trolls. With “common sense,” we can see that image data taken objectively by an MRI machine is more objective than tweets from people responding to events. The source of the data that the Ai system learns from introduces the bias.\nOne of the biggest biases that may be introduced from subjective information is the social context of the data. In an Ai that is used to analyze tweets, each tweet comes with it not only the author’s opinions, it also carries with it the context in which the tweet was written. For instance, the tweets that the author might have read before inking the tweet in question will alter the meaning of the tweet. Another example is the concept of dark humor. Dark humor can be perceived as negative comments on social media. Identifying humor is very subjective and based on the context of the text. If taken out of context, “dark humor” using words with negative connotations can be perceived as harassment.\nAbility To Forget Is Central To Solve Ai’s Bias In the land of biased data, we, humans have the unique ability to forget. This ability allows us to forget about past events that are anomalies in favor of new norms established. This ability allows us to forget about our perceived biases when new values are learned and internalized. This ability to forget allows us to become “better” humans. Ai is not so lucky. It does not possess this ability to forget. AI are created to learn. It will learn as much as it can. This means that the inherent biases introduced by data within the machine will stay there. Even though new information acquired can cause the system to place less importance on the data, it is still, nevertheless, there. It can still affect the outcomes of decisions made under certain conditions. When certain decisions that may place a higher emphasis on biased information, without adequate checks and balances, these types of decisions will not be reliable.\nAbility To Make “fair” Judgments Is Central To Solve Ai’s Bias If an Ai is used to decide on the mental health of an individual without checks and balances, the diagnosis can be made with biases. Mental Health diagnoses often need multiple professionals to confirm. Mental Health diagnosis is usually made with data that’s not only subjective but also can contain a myriad of social contexts. A Mental Health diagnosis can potentially impact a humnan’s employment and quality of life. It needs to be made with caution. The question becomes how do we establish fairness in Ai systems’ treatment of data? The question of “fairness” is often based on a line that is drawn based on established norms. By questioning our established norms, we can question our perceived “fairness” we use to judge the effectiveness of Ai systems’ decisions.\nSuccessful Ai Have One Of Two Notable Features Due to the biases inherent in today’s Ai systems, the systems that are highly effective in the market place have one of the two notable features:\n The system uses observed data that is inherently highly objective. The system’s decisions don’t have a critical impact on human’s lives.  Companies that are utilizing Ai based on highly subjective data are now trying to establish processes and procedures to check and balance the decisions made by the Ai systems. Researchers, on the other hand, are trying to develop more sophisticated methods for Ai to “unlearn” data, to detect “context,” and internalize norms. These combined efforts will allow us to understand individual cases of biases related to particular Ai systems usage.\nHow “Common Sense” Can Help? Ai systems that are using highly “subjective” data, testing the data with real-world scenarios means injecting “common sense” into decision making. Humans have the unique ability to process data using our cognitive mechanisms, and emotional mechanisms to gain unparalleled understanding. Through the process of understanding, we are discarding unwanted information, focusing on important information, putting information into social context, and injecting needed ethical boundaries to simplify information to make “common sense” decisions. Ai is attempting to replicate the human process of decision making, but it is only able to replicate some of this process. This is when “common sense” testing of real-world scenarios will be helpful. When groups of possible biases related to outcomes are identified for review based on real-world scenarios, it is much easier to understand the places where Ai needs improvement before being able to function without human checks and balances.\nFor instance, in an AI system used to make judgments about whether to identify possible criminals for scrutiny, there might be “common sense” testing involving “observed” data in daily life. When a minority teenager who just turned 18 years old is identified by the system as being a possible criminal, the system looked at the fact that this teenager lives in a housing project rampant with crime, the teenager’s mother has substance abuse issues, the teenager is of African American descent from a low-income family, the teenager goes to a school with high crime rate and low graduation rate, and the teenager’s brother has a long criminal record. The only positive factor in this minority teenager’s life is that this teenager maintains all A’s at school and spent most of his life in his grandmother’s house. The grandmother taught this teenager to play the violin and enriches the teenager’s life beyond his current circumstances. But, since the negative factors in this teenager’s life outweigh the positive factors, this teenager was identified as a possible criminal. “Common sense” might suggest an additional evaluation of this teenager’s life by an objective bystander. If a bystander just randomly went up to the teenager and spent an afternoon talking to the teenager, this bystander will see that the teenager is well-mannered, has aspirations, is focused on studies, and is actively working toward a better life. In this case, the “common sense” judgment can add additional positive factors to be considered in this teenager’s case. These positive factors will help the system make a more informed decision in this teenager’s case. From the testing, we can see that the Ai system lacks other critical information that might need to be considered such as behaviors inside the school, outside the school, reality of living arrangements, and social circles. Even though our “common sense” testing added another layer of complexity to the data, it also allowed for a better decision to be made. It may not be trivial to obtain this information with Ai systems, it may be trivial to obtain this information by a human. If the person obtaining the information is “objective enough”, then the additional layer of “common sense” checks has made Ai’s ultimate decision in this case much more well-rounded and less biased. Not every case, every criteria should be evaluated by Ai in a system assisted by Ai. Certain criterias evaluated by humans do not taint the decisions by Ai, rather they help to give the Ai a more well-rounded picture.\nResponsibility Of Lawmakers When Ai system implementations can contain many biases and inject “common sense” testing is not trivial, resources needed can quickly multiply on Ai projects. The lawmaker’s job is not to put limits on the proliferation of Ai systems. The lawmakers become directors. The lawmakers can direct the trend of proliferation of Ai to safeguard an individual’s liberties as defined by our Constitution. By placing specific “regulations” to delay certain aspects of Ai systems proliferations in certain industries that use highly “subjective” data to make decisions that have big impact on people’s lives rather than outright “bans,” lawmakers will allow researchers more time to catch up on developing Ai technologies, lawmakers will also allow corporations to put real-life scenarios into place to evaluate the Ai systems thoroughly with “common sense.” In this case, to be responsible and to safeguard our liberties in the age of Ai means setting standards in industries for testing with real-life scenarios that will inject “common sense” into the decision process.\nConclusion Ai bias is difficult to overcome. But it is a joint effort of corporations, researchers, lawmakers, media, and society. When many eyes are on the issues, we may have a lot more data, opinions about the data, and judgments passed, but with “common sense,” we come closer to equality, to human kindness, and to the protection of our constitutional liberties. That to me is an opportunity to exercise our democratic process.\nYou Rock! ","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"8f1c2f28592d3c6ac0110c5e218d3915","permalink":"/post/biasadded/","publishdate":"2019-10-10T00:00:00Z","relpermalink":"/post/biasadded/","section":"post","summary":"One of the biggest challenges with artificial intelligence and data science is the integrity of our data. Even if we did all the right things in our models, and our testing, data might conform to some technical standard of “cleanliness,” there might still be biases in our data as well as “common sense” challenges. With Big Data, it is difficult to get to a certain granularity of data validity without proper real-world testing.","tags":null,"title":"Ai Common Sense","type":"post"},{"authors":null,"categories":null,"content":" The Ai Hype Train Has Left the Station We know we’re in the middle of a hype cycle when every industry has products that use some buzzword, regardless of whether or not they have the technical merit or legitimacy to actually use that buzzword. This is where we currently are with regards to the market for Ai (Artificial Intelligence) products and services.\nWe hear briefings from vendors pitching technology across a wide range of solution areas, as well as from professional services and government contracting firms with new solution offerings and end users looking for guidance and advice. On the whole, we’ve seen a lot of great, new innovation that’s pushing the industry forward towards more intelligent, autonomous systems capable of addressing more of the challenge areas that have previously not been able to be solved due to extreme complexity or the need for human labor.\nHowever, we’ve also seen more than our fair share of vendors, so-called Ai Demo Days, customer engagements, and service pitches that claim to be Ai-enabled when all they have done is put some thin capability (usually a third-party library or API) that somehow miraculously transforms their automation or analytics or cybesecurity product of yesterday into some new-fangled Ai miracle. We scratch our heads and wonder if this new offering is making anything truly intelligent or is this just the same old product with primarily the same old features sold to the same old customers providing the same old benefits with some new thing added on. Recently, we spent time examining an entire category of product solution that has been known to call itself plainly an automation offering, robotic in fact. This whole category is also currently attempting to rebrand itself as intelligent and Ai-enabled because they’ve added OCR or some other add-on. Baloney. Automation is not intelligence.\nWhat is Automation? Let’s be clear – automation is not a bad word. In fact, the whole movement of the industrial revolution was to take much of what humanity was doing at the time and automate it so that we could achieve\n significantly greater productivity quality of life, and transformation of society through technology  The steam engine, train, factory line, and then computers and the internet have truly revolutionized the way we work, live, and exist. However, these are not intelligent technologies. We can’t walk up to a steam engine and ask it to recognize who we are or answer a random question or even learn from its experiences. A web server is just a web server no matter how many times it’s served the same content to the same sort of people.\n Automation is the process of applying technology to some repeatable task or process so that the task or process can be accomplished with predictable repeatability, lower total cost of operation, increase safety, and provide better efficiency.\n This is what we demand of most of our technology, and technology has delivered that value. In fact, technology continues to deliver increasingly greater value to enterprises and individuals, squeezing more efficiency and capabilities and increasing productivity on a daily basis. So, automation is good. There’s nothing bad about it.\nWhat is Intelligence? However, we demand more from intelligence than simply automation. From the beginnings of what researchers have been attempting to do with Ai, we’ve been striving for systems that can understand and comprehend their surroundings, learn from their experiences, make judgements and decisions that are based on rational thinking, handle new situations and apply their learning from previous experience, and perhaps even address bigger questions of self-awareness, consciousness, and more. These are not easy problems Ai researchers are trying to solve, and are fundamental questions of what does it mean to perceive, understand, rationalize, and be aware.\nThe talk about the difference between narrow applications of Ai to single problem areas versus the strong / broad application of Ai which attempts to create a generally intelligent system. Clearly we’re nowhere near the goal of an AGI (Artificial General Intelligence), and so it would follow that all of the innovations in the market currently are narrow (some say weak, but we don’t) applications of Ai. Yet, that doesn’t mean that these narrow applications are any less Ai solutions. Image recognition, applications that reason with self-learning, and more are building the techniques we need to achieve general intelligence by creating solutions that apply theories of intelligence and learning to solve those problems. While they may be narrow, they are still intelligent and thus qualify as Ai solutions. But no one would argue that Facial Recognition is automation. Because intelligence is not automation.\nAsk More Questions Adding OCR or voice recognition of NLP (natural language processing) doesn’t make a system intelligent. Sure, the NLP library might be intelligent, but the system or solution could be just as dumb as ever, with a nice shiny new NLP system on it. Our old-hat integration or automation solution is still just an old-hat automation integration solution even if it has OCR (Optical Character Recognition). I could put an Alexa interface on top of my steam engine, but that doesn’t mean I can call my steam engine Ai-enabled. It’s still just expanding pistons and pushing rods. It’s automation, not intelligence. Talking to your car doesn’t mean your car is intelligent or Ai-enabled. Unless it’s autonomous, you’re still the one pressing the pedals and moving the steering wheel. You may be fooling some of your customers, your investors, your partners, and your employees, but you’re not fooling us.\nYet it’s worse than that. Vendors who push automation solutions as intelligence are hurting the industry. If customers are lead to believe that the automation solutions are what they can expect out of Ai systems are these weak automation systems that use the narrowest definition of Ai to call their systems intelligent, then the industry is heading for a rapid roadblock. If we want the desired outcome of Ai that we’re hoping for and expecting, in which we’re solving increasingly harder problems that have not before been able to be done with systems that can learn and adapt, then we have to ask more of the systems that call themselves artificially intelligent. When vendors say their products have Ai capabilities, don’t take them at face value. Ask how their offering is going to learn and adapt and perceive the environment. More than half of the time the response will be that they’re just doing automation with the thinnest possible veneer of Ai. The ones that are truly building Ai-enabled products in the way that will help the industry mature are the ones that are worth following. Let’s leave the hype train behind at the station. Where we’re going, there aren’t any tracks. (Apologies to Back to the Future for the misquote).\nYou Rock! ","date":1570579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570579200,"objectID":"b816f2ce99adb55252ddf0b519e54538","permalink":"/post/aihype.-dont-be-fooled/","publishdate":"2019-10-09T00:00:00Z","relpermalink":"/post/aihype.-dont-be-fooled/","section":"post","summary":"The Ai Hype Train Has Left the Station We know we’re in the middle of a hype cycle when every industry has products that use some buzzword, regardless of whether or not they have the technical merit or legitimacy to actually use that buzzword. This is where we currently are with regards to the market for Ai (Artificial Intelligence) products and services.\nWe hear briefings from vendors pitching technology across a wide range of solution areas, as well as from professional services and government contracting firms with new solution offerings and end users looking for guidance and advice.","tags":null,"title":"Ai Hype. Don't Be Fooled","type":"post"},{"authors":null,"categories":null,"content":" Science and Sci-fi can’t seem to agree on the way we should think about artificial intelligence. Sci-fi wants to portray artificial intelligence agents as thinking machines, while organizations today use Ai for more mundane tasks like filling out forms with robotic process automation or driving your car. When interacting with these Ai interfaces at our current level of Ai technology, our human inclination is to treat them like vending machines, rather than to treat them like a human. Why? Because thinking of Ai like a human (anthropomorphizing) leads to immediate disappointment. Today’s Ai is very narrow, and so straying across the invisible line between what these systems can and can’t do leads to generic responses like “I don’t understand that” or “I can’t do that yet”. Although the technology is extremely cool, it just doesn’t think in the way that you or I think of as thinking.\nLet’s look at how that “thinking” process works, and examine how there are different kinds of thinking going on inside Ai systems.\nFirst, let me convince you that thinking is a real thing. Putting aside the whole conversation about consciousness, there is a pretty interesting philosophical argument that thinking is just computing inside your head. As it turns out, this has been investigated, and we can make some conclusions beyond just imagining what thinking might really be. In the book “Thinking Fast and Slow”, Nobel laureate Daniel Kahneman talks about the two systems in our brains that do thinking: A fast automated thinking system (System 1), and a slow more deliberative thinking system (System 2). Just like we have a left and right brain stuck in our one head, we also have these two types of thinking systems baked into our heads, talking to each other and shaping the way we see the world. And so thinking is not as much about being right, as it is a couple of ways for making decisions. Today\u0026rsquo;s Ai systems learn to think fast and automatically (like System 1), but artificial intelligence as a science doesn’t yet have a good handle on how to do the thinking slow approach we get from System 2. Also, today’s Ai systems make the same sorts of mistakes as System 1, where biases, shortcuts, and generalizations get baked into the “thinking” machine during learning. With today’s Ai, there is no deliberative step by step thinking process going on. For example, How can Ai “think”, when a major component of what thinking is all about isn’t ready for primetime?\nNow that we have a bit more definition about what thinking is, how can we make more human-like artificial intelligence? Maybe representing feedback loops will get us to a sort of thinking machine like System 2. Well, as it turns out, we have not cracked that yet. Ai syatems don’t contain common knowledge about the world. For example, I recall Yann Lecun, a “founding father” of modern Ai, gave an example sentence “He walked through the door” and pointed out that today’s Ai models can’t decide what this means. There is a silly interpretation where we can conclude that a person crashed through a door like a superhero, smashing it to pieces. There is another interpretation where either the door was open or the person opens the door to walk through the doorway. Unfortunately, without common knowledge, you don’t really know which situation is more likely. This shows us that even “thinking fast” situations can go poorly using the tools we have available today.\nWe live in a world where fast thinking Ai is the norm, and the algorithms are slowly trained on huge amounts of data. The reason you can’t make a better search engine than Google is not the secrecy of their search algorithms. Rather, it is the fact that they have data you don’t have, from excellent web crawlers to cameras on cars driving around your neighborhood. Presently, the value in Ai is the data, and the algorithms are mostly free and open source. Gathering masses of data is not necessarily enough to ensure a feature works. Massive efforts at human labor are often required. In the future, thinking algorithms that teach themselves may themselves represent most of the value in an Ai system, but for now, you still need data to make an Ai system, and the data is the most valuable part of the project.\nThinking is not easily separated from the human condition, but we humans are also far from perfect. We may be smart on average, but as individuals, we are not built to do statistics. There\u0026rsquo;s some evidence for the wisdom of crowds, but a crowd holding pitchforks and torches may change your mind. As it turns out, we are adapted through the generations to avoid being eaten by lions, rather than being adapted to be the best at calculus. We humans also have many biases and shortcuts built into our hardware. It’s well documented. For example, correlation is not causation, but we often get them mixed up. A colleague of mine has a funny story from her undergraduate math degree at a respected university, where the students would play a game called “stats chicken”, where they delay taking their statistics course until the fourth year, hoping every year that the requirement to take the course will be dropped from the program.\nGiven these many limitations on our human thinking, we are often puzzled by the conclusions reached but our machine counterparts. We “think” so differently from each other. When we see a really relevant movie or product recommendation, we feel impressed by this amazing recommendation magic trick, but don’t get to see the way the magic trick is performed. And one is tempted to conclude that machine-based thinking is better or cleaner than our messy biological process, because it is build on so much truth and mathematics. In many situations that’s true, but that truth hides a dark underlying secret. In many cases, it is not so clear why artificial intelligence works so well. The engineering got a bit ahead of the science, and we are playing with tools we don’t fully understand. We know they work, and we can test them, but we don’t have a good system for proving why things work. In fact, there are some accusations even in respected academic circles slide 24, here that the basic theory of artificial intelligence as a field of science is not yet rigerously defined. It is not just name-calling or jealousy being hurled by the mathematicians at the engineers. Ai is a bunch of fields stuck together, and there really is a lack of connection in the field between how to make things work and proving why they work. And so the question about thinking and Ai is also a question about knowledge. You can drive a car if you don’t know exactly how it works inside, and so maybe you can think, even if you don’t know why your thinking works.\nAssuming we don’t have a concrete theory underlying the field of artificial intelligence, how can engineers get anything done? Well, there are very good ways to test and train Ai systems, which is good enough for today’s economy. There are many types of Ai, including supervised learning, unsupervised learning, reinforcement learning, and more. Engineers don’t tend to ask questions like “is it thinking?”, and instead ask questions like “is it broken?” and “what is the test score?”\nSupervised learning is a very popular type of machine learning that makes fast predictions in some narrow domain. The state-of-the-art machinery for doing supervised learning on large datasets is feed-forward deep neural networks. This type of system does not really think. Instead, it learns to pick a label (for classification) or a number (for regression) based upon a set of observations. The way decisions are baked into neural networks during “learning” is not obvious without a strong validation step. More transparent models have been around for a long time, for example, in areas such as game theory for military planning. Explicit models like decision trees are a common approach to developing an interpretable Ai system, where a set of rules is learned that defines your path from observation to prediction, like a choose your own adventure story where each piece of data follows a path from the beginning of the book to the conclusion.\nAnother type of machine learning called reinforcement learning involves learning the transition from one decision to the next based on what’s going on in the environment and what happened in the past. We know that without much better “environment” models of the world, these methods are going to learn super slowly, to do even the most basic tasks. Systems that learn to solve problems this way rely heavily on accurate models of how the world works. When dealing with a problem related to humans, they need lots of data on what those humans do, or like, or think. For example, you can’t learn to generate amazing music without data on what humans like to listen to. In a game playing simulator an Ai model can play against itself very quickly to get smart, but in human-related applications the slow pace of data collection gums up the speed of the project. And so in a broad sense, the Ai field is still under construction at the same time as we are plugging lots of things into it.\nRegardless of the underlying technical machinery, when you interact with a trained machine learning model in the vast majority of real-life applications today, the model is pre-trained and is not learning on the fly. This is done to improve the stability of your experience, but also hides the messiness of the underlying technology. The learning tends to happen in a safe space where things can be tested, and you experience only the predictions (also called inference) as a customer of the Ai system.\nDespite the hype, Ai models that think like we do are not coming around the corner to exceed humanity in every way. Truly thinking machines are definitely worthy of research, but they are not here just yet. Today, Ai models and human analysts work side-by-side, where the practitioner gives their opinion and is assisted by an Ai model. It is useful to think about more general mathematical models like rainfall estimation and sovereign credit risk modeling to think about how mathematical models are carefully designed by humans, encoding huge amounts of careful and deliberative human thinking. The practice of building Ai systems involves a lot of reading and creativity. It\u0026rsquo;s not just coding away at the keyboard.\nI feel that Ai software developers gradually build a sense for how to think about what an Ai model is doing, and it ins’t “thinking”. I wanted to get some input from someone unrelated to me in the artificial intelligence field, to see if they feel the same way. Through the CEO of DTA, I set up a talk with Kurt Manninen about his work on an AI product called AstraLaunch. I asked Kurt a lot of technology questions, leading up to the question “Does the system think like people do?”\nAstraLaunch is a pretty advanced product involving both supervised and unsupervised learning for matching technologies with company needs on a very technical basis. A complicated technology like this is a good area to be thinking about “thinking”. The system has an intake process that leads into a document collection stage, and then outputs a chart of sorted relevant documents and technologies. What I wanted to understand from Kurt is the way he thinks about what the matching technology. Is the system thinking when it maps the needs of NASA to the technology capabilities of companies? When diagnosing an incorrect prediction, does he think about the model as making a mistake, or is the origin of the mistake with the model maker and/or the data?\nKurt’s answer was really close to what I expected from my own experience. Technology like AstraLaunch involves humans and Ai models working together to leverage the strength information processing approach. But, Kurt felt strongly, as I do, that bugs in Ai models are the fault of people, not the model. An Ai developer can see where the training wasn’t set up properly to understand language, or vocabulary, or where the dataset collection went wrong, etc.\nReturning to the original question about artificial intelligence and thinking, I think we can solidly conclude that these systems don’t do thinking at all. If we only have fast and automatic (System 1) artificial intelligence to work with, can we think of an Ai model as a gifted employee that thinks differently about the world? Well, no. Ai will probably cheat if the training is unmanaged, and so it is a lazy, deceptive employee. It will use the easy way out to get the highest score on every test, even if the approach is silly or wrong. As we try and build a “System 2” that thinks more like us, we need to remember that thinking is not about passing a test.\nDisclosure: I have no financial interest in any of these companies or products.\nYou Rock! ","date":1570320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570320000,"objectID":"d32969d81a8891f9e721caae2911f395","permalink":"/post/thinkingai/","publishdate":"2019-10-06T00:00:00Z","relpermalink":"/post/thinkingai/","section":"post","summary":"Science and Sci-fi can’t seem to agree on the way we should think about artificial intelligence. Sci-fi wants to portray artificial intelligence agents as thinking machines, while organizations today use Ai for more mundane tasks like filling out forms with robotic process automation or driving your car. When interacting with these Ai interfaces at our current level of Ai technology, our human inclination is to treat them like vending machines, rather than to treat them like a human.","tags":null,"title":"Thinking Ai?","type":"post"},{"authors":null,"categories":null,"content":" Organizations use paper. Businesses in every vertical have always relied upon a certain quantity of paper since the dawn of time and, despite the rise of digital, that basic truth isn’t going to change. Although invoices are now largely digital, boarding passes live on smartphones and there’s even an app (or two) for To-Do lists and notepads, a certain amount of paper will always feature as part of any firm’s operational tasks.\nWhile Western Civilization replaces some of its paper consumption with technology, business growth in developing nations and the use of paper in packaging is thought to offset the trend for digitization.\nNever been ‘inked’ If we accept these basic propositions, then it should logically follow that the technology industry seeks to provide solutions to working with data in its digital form. What was once printed paper, is now being digitized through OCR (Optical Character Recognition) and document capturing technologies. What was once spooled off of some form of printer, is now ‘digitally native’ data that is potentially never exposed to the wonders of ink, because its digital form has been validated for whatever enterprise use case it needs to perform.\nWith its roots in Eastern Europe and a foundation in document capture and management technology, Abbyy positions itself as something rather more than a content capture specialist. The company acquired Philadelphia, Pennsylvania-based TimelinePI back in May 2019 for its process intelligence competency. Add document management to business process analytics and you get a method for visually modelling data and events as they flow through a business. Now a product rather than a stand alone organization, Abbyy Timeline is a key part of what the company calls Digital IQ.\n“When we digitize successfully, we (as businesses) start to find that traditional use cases of technology (that might have previously only been touched by the IT department) start to become ‘subsumed’ into other line of business applications, which is why we talk about ‘citizen developers’ now creating new application services because they can more directly ‘shape’ the way powerful back end technologies function,” added Macciola.\nOnce documents (or images) have been scanned and digitized, then metadata can be applied to the stock of information that exists inside an organization using this technology. Metadata is sometimes described as \u0026lsquo;information about information\u0026rsquo; and it works to classify data into various groups and schema, which allows it to be more accurately stored in data stores.\nInformation ‘Pathways’ Denote Processes A typical company might have several hundred different ‘paths\u0026rsquo; down which data might flow within the business. These pathways denote, describe and define the shape of the processes inside the organization itself. Abbyy Timeline works to define a set of rules that a customer can use to look inside any single ‘process instance’ and see how work is actually executed. Some process workflows are repeatable and easier to label, some are ad hoc and more random in nature and some are in between the two i.e. not massively repeated, but still somewhat variable. Abbyy Timeline ingests all the event data inside these work processes and then builds a visual model (on screen) to identify bottlenecks, where work may still happen, but not in a way that is as fluid as it could be.\nOnce a business starts to get all the records, documents, events, images and other operational data appropriately digitized for more automated business intelligence, it will typically need to clear out anomalies which stand out. On the road from analogue to digital, some data gets missed, some data isn’t formatted as it should be and some data gets badly scanned.\nOver and above data ingestion and the chance to produce nice on-screen images and tools to explore that data with, Abbyy also focuses on the validation of the business data and the classification of the data. The company says that these stages need to be executed before the onward management and analysis of operational information. Company CEO Ulf Persson explains that all companies know that they have all these processes and they know that they have desired outcomes from those processes but that (most companies) fail to understand how complex they are in terms of how many steps, stages, people, products and things that are involved.\n\u0026laquo;Whether you look at your data using enterprise reporting, business intelligence dashboards or even cool tools like geospatial visualizations you are still limited to only looking at a single step of a business process at a time. With the advent of first-generation process mining software this improved as it allowed you to visualize all steps of a business process end-to-end – as long as the process was very well defined and consistently executed,\u0026raquo; said Scott Opitz, president of Abbyy Timeline. \u0026laquo;The problem is, when you are dealing with more convoluted or complex business scenarios where the steps of the process are performed in a more ad hoc manner such as a hospital emergency room or a wide-ranging customer service situation, where you can\u0026rsquo;t always resolve the process to a nice neat diagram. In these situations you need a new approach to analyze the process which is based on the complete process timeline – not some mythical pretty process diagram.\u0026raquo;\nOpitz uses this proposition to attempt to validate and justify the need for the types of process tools that his company specializes in. Going further, Opitz says that Abbyy Timeline has also implemented machine learning techniques to predict process outcomes and prescribe actions based upon user-defined priorities. So in the real world, this means that the system \u0026lsquo;maps\u0026rsquo; the way processes are carried out and accommodates for the fact that different employees or contractors will execute any given process performing steps in different orders and combinations, at different speeds, at different costs, at different levels of competency — and the software \u0026lsquo;learns\u0026rsquo; each time a process is digitally mapped to be able to make the system cumulatively smarter over time.\nAlongside its Timeline product, Abbyy also points to its Vantage tool. This software offers human-like cognitive skills to automation platforms such as RPA (Robotic Process Automation) and BPA (Business Process Automation). The technology delivers text recognition, machine learning in the form of ‘consumable skills’ for non-technical workers. The company insists that Vantage addresses common content-centric pain points such as lengthy set-up and configuration, shortage of training data, requiring skilled experts in areas like machine learning for automating unstructured content.\nDigital Business Codification Where all this takes us to is digital enterprises. The marketing people love to call it ‘digital transformation’, but it might be more accurate to call it digital codification i.e. we’re taking existing elements of business and driving them into a place where we have coded the information in the processes that drive them.\nSome parts of business are horizontal (in that they apply to every industry vertical) such as invoicing or facilities management. Some parts of business are vertical (in that they have a specific shape, role and context within a specific type of business) such as insurance claims management, cake baking or root canal treatment. All of these parts of business form a process and so we can digitize the paper-based documents that used to accompany them.\nEven when a business process is ‘very human’ and doesn’t have a defined shape or piece of paper attached to it (such as a worker going to a different department and having an informal chat with another team), we can usually ‘infer’ that something has happened in a work process without knowing its exact shape based upon later results.\nPaper isn’t dead, but work has gone digital… and you can make a note of that!\nYou Rock! ","date":1569974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569974400,"objectID":"b682c61758e13be1f5bd988e57e4692b","permalink":"/post/papertodigital/","publishdate":"2019-10-02T00:00:00Z","relpermalink":"/post/papertodigital/","section":"post","summary":"Organizations use paper. Businesses in every vertical have always relied upon a certain quantity of paper since the dawn of time and, despite the rise of digital, that basic truth isn’t going to change. Although invoices are now largely digital, boarding passes live on smartphones and there’s even an app (or two) for To-Do lists and notepads, a certain amount of paper will always feature as part of any firm’s operational tasks.","tags":null,"title":"Paper To Digital Intelligence","type":"post"},{"authors":null,"categories":null,"content":" Artificial Intelligence (Ai) as a market is full of hype, with vendors, customers, and press all speaking breathlessly about the capabilities for Ai in general and their offerings specifically. Likewise, blockchain is also a widely hyped market, with technology providers and customers claiming all sorts of capabilities that may or may not be possible. Combining Ai and blockchain then must be double the hype? On the other hand, Ai is providing real, tangible value in many myriad ways we talk about every day. Likewise, blockchain is starting to show value across a range of applications and industry. So, perhaps combining Ai and blockchain will also show twice the value combined together.\nThe Role Of Blockchain In An Ai Context Blockchain is a decentralized, distributed ledger of transactions that has elements of transparency, trust, verifiability, and something called smart contracts. Decentralized and distributed means that information that is stored across the network in such a way that each end point has access to the data without requiring access to a central server. The network is also distributed because the transactions happen at each end point without requiring centralized coordination. A ledger is a record of transactions. Blockchain records a ledger of interactions between two separate parties whether it be a financial exchange or even a chain of custody showing when things have changed hands over time.\nSince every block in the blockchain contains a different piece of information that is encrypted or encoded, the blockchain can help guarantee trust and verifiability of data. The concept of the chain and block in blockchain is that each block has its own information and that information contains a link to the block before it, which develop the chain and provides a verifiable chain of custody. No individual actor can change the information in a block without invalidating the entire chain of information in a particular block and thus messing up the chain. Since the chain is distributed in myriad of other places and between other parties, to make changes to the chain, a consensus of all the parties would need to agree to the changes being made.\nAdding to the concept of the blockchain are smart contracts, which are decentralized pieces of code that can be triggered when a specific chain of actions has been met. In this way, when you have two parties wanting to execute a secure, trusted transaction without the use of an intermediary this is what blockchain ideally would be used for.\nSo how can blockchain help with Ai? The first benefit is that you can share machine learning models among all parties without an intermediary. A good example is with facial recognition software. If one device knows what a person looks like and uploads it to the chain, other devices hooked up also know what that person looks like. When other devices upload their own facial recognition data, the other devices will gain the ability to use that for the facial recognition model. Since this happens on the blockchain, there is no central control over facial recognition, and as such no one company owns or stores the data. This approach also allows for everyone to learn faster and collaboratively through the use of integrated Ai with blockchain.\nAi systems can also use blockchain to facilitate the sharing of data used across multiple models. A great example is the use of machine learning models for product recommendations in online retail. If an online store knows the preferences of one shopper, and then that customer goes to a different store website, these two sites can be connected through a blockchain to share trusted personalization information. This could potentially become a place of competition for smaller ecommerce sites that want to share with each other personalization information. Instead of each company gathering their own personalization data, they can share it amongst each other in a blockchain, providing competition to sites like Amazon and Walmart who have already developed their own data systems to gather this information about their customers. The benefit for this to the customer is that in exchange for sharing their information, they can get everything from better pricing to customized shopping experiences. This could also prevent data breaches if all information and payment systems are stored and shared through a blockchain rather than a centralized data server.\nHow Blockchain Benefit Ai Not only can blockchains be used to share models and data, but blockchains can help serve a role as a “master brain” in a manner shared across multiple Ai systems. If we can put all of these shared learning benefits and blockchain and Ai together, the possibility may also be there to combine all of these things that can learn from their surroundings and then share that learning with all the Ai systems on the network. A major benefit would be that no one owns it and there’s no government control over the bot or the shared brain. It could potentially be unbiased because of the sheer amount of information coming in from different areas and different angles.\nAnother application is addressing the challenge of explainable Ai. One of the more significant problems with deep learning is that there isn’t a clear idea about what inputs result in what output and how that affected the whole sequence. If something goes wrong in a deep learning neural network, we don’t have a clear idea of how to identify the problem and correct it. This is the problem of neural networks in effect being a “black box” without any real transparency or explainability. However, if we use blockchains, we can record how individual actions result in a final decision in a non-reputable manner, which allows us to go back and see where things went wrong and then fix the problem. The blockchain would be used to record events, such as autonomous vehicle decisions and actions in a way that will not be modified later. This can also increase trust since the blockchain element is unbiased and is just for storage and analysis, anyone can go in and see what has happened.\nFinally, Ai systems can be used to improve blockchains in general. Machine learning models can keep an eye on what is happening in the blockchain. It can look for patterns and anomalies in the types of data being stored and actions being performed on a particular server and be used to alert users when something may be happening. The systems can look for normal behavior and flag what seems to be unusual. Ai systems can help keep blockchain more secure, more reliable, and more efficient.\nWhile it’s quite possible that the worlds of Ai and blockchain are full of hype, there are actual, tangible, realistic ways in which the two emerging technologies can be used in ways that benefit each other and provide real outcomes for those looking to implement the technologies today in their environments.\nYou Rock! ","date":1569715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569715200,"objectID":"1e077832bef38ad8a354932c18b5a3e9","permalink":"/post/blockchainai/","publishdate":"2019-09-29T00:00:00Z","relpermalink":"/post/blockchainai/","section":"post","summary":"Artificial Intelligence (Ai) as a market is full of hype, with vendors, customers, and press all speaking breathlessly about the capabilities for Ai in general and their offerings specifically. Likewise, blockchain is also a widely hyped market, with technology providers and customers claiming all sorts of capabilities that may or may not be possible. Combining Ai and blockchain then must be double the hype? On the other hand, Ai is providing real, tangible value in many myriad ways we talk about every day.","tags":null,"title":"Blockchain \u0026 Ai","type":"post"},{"authors":null,"categories":null,"content":" Much like the internet, the rise of blockchain gives individuals and organizations the chance to achieve vast improvements in productivity. For instance, blockchain can improve transactional processes among individuals, companies, suppliers and legislators to create more scalable, efficient solutions.\nThe Appeal Blockchain is a decentralized, distributed ledger of transactions that has elements of transparency, trust, verifiability, and something called smart contracts. Decentralized and distributed means that information that is stored across the network in such a way that each end point has access to the data without requiring access to a central server. The network is also distributed because the transactions happen at each end point without requiring centralized coordination. A ledger is a record of transactions. Blockchain records a ledger of interactions between two separate parties whether it be a financial exchange or even a chain of custody showing when things have changed hands over time.\nThe value it provides is security to the underlying data, allowing companies to significantly reduce the cost of trust and collaboration, things that are often expensive for businesses.\nIn a Harvard Business Review article, experts discussed how blockchain offers a distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. It is designed to be immutable, meaning that once a piece of information goes in, it can’t be changed. With the database being stored in numerous locations, if a hacker or a bad actor were to modify one of the databases, the tampering would be detected through cryptography.\nBeyond the attention-grabbing headlines, blockchain technology is increasingly sparking R\u0026amp;D (research and development) among enterprises. Across numerous sectors, new opportunities are surfacing to reduce costs, streamline processes, improve data tracking and security, increase safety and mitigate fraud.\nReal-World Application In supply chain management, companies are already using blockchain to track items through complex supply chains. Everledger, a London-based startup, is using blockchain systems to help eradicate blood diamonds that are mined through child labor and operations funding drug lords. The company has developed a platform that tracks origin and ownership changes along the supply chain and recently secured $20 million in funding.\nReducing counterfeit products is a major concern for many large brands, especially within the fashion world. The global market for counterfeit clothing amounts to an incredible $450 billion, and blockchain offers a clear solution. If items are registered on the system, manufacturers and suppliers can provide data about the products’ origins, records and chain of ownership. Tagging assets on a blockchain can give buyers confidence, knowing that their item is authentic.\nCompanies are also considering blockchain and other technologies to streamline and innovate within the auto industry. Solutions are emerging to address issues with vehicle-to-vehicle communication, cashless payments at highway tolls and automotive insurance. A specific case within the industry highlights the need to objectively measure vehicle value. Beijing Mercedes-Benz Sales Service recently partnered with a blockchain-based used car value management platform to help understand the value depreciation of its cars and automate the process in real time.\nWithin the financial services sector, blockchain activity has been advancing for a number of years. RBC, JP Morgan, Citibank, American Express, Visa and MasterCard are among the long list of enterprises conducting multiple blockchain-related efforts. Central banks are also following suit: A recent white paper from the World Economic Forum describes several use cases and how central banks are using blockchain technology to address long-standing issues within the industry, such as financial inclusion and payment efficiency.\nWeighing the Options In spite of these exciting applications, blockchain isn’t necessarily the answer for everyone. Blockchain and decentralized technologies enable data robustness and integrity and eliminate the need for intermediaries. However, they can also have certain disadvantages when compared to more centralized databases and systems.\nIn some cases, security vulnerabilities have given certain blockchain applications, such as smart contracts, a bad reputation and slowed enterprise adoption. In order to unleash its potential, security issues need to be addressed. Additionally, some technologies are unready and untested for large-scale enterprise implementation.\nSo how can companies test the waters and determine if a blockchain solution is right for them? I suggest conducting small, low-risk experiments to understand the full potential of blockchain for your organization.\nPilots are a great place to start. They are often less complex to implement and provide a starting point for companies to assess and expand once more information is known. Explore, hypothesize, make mistakes, observe and, if a pilot proves successful, be prepared to scale.\nWhat\u0026rsquo;s Next The examples above highlight just a few of the industries being impacted by blockchain solutions. From pharmaceuticals to food safety, there is enormous potential for enterprises to reinvent the way they operate and get ahead of the competition. We can create efficiencies and solve problems that we haven’t been able to solve previously with other technology, leading to substantial competitive advantages.\nEven with all these existing use cases, we\u0026rsquo;re still at an early stage in this industry. As blockchain technology becomes more mainstream, I expect further investment in blockchain education and R\u0026amp;D. I believe that this will, in turn, create unprecedented opportunities for growth.\nYou Rock! ","date":1567987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567987200,"objectID":"a4a21d16924a71aae0362b87acb41bf6","permalink":"/post/innovesecurely/","publishdate":"2019-09-09T00:00:00Z","relpermalink":"/post/innovesecurely/","section":"post","summary":"Much like the internet, the rise of blockchain gives individuals and organizations the chance to achieve vast improvements in productivity. For instance, blockchain can improve transactional processes among individuals, companies, suppliers and legislators to create more scalable, efficient solutions.\nThe Appeal Blockchain is a decentralized, distributed ledger of transactions that has elements of transparency, trust, verifiability, and something called smart contracts. Decentralized and distributed means that information that is stored across the network in such a way that each end point has access to the data without requiring access to a central server.","tags":null,"title":"Blockchain Aids the World's Largest Organizations Innovate Securely","type":"post"},{"authors":null,"categories":null,"content":" It’s no secret the insurance industry is primed for disruption. All you need to do is type terms like “ai for insurance” or “latest insurtech” into Google search, and a veritable cornucopia of articles will pop up preaching the advantages of adopting new, intelligent technologies. With so much information available, it’s no wonder many insurers are struggling to figure out how to embark on their Ai journey in a way that ensures projects yield the desired results.\nMy study has reveal some common questions put forward by these insurers and this post is an attempt at anwsering them:\n We need to be thinking about Ai, but we’re not sure where to start. What are some of the things we can do to kick-start the process?\n Like most large-scale projects involving multiple stakeholders, developing a strategic roadmap for rolling out Ai is absolutely crucial. It start by identifying use cases and ensuring they are relevant by engaging the right business stakeholders and users in the process. Each use case should be carefully framed, with clearly specified objectives and desired outcomes. Once identified, the use cases can be assessed and prioritized based on a set of key criteria, including desirability, viability, feasibility and risk:\n Desirability reflects the importance of the specific problem considered for end users (i.e. how badly do we want it solved?)\n Viability evaluates the expected economic benefits or ROI (return on investment)\n Feasibility assesses whether a solution to the problem is achievable, including data, technology and human intelligence\n Risk gauges the potential adverse consequences of a model failure and the ability to mitigate against them\n  Its crucial to choosing a manageable scope, as developing an Ai roadmap for the company, one line of business, or a specific workflow will require wildly different efforts. Its advisable to commence with a more focused scope and expand across the organization in successive waves, each leveraging the learnings from prior iterations, rather than attempt to do it all at once.\n What kind of data is needed to deploy an Ai product?\n Data is critical to deploying any Ai technology. Data can be grouped into two buckets: structured and unstructured data.\nStructured data is highly-organized and formatted in such a way that it\u0026rsquo;s easily searchable in relational databases. Structured data includes tables and spreadsheets \u0026ndash;claims and payments that is stored in a company’s relational database.\nUnstructured data has no pre-defined format or structure, making it much more difficult to collect, process and analyze. It can be in the format of text, images, audio or video and is typically much more of a challenge to search. Examples of unstructured data could include emails from brokers, financial statements, call center recordings, pictures of damaged vehicles and social media posts.\nWhile most companies bave a mix of both structured and unstructured data, the majority of insurers\u0026rsquo; data is unstructured. While structured data is preferable when deploying an Ai product, modern algorithms can leverage both types. In fact, recent advances allow Ai to effectively process many types of digital information, including scanned documents, images, videos and even audio recordings.\n What can insurers do that lack data for deployment in Ai?\n The first thing that needs to be determined is whether there is truly a lack of data or if what is really missing is labeled (structured) data.\nUnlabeled (unstructured) data has not been tagged with one or more labels, and it’s these labels that enable Ai systems to accurately read the data and understand what it represents. Photos, audio recordings, videos and, x-rays can all be forms of unlabelled data. If it is an issue of data not being labeled, there are tools that can accelerate the labeling process and help insurers build Ai-ready datasets.\nIf the issue does indeed stem from a lack of sufficient data, then an insurer needs to rely more heavily on pretrained, off-the-shelf models. These products, however, tend to have lower predictive accuracy, since pretrained models do not account for the insurer’s unique specificities.\n If we deploy an AI product within our business, will we be forced to migrate from our current digital platforms?\n Migrations are messy, expensive, time-consuming and generally a painful process for most involved. This is one of the reasons why it’s crucial for Ai products to integrate seamlessly into your current workflows.\nIn order for algorithms to access data and provide a recommendation or automate a task, data needs to be available in digital format. This is certainly the case when a digital platform has been deployed. Even in the absence of digital platforms, some insurers have managed to apply Ai for certain use cases, but these cases are limited to areas where the data is available in a proper format. So in short, no migration necessary!\n The insurance industry is highly regulated, and regulations vary by state. How does Ai take this into account?\n Ai systems are not plug-and-play products. They learn continuously through mechanisms designed to capture human judgment, knowledge and experience, further improving the performance of the model over time.\nThanks to this continuous learning, industry regulations, including state-specific regulations, can be applied, for example by imposing constraints on the model structure or by limiting the data that is made available to the model.\n Our company is currently deploying another new system. Is it possible to deploy an Ai model simultaneously?\n In short, yes. In some cases it can be done. When deploying a model for the first time within your firm, the model will rely on historical data to be trained. This data would come from the older systems.\nInsurers deploying multiple systems or platforms at the same time as an Ai model must make sure that the data coming from the new system is represented the same way as the data coming from the older system. Typically, newer systems provide more data with more granularity than older systems (which, when it comes to Ai, is preferable). It is therefore possible to deploy a model while deploying or migrating to a new system.\n Our actuaries are already using advanced analytics and ML. Do we really need to look outside for other Ai solutions?\n While some insurers’ in-house analytics teams have seen success developing Ai and ML models for specific uses, the reality is that it is very difficult to bring analytics to production. According to Gartner, by 2022, 85% of Ai projects will fail in production.\nGiven the level of complexity of advanced analytics and ML, compounded by the high probability of delays or failures of Ai projects, in the long run working with an Ai product provider can be a time - and cost - saving solution. Ai products can be deployed on-site quickly and they are highly customizable, thanks to human-in-the-loop features like tolerance thresholds settings for automation and corrective feedback provided by the end users.\n What business areas are most primed for Ai?\n There are opportunities to deploy Ai across every stage of the insurance lifecycle, with the technology currently available to us today, the areas where insurers will see the most return when applying Ai are in the underwriting and claims processes. However, there are also opportunities to apply Ai to other areas, such as marketing, customer targeting or product recommendation, to name a few.\nAt the end of the day, the best use cases to select are those that will best serve the organization’s priorities.\n What happens when we are ready to deploy an Ai product?\n Once the team has selected the use case, prepared the data and fine-tuned a model the next step is to integrate the Ai product into your tech environment, including all the systems it will need to interact with within the organization. This is done through API (application programming interface) calls. What’s an API, I hear you say ? It’s essentially a set of tools and procedures that help developers build software applications. The API will specify how the different software components will interact. An API call is the API being put to work. Any time you perform an action at your computer (e.g., send an email, download an app, enter a password), you are making an API call. In the context of an Ai product interacting with, say, your database, an example of an API call is requesting a price quote for a new business submission.\nIt can take anywhere from eight weeks to a few months to complete this deployment process. Unlike traditional RPA (robotic process automation) software, which is rules based and static, the model, once it is set up, will continuously learn and improve its decision-making over time based on thresholds you determine and control, as well as on corrective feedback provided by end users (e.g., administrators, underwriters, adjusters).\n How should insurers plan for change management when adopting Ai tech?\n Beyond developing an Ai strategy, securing technical talent, investing in data and technology infrastructure and putting in place the right governance framework and controls, insurers should not underestimate the importance of change management in ensuring the Ai solutions they adopt are successful (whether developed in-house or by third parties). As with any major initiative, insurers can plan for effective change management by identifying and empowering internal champions within the organization, in addition to developing and delivering a thoughtful communication strategy, both internally and externally, and providing adequate training and coaching, before and after deployment.\nA practical list of activities and deliverables that insurers can start with to effectively plan for change management:\n Preview your use cases to ensure that impacted business processes and SOP (standard operating procedures) are up-to-date. Its crucial to know what you are changing from, to understand what support, training and organization change will be required to be effective in the future\n Update job descriptions and capture the tasks associated with impacted roles. This will help identify training requirements and determine whether one needs to reorganize job tasks or work assignments\n Review the content and completeness of internal training and onboarding processes for the job roles affected. Once Ai is deployed, certain staff may be taking on new responsibilities and may require training or support to be set up for success\n Communicate, communicate, communicate. It can’t be said enough! Companies should not only share the vision and roadmap for how Ai will be developed in the future but also how employees will be affected and the support systems that will be available to them to ensure a smooth transition for all\n  You Rock! ","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"fa2adfffc4db064b1f03405b4487b173","permalink":"/post/insurance/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/post/insurance/","section":"post","summary":"It’s no secret the insurance industry is primed for disruption. All you need to do is type terms like “ai for insurance” or “latest insurtech” into Google search, and a veritable cornucopia of articles will pop up preaching the advantages of adopting new, intelligent technologies. With so much information available, it’s no wonder many insurers are struggling to figure out how to embark on their Ai journey in a way that ensures projects yield the desired results.","tags":null,"title":"Ai Deployment In FinTech","type":"post"},{"authors":null,"categories":null,"content":" Companies widely recognize the potential power of artificial intelligence (Ai). They instinctively understand that it feels like we’re on the cusp of something that will change our lives and our businesses in a profound way. Yet, many struggle with where to apply it. Executives can’t shake the feeling that they should have use cases for Ai and use it productively today, even recognizing that Ai is not mature yet and will be far more powerful tomorrow and in the future. If you’re looking for how and where your company should use Ai, let me give you a perspective on a great application of Ai today: your digital platforms.\nA good way to understand what is happening in digital transformation is that organizations are shifting from process orientation to platform orientation. They are doing this in a lot of different ways, but a primary action is building platforms. Platforms allow them to delight their customers by changing their customer experience. They build platforms that can better serve employees and so they can better accommodate and respond to regulatory changes.\nWhy Stakeholders Matter In Using Platforms When you build a digital platform at your company, it typically will have at least five distinct stakeholder groups that it serves:\n End users\u0026ndash;employees or customers People in the organization who are part of delivering the experience to the end users Company administrators and executives that fund and oversee the platform Ecosystem partners that provide services, tools and products used in the platform Ecosystem partners that are increasingly part of the monetization of the platform\u0026ndash;their platforms intertwine or benefit from your company’s platform  No organization is an island, and businesses increasingly go to market as ecosystems, not just as companies. They compete and collaborate across ecosystems. There could be more than five stakeholder groups, but typically there are at least five. Each stakeholder group has a different interest in the platform.\nWhat’s The Key to Success In A Platform? The technology is the easiest part. User adoption is the most difficult part of platforms. The stakeholder groups need to adopt the platform for it to succeed. The stakeholders must want to adopt the platform and be willing to do it. No amount of management change or messaging will make it happen; they must want to do it. The only way they will want to use the platform is if they get something from it. They want the experience that the platform brings.\nAlthough there are many dimensions to a stakeholder’s experience, at the center of each experience is ACT. When you design a digital platform, there are three attributes that define its success:\n A – The platform needs to anticipate a user’s needs C – The platform then needs to give the user transactions that are a complete experience T – The transaction on the platform must also satisfy the user’s needs in a timely fashion  These three attributes are key fundamentals of great user experiences, and they must converge on the platform to drive user adoption. It is crucial that companies design their platform for the “moments that matter” to the stakeholders – the most important tasks the platform will perform for each stakeholder group. These are the “absolute musts” that the platform must deliver to delight the user with the experience.\nAI Role The three ACT attributes are also where artificial intelligence can play a profound role. Consider the following example with a customer as the stakeholder for your company’s platform. To anticipate the customer’s needs, you first must be able to understand who the customer is. Therefore, you need data around the person (not a persona). It’s not enough to know that this is a credit card customer with a high net worth. To anticipate this customer’s needs, you need to know that it’s Ryder and how he has interacted with your company in the past. Ai can take that personal information and determine the next thing you should help Ryder with.\nAi could be a vital link in helping you create a superb experience for the user. This is a great – and natural – place to start the use of Ai. Think of your company as platforms. Think about the stakeholders that use those platforms. Recognize that you need to cater to each stakeholder to deliver a great experience. That means you need to be able to anticipate their needs, provide complete experiences and in a timely fashion.\nConclusion If you’re looking for how and where your company should adopt artificial intelligence, look at your platforms. Look at the user experiences. Look at the point at which you’re trying to anticipate the stakeholder’s needs. This is the point where you should use Ai.\nYou Rock! ","date":1566259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566259200,"objectID":"b03543d0c1ada8f3252ab7cd8580df63","permalink":"/post/aiessential/","publishdate":"2019-08-20T00:00:00Z","relpermalink":"/post/aiessential/","section":"post","summary":"Companies widely recognize the potential power of artificial intelligence (Ai). They instinctively understand that it feels like we’re on the cusp of something that will change our lives and our businesses in a profound way. Yet, many struggle with where to apply it. Executives can’t shake the feeling that they should have use cases for Ai and use it productively today, even recognizing that Ai is not mature yet and will be far more powerful tomorrow and in the future.","tags":null,"title":"Ai Essential","type":"post"},{"authors":null,"categories":null,"content":" Data Scientists have a strange predicament in an age of Ai: They are meant to solve problems for companies by marshaling the relevant data on customers and transactions, but the data itself is going to raise new, unexpected questions.\nIncreasingly, the data that is relevant for companies’ Ai system efforts will be not just some data, but all of it; anything less risks missing what could conceivably be the critical insight down the road, the answer to questions as yet unasked.\nUntil recently, the era of “big data,” has been about providing only the requisite information to answer some straightforward question, where the “known unknowns” are all that matters.\nFor instance, if you’re a retailer, you might want to know how many of your customers would be likely to return items they’ve bought based on patterns of purchases. In fact, a group from Indian online apparel retailer Myntra this summer showed off a machine learning model for just such an application. The scientists at the firm were able to show with a high degree of confidence that they could predict which customers would return items before they’d even checked out of the store.\nThat kind of research is still framed within the structure of conventional statistics. Given an independent variable, such as past patterns of purchases, and a dependent variable of interest, such as return rates, find the relationship between the two. But that presumes the question of returns is the most relevant one to be asking.\nThe piling up of data implies another kind of question-asking, one where neither the independent variables nor the dependent variables may be known. Big data is ushering in a period of “known unknowns.”\nIf you’re a retailer, calculating the known unknown of product returns in order to reduce the rate of returns could be a very meaningful immediate contributor to financial performance. It may suggest practical steps you can take to reduce returns, which saves money.\nBut the known unknown traps you in the question you’ve formulated, whereas there might be better questions to ask, and better models to construct. What if the presence of returns is one behavior your shoppers exhibit within a broader trend of sub-optimal consumption? They’re returning half their purchases because the total assortment, say, is poor. Is the best thing to predict individual patterns, or to better asses aggregate demand? Trying to avert returns, in other words, might be a less-useful strategy than trying to boost overall demand.\nThe age of known unknowns has been anticipated by scholars of Ai. Among the best examples of such deep thinkers is Vladimir Vapnik, a Russian emigre best known for developing over the course of forty years something called “statistical learning theory.” Vapnik’s work would lead in the 1990s to one of the most popular forms of Ai technology prior to today’s deep learning, known as “support vector machines.” Today, Vapnik is a member of Facebook’s Ai research group.\nEven with fairly simple problems such as supervised linear regression, Vapnik has shown in numerous written works over the years that there might not be full information about some of the variables. For instance, in a study of cancer patients, just comparing prevalence of cancer among a population group might leave out other variables about patients that are “hidden” but that are crucial to making medical predictions.\nOf course, designing a good experiment is something statisticians have always struggled with, how to pick the right variables and such. But Vapnik has gone farther than assessing study design, he has offered a much broader philosophy of science.\nIn an interview in 2014 with the staff of the Association for Computer Learning, Vapnik observed that most of science classically operated on the principle that there’s a “simple rule” underlying natural phenomena. Quoting Einstein, Vapnik says, “when the solution is simple, God is answering,” meaning, “if a law is simple we can find it.” But, says Vapnik, most of the world is not so simple, and it doesn’t yield to simple rules that take the form of fitting curves to points on a graph.\n“So the question is what is the real-world? Is it simple or complex?” asks Vapnik, rhetorically. His conclusion is that “machine learning shows that there are examples of complex worlds,” and that “we should approach complex worlds from a completely different position than simple worlds.”\nWhat does that mean? For industries, it means that assuming one knows the question is probably a premature conclusion. The mass of data inside corporations is going to yield entirely new kinds of questions that have nothing to do with the questions currently being asked by either conventional statistics or current machine learning models.\nPrepare for the unknown, you can count on it!\nYou Rock! ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"9886d2ba4feaf40cabc7cf7229399b3e","permalink":"/post/knownunkowns/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/post/knownunkowns/","section":"post","summary":"Data Scientists have a strange predicament in an age of Ai: They are meant to solve problems for companies by marshaling the relevant data on customers and transactions, but the data itself is going to raise new, unexpected questions.\nIncreasingly, the data that is relevant for companies’ Ai system efforts will be not just some data, but all of it; anything less risks missing what could conceivably be the critical insight down the road, the answer to questions as yet unasked.","tags":null,"title":"Known Unknowns","type":"post"},{"authors":null,"categories":null,"content":" We tend to think that people can easily tell what we’re thinking and feeling. They can’t. Understanding the illusion of transparency bias can improve relationships, career performance, and more.\n“A wonderful fact to reflect upon, that every human creature is constituted to be that profound secret and mystery to every other.” ― Charles Dickens, A Tale of Two Cities\nWhen we experience strong emotions, we tend to think it’s obvious to other humans, especially those who know us well. When we’re angry or tired or nervous or miserable, we may assume that anyone who looks at our face can spot it straight away.\nThat’s not true. Most of the time, other people can’t correctly guess what we’re feeling or thinking. Our emotions are not written all over our face all the time. The gap between our subjective experience and what other people pick up on is known as the illusion of transparency. It’s a fallacy that leads us to overestimate how easily we convey our emotions and thoughts.\nFor instance, we arrive at the office exhausted after a night with too little sleep. We drift around all day, chugging espressos, feeling sluggish and unfocused. Everything we do seems to go wrong. At the end of the day, we sheepishly apologize to a coworker for being “useless all day.”\nThey look at you, slightly confused. ‘Oh,’ they say. ‘You seemed fine to me.’ Clearly, they’re just being polite. There’s no way your many minor mistakes during the day could have escaped their notice. It must be extra apparent considering our coworkers all show up looking fresh as a daisy every single day.\nOr imagine that we have to give a talk in front of a big audience and we’re terrified. As one steps on stage, our hands shake, our voice keeps catching in our throat, we’re sweating and flushed. Afterward, you chat to someone from the audience and remark: ‘So that’s what a slow-motion panic attack looks like.’\n‘Well, you seemed like a confident speaker,’ they say. ‘You didn’t look nervous at all. I wish I could be as good at public speaking.’ Evidently, they were sitting at the back or they have bad eyesight. Your shaking hands and nervous pauses were far too apparent. Especially compared to the two wonderful speakers who came after you.\nNo One Cares “Words are the source of misunderstandings.” ― Antoine de Saint-Exupéry, The Little Prince\nThe reality is that other humans pay much less attention to you than you think. They’re often far too absorbed in their own subjective experiences to pick up on subtle cues related to the feelings of others. If you’re annoyed at your partner, they’re probably too busy thinking about what they need to do at work tomorrow or what they’re planning to cook for dinner to scrutinize your facial expressions. They’re not deliberately ignoring you, they’re just thinking about other things. While you’re having a bad day at work, your coworkers are probably distracted by their own deadlines and personal problems. You could fall asleep sitting up and many of them wouldn’t even notice. And when you give a talk in front of people, most of them are worrying about the next time they have to do any public speaking or when they can get a coffee.\nIn our own subjective experience, we’re in the eye of the storm. But what other humans have to go on are things like our tone of voice, facial expressions, and body language. The clues these provide can be hard to read. Unless someone is trying their best to figure out what you’re thinking or feeling, they’re not going to be particularly focused on our body language. If you make even the slightest effort to conceal your inner state, you’re quite able to hide it altogether from everyone.\nOur tendency to overestimate how much attention people are paying to us is a result of seeing our own perspective as the only perspective. If we’re feeling a strong emotion, we assume other people care about how we feel as much as we do. This egocentric bias leads to the spotlight effect—in social situations, we feel like there’s a spotlight shining on us. It’s not self-obsession, it’s natural. But overall, this internal self-focus is what makes you think other people can tell what you’re thinking.\nTake the case of lying. Even if we try to err on the side of honesty, we all face situations where we feel we have no option except to tell a lie. Setting aside the ethics of the matter, most of us probably don’t feel good about lying. It makes us uncomfortable. It’s normal to worry that whoever you’re lying to will easily be able to tell. Again, unless you’re being very obvious, the chances of someone else picking up on it are smaller than you think. In one study, participants asked to lie to other participants estimated they’d be caught about half the time. In fact, people only guessed they were lying about a quarter of the time—a rate low enough for random chance to account for it.\nTactics “Even if one is neither vain nor self-obsessed, it is so extraordinary to be oneself—exactly oneself and no one else—and so unique, that it seems natural that one should also be unique for someone else.” ― Simone de Beauvoir\nUnderstanding how the illusion of transparency works can help you navigate otherwise challenging situations with ease.\nStart with accepting that other humans don’t usually know what we’re feeling and thinking. If you want someone to know your mental state, you need to tell them in the clearest terms possible. You can’t make assumptions. Being subtle about your feelings is not the best idea, especially in high-stakes situations. Err on the side of caution whenever possible by communicating plainly in words about your feelings or views.\nLikewise, if you think you know how someone else feels, you should ask them to confirm. You shouldn’t assume you’ve got it right—you probably haven’t. If it’s important, one need to double check. The person who seems calm on the surface might be frenzied underneath. Some of us just appear unhappy to others all the time, no matter how we’re feeling. If you can’t pick up on someone’s mental state, they might not be vocalizing it because they think it’s obvious. So ask.\nAs Dylan Evans writes in Risk Intelligence: How To Live With Uncertainty,\n The first and most basic remedy is simply to treat all your hunches about the thoughts and feelings of other people with a pinch of salt and to be similarly skeptical about their ability to read your mind. It can be hard to resist the feeling that someone is lying to you, or that your own honesty will shine through, but with practice it can be done.\n The illusion of transparency doesn’t go away just because you know someone well. Even partners, family members and close friends have difficulty reading each other’s mental states. The problem compounds when we think they should be able to do this. We can easily become annoyed when they can’t. If you’re upset or angry and someone close to you doesn’t make any attempt to make you feel better, they are not necessarily ignoring you. They just haven’t noticed anything is wrong, or they may not know how you want them to respond. It’s best not to assume malicious intent. Understanding this can help avoid arguments that spring up based on thinking we’re communicating clearly when we’re not.\n“Much unhappiness has come into the world because of bewilderment and things left unsaid.” ― Fyodor Dostoevsky\nSet Yourself Free Knowing about the illusion of transparency can be liberating. Guess what? No one really cares. Or almost no one. If you’ve got food stuck between your teeth or you stutter during a speech or you’re exhausted at work, you might as well assume no one has noticed. Most of the time, they haven’t.\nBack to public speaking: We get it all wrong when we think people can tell we’re nervous about giving a talk. In a study entitled “The illusion of transparency and the alleviation of speech anxiety,” Kenneth Savitskya and Thomas Gilovich tested how knowing about the effect could help people feel less scared about public speaking. When participants were asked to give a speech, their self-reported levels of nervousness were well above what audience members guessed they were experiencing. Inside, they felt like a nervous wreck. On the outside, they looked calm and collected.\nBut when speakers learned about the illusion of transparency beforehand, they were less concerned about audience perceptions and therefore less nervous. They ended up giving better speeches, according to both their own and audience assessments. It’s a lot easier to focus on what we’re saying if we’re not so worried about what everyone else is thinking.\nThe Sun Revolves Around Me, doesn’t it? In psychology, anchoring refers to our tendency to make an estimated guess by selecting whatever information is easily available as our “anchor,” then adjusting from that point. Often, the adjustments are insufficient. This is exactly what happens when you try to guess the mental state of others. If we try to estimate how a friend feels, we take how we feel as our starting point, then adjust our guess from there.\nAccording to the authors of a paper entitled “The Illusion of Transparency: Biased Assessments of Other’s Ability to Read One’s Emotional States,”\n People are typically quite aware of their own internal states and tend to focus on them rather intently when they are strong. To be sure, people recognize that others are not privy to the same information as they are, and they attempt to adjust for this fact when trying to anticipate another’s perspective. Nevertheless, it can be hard to get beyond one’s own perspective even when one knows that.\n This is similar to hindsight bias, where things seem obvious in retrospect, even if they weren’t beforehand. When we look back on an event, it’s hard to disentangle what we knew then from what we know now. We can only use our current position as an anchor, a perspective which is inevitably skewed.\nIf you’re trying to hide your mental state, you’re probably doing better than you think. Unless you’re talking to, say, a trained police interrogator or professional poker player, other humans are easy to fool. They’re not looking that hard, so a mild effort to hide your emotions is likely to work well. Humans can’t read your mind, whether you’re trying to pretend you don’t hate the taste of a trendy new beverage, or trying to conceal your true standing in a negotiation to gain more leverage.\nThe illusion of transparency explains why, even once you’re no longer a teenager, it still seems like few people understand you. It’s not that other people are ambivalent or confused. Your feelings just aren’t as clear as you think. Often you can’t see beyond the confines of your own head and neither can anyone else. It’s best to make allowances for that.\nYou Rock! ","date":1564963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564963200,"objectID":"c9f5b46a50686ad644897bda84b182d4","permalink":"/post/illusionoftransparency/","publishdate":"2019-08-05T00:00:00Z","relpermalink":"/post/illusionoftransparency/","section":"post","summary":"We tend to think that people can easily tell what we’re thinking and feeling. They can’t. Understanding the illusion of transparency bias can improve relationships, career performance, and more.\n“A wonderful fact to reflect upon, that every human creature is constituted to be that profound secret and mystery to every other.” ― Charles Dickens, A Tale of Two Cities\nWhen we experience strong emotions, we tend to think it’s obvious to other humans, especially those who know us well.","tags":null,"title":"My Poker Face Is Better Than You Think","type":"post"},{"authors":null,"categories":null,"content":" My morning ritual on days when I work in Manhattan is to get off the train at Grand Central Station, use my phone to order my coffee on the Starbucks app with an invisible pre-funded payment, and then join the crowd at the pickup line. I’m not alone: Mobile Order and Pay represents 12% of Starbucks’ U.S. payments and more people use Starbucks’ proprietary payment system than use Apple Pay. After loading cash onto the app, payments are instant, invisible, and free: three words that are a chilling combination for any high street bank in the payments business.\nThe problem for banks is that Starbucks isn’t in the payments business — it’s in the latte business. Its payments app is just an enabler of a great coffee-buying experience. Similarly, the biggest originator of small business checking accounts in the U.S. today is not a well-known bank, but Uber, which opens bank accounts for new drivers so that payments to them can be instant, invisible, and free.\nAs a growing number of companies learn from Starbucks and Uber and focus on payments as an enabler of their core business model, banks will face significant disruption. If some banks are currently complacent, perhaps that’s because things haven’t changed much for banks in the U.S. — yet.\nOver the past 13 years, 3,000 new fintechs have sprouted up in major banking markets (60% of them are licensed payments companies such as Stripe, Square, Klarna and Revolut) and they now make up 17% of active financial institutions. However, despite about $29 billion worth of venture capital pouring into new payments ventures over the last 7 years, these new entrants have only captured 5% of revenues and even less profit. In the United Kingdom, the most disrupted developed market, new entrants take about 15% of total retail financial services revenue, but in a stable market like Canada, the number is less than 2% and less than 1% of payments revenue.\nHowever, things are moving much more quickly in the developing world. Looking at China suggests that banks elsewhere should gird for disruption. China has quickly transitioned from cash to mobile, bootstrapped via the QR code systems of Ant Financial and Tencent, completely skipping the card-based payment systems that are still ubiquitous in the West. In 2017, over half a billion people in China made at least one mobile payment transaction, processing nearly $13 trillion worth of mobile payments. To put that number in perspective, it is two thirds of all mobile payments globally.\nIn some technologically advanced coastal cities in China, 90% of consumer payments are now mobile, and it is not uncommon to see beggars on the streets with QR codes in front of them because they know spare change is becoming a thing of the past. And the growth is likely to continue, with Alipay building a system that can process 100 billion transactions daily. By 2021, 624 million people in China will make regular mobile payments, according to eMarketer. While China is a beacon for how fast mobile payments can take hold, it is not alone. In India mobile wallet penetration is now at 50% of consumers and mobile grew 22-fold last year, enabled by the new UPI payments interface.\nAs this disruption comes to developed economies, banks must adapt because, as we’ve seen in other areas of the economy, the spoils of disruption are not evenly shared. Yes, Google, Apple, Facebook, and Amazon are thriving, but 90 of the top 100 consumer brands in the U.S. lost market share in 2017 – and with the number of retail store closings accelerating, the term retail-apocalypse has entered into the business vocabulary. The tough lesson from China is that in a digital world, privacy in social platforms, retailing, and lifestyle management creates strategic control for everything else, including payments. As a result, if banks become the dumb funding platform for other people’s payments businesses, value will migrate to the likes of Tencent, Starbucks and Amazon.\nBanks that want to stay relevant in payments have three choices:\n focus on upfront payment initiation, add value around individual payments, develop services based on overall payment flows.  Banks focusing on upfront payments can embed their payment solution in third-party products and services, build Request-to-Pay services that shorten the cash cycle for businesses, or create services to reduce fraud. Those that try to add value around individual payments can focus on rewards, digitizing merchant-funded coupons, or by offering point-of-sale services, such as purchase insurance or installment credit. Banks can also develop services based on overall payment flows, such as cashflow management and advice for consumers and businesses, extracting insights from payments and selling them back to merchants in the way American Express, Visa and Mastercard already do, or by selling insurance and payment protection services underpinned by those same insights.\nBanks that are complacent will fall victim to more proactive enterprises. One vision of the future can be seen in two blockchain patents that Walmart has filed — one to manage vendor payments and the other for digital customer payments through Walmart Pay. The patent filings suggest Walmart might be about to disrupt itself, changing into a consignment store where vendors are only paid when their product leaves the store. At the same time, Walmart will capture a rich information stream on every customer transaction, while the information flow to traditional payment intermediaries will be limited in the same way that Starbucks turns the gap between mobile account funding into an information black hole for the banks.\nBanks paying attention to China and Walmart know that the payments business is changing at an accelerating rate towards being instant, invisible, and free. It won’t happen tomorrow, but the trajectory is secular and inexorable. Thinking through these issues – and working back from instant, invisible, and free – will make banks better prepared to protect the economics of their payments business that will otherwise shed revenue at an alarming rate in the coming years.\nYou Rock! ","date":1564272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564272000,"objectID":"7559376121e606eda58ed024139780d0","permalink":"/post/freeinvisibleinstantpayments/","publishdate":"2019-07-28T00:00:00Z","relpermalink":"/post/freeinvisibleinstantpayments/","section":"post","summary":"My morning ritual on days when I work in Manhattan is to get off the train at Grand Central Station, use my phone to order my coffee on the Starbucks app with an invisible pre-funded payment, and then join the crowd at the pickup line. I’m not alone: Mobile Order and Pay represents 12% of Starbucks’ U.S. payments and more people use Starbucks’ proprietary payment system than use Apple Pay.","tags":null,"title":"Free, Invisible \u0026 Instant Payments","type":"post"},{"authors":null,"categories":null,"content":" If you ever feel that the world is against you, you are not alone.\nWe all have a tendency to assume that when anything goes wrong, the fault lies within some great conspiracy against us. A co-worker fails to give you a report in time? They must be trying to derail your career and beat you to a promotion. Your child drops and breaks an expensive plate? They must be trying to annoy you and waste your time. WiFi in a coffee shop not working? The staff must be lying about having it to lure you in and sample their crappy espresso.\nBut the simple fact is that these explanations which we tend to jump to are rarely true. Maybe our co-worker thought today was Tuesday, not Wednesday. Maybe your child had sticky hands from playing with play-doh. Maybe the WiFi router was just broken. This is where Hanlon’s razor comes in.\nThe Basics Of Hanlon’s Razor Hanlon’s Razor is a useful mental model which can be best summarized as such:\n‘Never attribute to malice that which can be adequately explained by neglect.’\nLike Occam’s razor, this heuristic is a useful tool for rapid decision-making and intelligent cognition.\nApplying Hanlon’s razor in our day-to-day lives, allows us to better develop relationships, become less judgmental, and improves rationality. Hanlon’s razor allows us to give people the benefit of the doubt and have more empathy. In this way, the value of Hanlon’s razor is pronounced in relationships and business matters.\nIt’s a simple fact that most of us spend a large part of our day communicating with others and making choices based on that. We all lead complex lives wherein (as Murphy’s law states) things are constantly going wrong. When this occurs, a common response is to blame the nearest human and assume they have malicious intent. People are quick to accuse corporations, politicians, their bosses, employees, coffee shop workers and even family of trying to derail them. When someone messes up around us, we forget how many times we too have done the same. We forget how many times we have elbowed someone in the street, knocked over a drink at a relative’s house or forgotten to meet a friend at the right time. Instead, the perpetrator becomes a source of intense irritation.\nTo assume intent in such an event is likely to worsen the problem. None of us can ever know what someone else wanted to happen. The smartest humans make a lot of mistakes. Inability or neglect is far more likely to be the cause than malice. When a situation causes us to become angry or frustrated, it can be valuable to consider if those emotions are justified. Often, the best way to react to other people causing us problems is by seeking to educate them, not to disdain them. In this way, we can avoid repeats of the same situation.\nOrigins Of Hanlon’s Razor The phrase ‘Hanlon’s razor’ was coined by Robert J. Hanlon, but it has been voiced by many humans throughout history, as far back as 1774.\nNapoleon Bonaparte famously declared:\n‘Never ascribe to malice that which is adequately explained by incompetence.’\nGoethe wrote similarly in The Sorrows of Young Werther in 1774:\n Misunderstandings and neglect create more confusion in this world than trickery and malice. At any rate, the last two are certainly much less frequent.\n The German general Kurt von Hammerstein-Equord used Hanlon’s razor to assess his men, saying:\n I divide my officers into four groups. There are clever, diligent, stupid, and lazy officers. Usually two characteristics are combined. Some are clever and diligent – their place is the General Staff. The next lot are stupid and lazy – they make up 90 percent of every army and are suited to routine duties. Anyone who is both clever and lazy is qualified for the highest leadership duties, because he possesses the intellectual clarity and the composure necessary for difficult decisions. One must beware of anyone who is stupid and diligent – he must not be entrusted with any responsibility because he will always cause only mischief.\n The Place Of Hanlon’s Razor In A Latticework Of Knowledge Hanlon’s razor works best when combined and contrasted with other mental models in our latticework of knowledge. Here are some examples of the useful interactions:\n The availability heuristic. This mental model states we misjudge the frequency of recent events. In particular, this occurs if they are vivid and memorable. Many people have a tendency to keep an internal scorecard of other people’s mistakes. For instance, imagine that a Uber driver takes a wrong turn and makes a journey more expensive. A month later, the same thing occurs with a different driver. We are likely to recall the previous event and react by seeing all Uber drivers as malicious. Instead of accepting both as simple mistakes, the availability of the memory makes us imagine malicious intent. By combining these two mental models, we can understand why certain situations provoke such strong emotions. When a memory is vivid and easy to recall, we may ignore Hanlon’s razor.\n Confirmation bias. We all have a tendency to look for information which confirms preexisting beliefs. When cognitive dissonance arises, we aim to realign our worldviews. Overcoming confirmation bias is a huge step towards making better choices motivated by logic, not emotions. Hanlon’s razor assists with this. If we expect malicious intent, we are likely to attribute it wherever possible. For example, if someone sees a certain politician as corrupt, they will look for information which confirms that. They become unable to identify when mistakes are the result of incompetence or accident.\n Bias from disliking/hating. Hanlon’s razor can provide insights when we deal with humans, institutions, or entities which we dislike. The more we dislike someone or something, the more likely we are to attribute their actions to malice. When someone we dislike makes a mistake, reacting with empathy and understanding tends to be the last response. Acting in an emotional way is natural, yet immature. It can only worsen the situation. The smartest solution is, no matter how much we dislike someone, to assume neglect or incompetence.\n We also like to attribute our own flaws and failures to someone else, which is a cheap psychological protective mechanism called projection. This allows us to maintain a positive self-image and view friction as someone else’s fault rather than our own. It’s best to run a reality check before blaming others.\n  Applications Of Hanlon’s Razor The Media Modern media treats outrage as a profitable commodity. This often takes the form of articles which attribute malice to that which could be explained by incompetence or ignorance. We see examples of this play out in the media multiple times a day. Humans rush to take offense at anything which contradicts their worldview or which they imagine to do so. Media outlets are becoming increasingly skilled at generating assumptions of malicious intent. When looking at newspapers, websites, and social media, it can be beneficial to apply Hanlon’s razor to what we see.\nFor example, when Apple’s Siri voice search launched, humans noticed that it could not search for abortion clinics. This was immediately taken up as proof of misogyny within the company when in fact, a programming error caused the problem.\nA similar issue has occurred a number of times with YouTube content policies. When videos discussing LGBTQ matters were filtered on the restrictive viewing mode, many people took extreme offense at this. The reality is that again, this was an algorithm error and not a case of homophobia on the part of their programmers. Countless videos which do not discuss anything related to LGBTQ issues have also been filtered. This shows it to be a case of confirmation bias, wherein humans see the malice they expect to see.\nCommunication \u0026amp; Relationships One of the most valuable uses of Hanlon’s razor is in communication and relationships. It is common for people to damage relationships by believing other people are intentionally trying to cause problems for them, or behaving in a way intended to be annoying. In most cases, these situations are the result of inability or accidental mistakes.\nDouglas Hubbard expanded upon the idea in Failure of Risk Management: Why it’s Broken and How to Fix it:\n I would add a clumsier but more accurate corollary to this: ‘Never attribute to malice or stupidity that which can be explained by moderately rational individuals following incentives in a complex system of interactions.’ People behaving with no central coordination and acting in their own best interest can still create results that appear to some to be clear proof of conspiracy or a plague of ignorance.\n A further example can be seen when semantic barriers interfere with communication. We have all encountered people struggling to speak our native language, perhaps because they are a tourist or have recently moved to a new state. We have probably seen someone get frustrated at them or even been the one getting annoyed. Or if you have ever traveled to or lived in a country where you are not fluent in the language, you might have been the one people got annoyed at. Realistically, the person asking you for directions or struggling to order their coffee is not mixing up their nouns and speaking in a strong accent on purpose.\nHanlon’s razor tells us they are merely inarticulate and are not trying to waste anyone’s time. The same issues occur when a person uses language which is considered too complex or too basic. This may form a semantic barrier, as other people assume they are trying to confuse them or are being blunt.\nA short-cut to regulating what can be strong reactions to inadvertent events is to conscientiously reframe the perpetrator as a toddler knocking over a vase. Their actions are rendered unintentional and clumsy, highlighting their need for help, maturation or supervision, allowing you to rapidly regain composure and not take it personally.\nExceptions \u0026amp; Issues Like any mental model, Hanlon’s razor has its limitations and its validity has been contested. Some critics consider Hanlon’s razor to be an overly naive idea which can blind people to true malice. While people have malicious intent far less often than we think, it is still something which must be taken into account. Sometimes actions which could be attributed to incompetence are in fact consciously or unconsciously malicious.\nAn instance of Hanlon’s razor being proven wrong is the mafia. Prior to the 1960s, the existence of the mafia was considered to be a conspiracy theory. Only when a member contacted law enforcement, did police realize that the malice being perpetrated was carefully orchestrated.\nTo make the best use of Hanlon’s razor, we must be sure to put it in context, taking into account logic, experience, and empirical evidence. Make it a part of your latticework of mental models, but do not be blind to behavior which is intended to be harmful.\nYou Rock! ","date":1563148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563148800,"objectID":"6d9a3e4df37d643a0d95103a99f31019","permalink":"/post/hanlonrazor/","publishdate":"2019-07-15T00:00:00Z","relpermalink":"/post/hanlonrazor/","section":"post","summary":"If you ever feel that the world is against you, you are not alone.\nWe all have a tendency to assume that when anything goes wrong, the fault lies within some great conspiracy against us. A co-worker fails to give you a report in time? They must be trying to derail your career and beat you to a promotion. Your child drops and breaks an expensive plate? They must be trying to annoy you and waste your time.","tags":null,"title":"The Great Conspiracy Against Us","type":"post"},{"authors":null,"categories":null,"content":" One of the most common lines of questioning from data scientists today goes something like this: “How do I know if my model is good? Should I just take R^2 score? Should I take AUC? What does it mean for a given model to know if it’s good or not?”\nThe best way to decide whether or not a model is good is to have the business interests directly plugged into that decision.\nThat means instead of:\n My accuracy is very high.\n The question or concern should be something like:\n How much money do I gain (or lose) from each prediction?\n Data scientists who can’t answer whether the business is gaining or losing money from a given prediction, shouldn’t even think about putting that model into production.\nAnother component of knowing whether a model is good or not comes down to proper monitoring. Often times when putting together a dataset that will be used to build a model, data scientists think about building the model that one time, but they don’t think about how they’re going to keep track of how well it’s actually doing in the future. How does one get the information - the baseline - to compare the model against?\nIt’s a tricky feedback loop that must be built into workflows, but it gets even harder:\n The more rare the thing you’re trying to predict is OR\n The longer it takes for your prediction to be proven right or wrong\n  So for example, when trying to predict whether somebody will default on a loan, one might not know for 10 years if the prediction was correct or not. Nevertheless, (s)he must think about how to feed that back into the model.\nThe bottom line is making sure to work within a framework when building models instead of building in isolation and then figuring out monitoring - or even pushing to production - after the fact.\nYou Rock! ","date":1562112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562112000,"objectID":"fb2fd0e5db700810f59eb7e758c462db","permalink":"/post/modeltest/","publishdate":"2019-07-03T00:00:00Z","relpermalink":"/post/modeltest/","section":"post","summary":"One of the most common lines of questioning from data scientists today goes something like this: “How do I know if my model is good? Should I just take R^2 score? Should I take AUC? What does it mean for a given model to know if it’s good or not?”\nThe best way to decide whether or not a model is good is to have the business interests directly plugged into that decision.","tags":null,"title":"How Do I Know If My Model Is Good?","type":"post"},{"authors":null,"categories":null,"content":" No action exists in a vacuum. There are ripples that have consequences that we can and can’t see. Here are the three types of externalities that can help us guide our actions so they don’t come back to bite us.\nAn externality affects someone without them agreeing to it. As with unintended consequences, externalities can be positive or negative. Understanding the types of externalities and the impact they have in our lives can help us improve our decision making, and how we interact with the world.\nExternalities provide useful mental models for understanding complex systems. They show us that systems don’t exist in isolation from other systems. Externalities may affect uninvolved third parties which make them a form of market failure-—an inefficient allocation of resources.\nWe both create and are subject to externalities. Most are very minor but compound over time. They can inflict numerous second-order effects. Someone reclines their seat on an airplane. They get the benefit of comfort. The person behind bears the cost of discomfort by having less space. One family member leaves their dirty dishes in the sink. They get the benefit of using the plate. Someone else bears the cost of washing it later. We can’t expect to interact with any system without repercussions. Over time, even minor externalities can cause significant strain in our lives and relationships.\nThe First Law of Ecology To understand externalities it is first useful to consider second-order consequences. In Filters Against Folly, Garrett Hardin describes what he considers to be the First Law of Ecology: We can never do one thing. Whenever we interact with a system, we need to ask, “And then what? What will the wider repercussions of our actions be?” There is bound to be at least one externality.\nHardin gives the example of the Prohibition Amendment in the U.S. In 1920, lawmakers banned the production and sale of alcoholic beverages throughout the entire country. This was in response to an extended campaign by those who believed alcohol was evil. It wasn’t enough to restrict its consumption—-it needed to go.\nThe addition of 61 words to the American Constitution changed the social and legal landscape for over a decade. Policymakers presumably thought they could make the change and humans would stop drinking. But Prohibition led to numerous externalities. Alcohol is an important part of many people’s lives. Few were willing to suddenly give it up without a fight. The demand was more than strong enough to ensure a black-market supply re-emerged.\nWealthy people stockpiled alcohol in their homes before the ban went into effect. Thousands of speakeasies and gin joints flourished. Walgreens grew from 20 stores to 500, in large part due to its sales of ‘medicinal’ whiskey. Former alcohol producers simply sold the ingredients for people to make their own. Gangsters like Al Capone made their fortune smuggling, and murdered his rivals in the process. Crime gangs undermined official institutions. Tax revenues plummeted. People lost their jobs. Prisons became overcrowded and bribery commonplace. Thousands died from crime and drinking unsafe homemade alcohol.\nPolicymakers did not fully ask, “And then what?” before legislating. Drinking did decrease during this time, on average by about half. But this was far from the hope of a total ban. The second-order consequences outweighed any benefits.\nAs economist Gregory Mankiw explains in Principles of Microeconomics,\n In the presence of externalities, society’s interest in a market outcome extends beyond the well-being of buyers and sellers who participate in the market; it also includes the well-being of bystanders who are affected indirectly…. The market equilibrium is not efficient when there are externalities. That is, the equilibrium fails to maximize the total benefit to society as a whole.\n Negative Externalities Negative externalities can occur during the production or consumption of a service or good. Pollution is a useful example. If a factory pollutes nearby water supplies, it causes harm without incurring costs. The costs to society are high and are not reflected in the price of whatever the factory makes. Economists often view environmental damage as another factor in a production process. But even if pollution is taxed, the harmful effects don’t go away.\nTransport and manufacturing release toxins into the environment, harming our health and altering our climate. The reality though, is these externalities are hard to see, and it is often difficult to trace them back to their root causes. There’s also the question of whether we are responsible for externalities or not.\nImagine you’re driving down the road. As you go by an apartment, the noise disturbs someone who didn’t agree to it. Your car emits air pollution, which affects everyone living nearby. Each of these small externalities will affect people you don’t see and who didn’t choose them. They won’t receive any compensation from you. Are you really responsible for the externalities you cause? If you’re not being outright careless or malicious, isn’t it just part of life? How much responsibility do we have as individuals, anyway?\nCalling something a negative externality can be a convenient way of abdicating responsibility.\nPositive Externalities A positive externality imposes an unexpected benefit on a third party. The producer doesn’t agree to this, nor do they receive compensation for it.\nScientific research often leads to positive externalities. Research findings can have applications beyond their initial scope. The resulting information becomes part of our collective knowledge base. However, the researcher who makes a discovery cannot receive the full benefits. Nor do they necessarily feel entitled to them.\nBlaise Pascal and Pierre de Fermat developed probability theory to solve a gambling dispute. Their work went on to inform numerous disciplines (like the field of calculus) and transform our understanding of the world. Probabilities are now a core part of how we think. Pascal and Fermat created a positive externality.\nSomeone who comes up with an equation cannot expect compensation each time it gets used. As a result, the incentives to invest the time and effort to discover new equations are reduced. Algorithms, patents, and copyright laws change this by allowing creators to protect and profit from their ideas for years before other people can freely use them. We all benefit, and researchers have an incentive to continue their work.\nNetwork effects are an example of a positive externality. Silicon Valley understands this well. Each person who joins a network, like a marketplace app, increases the value to all other users. Those who own the network have an incentive improve it to encourage new users. Everyone benefits from being able to communicate with more people. While we might not join a new network intending to improve it for other people, that is what normally happens. (On the flipside, network effects can also produce negative externalities, as too many members can decrease the value of a network.)\nPositive externalities often lead to the “free rider” problem. When we enjoy something that we aren’t paying for, we tend not to value it. Not paying can remove the incentive to look after a resource and leads to a Tragedy of the Commons situation. As Aristotle put it, “For that which is common to the greatest number has the least care bestowed upon it.” A good portion of online content succumbs to the free rider problem. We enjoy it and yet we don’t pay for it. We expect it to be free and yet, if users weren’t willing to support sites they would likely fold, start publishing lower quality articles, or sell readers to advertisers who collect their data. The end result, as we see too frequently, is low-quality content funded by page-view advertising.\nPositional Externalities Positional externalities are a form of second-order effects. They occur when our decisions alter the context of future perception or value.\nFor instance, consider what happens when a person decides to start staying at the office an hour late. Perhaps they want a promotion and think it will endear them to managers. Parkinson’s Law states that tasks expand to fit the time allocated to them. What this person would otherwise get done by 5pm, now takes until 6pm. Staying late becomes their norm. Their co-workers notice and start to also stay late. Before long, staying at the office until 6pm becomes the standard for everyone. Anyone who leaves at 5pm is perceived as lazy. Now that 6pm is the norm, everyone suffers. They are forced to work more without deriving any real benefits. It’s a lose-lose situation for everyone.\nSomeone we know once made an investment with a nearly unlimited return by gaming the system. He worked for an investment firm that valued employees according to a perception of how hard they worked and not necessarily by their results. Each Monday he brought in a series of sport coats and left them in the office. He paid the cleaning staff $20 a week to change the coat hanging on his chair and to turn on his computer. No matter what happened, it appeared he was always the first one into the office even though he often didn’t show up from a “client meeting” until 10. When it came to bonus time, he’d get an enormous return on that $20 investment.\nPurchasing luxury goods can create positional externalities. Veblen goods are items we value because of their scarcity and high cost. Diamonds, Lamborghinis, tailor-made suits — owning them is a status symbol, and they lose their value if they become cheaper or if too many people have them. As Luca Lambertini puts it in The Economics of Vertically Differentiated Markets,\n The utility derived from consumption is a function of the quantity purchased relative to the average of the society or the reference group to whom the consumer compares.” In other words, a shiny new car seems more valuable if all your friends are driving battered old wrecks. If they have equally (or more) fancy cars, the value of yours drops. At some point, it seems worthless and it’s time to find a new one. In this way, the purchase of a Veblen good confers a positional externality on other people who own it too.\n That utility can also be a matter of comparison. A person earning $40,000 a year while their friends earn $30,000 will be happier than one earning $60,000 when their friends earn $70,000. When someone’s salary increases, it raises the bar, giving others a new point of reference.\nWe can confer positional externalities on ourselves by changing our attitudes. Let’s say someone enjoys wine but is not a connoisseur. A $10 bottle and a $100 bottle make them equally happy. When they decide to go on a course and learn the subtleties and technicalities of fine wines, they develop an appreciation for the $100 wine and a distaste for the $10. They may no longer be able to enjoy a cheap drink because they raised their standards.\nConclusion Externalities are everywhere. It’s easy to ignore the impact of our decisions—to recline an airplane seat, to stay late at the office, or drop litter. Eventually though, someone always ends up paying. Like the villagers in Hardin’s Tragedy of the Commons, who end up with no grass for their animals, we run the risk of ruining a good thing if we don’t take care of it. Keeping the three types of externalities in mind is a useful way to make decisions that won’t come back to bite us. Whenever we interact with a system, we should remember to ask Hardin’s question: and then what?\nYou Rock! ","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"86ecbbfabf8a2885baf3e5338dac9ee7","permalink":"/post/externalities/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/post/externalities/","section":"post","summary":"No action exists in a vacuum. There are ripples that have consequences that we can and can’t see. Here are the three types of externalities that can help us guide our actions so they don’t come back to bite us.\nAn externality affects someone without them agreeing to it. As with unintended consequences, externalities can be positive or negative. Understanding the types of externalities and the impact they have in our lives can help us improve our decision making, and how we interact with the world.","tags":null,"title":"Why Humans Can Never Do One Thing","type":"post"},{"authors":null,"categories":null,"content":" We can select truths that engage people and inspire action, or we can deploy truths that deliberately mislead. Truth comes in many forms, and experienced communicators can exploit its variability to shape our impression of reality.\nThe truth is not as straightforward as it seems. There are many truths, some of them more honest than others. “On most issues,” writes Hector Macdonald in his book Truth: How the Many Sides to Every Story Shape Our Reality, “there are multiple truths we can choose to communicate. Our choice of truth will influence how those around us perceive an issue and react to it.”\nWe are often left with several truths, some more flattering to us than others. What we choose to see, and what we share with others, says a lot about who we are.\n“There is no worse lie than a truth misunderstood by those who hear it.” — William James\nCompeting Truths According to MacDonald, there are often many legitimate ways of describing a situation. Of course, it’s possible for anyone to cherry-pick the facts or truths they prefer, shaping the story to meet their needs. MacDonald offers an apt demonstration.\n A few years ago, I was asked to support a transformation programme at a global corporation that was going through a particularly tough patch. … I interviewed the corporation’s top executives to gather their views on the state of their industry and their organization. After consolidating all the facts they’d given me, I sat down with the CEO in a plush Manhattan executive suite and asked him whether he wanted me to write the ‘Golden Opportunity’ story or the ‘Burning Platform’ story of his business.\n These two phrases, “Golden Opportunity” and “Burning Platform,” describe two different approaches to telling the same story, or in this case, promoting the same plan. The first describes the incredible potential the client company can realize by transforming itself to meet growing demand. The profit is out there for them if they work together to make the necessary changes! The second phrase refers to internal struggles at the company and a potential downward spiral that can only be arrested if the company transforms itself to correct the problems. Both stories are true, and both are intended to create the same outcome: supporting a painful and difficult transformation. Yet they can create very different impressions in the minds of employees.\nMacDonald illustrates how, when we interact with someone, especially someone who knows more than we do, they have an opportunity to shape our reality. That is, they can shape how we think, our ideas and opinions about a subject. Our perception of reality changes and “because we act on the basis of our perceptions” they change not only our thinking but our actions.\nSpin Masters I remember watching ads on TV when I was a kid claiming that 80 percent of dentists recommended Colgate-Palmolive. I wondered if my mom was trying to kill me by giving me Crest. I wasn’t the best in math, but I reasoned that if 80% of dentists were recommending Colgate, at most 20% were recommending Crest.\nOf course, that’s exactly what Colgate wanted people to think—the survey was in comparison to other brands. But that wasn’t the whole story. The survey actually asked dentists which brands they would recommend, and almost all of them listed several. Colgate wasn’t lying—but they were using a very distorted version of the truth, designed to mislead. The Advertising Standards Authority eventually banned the ad.\nPeople use this sort of spin all the time. Everyone has an agenda. You can deceive without ever lying. Politicians get elected on how effective they are at “spinning truths in a way that create a false impression.” It’s only too easy for political agendas to trump impartial truth.\nThree Types of Communicators “It’s not simply that we’re being lied to; the more insidious problem is that we are routinely misled by the truth.”\nIn Truth, Macdonald explores the effects of three types of communicators: advocates, misinformers, and misleaders.\nAdvocates select competing truths that create a reasonably accurate impression of reality in order to achieve a constructive goal.\nMisinformers innocently propagate competing truths that unintentionally distort reality.\nMisleaders deliberately deploy competing truths to create an impression of reality that they know is not true.\nWe may feel better believing there is one single truth, and thinking everyone who doesn’t see things the way we do simply doesn’t have the truth. That’s not…true. Everyone, including you and me, has a lens on the situation that’s distorted by what they want, how they see the world, and their biases. The most dangerous truths are the credible ones that we convince ourselves are correct.\nOne idea I find helpful when faced with a situation is perspective-taking. I construct a mental room that I fill with all the participants and stakeholders around a table. I then put myself into their seats and try to see the room through their eyes. Not only does this help me better understand reality by showing me my blind spots, but it shows me what other people care about and how I can create win-wins.\nTruth: How the Many Sides to Every Story Shape Our Reality, goes on to explore partial truths, subjective truths, artificial truths, and unknown truths. It’s a terrific read for checking your own perspective on truth, and understanding how truth can be used to both inform and mislead you.\nYou Rock! ","date":1558137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558137600,"objectID":"426a5e780c6da61da62b4e8a2f81acd6","permalink":"/post/stories/","publishdate":"2019-05-18T00:00:00Z","relpermalink":"/post/stories/","section":"post","summary":"We can select truths that engage people and inspire action, or we can deploy truths that deliberately mislead. Truth comes in many forms, and experienced communicators can exploit its variability to shape our impression of reality.\nThe truth is not as straightforward as it seems. There are many truths, some of them more honest than others. “On most issues,” writes Hector Macdonald in his book Truth: How the Many Sides to Every Story Shape Our Reality, “there are multiple truths we can choose to communicate.","tags":null,"title":"Stories That Shape Our Reality","type":"post"},{"authors":null,"categories":null,"content":" Probabilistic thinking is essentially trying to estimate, using some tools of math and logic, the likelihood of any specific outcome coming to pass. It is one of the best tools we have to improve the accuracy of our models. In a world where each moment is determined by an infinitely complex set of factors, probabilistic thinking helps us identify the most likely outcomes. When we know these our decisions can be more precise and effective.\nAre We Going To Get Struck By lightning Or Not? Why we need the concept of probabilities at all is worth thinking about. Things either are or are not, right? We either will get hit by lightning today or we won’t. The problem is, we just don’t know until we live out the day, which doesn’t help us at all when we make our decisions in the morning. The future is far from determined and we can better navigate it by understanding the likelihood of events that could impact us.\nOur lack of perfect information about the world gives rise to all of probability theory, and its usefulness. We know now that the future is inherently unpredictable because not all variables can be known and even the smallest error imaginable in our data very quickly throws off our predictions. The best we can do is estimate the future by generating realistic, useful probabilities. So how do we do that?\nProbability is everywhere, down to the very bones of the world. The probabilistic machinery in our minds—the cut-to-the-quick heuristics made so famous by the psychologists Daniel Kahneman and Amos Tversky—was evolved by the human species in a time before machines, factories, traffic, middle managers, and the stock market. It served us in a time when human life was about survival, and still serves us well in that capacity.\nBut what about today—-a time when, for most of us, survival is not so much the issue? We want to thrive. We want to compete, and win. Mostly, we want to make good decisions in complex social systems that were not part of the world in which our brains evolved their (quite rational) heuristics.\nFor this, we need to consciously add in a needed layer of probability awareness. What is it and how can I use it to my advantage?\nThere are three important aspects of probability that we need to explain so we can integrate them into our thinking to get into the ballpark and improve our chances of catching the ball:\n Bayesian thinking, Fat-tailed curves Asymmetries  Thomas Bayes and Bayesian thinking: Bayes was an English minister in the first half of the 18th century, whose most famous work, “An Essay Toward Solving a Problem in the Doctrine of Chances” was brought to the attention of the Royal Society by his friend Richard Price in 1763—two years after his death. The essay, the key to what we now know as Bayes’s Theorem, concerned how we should adjust probabilities when we encounter new data.\nThe core of Bayesian thinking (or Bayesian updating, as it can be called) is this: given that we have limited but useful information about the world, and are constantly encountering new information, we should probably take into account what we already know when we learn something new. As much of it as possible. Bayesian thinking allows us to use all relevant prior information in making decisions. Statisticians might call it a base rate, taking in outside information about past situations like the one you’re in.\nConsider the headline “Violent Stabbings on the Rise.” Without Bayesian thinking, you might become genuinely afraid because your chances of being a victim of assault or murder is higher than it was a few months ago. But a Bayesian approach will have you putting this information into the context of what you already know about violent crime.\nYou know that violent crime has been declining to its lowest rates in decades. Your city is safer now than it has been since this measurement started. Let’s say your chance of being a victim of a stabbing last year was one in 10,000, or 0.01%. The article states, with accuracy, that violent crime has doubled. It is now two in 10,000, or 0.02%. Is that worth being terribly worried about? The prior information here is key. When we factor it in, we realize that our safety has not really been compromised.\nConversely, if we look at the diabetes statistics in the United States, our application of prior knowledge would lead us to a different conclusion. Here, a Bayesian analysis indicates you should be concerned. In 1958, 0.93% of the population was diagnosed with diabetes. In 2015 it was 7.4%. When you look at the intervening years, the climb in diabetes diagnosis is steady, not a spike. So the prior relevant data, or priors, indicate a trend that is worrisome.\nIt is important to remember that priors themselves are probability estimates. For each bit of prior knowledge, you are not putting it in a binary structure, saying it is true or not. You’re assigning it a probability of being true. Therefore, you can’t let your priors get in the way of processing new knowledge. In Bayesian terms, this is called the likelihood ratio or the Bayes factor. Any new information you encounter that challenges a prior simply means that the probability of that prior being true may be reduced. Eventually, some priors are replaced completely. This is an ongoing cycle of challenging and validating what you believe you know. When making uncertain decisions, it’s nearly always a mistake not to ask: What are the relevant priors? What might I already know that I can use to better understand the reality of the situation?\nNow we need to look at fat-tailed curves: Many of us are familiar with the bell curve, that nice, symmetrical wave that captures the relative frequency of so many things from height to exam scores. The bell curve is great because it’s easy to understand and easy to use. Its technical name is “normal distribution.” If we know we are in a bell curve situation, we can quickly identify our parameters and plan for the most likely outcomes.\nFat-tailed curves are different. Take a look:\nAt first glance they seem similar enough. Common outcomes cluster together, creating a wave. The difference is in the tails. In a bell curve the extremes are predictable. There can only be so much deviation from the mean. In a fat-tailed curve there is no real cap on extreme events.\nThe more extreme events that are possible, the longer the tails of the curve get. Any one extreme event is still unlikely, but the sheer number of options means that we can’t rely on the most common outcomes as representing the average. The more extreme events that are possible, the higher the probability that one of them will occur. Crazy things are definitely going to happen, and we have no way of identifying when.\nThink of it this way. In a bell curve type of situation, like displaying the distribution of height or weight in a human population, there are outliers on the spectrum of possibility, but the outliers have a fairly well defined scope. You’ll never meet a man who is ten times the size of an average man. But in a curve with fat tails, like wealth, the central tendency does not work the same way. You may regularly meet people who are ten, 100, or 10,000 times wealthier than the average person. That is a very different type of world.\nLet’s re-approach the example of the risks of violence we discussed in relation to Bayesian thinking. Suppose you hear that you had a greater risk of slipping on the stairs and cracking your head open than being killed by a terrorist. The statistics, the priors, seem to back it up: 1,000 people slipped on the stairs and died last year in your country and only 500 died of terrorism. Should you be more worried about stairs or terror events?\nSome use examples like these to prove that terror risk is low—since the recent past shows very few deaths, why worry?[1] The problem is in the fat tails: The risk of terror violence is more like wealth, while stair-slipping deaths are more like height and weight. In the next ten years, how many events are possible? How fat is the tail?\nThe important thing is not to sit down and imagine every possible scenario in the tail (by definition, it is impossible) but to deal with fat-tailed domains in the correct way: by positioning ourselves to survive or even benefit from the wildly unpredictable future, by being the only ones thinking correctly and planning for a world we don’t fully understand.\nAsymmetries: Finally, you need to think about something we might call “metaprobability” —the probability that your probability estimates themselves are any good.\nThis massively misunderstood concept has to do with asymmetries. If you look at nicely polished stock pitches made by professional investors, nearly every time an idea is presented, the investor looks their audience in the eye and states they think they’re going to achieve a rate of return of 20% to 40% per annum, if not higher. Yet exceedingly few of them ever attain that mark, and it’s not because they don’t have any winners. It’s because they get so many so wrong. They consistently overestimate their confidence in their probabilistic estimates. (For reference, the general stock market has returned no more than 7% to 8% per annum in the United States over a long period, before fees.)\nAnother common asymmetry is people’s ability to estimate the effect of traffic on travel time. How often do you leave “on time” and arrive 20% early? Almost never? How often do you leave “on time” and arrive 20% late? All the time? Exactly. Your estimation errors are asymmetric, skewing in a single direction. This is often the case with probabilistic decision-making.[2]\nFar more probability estimates are wrong on the “over-optimistic” side than the “under-optimistic” side. You’ll rarely read about an investor who aimed for 25% annual return rates who subsequently earned 40% over a long period of time. You can throw a dart at the Wall Street Journal and hit the names of lots of investors who aim for 25% per annum with each investment and end up closer to 10%.\nThe Spy World Successful spies are very good at probabilistic thinking. High-stakes survival situations tend to make us evaluate our environment with as little bias as possible.\nWhen Vera Atkins was second in command of the French unit of the Special Operations Executive (SOE), a British intelligence organization reporting directly to Winston Churchill during World War II[3], she had to make hundreds of decisions by figuring out the probable accuracy of inherently unreliable information.\nAtkins was responsible for the recruitment and deployment of British agents into occupied France. She had to decide who could do the job, and where the best sources of intelligence were. These were literal life-and-death decisions, and all were based in probabilistic thinking.\nFirst, how do you choose a spy? Not everyone can go undercover in high-stress situations and make the contacts necessary to gather intelligence. The result of failure in France in WWII was not getting fired; it was death. What factors of personality and experience show that a person is right for the job? Even today, with advancements in psychology, interrogation, and polygraphs, it’s still a judgment call.\nFor Vera Atkins in the 1940s, it was very much a process of assigning weight to the various factors and coming up with a probabilistic assessment of who had a decent chance of success. Who spoke French? Who had the confidence? Who was too tied to family? Who had the problem-solving capabilities? From recruitment to deployment, her development of each spy was a series of continually updated, educated estimates.\nGetting an intelligence officer ready to go is only half the battle. Where do you send them? If your information was so great that you knew exactly where to go, you probably wouldn’t need an intelligence mission. Choosing a target is another exercise in probabilistic thinking. You need to evaluate the reliability of the information you have and the networks you have set up. Intelligence is not evidence. There is no chain of command or guarantee of authenticity.\nThe stuff coming out of German-occupied France was at the level of grainy photographs, handwritten notes that passed through many hands on the way back to HQ, and unverifiable wireless messages sent quickly, sometimes sporadically, and with the operator under incredible stress. When deciding what to use, Atkins had to consider the relevancy, quality, and timeliness of the information she had.\nShe also had to make decisions based not only on what had happened, but what possibly could. Trying to prepare for every eventuality means that spies would never leave home, but they must somehow prepare for a good deal of the unexpected. After all, their jobs are often executed in highly volatile, dynamic environments. The women and men Atkins sent over to France worked in three primary occupations: organizers were responsible for recruiting locals, developing the network, and identifying sabotage targets; couriers moved information all around the country, connecting people and networks to coordinate activities; and wireless operators had to set up heavy communications equipment, disguise it, get information out of the country, and be ready to move at a moment’s notice. All of these jobs were dangerous. The full scope of the threats was never completely identifiable. There were so many things that could go wrong, so many possibilities for discovery or betrayal, that it was impossible to plan for them all. The average life expectancy in France for one of Atkins’ wireless operators was six weeks.\nFinally, the numbers suggest an asymmetry in the estimation of the probability of success of each individual agent. Of the 400 agents that Atkins sent over to France, 100 were captured and killed. This is not meant to pass judgment on her skills or smarts. Probabilistic thinking can only get you in the ballpark. It doesn’t guarantee 100% success.\nThere is no doubt that Atkins relied heavily on probabilistic thinking to guide her decisions in the challenging quest to disrupt German operations in France during World War II. It is hard to evaluate the success of an espionage career, because it is a job that comes with a lot of loss. Atkins was extremely successful in that her network conducted valuable sabotage to support the allied cause during the war, but the loss of life was significant.\nConclusion Successfully thinking in shades of probability means roughly identifying what matters, coming up with a sense of the odds, doing a check on our assumptions, and then making a decision. We can act with a higher level of certainty in complex, unpredictable situations. We can never know the future with exact precision. Probabilistic thinking is an extremely useful tool to evaluate how the world will most likely look so that we can effectively strategize\nYou Rock! ","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"ff66f48377ad4bf6335ec2caf0de2c98","permalink":"/post/probabilisticvalues/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/post/probabilisticvalues/","section":"post","summary":"Probabilistic thinking is essentially trying to estimate, using some tools of math and logic, the likelihood of any specific outcome coming to pass. It is one of the best tools we have to improve the accuracy of our models. In a world where each moment is determined by an infinitely complex set of factors, probabilistic thinking helps us identify the most likely outcomes. When we know these our decisions can be more precise and effective.","tags":null,"title":"Probabilistic Value","type":"post"},{"authors":null,"categories":null,"content":" Implementation of Naive Bayes Classifier\nWhat is Naive Bayes?\nNaive Bayes is a classification technique based on the Bayes’ Theorem.\nAccording to Wikipedia, Bayes’ Theorem is defined as the probability of an event, based on prior knowledge of conditions that might be related to the event.\nFor instance, if a disease like diabetes is related to the age of a person, then a person’s age can be used to more accurately assess the likelihood of the person having diabetes based on his age as opposed to predicting probability without making an assessment without prior information.\nThe Naive Bayes Theorem can be mathematically defined as:\nP(A | B) = P(A) * P(B | A) / P(B)\nFor better understanding, this means: P(diabetes/age) = P(diabetes) * P(age/diabetes) / P(age)\nThus, a Naive Bayes Classifier determines the outcome based on prior information or evidence using this formula.\nThe assumption is that some attributes like age and likelihood of diabetes are dependent on each other and this assumption helps in determining the probability of new data.\nImplementaton in R:\nAbout the dataset: Attribute Information: Age of patient at time of operation (numerical) Patient’s year of operation (year - 1900, numerical) Number of positive axillary nodes detected (numerical) Survival status (class attribute) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year  Step 1: Read data\ndf_haberman \u0026lt;- read.csv(\u0026quot;haberman_data.csv\u0026quot;) str(df_haberman) \u0026#39;data.frame\u0026#39;: 306 obs. of 4 variables: $ Age : int 30 30 30 31 31 33 33 34 34 34 ... $ Year_of_operation: int 64 62 65 59 65 58 60 59 66 58 ... $ auxiliary_nodes : int 1 3 0 2 4 10 0 0 9 30 ... $ Survival : int 1 1 1 1 1 1 1 2 2 1 ... df_haberman$Survival \u0026lt;- as.factor(df_haberman$Survival) Aim is to predict whether enrolment was rejected or accepted based on other attributes using Naive Bayes.\nSplitting data into train and testing set sets(80-20)\nset.seed(1234) sample = sample.split(df_haberman, SplitRatio = 0.8) df_train \u0026lt;- subset(df_haberman, sample == TRUE) df_test \u0026lt;- subset(df_haberman, sample == FALSE) df_test_data \u0026lt;- df_test[, c(1,2,3)] plot(as.factor(df_haberman$Survival)) ggplot(data = df_haberman, aes(x = Survival, y = Age)) + geom_point() + theme_classic() Naive Bayes’\nnaive_bayes \u0026lt;- naiveBayes(Survival~Age , data = df_train) summary(naive_bayes)  Length Class Mode apriori 2 table numeric tables 1 -none- list levels 2 -none- character isnumeric 1 -none- logical call 4 -none- call  Predict:\npredict_nb \u0026lt;- predict(naive_bayes, newdata = df_test$Age) Warning in predict.naiveBayes(naive_bayes, newdata = df_test$Age): Type mismatch between training and new data for variable \u0026#39;Age\u0026#39;. Did you use factors with numeric labels for training, and numeric values for new data? predict_nb  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [71] 1 1 1 1 1 1 Levels: 1 2 conf_matrix \u0026lt;- table(predict_nb, df_test$Survival) conf_matrix  predict_nb 1 2 1 58 18 2 0 0 The confusion matrix has predicted values as rows and the actual values as columns. From the confusion matrix, the conclusion is that 58 instances were correctly predicted as Class 1 whereas 18 were predicted as Class 1 but actually belonged to Class 2.\nThus, the model correctly classified all 58 instances of Class 1 but none of those for Class 2.\nThe overall ccuracy can be calculated as- 58/(58+18) = 76.31\nApplying the Naive Bayes’ model, the model has obtained a 76.3% acuracy.\nHowever, this model only considers Age as one of the factors. The next step will be to build a model with all three attributes to determine whether an improvement in accuracy can be achieved.\nnaive_bayes_all \u0026lt;- naiveBayes(Survival ~ ., data = df_train) Predict using test data\npredict_allAttributes \u0026lt;- predict(naive_bayes_all, df_test[, -1]) Warning in predict.naiveBayes(naive_bayes_all, df_test[, -1]): Type mismatch between training and new data for variable \u0026#39;Age\u0026#39;. Did you use factors with numeric labels for training, and numeric values for new data? predict_allAttributes  [1] 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 [71] 1 2 1 1 1 1 Levels: 1 2 confusion_matrix \u0026lt;- table(predict_allAttributes, df_test$Survival) confusion_matrix  predict_allAttributes 1 2 1 54 14 2 4 4 The model built using all the attributes in the data gives a different model than the one built using Age.\nHere, 54 out of 58(54+4) are corretly classified as Class 1 and 4 out of 18(4+14) of Class 2.\nHowever, the overall accuracy is still calculated as- (54+4)/76 = 76.31%\nThus, the overall prediction accuracy remains unchanged.\n  ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"04f2fdaae44d7e931e0a25acc0cd4960","permalink":"/project/naive_bayes/naive_bayes/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/naive_bayes/naive_bayes/","section":"project","summary":"The assumption is that some attributes like age and likelihood of diabetes are dependent on each other and this assumption helps in determining the probability of new data","tags":["R","Machine Learning"],"title":"Diabetes","type":"project"},{"authors":null,"categories":null,"content":" Regression is a statistical technique to analyze the relationship between a response variable and one or more predictors. If the relationship is linear, a straight line can be fit to the data using the following equation- y = mx + c Here, y is the outcome variable, m is the gradient or slope of the line, x is the predictor variable and c is the intercept on the line.\nFor example, studing the relationship between size of a house in square feet and its price. Linear modelling helps in predicting the price of a house based on the size. Thus, the above equation can be written as- price = m * size + intercept\nImplementation Dataset: Turnstile Data of NYC Subway\ndf_turnstile \u0026lt;- read.csv(\u0026quot;turnstile_data_master_with_weather.csv\u0026quot;) head(df_turnstile)  X UNIT DATEn TIMEn Hour DESCn ENTRIESn_hourly EXITSn_hourly 1 0 R001 2011-05-01 01:00:00 1 REGULAR 0 0 2 1 R001 2011-05-01 05:00:00 5 REGULAR 217 553 3 2 R001 2011-05-01 09:00:00 9 REGULAR 890 1262 4 3 R001 2011-05-01 13:00:00 13 REGULAR 2451 3708 5 4 R001 2011-05-01 17:00:00 17 REGULAR 4400 2501 6 5 R001 2011-05-01 21:00:00 21 REGULAR 3372 2122 maxpressurei maxdewpti mindewpti minpressurei meandewpti meanpressurei 1 30.31 42 35 30.23 39 30.27 2 30.31 42 35 30.23 39 30.27 3 30.31 42 35 30.23 39 30.27 4 30.31 42 35 30.23 39 30.27 5 30.31 42 35 30.23 39 30.27 6 30.31 42 35 30.23 39 30.27 fog rain meanwindspdi mintempi meantempi maxtempi precipi thunder 1 0 0 5 50 60 69 0 0 2 0 0 5 50 60 69 0 0 3 0 0 5 50 60 69 0 0 4 0 0 5 50 60 69 0 0 5 0 0 5 50 60 69 0 0 6 0 0 5 50 60 69 0 0 str(df_turnstile) ## \u0026#39;data.frame\u0026#39;: 131951 obs. of 22 variables: ## $ X : int 0 1 2 3 4 5 6 7 8 9 ... ## $ UNIT : Factor w/ 465 levels \u0026quot;R001\u0026quot;,\u0026quot;R002\u0026quot;,..: 1 1 1 1 1 1 2 2 2 2 ... ## $ DATEn : Factor w/ 30 levels \u0026quot;2011-05-01\u0026quot;,\u0026quot;2011-05-02\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ TIMEn : Factor w/ 32261 levels \u0026quot;00:00:00\u0026quot;,\u0026quot;00:00:03\u0026quot;,..: 1253 6369 11995 17547 23222 28407 1253 6369 11995 17547 ... ## $ Hour : int 1 5 9 13 17 21 1 5 9 13 ... ## $ DESCn : Factor w/ 1 level \u0026quot;REGULAR\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ ENTRIESn_hourly: num 0 217 890 2451 4400 ... ## $ EXITSn_hourly : num 0 553 1262 3708 2501 ... ## $ maxpressurei : num 30.3 30.3 30.3 30.3 30.3 ... ## $ maxdewpti : num 42 42 42 42 42 42 42 42 42 42 ... ## $ mindewpti : num 35 35 35 35 35 35 35 35 35 35 ... ## $ minpressurei : num 30.2 30.2 30.2 30.2 30.2 ... ## $ meandewpti : num 39 39 39 39 39 39 39 39 39 39 ... ## $ meanpressurei : num 30.3 30.3 30.3 30.3 30.3 ... ## $ fog : num 0 0 0 0 0 0 0 0 0 0 ... ## $ rain : num 0 0 0 0 0 0 0 0 0 0 ... ## $ meanwindspdi : num 5 5 5 5 5 5 5 5 5 5 ... ## $ mintempi : num 50 50 50 50 50 50 50 50 50 50 ... ## $ meantempi : num 60 60 60 60 60 60 60 60 60 60 ... ## $ maxtempi : num 69 69 69 69 69 69 69 69 69 69 ... ## $ precipi : num 0 0 0 0 0 0 0 0 0 0 ... ## $ thunder : num 0 0 0 0 0 0 0 0 0 0 ... summary(df_turnstile) ## X UNIT DATEn TIMEn ## Min. : 0 R549 :12198 2011-05-05: 4510 00:00:00: 7144 ## 1st Qu.: 32988 R550 : 6881 2011-05-12: 4503 04:00:00: 7141 ## Median : 65975 R541 : 5922 2011-05-19: 4503 20:00:00: 7112 ## Mean : 65975 R540 : 4420 2011-05-16: 4493 12:00:00: 7105 ## 3rd Qu.: 98962 R543 : 4146 2011-05-23: 4479 16:00:00: 7067 ## Max. :131950 R552 : 3509 2011-05-26: 4479 09:00:00: 6087 ## (Other):94875 (Other) :104984 (Other) :90295 ## Hour DESCn ENTRIESn_hourly EXITSn_hourly ## Min. : 0.0 REGULAR:131951 Min. : 0 Min. : 0.0 ## 1st Qu.: 5.0 1st Qu.: 39 1st Qu.: 32.0 ## Median :12.0 Median : 279 Median : 232.0 ## Mean :10.9 Mean : 1095 Mean : 886.9 ## 3rd Qu.:17.0 3rd Qu.: 1109 3rd Qu.: 847.0 ## Max. :23.0 Max. :51839 Max. :45249.0 ## ## maxpressurei maxdewpti mindewpti minpressurei ## Min. :29.74 Min. :39.00 Min. :22.00 Min. :29.54 ## 1st Qu.:29.96 1st Qu.:50.00 1st Qu.:38.00 1st Qu.:29.84 ## Median :30.03 Median :57.00 Median :51.00 Median :29.91 ## Mean :30.03 Mean :57.24 Mean :48.26 Mean :29.89 ## 3rd Qu.:30.10 3rd Qu.:64.00 3rd Qu.:55.00 3rd Qu.:29.97 ## Max. :30.31 Max. :70.00 Max. :66.00 Max. :30.23 ## ## meandewpti meanpressurei fog rain ## Min. :31.0 Min. :29.64 Min. :0.0000 Min. :0.0000 ## 1st Qu.:45.0 1st Qu.:29.91 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :54.0 Median :29.96 Median :0.0000 Median :0.0000 ## Mean :52.7 Mean :29.97 Mean :0.1671 Mean :0.3342 ## 3rd Qu.:60.0 3rd Qu.:30.05 3rd Qu.:0.0000 3rd Qu.:1.0000 ## Max. :68.0 Max. :30.27 Max. :1.0000 Max. :1.0000 ## ## meanwindspdi mintempi meantempi maxtempi ## Min. : 1.000 Min. :46.00 Min. :55.00 Min. :58.00 ## 1st Qu.: 5.000 1st Qu.:52.00 1st Qu.:60.00 1st Qu.:65.00 ## Median : 5.000 Median :54.00 Median :63.00 Median :71.00 ## Mean : 5.543 Mean :56.17 Mean :64.27 Mean :71.77 ## 3rd Qu.: 6.000 3rd Qu.:60.00 3rd Qu.:68.00 3rd Qu.:78.00 ## Max. :12.000 Max. :70.00 Max. :78.00 Max. :86.00 ## ## precipi thunder ## Min. :0.0000 Min. :0 ## 1st Qu.:0.0000 1st Qu.:0 ## Median :0.0000 Median :0 ## Mean :0.1723 Mean :0 ## 3rd Qu.:0.1000 3rd Qu.:0 ## Max. :2.1800 Max. :0 ##  The aim is to predict Entriesn_hourly i.e. number of hourly entries at the NYC Subway based on other predictor variables.\nSome of the predictor variables are not of any use; X is the row number which won’t be useful in analysis.\nDESCn only has one value in the entire dataset and can be omitted.\ndf_turnstile$X \u0026lt;- NULL df_turnstile$DESCn \u0026lt;- NULL  Exploratory Data Analysis: Scatterplots are a good way of understand the linear relationship between two variables. Following is a scatterplot between the Hour of the day and the number of entries, colored by whether there was rain or not.\nggplot(df_turnstile, aes(x = as.Date(DATEn), y = ENTRIESn_hourly)) + geom_point() + scale_x_date(breaks = seq(as.Date(\u0026quot;2011-05-01\u0026quot;), as.Date(\u0026quot;2011-05-30\u0026quot;), by = \u0026quot;1 day\u0026quot;),labels=date_format(\u0026quot;%d\u0026quot;)) + theme_classic() This first graph shows the distribution of the number of entries over the month of May 2011. The distribution shows 5 days of high entries and 2 days where it is comparitively less; explaining the weekdays and weekend entries.\nStudying the number of turnstile entries based on the Hour of the day.\nggplot(df_turnstile, aes(x = Hour, y = ENTRIESn_hourly)) + geom_point(color = \u0026quot;red\u0026quot;, alpha = 0.1) + scale_x_continuous(limits = c(0,25), breaks = seq(0,25,1)) + theme_classic() This graph shows the number of entries throughout the day. (Hour 1 is the hour from Midnight to 12.59 am, Hour 2 is 1am to 1.59 am and so on)\nThe transparency in the graph shows the level of number of entries; for example, the darker the red color, the more number of data points lie at that point in the graph.\nThere are some spikes in the data like at Hour 1, hour 13 and Hour 22.\nThe next step would be to investigate this relationship further and see how other factors come into play.\ndf_turnstile$DATEn \u0026lt;- as.Date(df_turnstile$DATEn) df_turnstile$dayofweek \u0026lt;- weekdays(df_turnstile$DATEn) ggplot(data = df_turnstile, aes(x = Hour, y = ENTRIESn_hourly, color = dayofweek)) + geom_point() + scale_x_continuous(limits = c(0,25), breaks = seq(0,25,1)) + theme_classic() The above graph is an add on to the previous one. The graph is colored by the day of the week. Now, it can be seen that the spike at Hour 1 is on Saturday. (Maybe people returning home after Saturday night partying).\nHowever, the spikes at Hour 13 and Hour 22 are on Weekdays (Hour 13 is Lunch hour and people going home from work between 10 and 11 pm may be the explanation).\nggplot(data = df_turnstile, aes(x = factor(dayofweek), y = ENTRIESn_hourly)) + geom_point() + theme_classic() This graph shows the distribution of the number of entries over the week. Weekdays have a similar number of entries whereas weekends are less than the rest. This makes sense intuitively since people take the subway on weekdays to commute to work; whereas the frequency of trains is usually less on weekends hence the lesser number of entries.\nThe next step is to study how weather conditions affect these entires over the week.\nggplot(df_turnstile, aes(x = Hour, y = ENTRIESn_hourly, color = factor(rain))) + geom_point() + scale_x_continuous(limits = c(0,25), breaks = seq(0,25,1)) + theme_classic() This graph explains the spikes at Hour 13 and Hour 22; the reason being more people using the subway due to the rain.\nggplot(data = df_turnstile, aes(x = maxtempi, y = ENTRIESn_hourly, shape = factor(rain))) + geom_point(aes(color = Hour)) + scale_x_continuous(limits = c(55,90), breaks = seq(55,90,5)) + theme_classic()  Building a correlation matrix: A correlation matrix is the best way to understand if two factors are collinear. It calculates the correlation coefficient between each factor and others. Correlation basically shows how change in one factor is likely to affect another.\nThe correlation coefficient is a number between -1 and 1. A value of 0 denotes no correlation between two factors; coeffiecient 1 denotes complete position correlation and a value of -1 denotes complete negative correlation.\nThus, if two predictor variables are highly correlated, positively or negatively (i.e. \u0026gt;=0.5 or \u0026lt;=(-0.5)), considering both in the analyses will only increase complexity without giving any real value. In such cases, any one of the factors can be omitted.\ndf_turnstile$rain \u0026lt;- as.numeric(df_turnstile$rain) df_turnstile$Hour \u0026lt;- as.numeric(df_turnstile$Hour) nums \u0026lt;- sapply(df_turnstile, is.numeric) correlation_matrix \u0026lt;- cor(df_turnstile[, nums], method = \u0026quot;spearman\u0026quot;) Warning in cor(df_turnstile[, nums], method = \u0026quot;spearman\u0026quot;): the standard deviation is zero require(corrplot) corrplot(correlation_matrix, method = \u0026quot;color\u0026quot;, type = \u0026quot;lower\u0026quot;) From the above plot, it is evident that rain and fog are highly correlated(0.44) and hence either one of them can be neglected while building the model.\n Significance Tests Chi Square Test/ Pearsons Chi square test\n Goodness of fit Independence of variables (if dependent and independent are statistically related)\nMann Whitney U test\n Building a model: Split data into train and test set.\ndf_turnstile$rain \u0026lt;- as.factor(df_turnstile$rain) df_turnstile$Hour \u0026lt;- as.numeric(df_turnstile$Hour) set.seed(1234) require(caTools) Loading required package: caTools sample \u0026lt;- sample.split(df_turnstile, SplitRatio = 0.75) df_train \u0026lt;- subset(df_turnstile, sample == TRUE) df_test \u0026lt;- subset(df_turnstile, sample == FALSE) f = ENTRIESn_hourly ~ Hour + meandewpti + meanpressurei + meandewpti + rain + precipi + meantempi linear_model1 \u0026lt;- lm(formula = f, data = df_train) summary(linear_model1)  Call: lm(formula = f, data = df_train) Residuals: Min 1Q Median 3Q Max -1927 -963 -595 29 50217 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 10114.589 1796.834 5.629 1.82e-08 *** Hour 59.503 1.088 54.693 \u0026lt; 2e-16 *** meandewpti -1.167 1.272 -0.917 0.358964 meanpressurei -306.446 59.626 -5.139 2.76e-07 *** rain1 -36.115 23.274 -1.552 0.120718 precipi 58.822 21.466 2.740 0.006142 ** meantempi -6.551 1.808 -3.623 0.000292 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2301 on 94244 degrees of freedom Multiple R-squared: 0.03157, Adjusted R-squared: 0.03151 F-statistic: 512.1 on 6 and 94244 DF, p-value: \u0026lt; 2.2e-16 f = ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + maxdewpti + rain + precipi + maxtempi linear_model2 \u0026lt;- lm(formula = f, data = df_train) summary(linear_model2)  Call: lm(formula = f, data = df_train) Residuals: Min 1Q Median 3Q Max -1928 -962 -594 27 50189 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 10390.409 1861.345 5.582 2.38e-08 *** Hour 59.505 1.088 54.687 \u0026lt; 2e-16 *** maxdewpti -2.352 1.220 -1.928 0.05387 . maxpressurei -320.621 61.591 -5.206 1.94e-07 *** rain1 -16.255 22.766 -0.714 0.47522 precipi 56.349 21.568 2.613 0.00899 ** maxtempi -2.561 1.329 -1.927 0.05396 . --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2301 on 94244 degrees of freedom Multiple R-squared: 0.03131, Adjusted R-squared: 0.03125 F-statistic: 507.7 on 6 and 94244 DF, p-value: \u0026lt; 2.2e-16 f = ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + maxdewpti + rain + precipi + maxtempi + mindewpti + minpressurei + mintempi + meandewpti + meanpressurei + meantempi linear_model3 \u0026lt;- lm(formula = f, data = df_train) summary(linear_model3)  Call: lm(formula = f, data = df_train) Residuals: Min 1Q Median 3Q Max -2169 -977 -579 36 50233 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 12754.788 2104.365 6.061 1.36e-09 *** Hour 59.561 1.086 54.823 \u0026lt; 2e-16 *** maxdewpti 21.654 6.591 3.285 0.00102 ** maxpressurei -2259.680 285.300 -7.920 2.39e-15 *** rain1 -81.456 26.213 -3.107 0.00189 ** precipi 22.064 23.186 0.952 0.34130 maxtempi 35.348 16.576 2.132 0.03297 * mindewpti 5.976 6.068 0.985 0.32470 minpressurei -1710.971 227.800 -7.511 5.93e-14 *** mintempi -29.708 17.853 -1.664 0.09611 . meandewpti -6.414 11.166 -0.574 0.56567 meanpressurei 3589.144 404.039 8.883 \u0026lt; 2e-16 *** meantempi -44.866 33.218 -1.351 0.17681 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2298 on 94238 degrees of freedom Multiple R-squared: 0.03437, Adjusted R-squared: 0.03425 F-statistic: 279.5 on 12 and 94238 DF, p-value: \u0026lt; 2.2e-16 f = ENTRIESn_hourly ~ rain + precipi + maxtempi + mindewpti + mintempi + meandewpti + meantempi linear_model3 \u0026lt;- lm(formula = f, data = df_train) summary(linear_model3)  Call: lm(formula = f, data = df_train) Residuals: Min 1Q Median 3Q Max -1389 -1027 -798 24 50810 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 1421.505 96.171 14.781 \u0026lt; 2e-16 *** rain1 -121.264 25.048 -4.841 1.29e-06 *** precipi 65.162 21.948 2.969 0.00299 ** maxtempi 16.947 16.116 1.052 0.29298 mindewpti -24.309 4.339 -5.602 2.13e-08 *** mintempi -32.259 17.815 -1.811 0.07018 . meandewpti 41.276 5.303 7.784 7.10e-15 *** meantempi -10.937 32.776 -0.334 0.73861 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2336 on 94243 degrees of freedom Multiple R-squared: 0.00199, Adjusted R-squared: 0.001916 F-statistic: 26.85 on 7 and 94243 DF, p-value: \u0026lt; 2.2e-16  Predict The predict() function is used to test the model fit against the test set. The difference between the predicted values and actual values will help determine the accuracy of the model.\npredict_linear \u0026lt;- predict(linear_model3, newdata = df_test[, c(\u0026quot;rain\u0026quot;, \u0026quot;precipi\u0026quot;, \u0026quot;maxtempi\u0026quot;, \u0026quot;mindewpti\u0026quot;, \u0026quot;mintempi\u0026quot;,\u0026quot;meandewpti\u0026quot;, \u0026quot;meantempi\u0026quot;)]) mse \u0026lt;- mean((df_test$ENTRIESn_hourly - predict_linear)^2) sqrt(mse) [1] 2331.393  Stepwise Regression Stepwise Regression can be used to find the best subset of factors instead of manually modifying the formula each time. Stepwise regression performs feature selection and builds the model. The subset is selected based on a parameter called AIC(Akaike Information Criterion) and RSS(Residual Sum of Squares)\nf = ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + maxdewpti + rain + precipi + maxtempi + mindewpti + minpressurei + mintempi + meandewpti + meanpressurei + meantempi model_stepwise \u0026lt;- lm(f, data = df_train) fit \u0026lt;- step(model_stepwise) Start: AIC=1458969 ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + maxdewpti + rain + precipi + maxtempi + mindewpti + minpressurei + mintempi + meandewpti + meanpressurei + meantempi Df Sum of Sq RSS AIC - meandewpti 1 1.7424e+06 4.9760e+11 1458967 - precipi 1 4.7816e+06 4.9760e+11 1458968 - mindewpti 1 5.1214e+06 4.9760e+11 1458968 - meantempi 1 9.6324e+06 4.9761e+11 1458969 \u0026lt;none\u0026gt; 4.9760e+11 1458969 - mintempi 1 1.4621e+07 4.9761e+11 1458970 - maxtempi 1 2.4011e+07 4.9762e+11 1458972 - rain 1 5.0989e+07 4.9765e+11 1458977 - maxdewpti 1 5.6986e+07 4.9765e+11 1458978 - minpressurei 1 2.9787e+08 4.9789e+11 1459024 - maxpressurei 1 3.3124e+08 4.9793e+11 1459030 - meanpressurei 1 4.1666e+08 4.9801e+11 1459046 - Hour 1 1.5870e+10 5.1347e+11 1461926 Step: AIC=1458967 ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + precipi + maxtempi + mindewpti + minpressurei + mintempi + meanpressurei + meantempi Df Sum of Sq RSS AIC - precipi 1 4.3774e+06 4.9760e+11 1458966 - mindewpti 1 6.1022e+06 4.9760e+11 1458967 - meantempi 1 9.8265e+06 4.9761e+11 1458967 \u0026lt;none\u0026gt; 4.9760e+11 1458967 - mintempi 1 1.4824e+07 4.9761e+11 1458968 - maxtempi 1 2.4880e+07 4.9762e+11 1458970 - rain 1 5.3047e+07 4.9765e+11 1458976 - maxdewpti 1 1.3688e+08 4.9774e+11 1458991 - minpressurei 1 2.9613e+08 4.9789e+11 1459022 - maxpressurei 1 3.5576e+08 4.9795e+11 1459033 - meanpressurei 1 4.2195e+08 4.9802e+11 1459045 - Hour 1 1.5870e+10 5.1347e+11 1461924 Step: AIC=1458966 ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + mindewpti + minpressurei + mintempi + meanpressurei + meantempi Df Sum of Sq RSS AIC - mindewpti 1 6.5889e+06 4.9761e+11 1458966 - meantempi 1 8.7840e+06 4.9761e+11 1458966 \u0026lt;none\u0026gt; 4.9760e+11 1458966 - mintempi 1 1.7097e+07 4.9762e+11 1458968 - maxtempi 1 2.3165e+07 4.9763e+11 1458969 - rain 1 4.8873e+07 4.9765e+11 1458974 - maxdewpti 1 1.4351e+08 4.9775e+11 1458991 - minpressurei 1 2.9223e+08 4.9789e+11 1459020 - maxpressurei 1 4.2009e+08 4.9802e+11 1459044 - meanpressurei 1 4.4393e+08 4.9805e+11 1459048 - Hour 1 1.5870e+10 5.1347e+11 1461923 Step: AIC=1458966 ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + minpressurei + mintempi + meanpressurei + meantempi Df Sum of Sq RSS AIC - meantempi 1 6.6694e+06 4.9762e+11 1458965 \u0026lt;none\u0026gt; 4.9761e+11 1458966 - mintempi 1 1.8191e+07 4.9763e+11 1458967 - maxtempi 1 1.8969e+07 4.9763e+11 1458967 - rain 1 5.0144e+07 4.9766e+11 1458973 - minpressurei 1 3.0167e+08 4.9791e+11 1459021 - maxpressurei 1 4.1365e+08 4.9802e+11 1459042 - meanpressurei 1 4.4210e+08 4.9805e+11 1459047 - maxdewpti 1 4.4868e+08 4.9806e+11 1459048 - Hour 1 1.5869e+10 5.1348e+11 1461922 Step: AIC=1458965 ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + minpressurei + mintempi + meanpressurei Df Sum of Sq RSS AIC \u0026lt;none\u0026gt; 4.9762e+11 1458965 - rain 1 5.6258e+07 4.9767e+11 1458973 - maxtempi 1 2.9480e+08 4.9791e+11 1459019 - minpressurei 1 2.9848e+08 4.9791e+11 1459019 - maxpressurei 1 4.0799e+08 4.9802e+11 1459040 - meanpressurei 1 4.3771e+08 4.9805e+11 1459046 - maxdewpti 1 5.1002e+08 4.9813e+11 1459059 - mintempi 1 1.1998e+09 4.9882e+11 1459190 - Hour 1 1.5869e+10 5.1348e+11 1461921 summary(fit)  Call: lm(formula = ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + minpressurei + mintempi + meanpressurei, data = df_train) Residuals: Min 1Q Median 3Q Max -2188 -978 -580 37 50211 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) 12084.136 1885.501 6.409 1.47e-10 *** Hour 59.558 1.086 54.821 \u0026lt; 2e-16 *** maxdewpti 22.558 2.295 9.828 \u0026lt; 2e-16 *** maxpressurei -2316.325 263.511 -8.790 \u0026lt; 2e-16 *** rain1 -79.208 24.266 -3.264 0.0011 ** maxtempi 12.496 1.672 7.472 7.96e-14 *** minpressurei -1580.716 210.243 -7.519 5.59e-14 *** mintempi -52.348 3.473 -15.074 \u0026lt; 2e-16 *** meanpressurei 3536.013 388.370 9.105 \u0026lt; 2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 2298 on 94242 degrees of freedom Multiple R-squared: 0.03433, Adjusted R-squared: 0.03425 F-statistic: 418.8 on 8 and 94242 DF, p-value: \u0026lt; 2.2e-16  Residual Sum of Squares It is the sum of the squared difference between the predicted and the actual values. Lower the RSS, better the fit.\n AIC- Akaike Information Criterion According to Wikipedia, IC provides a means for model selection. It estimates the quality of the model, relative to other models.\nAIC enables comparison of different models built on the same dataset and selects the best. A lower AIC value signifies a better model. However, it is a relative comparison technique i.e. it gives the best model with respect to others; however there is no way to determine if the best model is actually applicable and useful.\nIn addition, the AIC also penalises extra variables and hence gives preference to simpler models.\nHowever, AIC does not help in solving the problem of multicollinearity. Thus, the factors that are collinear must first be dealt with before performing stepwise regression and using AIC to select best fit.\nFrom the above summary, the model with best fit is the one that uses the formula- ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + minpressurei + mintempi + meanpressurei + meantempi\nf = ENTRIESn_hourly ~ Hour + maxdewpti + maxpressurei + rain + maxtempi + minpressurei + mintempi + meanpressurei + meantempi model_final \u0026lt;- lm(f, data = df_train) predict_final \u0026lt;- predict(model_final, newdata = df_test[, c(\u0026quot;Hour\u0026quot;, \u0026quot;maxdewpti\u0026quot;, \u0026quot;maxpressurei\u0026quot;, \u0026quot;rain\u0026quot;, \u0026quot;maxtempi\u0026quot;, \u0026quot;minpressurei\u0026quot;, \u0026quot;mintempi\u0026quot;, \u0026quot;meanpressurei\u0026quot;, \u0026quot;meantempi\u0026quot;)]) mse \u0026lt;- mean((df_test$ENTRIESn_hourly - predict_final)^2) sqrt(mse) [1] 2292.802 Thus, the RMSE went down from 2331 to 2292 which shows that this model is better than the previous one.\n ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"03a2f3d8b6c3138cf39b3c877e48082d","permalink":"/project/linear_regression/linear_regression/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/linear_regression/linear_regression/","section":"project","summary":"Predict Entriesn_hourly i.e. number of hourly entries at the NYC Subway based on other predictor variables.","tags":["R","regression","Machine Learning"],"title":"NYC Subway","type":"project"},{"authors":null,"categories":null,"content":" The analysis consists of two parts:\nA simulation exercise. Basic inferential data analysis.  Analysis 1. A simulation exercise. In this analysis I’ll investigate the exponential distribution in R and compare it with the Central Limit Theorem. The exponential distribution can be simulated in R with rexp(n, lambda) where lambda is the rate parameter. The mean of exponential distribution is 1/lambda and the standard deviation is also 1/lambda. Set lambda = 0.2 for all of the simulations. I’ll investigate the distribution of averages of 40 exponentials. Note that you will need to do a thousand simulations.\nIllustrate via simulation and associated explanatory text the properties of the distribution of the mean of 40 exponentials. You should\nShow the sample mean and compare it to the theoretical mean of the distribution. Show how variable the sample is (via variance) and compare it to the theoretical variance of the distribution. Show that the distribution is approximately normal.    Task # set seed for reproducability set.seed(31) # set lambda to 0.2 lambda \u0026lt;- 0.2 # 40 samples n \u0026lt;- 40 # 1000 simulations simulations \u0026lt;- 1000 # simulate simulated_exponentials \u0026lt;- replicate(simulations, rexp(n, lambda)) # calculate mean of exponentials means_exponentials \u0026lt;- apply(simulated_exponentials, 2, mean)  Question 1 Show where the distribution is centered at and compare it to the theoretical center of the distribution.\nanalytical_mean \u0026lt;- mean(means_exponentials) analytical_mean [1] 4.993867 # analytical mean theory_mean \u0026lt;- 1/lambda theory_mean [1] 5 # visualization hist(means_exponentials, xlab = \u0026quot;mean\u0026quot;, main = \u0026quot;Exponential Function Simulations\u0026quot;) abline(v = analytical_mean, col = \u0026quot;red\u0026quot;) abline(v = theory_mean, col = \u0026quot;orange\u0026quot;) The analytics mean is 4.993867 the theoretical mean 5. The center of distribution of averages of 40 exponentials is very close to the theoretical center of the distribution.\n Question 2 Show how variable it is and compare it to the theoretical variance of the distribution..\n# standard deviation of distribution standard_deviation_dist \u0026lt;- sd(means_exponentials) standard_deviation_dist [1] 0.7931608 # standard deviation from analytical expression standard_deviation_theory \u0026lt;- (1/lambda)/sqrt(n) standard_deviation_theory [1] 0.7905694 # variance of distribution variance_dist \u0026lt;- standard_deviation_dist^2 variance_dist [1] 0.6291041 # variance from analytical expression variance_theory \u0026lt;- ((1/lambda)*(1/sqrt(n)))^2 variance_theory [1] 0.625 Standard Deviation of the distribution is 0.7931608 with the theoretical SD calculated as 0.7905694. The Theoretical variance is calculated as ((1 / ??) * (1/???n))2 = 0.625. The actual variance of the distribution is 0.6291041\n Question 3 Show that the distribution is approximately normal.\nxfit \u0026lt;- seq(min(means_exponentials), max(means_exponentials), length=100) yfit \u0026lt;- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(n))) hist(means_exponentials,breaks=n,prob=T,col=\u0026quot;orange\u0026quot;,xlab = \u0026quot;means\u0026quot;,main=\u0026quot;Density of means\u0026quot;,ylab=\u0026quot;density\u0026quot;) lines(xfit, yfit, pch=22, col=\u0026quot;black\u0026quot;, lty=5) # compare the distribution of averages of 40 exponentials to a normal distribution qqnorm(means_exponentials) qqline(means_exponentials, col = 2) Due to Due to the central limit theorem (CLT), the distribution of averages of 40 exponentials is very close to a normal distribution.\n2. Basic inferential data analysis. Now in the second portion of this analysis, we’re going to analyze the ToothGrowth data in the R datasets package.\n Load the ToothGrowth data and perform some basic exploratory data analyses Provide a basic summary of the data. Use confidence intervals and/or hypothesis tests to compare tooth growth by supp and dose. (Only use the techniques from class, even if there’s other approaches worth considering) State your conclusions and the assumptions needed for your conclusions.    Load the ToothGrowth data and perform some basic exploratory data analyses # load the data ToothGrowth data(ToothGrowth) # preview the structure of the data str(ToothGrowth) \u0026#39;data.frame\u0026#39;: 60 obs. of 3 variables: $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... $ supp: Factor w/ 2 levels \u0026quot;OJ\u0026quot;,\u0026quot;VC\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ... $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... # preview first 5 rows of the data head(ToothGrowth, 5)  len supp dose 1 4.2 VC 0.5 2 11.5 VC 0.5 3 7.3 VC 0.5 4 5.8 VC 0.5 5 6.4 VC 0.5  Provide a basic summary of the data. # data summary summary(ToothGrowth)  len supp dose Min. : 4.20 OJ:30 Min. :0.500 1st Qu.:13.07 VC:30 1st Qu.:0.500 Median :19.25 Median :1.000 Mean :18.81 Mean :1.167 3rd Qu.:25.27 3rd Qu.:2.000 Max. :33.90 Max. :2.000  # compare means of the different delivery methods tapply(ToothGrowth$len,ToothGrowth$supp, mean)  OJ VC 20.66333 16.96333  # plot data graphically ggplot(ToothGrowth, aes(factor(dose), len, fill = factor(dose))) + geom_boxplot() + # facet_grid(.~supp)+ facet_grid(.~supp, labeller = as_labeller( c(\u0026quot;OJ\u0026quot; = \u0026quot;Orange juice\u0026quot;, \u0026quot;VC\u0026quot; = \u0026quot;Ascorbic Acid\u0026quot;))) + labs(title = \u0026quot;Tooth growth of 60 guinea pigs by dosage and\\nby delivery method of vitamin C\u0026quot;, x = \u0026quot;Dose in milligrams/day\u0026quot;, y = \u0026quot;Tooth Lengh\u0026quot;) + scale_fill_discrete(name = \u0026quot;Dosage of\\nvitamin C\\nin mg/day\u0026quot;) + theme_classic()  Use confidence intervals and/or hypothesis tests to compare tooth growth by supp and dose. # comparison by delivery method for the same dosage t05 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 0.5) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 0.5) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) t1 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 1) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 1) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) t2 \u0026lt;- t.test(len ~ supp, data = rbind(ToothGrowth[(ToothGrowth$dose == 2) \u0026amp; (ToothGrowth$supp == \u0026quot;OJ\u0026quot;),], ToothGrowth[(ToothGrowth$dose == 2) \u0026amp; (ToothGrowth$supp == \u0026quot;VC\u0026quot;),]), var.equal = FALSE) # summary of the conducted t.tests, which compare the delivery methods by dosage, # take p-values and CI summaryBYsupp \u0026lt;- data.frame( \u0026quot;p-value\u0026quot; = c(t05$p.value, t1$p.value, t2$p.value), \u0026quot;Conf.Low\u0026quot; = c(t05$conf.int[1],t1$conf.int[1], t2$conf.int[1]), \u0026quot;Conf.High\u0026quot; = c(t05$conf.int[2],t1$conf.int[2], t2$conf.int[2]), row.names = c(\u0026quot;Dosage .05\u0026quot;,\u0026quot;Dosage 1\u0026quot;,\u0026quot;Dosage 2\u0026quot;)) # show data table summaryBYsupp  p.value Conf.Low Conf.High Dosage .05 0.006358607 1.719057 8.780943 Dosage 1 0.001038376 2.802148 9.057852 Dosage 2 0.963851589 -3.798070 3.638070  Conclusion With 95% confidence we reject the null hypothesis, stating that there is no difference in the tooth growth by the delivery method for .5 and 1 milligrams/day. We observe p-values less than the treshold of .05 and the confidence levels don’t include 0. So, for dosage of .5 milligrams/day and 1 milligrams/day does matter the delivery method. With 95% confidence we fail to reject the null hypothesis, stating that there is no difference in the tooth growth by the delivery method for 2 milligrams/day. We observe p-values more than the treshold of .05 and the confidence levels include 0. So, for dosage of 2 milligrams/day the delivery method doesn’t matter.\n ","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"8256f7fe219884432862c8d02845ba97","permalink":"/project/exponential_distribution/stats/","publishdate":"2019-04-30T00:00:00Z","relpermalink":"/project/exponential_distribution/stats/","section":"project","summary":"Central limit theorem","tags":["R","Data Analytics"],"title":"Investigating The Exponential Distribution \u0026 Compare It With The Central Limit Theorem","type":"project"},{"authors":null,"categories":null,"content":" “Knowledge is indeed highly subjective, but we can quantify it with a bet. The amount we wager shows how much we believe in something.” — Sharon Bertsch McGrayne\nThe quality of your life will, to a large extent, be decided by whom you elect to spend your time with. Supportive, caring, and funny are great attributes in friends and lovers. Unceasingly negative cynics who chip away at your self-esteem? We need to jettison those people as far and fast as we can.\nThe problem is, how do we identify these people who add nothing positive — or not enough positive — to our lives?\nFew of us keep relationships with obvious assholes. There are always a few painfully terrible family members we have to put up with at weddings and funerals, but normally we choose whom we spend time with. And we’ve chosen these people because, at some point, our interactions with them felt good.\nHow, then, do we identify the deadweight? The people who are really dragging us down and who have a high probability of continuing to do so in the future? We can apply the general thinking tool called Bayesian Updating.\nBayes’s theorem can involve some complicated mathematics, but at its core lies a very simple premise. Probability estimates should start with what we already know about the world and then be incrementally updated as new information becomes available. Bayes can even help us when that information is relevant but subjective.\nHow? As McGrayne explains in the quote above, from The Theory That Would Not Die, you simply ask yourself to wager on the outcome.\nLet’s take an easy example. You are going on a blind date. You’ve been told all sorts of good things in advance — the person is attractive and funny and has a good job — so of course, you are excited. The date starts off great, living up to expectations. Halfway through you find out they have a cat. You hate cats. Given how well everything else is going, how much should this information affect your decision to keep dating?\nQuantify your belief in the most probable outcome with a bet. How much would you wager that harmony on the pet issue is an accurate predictor of relationship success? Ten cents? Ten thousand dollars? Do the thought experiment. Imagine walking into a casino and placing a bet on the likelihood that this person’s having a cat will ultimately destroy the relationship. How much money would you take out of your savings and lay on the table? Your answer will give you an idea of how much to factor the cat into your decision-making process. If you wouldn’t part with a dime, then I wouldn’t worry about it.\nThis kind of approach can help us when it comes to evaluating our interpersonal relationships. Deciding if someone is a good friend, partner, or co-worker is full of subjective judgments. There is usually some contradictory information, and ultimately no one is perfect. So how do you decide who is worth keeping around?\nLet’s start with friends. The longer a friendship lasts, the more likely it is to have ups and downs. The trick is to start quantifying these. A hit from a change in geographical proximity is radically different from a hit from betrayal — we need to factor these differences into our friendship formula.\nThis may seem obvious, but the truth is that we often give the same weight to a wide variety of behaviors. We’ll say things like “yeah, she talked about my health problems when I asked her not to, but she always remembers my birthday.” By treating all aspects of the friendship equally, we have a hard time making reasonable estimates about the future value of that friendship. And that’s how we end up with deadweight.\nFor the friend who has betrayed your confidence, what you really want to know is the likelihood that she’s going to do it again. Instead of trying to remember and analyze every interaction you’ve ever had, just imagine yourself betting on it. Go back to that casino and head to the friendship roulette wheel. Where would you put your money? All in on “She can’t keep her mouth shut” or a few chips on “Not likely to happen again”?\nUsing a rough Bayesian model in our heads, we’re forcing ourselves to quantify what “good” is and what “bad” is. How good? How bad? How likely? How unlikely? Until we do some (rough) guessing at these things, we’re making decisions much more poorly than we need to be.\nThe great thing about using Bayes’s theorem is that it encourages constant updating. It also encourages an open mind by giving us the chance to look at a situation from multiple angles. Maybe she really is sorry about the betrayal. Maybe she thought she was acting in your best interests. There are many possible explanations for her behavior and you can use Bayes’s theorem to integrate all of her later actions into your bet. If you find yourself reducing the amount of money you’d bet on further betrayal, you can accurately assume that the probability she will betray your trust again has gone down.\nUsing this strategy can also stop the endless rounds of asking why. Why did that co-worker steal my idea? Who else do I have to watch out for? This what-if thinking is paralyzing. You end up self-justifying your behavior by anticipating the worst possible scenarios you can imagine. Thus, you don’t change anything, and you step further away from a solution.\nIn reality, who cares? The why isn’t important; the most relevant task for you is to figure out the probability that your coworker will do it again. Don’t spend hours analyzing what to do, get upset over the doomsday scenarios you have come up with, or let a few glasses of wine soften the experience.\nHead to your mental casino and place the bet, quantifying all the subjective information in your head that is messy and hard to articulate. You will cut through the endless “but maybes” and have a clear path forward that addresses the probable future. It may make sense to give him the benefit of the doubt. It may also be reasonable to avoid him as much as possible. When you figure out how much you would wager on the potential outcomes, you’ll know what to do.\nSometimes we can’t just get rid of people who aren’t good for us — family being the prime example. But you can also use Bayes to test how your actions will change the probability of outcomes to find ways of keeping the negativity minimal. Let’s say you have a cousin who always plans to visit but then cancels. You can’t stop being his cousin and saying “you aren’t welcome at my house” will cause a big family drama. So what else can you do?\nYour initial equation — your probability estimate — indicates that the behavior is likely to continue. In your casino, you would comfortably bet your life savings that it will happen again. Now imagine ways in which you could change your behavior. Which of these would reduce your bet? You could have an honest conversation with him, telling him how his actions make you feel. To know if he’s able to openly receive this, consider whether your bet would change. Or would you wager significantly less after employing the strategy of always being busy when he calls to set up future visits?\nAnd you can dig even deeper. Which of your behaviors would increase the probability that he actually comes? Which behaviors would increase the probability that he doesn’t bother making plans in the first place? Depending on how much you like him, you can steer your changes to the outcome you’d prefer.\nQuantifying the subjective and using Bayes’s theorem can help us clear out some of the relationship negativity in our lives.\nYou Rock! ","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"799a5fc85d6d74ac0f5ba20fa4ac75d0","permalink":"/post/deadweight/","publishdate":"2019-04-30T00:00:00Z","relpermalink":"/post/deadweight/","section":"post","summary":"“Knowledge is indeed highly subjective, but we can quantify it with a bet. The amount we wager shows how much we believe in something.” — Sharon Bertsch McGrayne\nThe quality of your life will, to a large extent, be decided by whom you elect to spend your time with. Supportive, caring, and funny are great attributes in friends and lovers. Unceasingly negative cynics who chip away at your self-esteem? We need to jettison those people as far and fast as we can.","tags":null,"title":"Using Statistics To Reduce Deadweight From Our Lives","type":"post"},{"authors":null,"categories":null,"content":" Introduction It is now possible to collect a large amount of data about personal movement using activity monitoring devices such as a Fitbit, Nike Fuelband, or Jawbone Up. These type of devices are part of the “quantified self” movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. But these data remain under-utilized both because the raw data are hard to obtain and there is a lack of statistical methods and software for processing and interpreting the data.\nThis analysis makes use of data from a personal activity monitoring device. This device collects data at 5 minute intervals through-out the day. The data consists of two months of data from an anonymous individual collected during the months of October and November, 2012 and include the number of steps taken in 5 minute intervals each day.\nThe data for this analysis can be downloaded from the course web site:\n Dataset: Activity monitoring data  The variables included in this dataset are:\n steps: Number of steps taking in a 5-minute interval (missing values are coded as 𝙽𝙰) date: The date on which the measurement was taken in YYYY-MM-DD format interval: Identifier for the 5-minute interval in which measurement was taken  The dataset is stored in a comma-separated-value (CSV) file and there are a total of 17,568 observations in this dataset.\n Loading and preprocessing the data Unzip data to obtain a csv file.\nfileUrl \u0026lt;- \u0026quot;https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip\u0026quot; download.file(fileUrl, destfile = paste0(getwd(), \u0026#39;/repdata%2Fdata%2Factivity.zip\u0026#39;), method = \u0026quot;curl\u0026quot;) unzip(\u0026quot;repdata%2Fdata%2Factivity.zip\u0026quot;,exdir = \u0026quot;data\u0026quot;)  Reading csv Data into Data.Table. activityDT \u0026lt;- data.table::fread(input = \u0026quot;data/activity.csv\u0026quot;)  What is mean total number of steps taken per day? Calculate the total number of steps taken per day.  Total_Steps \u0026lt;- activityDT[, c(lapply(.SD, sum, na.rm = FALSE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(date)] head(Total_Steps, 10)  date steps 1: 2012-10-01 NA 2: 2012-10-02 126 3: 2012-10-03 11352 4: 2012-10-04 12116 5: 2012-10-05 13294 6: 2012-10-06 15420 7: 2012-10-07 11015 8: 2012-10-08 NA 9: 2012-10-09 12811 10: 2012-10-10 9900 Make a histogram of the total number of steps taken each day.  ggplot(Total_Steps, aes(x = steps)) + geom_histogram(fill = \u0026quot;blue\u0026quot;, binwidth = 1000) + labs(title = \u0026quot;Daily Steps\u0026quot;, x = \u0026quot;Steps\u0026quot;, y = \u0026quot;Frequency\u0026quot;) + theme_classic() Warning: Removed 8 rows containing non-finite values (stat_bin). Calculate and report the mean and median of the total number of steps taken per day.  Total_Steps[, .(Mean_Steps = mean(steps, na.rm = TRUE), Median_Steps = median(steps, na.rm = TRUE))]  Mean_Steps Median_Steps 1: 10766.19 10765  What is the average daily activity pattern? Make a time series plot (i.e. 𝚝𝚢𝚙𝚎 = “𝚕”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis).  IntervalDT \u0026lt;- activityDT[, c(lapply(.SD, mean, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(interval)] ggplot(IntervalDT, aes(x = interval , y = steps)) + geom_line(color=\u0026quot;blue\u0026quot;, size=1) + labs(title = \u0026quot;Avg. Daily Steps\u0026quot;, x = \u0026quot;Interval\u0026quot;, y = \u0026quot;Avg. Steps per day\u0026quot;) + theme_classic() Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?  IntervalDT[steps == max(steps), .(max_interval = interval)]  max_interval 1: 835  Imputing missing values Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with 𝙽𝙰s)  activityDT[is.na(steps), .N ] [1] 2304 # alternative solution nrow(activityDT[is.na(steps),]) [1] 2304 Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc.  Filling in missing values with median of dataset.\nactivityDT[is.na(steps), \u0026quot;steps\u0026quot;] \u0026lt;- activityDT[, c(lapply(.SD, median, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;)] Create a new dataset that is equal to the original dataset, but with the missing data filled in.  data.table::fwrite(x = activityDT, file = \u0026quot;data/tidyData.csv\u0026quot;, quote = FALSE) Make a histogram of the total number of steps taken each day and calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?  # total number of steps taken per day Total_Steps \u0026lt;- activityDT[, c(lapply(.SD, sum)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(date)] # mean and median total number of steps taken per day Total_Steps[, .(Mean_Steps = mean(steps), Median_Steps = median(steps))]  Mean_Steps Median_Steps 1: 9354.23 10395 ggplot(Total_Steps, aes(x = steps)) + geom_histogram(fill = \u0026quot;blue\u0026quot;, binwidth = 1000) + labs(title = \u0026quot;Daily Steps\u0026quot;, x = \u0026quot;Steps\u0026quot;, y = \u0026quot;Frequency\u0026quot;) + theme_classic() Type of Estimate | Mean_Steps | Median_Steps\nFirst Part (with na) | 10765 | 10765 Second Part (fillin in na with median) | 9354.23 | 10395\n Are there differences in activity patterns between weekdays and weekends? Create a new factor variable in the dataset with two levels – “weekday” and “weekend” indicating whether a given date is a weekday or weekend day.  Just recreating activityDT from scratch then making the new factor variable. (No need to, just want to be clear on what the entire process is.)\nactivityDT \u0026lt;- data.table::fread(input = \u0026quot;data/activity.csv\u0026quot;) activityDT[, date := as.POSIXct(date, format = \u0026quot;%Y-%m-%d\u0026quot;)] activityDT[, `Day of Week`:= weekdays(x = date)] activityDT[grepl(pattern = \u0026quot;Monday|Tuesday|Wednesday|Thursday|Friday\u0026quot;, x = `Day of Week`), \u0026quot;weekday or weekend\u0026quot;] \u0026lt;- \u0026quot;weekday\u0026quot; activityDT[grepl(pattern = \u0026quot;Saturday|Sunday\u0026quot;, x = `Day of Week`), \u0026quot;weekday or weekend\u0026quot;] \u0026lt;- \u0026quot;weekend\u0026quot; activityDT[, `weekday or weekend` := as.factor(`weekday or weekend`)] head(activityDT, 10)  steps date interval Day of Week weekday or weekend 1: NA 2012-10-01 0 Monday weekday 2: NA 2012-10-01 5 Monday weekday 3: NA 2012-10-01 10 Monday weekday 4: NA 2012-10-01 15 Monday weekday 5: NA 2012-10-01 20 Monday weekday 6: NA 2012-10-01 25 Monday weekday 7: NA 2012-10-01 30 Monday weekday 8: NA 2012-10-01 35 Monday weekday 9: NA 2012-10-01 40 Monday weekday 10: NA 2012-10-01 45 Monday weekday Make a panel plot containing a time series plot (i.e. 𝚝𝚢𝚙𝚎 = “𝚕”) of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday, days or weekend days (y-axis).  activityDT[is.na(steps), \u0026quot;steps\u0026quot;] \u0026lt;- activityDT[, c(lapply(.SD, median, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;)] IntervalDT \u0026lt;- activityDT[, c(lapply(.SD, mean, na.rm = TRUE)), .SDcols = c(\u0026quot;steps\u0026quot;), by = .(interval, `weekday or weekend`)] ggplot(IntervalDT , aes(x = interval , y = steps, color=`weekday or weekend`)) + geom_line() + labs(title = \u0026quot;Avg. Daily Steps by Weektype\u0026quot;, x = \u0026quot;Interval\u0026quot;, y = \u0026quot;No. of Steps\u0026quot;) + facet_wrap(~`weekday or weekend` , ncol = 1, nrow=2) + theme_classic()  ","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"198ed71bfc4b71a3fddc384c9dbe710f","permalink":"/project/personal_movement/pmd/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/project/personal_movement/pmd/","section":"project","summary":"An analysis of personal activity monitoring device","tags":["R","Data Analytics"],"title":"Personal Movement Monitoring Devices","type":"project"},{"authors":null,"categories":null,"content":" The federal funds rate is the key interest rate that the U.S. Federal Reserve uses to influence economic growth. The Federal Open Market Committee meets regularly to decide whether to increase, decrease, or maintain the target interest rate. Their choice has important ramifications that cascade through the economy, so the announcement of the interest rates is eagerly awaited each month.\nIn this analysis, I’ll use analytics to try to predict when the Fed will raise interest rates. I’ll look at monthly economic and political data dating back to the mid-1960’s. In this analysis, the dependent variable will be the binary outcome variable RaisedFedFunds, which takes value 1 if the federal funds rate was increased that month and 0 if it was lowered or stayed the same. For each month, the file federalFundsRate.csv.xz contains the following independent variables:\n Date: The date the change was announced. Chairman: The name of the Federal Reserve Chairman at the time the change was announced. PreviousRate: The federal funds rate in the prior month. Streak: The current streak of raising or not raising the rate, e.g. +8 indicates the rate has been increased 8 months in a row, whereas -3 indicates the rate has been lowered or stayed the same for 3 months in a row. GDP: The U.S. Gross Domestic Product, in Billions of Chained 2009 US Dollars. Unemployment: The unemployment rate in the U.S. CPI: The Consumer Price Index, an indicator of inflation, in the U.S. HomeownershipRate: The rate of homeownership in the U.S. DebtAsPctGDP: The U.S. national debt as a percentage of GDP DemocraticPres: Whether the sitting U.S. President is a Democrat (DemocraticPres=1) or a Republican (DemocraticPres=0) MonthsUntilElection: The number of remaining months until the next U.S. presidential election.  Problem 1 - Loading the Data Use the read.csv function to load the contents of federalFundsRate.csv.xz file into a dataframe called fedFunds, using stringsAsFactors=FALSE.\nWhat proportion of months did the Fed raise the interest rate?\nfedFunds \u0026lt;- read.csv(\u0026quot;federalFundsRate.csv.xz\u0026quot;) str(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : int 1 1 1 1 1 1 1 1 1 1 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : int 1 1 1 1 1 1 1 0 1 1 ... summary(fedFunds)  Date Chairman PreviousRate 1966-02-01: 1 Greenspan, Alan :221 Min. : 0.070 1966-03-01: 1 Bernanke, Ben : 96 1st Qu.: 3.290 1966-04-01: 1 Burns, Arthur : 96 Median : 5.390 1966-05-01: 1 Volcker, Paul : 96 Mean : 5.651 1966-06-01: 1 Martin, William M.: 48 3rd Qu.: 7.880 1966-07-01: 1 Miller, G. William: 17 Max. :19.100 (Other) :579 (Other) : 11 Streak GDP Unemployment CPI Min. :-16.000 Min. : 4202 Min. : 3.400 Min. : 31.88 1st Qu.: -2.000 1st Qu.: 6039 1st Qu.: 5.000 1st Qu.: 63.40 Median : 1.000 Median : 8907 Median : 5.900 Median :129.10 Mean : 1.094 Mean : 9450 Mean : 6.181 Mean :127.71 3rd Qu.: 3.000 3rd Qu.:12956 3rd Qu.: 7.300 3rd Qu.:180.00 Max. : 27.000 Max. :16206 Max. :10.800 Max. :237.63 HomeownershipRate DebtAsPctGDP DemocraticPres MonthsUntilElection Min. :63.20 Min. : 30.60 Min. :0.0000 Min. : 0.00 1st Qu.:64.20 1st Qu.: 62.35 1st Qu.:0.0000 1st Qu.:12.00 Median :64.80 Median : 6039.16 Median :0.0000 Median :24.00 Mean :65.41 Mean : 6317.32 Mean :0.4256 Mean :23.58 3rd Qu.:66.50 3rd Qu.:10529.38 3rd Qu.:1.0000 3rd Qu.:35.00 Max. :69.20 Max. :16205.59 Max. :1.0000 Max. :47.00 RaisedFedFunds Min. :0.0000 1st Qu.:0.0000 Median :1.0000 Mean :0.5026 3rd Qu.:1.0000 Max. :1.0000  table(fedFunds$RaisedFedFunds)  0 1 291 294  294 / (291 + 294) [1] 0.5025641  Problem 2 - The Longest-Serving Fed Chair Which Fed Reserve Chair has presided over the most interest rate decisions?\ntable(fedFunds$Chairman)  Bernanke, Ben Burns, Arthur Greenspan, Alan 96 96 221 Martin, William M. Miller, G. William N/A 48 17 2 Volcker, Paul Yellen, Janet 96 9  Greenspan, Alan   Problem 3 - Converting Variables to Factors Convert the following variables to factors using the as.factor function:\n Chairman DemocraticPres RaisedFedFunds  Which of the following methods requires the dependent variables be stored as a factor variable when training a model for classification?\nstr(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : int 1 1 1 1 1 1 1 1 1 1 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : int 1 1 1 1 1 1 1 0 1 1 ... fedFunds$Chairman \u0026lt;- as.factor(fedFunds$Chairman) fedFunds$DemocraticPres \u0026lt;- as.factor(fedFunds$DemocraticPres) fedFunds$RaisedFedFunds \u0026lt;- as.factor(fedFunds$RaisedFedFunds) str(fedFunds) \u0026#39;data.frame\u0026#39;: 585 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 4.42 4.6 4.65 4.67 4.9 5.17 5.3 5.53 5.4 5.53 ... $ Streak : int 4 5 6 7 8 9 10 11 -1 1 ... $ GDP : num 4202 4202 4202 4219 4219 ... $ Unemployment : num 4 3.8 3.8 3.8 3.9 3.8 3.8 3.8 3.7 3.7 ... $ CPI : num 31.9 32.1 32.2 32.3 32.4 ... $ HomeownershipRate : num 63.5 63.5 63.5 63.2 63.2 63.2 63.3 63.3 63.3 63.8 ... $ DebtAsPctGDP : num 40.3 4201.9 4201.9 39.2 4219.1 ... $ DemocraticPres : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ... $ MonthsUntilElection: int 33 32 31 30 29 28 27 26 25 24 ... $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 1 2 2 ... Random forest (randomForest)   Problem 4 - Splitting the dataframe into a Training \u0026amp; Testing Set Obtain a random training/testing set split with:\nset.seed(201) library(caTools) spl \u0026lt;- sample.split(fedFunds$RaisedFedFunds, 0.7) Split months into a training dataframe called “training” using the observations for which spl is TRUE and a testing dataframe called “testing” using the observations for which spl is FALSE.\ntraining \u0026lt;- subset(fedFunds, spl == TRUE) testing \u0026lt;- subset(fedFunds, spl == FALSE) Why do we use the sample.split() function to split into a training and testing set? #### It balances the dependent variable between the training and testing sets\n Problem 5 - Training a Logistic Regression Model Train a logistic regression model using independent variables “PreviousRate”, “Streak”, “Unemployment”, “HomeownershipRate”, “DemocraticPres”, and “MonthsUntilElection”, using the training set to obtain the model.\nLogIntRate \u0026lt;- glm(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, family = binomial) Which of the following characteristics is the most statistically significant associated with an increased chance of the fed funds rate being raised?\nsummary(LogIntRate)  Call: glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, family = binomial, data = training) Deviance Residuals: Min 1Q Median 3Q Max -2.8177 -1.0121 0.2301 1.0491 2.5297 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.121012 5.155774 1.769 0.0769 . PreviousRate -0.003427 0.032350 -0.106 0.9156 Streak 0.157658 0.025147 6.270 3.62e-10 *** Unemployment -0.047449 0.065438 -0.725 0.4684 HomeownershipRate -0.136451 0.076872 -1.775 0.0759 . DemocraticPres1 0.347829 0.233200 1.492 0.1358 MonthsUntilElection -0.006931 0.007678 -0.903 0.3666 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 568.37 on 409 degrees of freedom Residual deviance: 492.69 on 403 degrees of freedom AIC: 506.69 Number of Fisher Scoring iterations: 4 A longer consecutive STREAK of months in which the fed funds rate was raised   Problem 6 - Predicting Using a Logistic Regression Model Imagine you are an analyst at a bank and your manager has asked you to predict whether the fed funds rate will be raised next month.\nYou know that the rate has been lowered for 3 straight months (Streak = -3) and that the previous month’s rate was 1.7%.\nThe unemployment rate is 5.1% and the homeownership rate is 65.3%.\nThe current U.S. president is a Republican and the next election will be held in 18 months. According to the logistic regression model you built in Problem 5.\nWhat is the predicted probability that the interest rate will be raised?\n9.121012 + PreviousRate(-0.003427) + Streak0.157658 + Unemployment(-0.047449) + HomeownershipRate(-0.136451) + DemocraticPres10.347829 + MonthsUntilElection(-0.006931) 9.121012 + 1.7*(-0.003427) - 3*0.157658 + 5.1*(-0.047449) + 65.3*(-0.136451) + 0*0.347829 + 18*(-0.006931) [1] -0.6347861  -0.6347861 ==\u0026gt; Need to plug it into the logistic response function problem6 \u0026lt;- training[1, ] problem6$PreviousRate \u0026lt;- 1.7 problem6$Streak \u0026lt;- -3 problem6$Unemployment \u0026lt;- 5.1 problem6$HomeownershipRate \u0026lt;- 65.3 problem6$DemocraticPres \u0026lt;- as.factor(0) problem6$MonthsUntilElection \u0026lt;- 18 problem6  Date Chairman PreviousRate Streak GDP Unemployment 1 1966-02-01 Martin, William M. 1.7 -3 4201.891 5.1 CPI HomeownershipRate DebtAsPctGDP DemocraticPres MonthsUntilElection 1 31.88 65.3 40.26076 0 18 RaisedFedFunds 1 1 str(problem6) \u0026#39;data.frame\u0026#39;: 1 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 1 $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 $ PreviousRate : num 1.7 $ Streak : num -3 $ GDP : num 4202 $ Unemployment : num 5.1 $ CPI : num 31.9 $ HomeownershipRate : num 65.3 $ DebtAsPctGDP : num 40.3 $ DemocraticPres : Factor w/ 1 level \u0026quot;0\u0026quot;: 1 $ MonthsUntilElection: num 18 $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 problem6PredProb \u0026lt;- predict(LogIntRate, newdata = problem6, type = \u0026quot;response\u0026quot;) problem6PredProb  1 0.3464297    Problem 7 - Interpreting Model Coefficients What is the meaning of the coefficient labeled “DemocraticPres1” in the logistic regression summary output?\nsummary(LogIntRate)  Call: glm(formula = RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, family = binomial, data = training) Deviance Residuals: Min 1Q Median 3Q Max -2.8177 -1.0121 0.2301 1.0491 2.5297 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.121012 5.155774 1.769 0.0769 . PreviousRate -0.003427 0.032350 -0.106 0.9156 Streak 0.157658 0.025147 6.270 3.62e-10 *** Unemployment -0.047449 0.065438 -0.725 0.4684 HomeownershipRate -0.136451 0.076872 -1.775 0.0759 . DemocraticPres1 0.347829 0.233200 1.492 0.1358 MonthsUntilElection -0.006931 0.007678 -0.903 0.3666 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 568.37 on 409 degrees of freedom Residual deviance: 492.69 on 403 degrees of freedom AIC: 506.69 Number of Fisher Scoring iterations: 4 When the president is Democratic, the odds of the fed funds rate increasing are 41.6% higher than in an otherise identical month (i.e. identical among the variables in the model). EXPLANATION: The coefficients of the model are the log odds associated with that variable; so we see that the odds of being sold are exp(0.347829)=1.41599 those of an otherwise identical month. This means the month is predicted to have 41.6% higher odds of being sold.\n  Problem 8 - Obtaining Test Set Predictions Using our logistic regression model, obtain predictions on the test-set. Then, using a probability threshold of 0.5, create a confusion matrix for the test-set.\nOn how many test-set observations does our logistic regression model make a different prediction than the prediction the naive baseline model would make?\n(Remember that the naive baseline model we use always predicts the most frequent outcome in the training set for all observations in the test-set.)\nstr(testing) \u0026#39;data.frame\u0026#39;: 175 obs. of 12 variables: $ Date : Factor w/ 585 levels \u0026quot;1966-02-01\u0026quot;,\u0026quot;1966-03-01\u0026quot;,..: 14 15 16 18 19 31 32 37 38 39 ... $ Chairman : Factor w/ 8 levels \u0026quot;Bernanke, Ben\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... $ PreviousRate : num 5 4.53 4.05 3.98 3.79 6.02 6.03 6.3 6.61 6.79 ... $ Streak : int 1 -1 -2 1 -1 -2 1 2 3 4 ... $ GDP : num 4325 4325 4329 4329 4366 ... $ Unemployment : num 3.8 3.8 3.8 3.9 3.8 3.7 3.5 3.4 3.4 3.4 ... $ CPI : num 33 33 33.1 33.3 33.4 34.9 35 35.7 35.8 36.1 ... $ HomeownershipRate : num 63.3 63.3 63.9 63.9 63.8 64.1 64.1 64.1 64.1 64.1 ... $ DebtAsPctGDP : num 4324.9 4324.9 37.9 4328.7 38.8 ... $ DemocraticPres : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 2 2 2 2 2 2 2 1 1 1 ... $ MonthsUntilElection: int 20 19 18 16 15 3 2 45 44 43 ... $ RaisedFedFunds : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 1 1 1 1 2 2 1 2 2 2 ... PredProb \u0026lt;- predict(LogIntRate, newdata = testing, type = \u0026quot;response\u0026quot;) table(testing$RaisedFedFunds, PredProb \u0026gt;= 0.5)  FALSE TRUE 0 60 27 1 31 57 table(training$RaisedFedFunds)  0 1 204 206  91 (60 + 31 were predicted less than 0.5)   Problem 9 - Computing Test-Set AUC What is the test-set AUC of the logistic regression model?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess PredTestLogROCR \u0026lt;- prediction(PredProb, testing$RaisedFedFunds) performance(PredTestLogROCR, \u0026quot;auc\u0026quot;)@y.values [[1]] [1] 0.704023  Problem 10 - Interpreting AUC What is the meaning of the AUC? #### The proportion of the time the model can differentiate between a randomly selected month during which the fed funds were raised and a randomly selected month during which the federal funds were not raised.\n Problem 11 - ROC Curves Which logistic regression threshold is associated with the upper-right corner of the ROC plot (true positive rate 1 and false positive rate 1)? #### 0\nEXPLANATION A model with threshold 0 predicts 1 for all observations, yielding a 100% true positive rate and a 100% false positive rate.\n  Problem 12 - ROC Curves Plot the colorized ROC curve for the logistic regression model’s performance on the test-set. At roughly which logistic regression cut-off does the model achieve a true positive rate of 85% and a false positive rate of 60%?\nROCRperf \u0026lt;- performance(PredTestLogROCR, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;) plot(ROCRperf, colorize = TRUE, print.cutoffs.at = seq(0, 1, by = 0.1), text.adj = c(-0.2, 1.7)) 0.37   Problem 13 - Cross-Validation to Select Parameters Which of the following best describes how 10-fold cross-validation works when selecting between 2 different parameter values? #### 20 models are trained on subsets of the training set and evaluated on a portion of the training set\n Problem 14 - Cross-Validation for a CART Model Set the random seed to 201 (even though you have already done so earlier in the problem).\nThen use the caret package and the train function to perform 10-fold cv with the training data set to select the best cp value for a CART model that predicts the dependent variable “RaisedFedFunds” using the independent variables “PreviousRate,” “Streak,” “Unemployment,” “HomeownershipRate,” “DemocraticPres,” and “MonthsUntilElection.” Select the cp value from a grid consisting of the 50 values 0.001, 0.002, …, 0.05.\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 library(e1071) set.seed(201) # define cross-validation experiment numFolds \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;, number = 10) cpGrid \u0026lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001))  Define cv experiment\nnumFolds \u0026lt;- trainControl(method = \u0026quot;cv\u0026quot;, number = 10) cpGrid \u0026lt;- expand.grid(.cp = seq(0.001, 0.05, 0.001))  Perform the cv\ntrainCV \u0026lt;- train(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = \u0026quot;rpart\u0026quot;, trControl = numFolds, tuneGrid = cpGrid) trainCV CART 410 samples 6 predictor 2 classes: \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 369, 368, 368, 369, 370, 370, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.001 0.6248461 0.2498288 0.002 0.6366928 0.2737781 0.003 0.6465099 0.2940348 0.004 0.6465099 0.2940348 0.005 0.6465099 0.2940348 0.006 0.6513298 0.3037278 0.007 0.6513298 0.3037278 0.008 0.6488298 0.2987278 0.009 0.6462108 0.2934897 0.010 0.6437718 0.2887290 0.011 0.6532956 0.3075752 0.012 0.6532956 0.3075752 0.013 0.6532956 0.3075752 0.014 0.6386527 0.2782894 0.015 0.6386527 0.2782894 0.016 0.6386527 0.2782894 0.017 0.6287718 0.2585275 0.018 0.6287718 0.2585275 0.019 0.6287718 0.2585275 0.020 0.6385918 0.2780499 0.021 0.6385918 0.2780499 0.022 0.6385918 0.2784431 0.023 0.6385918 0.2784431 0.024 0.6432956 0.2882859 0.025 0.6432956 0.2882859 0.026 0.6605575 0.3228097 0.027 0.6605575 0.3228097 0.028 0.6680575 0.3378097 0.029 0.6680575 0.3378097 0.030 0.6680575 0.3381290 0.031 0.6680575 0.3381290 0.032 0.6680575 0.3381290 0.033 0.6680575 0.3381290 0.034 0.6680575 0.3381290 0.035 0.6680575 0.3381290 0.036 0.6729355 0.3474661 0.037 0.6729355 0.3474661 0.038 0.6729355 0.3474661 0.039 0.6729355 0.3474661 0.040 0.6729355 0.3474661 0.041 0.6729355 0.3474661 0.042 0.6729355 0.3474661 0.043 0.6729355 0.3474661 0.044 0.6729355 0.3474661 0.045 0.6729355 0.3474661 0.046 0.6729355 0.3474661 0.047 0.6729355 0.3474661 0.048 0.6729355 0.3474661 0.049 0.6729355 0.3474661 0.050 0.6729355 0.3474661 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.05. What cp value maximizes the cv accuracy? #### 0.016\n Problem 15 - Train CART Model Create and plot the CART model trained with the parameter identified in Problem 14, again predicting the dependent variable using “PreviousRate”, “Streak”, “Unemployment”, “HomeownershipRate”, “DemocraticPres”, and “MonthsUntilElection”.\nWhat variable is used as the first (upper-most) split in the tree?\nlibrary(rpart) library(rpart.plot) TreeIntRate \u0026lt;- trainCV$finalModel prp(TreeIntRate) TreeIntRate n= 410 node), split, n, loss, yval, (yprob) * denotes terminal node 1) root 410 204 1 (0.4975610 0.5024390) 2) Streak\u0026lt; 2.5 300 115 0 (0.6166667 0.3833333) * 3) Streak\u0026gt;=2.5 110 19 1 (0.1727273 0.8272727) * TreeIntRate2 \u0026lt;- rpart(RaisedFedFunds ~ PreviousRate + Streak + Unemployment + HomeownershipRate + DemocraticPres + MonthsUntilElection, data = training, method = \u0026quot;class\u0026quot;, cp = 0.016) prp(TreeIntRate2) Streak   Problem 16 - Predicting Using a CART Model If you were to use the CART model you created in Problem 15 to explore the question asked of the analyst in Problem 6, what would you predict for next month?\nRemember: The rate has been lowered for 3 straight months (Streak = -3). The previous month’s rate was 1.7%. The unemployment rate is 5.1%. The homeownership rate is 65.3%. The current U.S. president is a Republican and the next election will be held in 18 months. #### The Fed will not raise the federal funds rate. The Fed will not raise the fed funds rate.\n Problem 17 - Test-Set Accuracy for CART Model Using the CART model you created in Problem 15, obtain predictions on the test-set (using the parameter type=“class” with the predict function).\nThen, create a confusion matrix for the test-set.\nPredClassTree \u0026lt;- predict(TreeIntRate2, newdata = testing, type = \u0026quot;class\u0026quot;) What is the accuracy of your CART model?\ntable(PredClassTree, testing$RaisedFedFunds)  PredClassTree 0 1 0 64 40 1 23 48 (64 + 48) / nrow(testing) [1] 0.64  ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"33bd0eebb3741850783dce947bc6ba4a","permalink":"/project/interest_forecast/interest_forecast/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/interest_forecast/interest_forecast/","section":"project","summary":"Predict when the Fed will raise interest rates","tags":["R","Data Analytics","Machine Learning"],"title":"Forecasting U.S. Federal Reserve Interest Rate Hikes","type":"project"},{"authors":null,"categories":null,"content":" The use of coal in the United States peaked in 2005, and since then has decreased by 25%, being replaced by renewable energy sources and more efficient use (Lovins, 2014). As the US pursues a portfolio of more diverse, sustainable and secure energy sources, there are many questions to consider.\nWhat are effective factors in incentivizing states to adopt more environmentally friendly energy generation methods? How do these factors vary by state? How can we direct resources to different places in the country and ensure that they effectively drive renewable energy sources adoption?  To derive insights and explore these questions, I’ll take a combination of generation, usage, and greenhouse emission data by state and combine it with macro-economic and political information.\nFor this analysis, I’ve gathered data from various sources to include the following information for each state within the U.S. for the years spanning year 2000 to year 2013. The aggregated dataset energy.csv.xz results in a total of 27 variables and 699 observations. Each observation contains one record per state per year. Here’s a detailed description of the variables:\n GenTotal: Annual generation of energy using all types of energy sources (coal, nuclear, hydroelectric, solar, etc.) normalized by the state population at a given year. GenTotalRenewable: Annual generation of energy using all renewable energy sources normalized by the state population at a given year. GenHydro, GenSolar: Annual generation of energy using each type of energy source as a percent of the total energy generation. GenTotalRenewableBinary, GenSolarBinary: 1 if generation from solar or other renewable energy sources increased between a year n and a year n+1. 0 if it did not increase. AllSourcesCO2, AllSourcesSO2 and AllSourcesNOx: Annual emissions per state in metric tons, normalized by the respective state population at a given year and caused by all energy generation sources. EPriceResidential, EPriceCommercial, EPriceIndustrial, EPriceTransportation, EPriceTotal: Average electricity price per state, per sector (residential, industrial, commercial, etc.) ESalesResidential, ESalesCommercial, ESalesIndustrial, ESalesTransportation, ESalesTotal: Annual normalized sales of electricity per state, per sector. CumlRegulatory, CumlFinancial: Number of energy-related financial incentives and regulations created by a state per year. Demographic data such as annual wages per capita and presidential results (0 if a state voted republican, 1 is democrat).  Problem 1 - Total Renewable Energy Generation Load energy.csv into a data frame called energy.\nenergy \u0026lt;- read.csv(\u0026quot;energy.csv.xz\u0026quot;) str(energy) \u0026#39;data.frame\u0026#39;: 699 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ YEAR : int 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 ... $ GenTotal : num 9.81 10.64 10.54 9.79 9.9 ... $ GenHydro : num 0.163 0.2 0.213 0.25 0.23 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.163 0.2 0.214 0.25 0.231 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 1 0 0 0 1 0 1 1 ... $ AllSourcesCO2 : num 7.26 7.31 6.94 6.15 7.25 ... $ AllSourcesSO2 : num 0.0224 0.01193 0.01148 0.00675 0.00659 ... $ AllSourcesNOx : num 0.0289 0.0277 0.0294 0.0242 0.0377 ... $ EPriceResidential : num 11.4 12.1 12.1 12 12.4 ... $ EPriceCommercial : num 9.77 10.29 10.13 10.49 10.99 ... $ EPriceIndustrial : num 7.56 7.61 7.65 7.86 8.33 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.1 10.5 10.5 10.5 11 ... $ EsalesResidential : num 0.349 0.347 0.354 0.357 0.356 ... $ EsalesCommercial : num 0.421 0.42 0.41 0.444 0.449 ... $ EsalesIndustrial : num 0.195 0.198 0.199 0.198 0.195 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0343 0.0356 0.0379 0 0 ... $ EsalesTotal : num 8.46 8.61 8.51 8.59 8.78 ... $ CumlFinancial : int 1 1 1 1 1 2 7 7 10 10 ... $ CumlRegulatory : int 1 1 1 1 1 1 2 2 2 3 ... $ Total.salary : num 17.6 18.7 19.6 20.4 21.4 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... summary(energy)  STATE YEAR GenTotal GenHydro AK : 14 Min. :2000 Min. : 4.591 Min. :0.00000 AL : 14 1st Qu.:2003 1st Qu.:10.037 1st Qu.:0.00888 AR : 14 Median :2006 Median :14.647 Median :0.02291 AZ : 14 Mean :2006 Mean :16.956 Mean :0.09853 CA : 14 3rd Qu.:2010 3rd Qu.:18.018 3rd Qu.:0.07041 CO : 14 Max. :2013 Max. :92.097 Max. :0.92076 (Other):615 GenSolar GenTotalRenewable GenSolarBinary Min. :-1.300e-08 Min. :0.00000 Min. :0.0000 1st Qu.: 0.000e+00 1st Qu.:0.02027 1st Qu.:0.0000 Median : 0.000e+00 Median :0.04475 Median :0.0000 Mean : 3.344e-04 Mean :0.12245 Mean :0.2318 3rd Qu.: 0.000e+00 3rd Qu.:0.10771 3rd Qu.:0.0000 Max. : 2.045e-02 Max. :0.92076 Max. :1.0000 GenTotalRenewableBinary AllSourcesCO2 AllSourcesSO2 Min. :0.0000 Min. : 0.01059 Min. :0.00003 1st Qu.:0.0000 1st Qu.: 4.84645 1st Qu.:0.00925 Median :1.0000 Median : 8.34005 Median :0.02375 Mean :0.5923 Mean :11.61430 Mean :0.03687 3rd Qu.:1.0000 3rd Qu.:12.99889 3rd Qu.:0.04197 Max. :1.0000 Max. :93.96429 Max. :0.34568 NA\u0026#39;s :50 NA\u0026#39;s :50 AllSourcesNOx EPriceResidential EPriceCommercial EPriceIndustrial Min. :0.00067 Min. : 5.130 Min. : 4.240 Min. : 3.010 1st Qu.:0.00604 1st Qu.: 7.965 1st Qu.: 6.760 1st Qu.: 4.695 Median :0.01428 Median : 9.490 Median : 8.100 Median : 5.820 Mean :0.02058 Mean :10.424 Mean : 8.969 Mean : 6.660 3rd Qu.:0.02288 3rd Qu.:11.825 3rd Qu.:10.065 3rd Qu.: 7.500 Max. :0.35610 Max. :37.340 Max. :34.880 Max. :30.820 NA\u0026#39;s :50 EPriceTransportation EPriceTotal EsalesResidential EsalesCommercial Min. : 0.000 Min. : 4.170 Min. :0.1594 Min. :0.1690 1st Qu.: 0.000 1st Qu.: 6.515 1st Qu.:0.3297 1st Qu.:0.2816 Median : 1.615 Median : 7.860 Median :0.3563 Median :0.3385 Mean : 4.131 Mean : 8.809 Mean :0.3595 Mean :0.3411 3rd Qu.: 7.940 3rd Qu.:10.095 3rd Qu.:0.3926 3rd Qu.:0.3877 Max. :15.440 Max. :34.040 Max. :0.5287 Max. :0.5381 NA\u0026#39;s :1 EsalesIndustrial EsalesTransportation EsalesOther EsalesTotal Min. :0.06372 Min. :0.0000000 Min. :0.0000 Min. : 6.745 1st Qu.:0.21331 1st Qu.:0.0000000 1st Qu.:0.0000 1st Qu.:10.336 Median :0.29978 Median :0.0000000 Median :0.0092 Median :13.153 Mean :0.29232 Mean :0.0010349 Mean :0.0170 Mean :13.145 3rd Qu.:0.35791 3rd Qu.:0.0002786 3rd Qu.:0.0216 3rd Qu.:15.353 Max. :0.59561 Max. :0.0229279 Max. :0.1074 Max. :31.336 NA\u0026#39;s :449 CumlFinancial CumlRegulatory Total.salary presidential.results Min. : 0.00 Min. : 0.000 Min. :10.65 Min. :0.0000 1st Qu.: 2.00 1st Qu.: 3.000 1st Qu.:16.84 1st Qu.:0.0000 Median : 8.00 Median : 6.000 Median :19.51 Median :0.0000 Mean : 17.58 Mean : 6.838 Mean :20.01 Mean :0.4578 3rd Qu.: 23.50 3rd Qu.:10.000 3rd Qu.:22.83 3rd Qu.:1.0000 Max. :154.00 Max. :42.000 Max. :33.85 Max. :1.0000 Import Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.3433 3rd Qu.:1.0000 Max. :1.0000  Renewable energy sources are considered to include geothermal, hydroelectric, biomass, solar and wind.\nWhich state in the US seems to have the highest total generation of energy from renewable sources (using the variable GenTotalRenewable)?\nenergy[which.max(energy$GenTotalRenewable), ]  STATE YEAR GenTotal GenHydro GenSolar GenTotalRenewable 169 ID 2000 9.165016 0.9207631 0 0.9207631 GenSolarBinary GenTotalRenewableBinary AllSourcesCO2 AllSourcesSO2 169 0 0 0.6368657 0.004590802 AllSourcesNOx EPriceResidential EPriceCommercial EPriceIndustrial 169 0.002714775 5.39 4.24 3.11 EPriceTransportation EPriceTotal EsalesResidential EsalesCommercial 169 0 4.17 0.3068433 0.3095307 EsalesIndustrial EsalesTransportation EsalesOther EsalesTotal 169 0.368208 0 0.01541804 17.57071 CumlFinancial CumlRegulatory Total.salary presidential.results Import 169 2 2 12.81243 0 1 Idaho (ID) Which year did the above state produce the highest energy generation from renewable resources? #### 2000\n  Problem 2 - Relationship Between Politics and Greenhouse Emissions What is the average CO2 emissions from all sources of energy for: Note Using na.rm = TRUE in our calculations!\n states during years in which they voted republican?  mean(subset(energy, presidential.results == 0)$AllSourcesCO2, na.rm = TRUE) [1] 16.44296  states during years in which they voted democrat?  mean(subset(energy, presidential.results == 1)$AllSourcesCO2, na.rm = TRUE) [1] 5.783781 States that voted democrat have on average higher NOx emissions than states that voted republican across all years.\nIs this statement true or false?\nmean(subset(energy, presidential.results == 0)$AllSourcesNOx, na.rm = TRUE) [1] 0.02985461 mean(subset(energy, presidential.results == 1)$AllSourcesNOx, na.rm = TRUE) [1] 0.009377028 False   Problem 3 - Relationship Between Greenhouse Emissions and Energy Sales What is the correlation between overall CO2 emissions and energy sales made to industrial facilities?\nNote that the variables AllSourcesCO2 and EsalesIndustrial contain NAs. Using the parameter: use=“complete” to handle NAs in this question!\ncor(energy$AllSourcesCO2, energy$EsalesIndustrial, use = \u0026quot;complete\u0026quot;) [1] 0.5385867 Choose the correct answers from the following statements:\ncor(energy$AllSourcesSO2, energy$EsalesIndustrial, use = \u0026quot;complete\u0026quot;) [1] 0.4812317 cor(energy$AllSourcesNOx, energy$EsalesResidential, use = \u0026quot;complete\u0026quot;) [1] -0.5038829 cor(energy$AllSourcesCO2, energy$EsalesCommercial, use = \u0026quot;complete\u0026quot;) [1] -0.373383 Overall SO2 emissions are likely higher with increased industrial energy sales   Problem 4 - Boxplot of Energy Prices per State Creating a boxplot of the total energy price (EPriceTotal) by State across the data, and a table summarizing the mean of EPriceTotal by State.\npriceBoxplot \u0026lt;- ggplot(energy, aes(STATE, EPriceTotal)) priceBoxplot + geom_boxplot() What observations do we make? #### - The boxplot shows a clear outlier, the state of Hawaii, with much higher energy price compared to the rest of the U.S. #### - When looking at the average energy prices, there seems to be three price tiers ($5-$9, $10-$14, and $20+)\nWhich state has the lowest average energy price of all? Using a table to explore this question.\nsum(is.na(energy$EPriceTotal)) [1] 0 mean(energy$EPriceTotal) [1] 8.809213 avgPriceByState \u0026lt;- aggregate(EPriceTotal ~ STATE, energy, mean) avgPriceByState[which.min(avgPriceByState$EPriceTotal), ]  STATE EPriceTotal 50 WY 5.51 Wyoming (WY) Is this state associated with the highest mean total energy generation (GenTotal)?\navgGenByState \u0026lt;- aggregate(GenTotal ~ STATE, energy, mean) avgGenByState  STATE GenTotal 1 AK 9.810218 2 AL 30.582000 3 AR 19.090133 4 AZ 17.211604 5 CA 5.576285 6 CO 10.367634 7 CT 9.358409 8 DE 8.256761 9 FL 12.027089 10 GA 13.969187 11 HI 8.623563 12 IA 16.270149 13 ID 8.358471 14 IL 15.193858 15 IN 19.664150 16 KS 16.732968 17 KY 22.399999 18 LA 21.384516 19 MA 6.434295 20 MD 8.352530 21 ME 13.148792 22 MI 11.201083 23 MN 10.199283 24 MO 15.062760 25 MS 16.377409 26 MT 28.942351 27 NC 13.915444 28 ND 49.909462 29 NE 18.470352 30 NH 15.671056 31 NJ 7.108700 32 NM 18.130563 33 NV 14.311625 34 NY 7.197383 35 OH 12.664461 36 OK 18.787146 37 OR 14.537263 38 PA 17.326080 39 RI 6.488628 40 SC 22.476570 41 SD 10.719863 42 TN 14.751132 43 TX 17.020082 44 UT 15.587630 45 VA 9.720367 46 VT 10.177005 47 WA 16.426394 48 WI 11.079302 49 WV 47.417141 50 WY 88.393083 avgGenByState[which.max(avgGenByState$GenTotal), ]  STATE GenTotal 50 WY 88.39308  True   Problem 5 - Prediction Model for Solar Generation We are interested in predicting whether states are going to increase their solar energy generation over the next year.\nLet’s subset our dataset into a training and a testing set by using the following code:\nset.seed(144) spl \u0026lt;- sample(1 : nrow(energy), size = 0.7 * nrow(energy)) train \u0026lt;- energy[spl, ] test \u0026lt;- energy[-spl, ] Let’s create a logistic regression model “mod” using the train set to predict the binary variable GenSolarBinary.\nTo do so, consider the following as potential predictive variables: GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary, Import.\nmod \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train, family = binomial) Which variable is most predictive in the model?\nsummary(mod)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -4.0165 -0.4374 -0.2398 -0.0719 2.6342 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -6.345e+00 9.148e-01 -6.935 4.05e-12 *** GenHydro -3.652e+00 1.114e+00 -3.279 0.00104 ** GenSolar 1.076e+03 3.322e+02 3.238 0.00121 ** CumlFinancial 2.814e-02 9.563e-03 2.943 0.00325 ** CumlRegulatory 1.996e-01 5.045e-02 3.955 7.65e-05 *** Total.salary 1.393e-01 4.465e-02 3.119 0.00181 ** Import -3.142e-01 3.447e-01 -0.912 0.36190 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 528.69 on 488 degrees of freedom Residual deviance: 282.82 on 482 degrees of freedom AIC: 296.82 Number of Fisher Scoring iterations: 7 CumlRegulatory   Problem 6 - Performance on the Test Set Computing the predictions on the test-set. Using a threshold of 0.5, what is the accuracy of our model on the test-set?\npredTest \u0026lt;- predict(mod, newdata = test, type = \u0026quot;response\u0026quot;) table(test$GenSolarBinary, predTest \u0026gt; 0.5)  FALSE TRUE 0 154 7 1 31 18 (154 + 18) / nrow(test) [1] 0.8190476 Cool!\nWhat is the accuracy for states voting republican?\ntestWithPred \u0026lt;- test testWithPred$predTest \u0026lt;- predTest str(testWithPred) \u0026#39;data.frame\u0026#39;: 210 obs. of 28 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... $ predTest : num 0.01417 0.0541 0.07459 0.15572 0.00879 ... testWithPredRep \u0026lt;- subset(testWithPred, presidential.results == 0) table(testWithPredRep$GenSolarBinary, testWithPredRep$predTest \u0026gt; 0.5)  FALSE TRUE 0 90 0 1 18 2 (90 + 2) / nrow(testWithPredRep) [1] 0.8363636 What is the accuracy for states voting democrat?\ntestWithPredDem \u0026lt;- subset(testWithPred, presidential.results == 1) table(testWithPredDem$GenSolarBinary, testWithPredDem$predTest \u0026gt; 0.5)  FALSE TRUE 0 64 7 1 13 16 (64 + 16) / nrow(testWithPredDem) [1] 0.8  Problem 7 - Clustering of the Observations We can perhaps improve our accuracy if we implement a cluster-the-predict approach. I’m interested in clustering the observations based on information about the regulatory and financial incentives, the elections outcome and the population wealth in each state across the years, in addition to whether the state was an energy importer or not.\nLet’s create a train.limited and test.limited datasets, where we only keep the variables CumlRegulatory, CumlFinancial, presidential.results, Total.salary, and Import.\nstr(train) \u0026#39;data.frame\u0026#39;: 489 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 3 36 37 25 16 32 2 37 4 41 ... $ YEAR : int 2006 2003 2002 2012 2010 2009 2011 2000 2011 2007 ... $ GenTotal : num 18.5 17.3 13.4 18.3 16.9 ... $ GenHydro : num 0.029722 0.029664 0.73065 0 0.000276 ... $ GenSolar : num 0 0 0 0 0 ... $ GenTotalRenewable : num 0.030363 0.030562 0.739667 0.000301 0.07246 ... $ GenSolarBinary : int 0 0 0 0 0 1 0 0 1 0 ... $ GenTotalRenewableBinary: int 1 1 0 0 1 1 0 0 0 0 ... $ AllSourcesCO2 : num 10.28 14.09 2.05 8.13 12.78 ... $ AllSourcesSO2 : num 0.0291 0.0316 0.004 0.0144 0.0144 ... $ AllSourcesNOx : num 0.01358 0.02472 0.00324 0.00786 0.01617 ... $ EPriceResidential : num 8.85 7.47 7.12 10.26 10.03 ... $ EPriceCommercial : num 6.96 6.38 6.59 9.33 8.25 ... $ EPriceIndustrial : num 5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 6.55 5.09 ... $ EPriceTransportation : num 0 0 6.5 0 0 0 0 6.68 0 0 ... $ EPriceTotal : num 6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 9.71 6.89 ... $ EsalesResidential : num 0.366 0.4 0.388 0.372 0.355 ... $ EsalesCommercial : num 0.248 0.336 0.329 0.281 0.382 ... $ EsalesIndustrial : num 0.386 0.264 0.272 0.347 0.263 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num NA 0 0.0111 NA NA ... $ EsalesTotal : num 16.6 14.4 12.9 16.2 14.2 ... $ CumlFinancial : int 4 1 12 19 3 23 15 5 42 5 ... $ CumlRegulatory : int 4 1 9 1 6 8 2 5 21 1 ... $ Total.salary : num 15.4 13.4 16.3 15.3 20.8 ... $ presidential.results : int 0 0 1 0 0 1 0 1 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 1 ... train.limited \u0026lt;- subset(train, select = CumlFinancial:Import) str(test) \u0026#39;data.frame\u0026#39;: 210 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... test.limited \u0026lt;- subset(test, select = CumlFinancial:Import) Using the “preProcess” function on the train.limited set, I can compute the train.norm and test.norm.\npreprocTrain \u0026lt;- preProcess(train.limited) train.norm \u0026lt;- predict(preprocTrain, train.limited) preprocTest \u0026lt;- preProcess(test.limited) test.norm \u0026lt;- predict(preprocTest, test.limited) Why didn’t I include the dependent variable GenSolarBinary in this clustering phase? #### - Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the cluster-then-predict methodology\nLet’s use kmeans clustering for this problem with a seed of 144, k=2 and keep the maximum number of iterations at 1,000.\nset.seed(144) k \u0026lt;- 2 trainKMC \u0026lt;- kmeans(train.norm, centers = k, iter.max = 1000) set.seed(144) testKMC \u0026lt;- kmeans(test.norm, centers = k, iter.max = 1000) Using the flexclust package, identifying the clusters and call train1 the subset of train corresponding to the first cluster, and train2 the subset of train corresponding to the second cluster.\ntrainKMC.kcca \u0026lt;- as.kcca(trainKMC, train.norm) # clusters(trainKMC.kcca) trainClusters \u0026lt;- predict(trainKMC.kcca) table(trainClusters) trainClusters 1 2 308 181  train1 \u0026lt;- subset(train, trainClusters == 1) train2 \u0026lt;- subset(train, trainClusters == 2) str(train1) \u0026#39;data.frame\u0026#39;: 308 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 3 36 37 25 16 32 2 37 41 28 ... $ YEAR : int 2006 2003 2002 2012 2010 2009 2011 2000 2007 2009 ... $ GenTotal : num 18.5 17.3 13.4 18.3 16.9 ... $ GenHydro : num 0.029722 0.029664 0.73065 0 0.000276 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.030363 0.030562 0.739667 0.000301 0.07246 ... $ GenSolarBinary : int 0 0 0 0 0 1 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 0 0 1 1 0 0 0 1 ... $ AllSourcesCO2 : num 10.28 14.09 2.05 8.13 12.78 ... $ AllSourcesSO2 : num 0.0291 0.0316 0.004 0.0144 0.0144 ... $ AllSourcesNOx : num 0.01358 0.02472 0.00324 0.00786 0.01617 ... $ EPriceResidential : num 8.85 7.47 7.12 10.26 10.03 ... $ EPriceCommercial : num 6.96 6.38 6.59 9.33 8.25 ... $ EPriceIndustrial : num 5.24 4.59 4.72 6.24 6.23 5.72 6.25 3.56 5.09 5.25 ... $ EPriceTransportation : num 0 0 6.5 0 0 0 0 6.68 0 0 ... $ EPriceTotal : num 6.99 6.35 6.32 8.6 8.35 8.09 9.1 4.89 6.89 6.63 ... $ EsalesResidential : num 0.366 0.4 0.388 0.372 0.355 ... $ EsalesCommercial : num 0.248 0.336 0.329 0.281 0.382 ... $ EsalesIndustrial : num 0.386 0.264 0.272 0.347 0.263 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num NA 0 0.0111 NA NA ... $ EsalesTotal : num 16.6 14.4 12.9 16.2 14.2 ... $ CumlFinancial : int 4 1 12 19 3 23 15 5 5 8 ... $ CumlRegulatory : int 4 1 9 1 6 8 2 5 1 4 ... $ Total.salary : num 15.4 13.4 16.3 15.3 20.8 ... $ presidential.results : int 0 0 1 0 0 1 0 1 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 1 0 ... str(train2) \u0026#39;data.frame\u0026#39;: 181 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 4 9 8 19 14 8 30 10 48 22 ... $ YEAR : int 2011 2011 2002 2007 2006 2006 2010 2011 2004 2011 ... $ GenTotal : num 16.7 11.61 7.46 7.24 15.13 ... $ GenHydro : num 0.08485 0.00082 0 0.01694 0.0009 ... $ GenSolar : num 0.000771 0.000567 0 0 0 ... $ GenTotalRenewable : num 0.08841 0.01191 0 0.04075 0.00531 ... $ GenSolarBinary : int 1 1 0 1 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 0 0 0 1 1 1 1 0 0 1 ... $ AllSourcesCO2 : num 8.27 5.99 7.54 3.96 7.95 ... $ AllSourcesSO2 : num 0.00459 0.00592 0.03869 0.00787 0.0243 ... $ AllSourcesNOx : num 0.00815 0.00434 0.01426 0.00307 0.00962 ... $ EPriceResidential : num 11.08 11.51 8.7 16.23 8.42 ... $ EPriceCommercial : num 9.5 9.85 7.15 15.2 7.95 ... $ EPriceIndustrial : num 6.55 8.55 4.85 13.03 4.69 ... $ EPriceTransportation : num 0 8.81 0 9.24 5.59 0 0 7.94 0 8.53 ... $ EPriceTotal : num 9.71 10.61 6.91 15.16 7.07 ... $ EsalesResidential : num 0.441 0.517 0.335 0.352 0.326 ... $ EsalesCommercial : num 0.394 0.408 0.315 0.475 0.355 ... $ EsalesIndustrial : num 0.165 0.075 0.345 0.165 0.315 ... $ EsalesTransportation : num 0 0.00038 0 0.00705 0.00364 ... $ EsalesOther : num NA NA 0.00498 NA NA ... $ EsalesTotal : num 11.58 11.78 14.94 8.79 11.2 ... $ CumlFinancial : int 42 59 2 33 6 2 32 39 3 37 ... $ CumlRegulatory : int 21 14 4 12 6 7 9 7 7 12 ... $ Total.salary : num 22 20.7 21.3 28.9 22.7 ... $ presidential.results : int 0 1 1 1 1 1 1 0 1 1 ... $ Import : int 0 1 1 1 0 1 0 1 1 0 ... testKMC.kcca \u0026lt;- as.kcca(testKMC, test.norm) # clusters(testKMC.kcca) testClusters \u0026lt;- predict(testKMC.kcca) table(testClusters) testClusters 1 2 130 80  test1 \u0026lt;- subset(test, testClusters == 1) test2 \u0026lt;- subset(test, testClusters == 2) str(test1) \u0026#39;data.frame\u0026#39;: 130 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ YEAR : int 2001 2007 2008 2012 2001 2006 2007 2013 2000 2001 ... $ GenTotal : num 10.64 10.03 9.88 9.5 28.08 ... $ GenHydro : num 0.1995 0.1893 0.173 0.2267 0.0667 ... $ GenSolar : num 0 0 0 0 0 0 0 0 0 0 ... $ GenTotalRenewable : num 0.1997 0.1909 0.1737 0.2325 0.0668 ... $ GenSolarBinary : int 0 0 0 0 0 0 0 1 0 0 ... $ GenTotalRenewableBinary: int 1 0 1 1 0 0 1 1 0 1 ... $ AllSourcesCO2 : num 7.31 6.39 6.38 5.89 17.65 ... $ AllSourcesSO2 : num 0.01193 0.00631 0.00514 0.0037 0.10361 ... $ AllSourcesNOx : num 0.0277 0.0253 0.0215 0.0233 0.0384 ... $ EPriceResidential : num 12.12 15.18 16.56 17.88 7.01 ... $ EPriceCommercial : num 10.29 12.19 13.64 14.93 6.53 ... $ EPriceIndustrial : num 7.61 12.63 14.17 16.82 3.79 ... $ EPriceTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EPriceTotal : num 10.5 13.3 14.7 16.3 5.6 ... $ EsalesResidential : num 0.347 0.334 0.337 0.337 0.35 ... $ EsalesCommercial : num 0.42 0.447 0.451 0.448 0.238 ... $ EsalesIndustrial : num 0.198 0.219 0.213 0.215 0.403 ... $ EsalesTransportation : num 0 0 0 0 0 0 0 0 0 0 ... $ EsalesOther : num 0.0356 NA NA NA 0.00931 ... $ EsalesTotal : num 8.61 9.31 9.23 8.78 17.78 ... $ CumlFinancial : int 1 7 10 12 1 8 8 16 0 0 ... $ CumlRegulatory : int 1 2 2 4 0 1 1 2 0 1 ... $ Total.salary : num 18.7 25.7 27.1 31.2 13.2 ... $ presidential.results : int 0 0 0 0 0 0 0 0 0 0 ... $ Import : int 0 0 0 0 0 0 0 0 0 0 ... str(test2) \u0026#39;data.frame\u0026#39;: 80 obs. of 27 variables: $ STATE : Factor w/ 50 levels \u0026quot;AK\u0026quot;,\u0026quot;AL\u0026quot;,\u0026quot;AR\u0026quot;,..: 4 5 5 6 6 6 7 7 7 7 ... $ YEAR : int 2009 2004 2010 2008 2010 2013 2001 2003 2005 2007 ... $ GenTotal : num 17 5.48 5.48 10.84 9.95 ... $ GenHydro : num 0.0574 0.1753 0.1638 0.0382 0.0311 ... $ GenSolar : num 0.000126 0.002931 0.003769 0.000343 0.000838 ... $ GenTotalRenewable : num 0.058 0.2787 0.2711 0.0996 0.1012 ... $ GenSolarBinary : int 1 0 1 1 1 1 0 0 0 0 ... $ GenTotalRenewableBinary: int 1 1 1 1 1 1 0 0 1 1 ... $ AllSourcesCO2 : num 8.12 1.69 1.49 8.45 7.95 ... $ AllSourcesSO2 : num 4.99e-03 6.28e-04 6.77e-05 1.11e-02 8.81e-03 ... $ AllSourcesNOx : num 0.00935 0.00276 0.00214 0.01272 0.01081 ... $ EPriceResidential : num 10.7 12.2 14.8 10.1 11 ... $ EPriceCommercial : num 9.35 11.64 13.09 8.57 9.13 ... $ EPriceIndustrial : num 6.65 9.27 9.8 6.65 6.9 ... $ EPriceTransportation : num 0 6.42 8.27 8.32 9.34 ... $ EPriceTotal : num 9.56 11.35 13.01 8.59 9.15 ... $ EsalesResidential : num 0.447 0.331 0.338 0.34 0.342 ... $ EsalesCommercial : num 0.4 0.472 0.469 0.394 0.37 ... $ EsalesIndustrial : num 0.153 0.194 0.191 0.265 0.287 ... $ EsalesTransportation : num 0 0.003572 0.003177 0.000932 0.000876 ... $ EsalesOther : num NA 0 NA NA NA ... $ EsalesTotal : num 11.15 7.09 6.94 10.58 10.39 ... $ CumlFinancial : int 33 17 138 30 58 70 3 3 4 10 ... $ CumlRegulatory : int 17 20 28 18 19 20 5 5 7 12 ... $ Total.salary : num 21.2 20.8 24 27 26.3 ... $ presidential.results : int 0 1 1 1 1 1 1 1 1 1 ... $ Import : int 0 1 1 0 1 1 1 1 0 1 ... Select the correct statement(s) below:\ntable(train1$presidential.results)  0 1 248 60  table(train2$presidential.results)  0 1 21 160  mean(train1$CumlFinancial) [1] 7.766234 mean(train2$CumlFinancial) [1] 34.87845 mean(train1$CumlRegulatory) [1] 3.983766 mean(train2$CumlRegulatory) [1] 12.28729 mean(train1$AllSourcesCO2, na.rm = TRUE) [1] 15.09418 mean(train2$AllSourcesCO2, na.rm = TRUE) [1] 5.841604 mean(train1$AllSourcesSO2, na.rm = TRUE) [1] 0.04923232 mean(train2$AllSourcesSO2, na.rm = TRUE) [1] 0.01533239 mean(train1$AllSourcesNOx, na.rm = TRUE) [1] 0.0282891 mean(train2$AllSourcesNOx, na.rm = TRUE) [1] 0.007871944 - On average, train1 contains more republican states than train2  - On average, train1 contains states that have recorded more CO2, SO2 and NOx emissions than train2   Problem 8 - Creating the Model on the First Cluster Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod1 using a logistic regression on train1.\nmod1 \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train1, family = binomial) What variable is most predictive?\nsummary(mod1)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train1) Deviance Residuals: Min 1Q Median 3Q Max -1.8654 -0.2715 -0.1692 -0.1194 2.8802 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -7.574e+00 1.945e+00 -3.893 9.89e-05 *** GenHydro -1.201e+00 2.082e+00 -0.577 0.56398 GenSolar 2.324e+04 1.104e+04 2.106 0.03524 * CumlFinancial 6.836e-02 2.159e-02 3.166 0.00154 ** CumlRegulatory 1.589e-01 1.145e-01 1.388 0.16521 Total.salary 1.549e-01 9.190e-02 1.686 0.09188 . Import -3.323e-01 9.628e-01 -0.345 0.72995 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 131.538 on 307 degrees of freedom Residual deviance: 90.376 on 301 degrees of freedom AIC: 104.38 Number of Fisher Scoring iterations: 7 CumlFinancial   Problem 9 - Evaluating the Model Obtained Using the First Cluster What is the accuracy on test1, the subset of test corresponding to the first cluster?\npredTest1 \u0026lt;- predict(mod1, newdata = test1, type = \u0026quot;response\u0026quot;) table(test1$GenSolarBinary, predTest1 \u0026gt; 0.5)  FALSE TRUE 0 114 1 1 11 4 (114 + 4) / nrow(test1) [1] 0.9076923 Wow!\nI’d like to know if mod1 gives us an edge over mod on the dataset test1.\nUsing mod, predicting GenSolarBinary for the observation in test1 and report the accuracy below:\npredTest1WithMod \u0026lt;- predict(mod, newdata = test1, type = \u0026quot;response\u0026quot;) table(test1$GenSolarBinary, predTest1WithMod \u0026gt; 0.5)  FALSE 0 115 1 15 115 / nrow(test1) [1] 0.8846154  Problem 10 - Creating the Model on the Second Cluster Using the variables GenHydro, GenSolar, CumlFinancial, CumlRegulatory, Total.salary and Import, creating mod2 using a logistic regression on train2.\nmod2 \u0026lt;- glm(GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, data = train2, family = binomial) Select the correct statement(s) below?\nsummary(mod2)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train2) Deviance Residuals: Min 1Q Median 3Q Max -3.6321 -0.7964 0.0083 0.8536 1.7143 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -4.14627 1.43969 -2.880 0.00398 ** GenHydro -3.88698 1.23196 -3.155 0.00160 ** GenSolar 951.59397 322.86360 2.947 0.00321 ** CumlFinancial 0.02077 0.01008 2.060 0.03939 * CumlRegulatory 0.17315 0.06541 2.647 0.00812 ** Total.salary 0.08472 0.05576 1.519 0.12869 Import -0.72189 0.40427 -1.786 0.07416 . --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 250.25 on 180 degrees of freedom Residual deviance: 173.65 on 174 degrees of freedom AIC: 187.65 Number of Fisher Scoring iterations: 7 summary(mod1)  Call: glm(formula = GenSolarBinary ~ GenHydro + GenSolar + CumlFinancial + CumlRegulatory + Total.salary + Import, family = binomial, data = train1) Deviance Residuals: Min 1Q Median 3Q Max -1.8654 -0.2715 -0.1692 -0.1194 2.8802 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -7.574e+00 1.945e+00 -3.893 9.89e-05 *** GenHydro -1.201e+00 2.082e+00 -0.577 0.56398 GenSolar 2.324e+04 1.104e+04 2.106 0.03524 * CumlFinancial 6.836e-02 2.159e-02 3.166 0.00154 ** CumlRegulatory 1.589e-01 1.145e-01 1.388 0.16521 Total.salary 1.549e-01 9.190e-02 1.686 0.09188 . Import -3.323e-01 9.628e-01 -0.345 0.72995 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 131.538 on 307 degrees of freedom Residual deviance: 90.376 on 301 degrees of freedom AIC: 104.38 Number of Fisher Scoring iterations: 7 - Unlike mod1, the number of regulatory policies is more predictive than the number of financial incentives in mod2   Problem 11 - Evaluating the Model Obtained Using the Second Cluster Using the threshold of 0.5, what is the accuracy on test2, the subset of test corresponding to the second cluster?\npredTest2 \u0026lt;- predict(mod2, newdata = test2, type = \u0026quot;response\u0026quot;) table(test2$GenSolarBinary, predTest2 \u0026gt; 0.5)  FALSE TRUE 0 40 6 1 14 20 (40 + 20) / nrow(test2) [1] 0.75 We would like to know if mod2 gives us an edge over mod on the dataset test2.\nUsing mod, predicting GenSolarBinary for the observation in test2 and report the accuracy below:\npredTest2WithMod \u0026lt;- predict(mod, newdata = test2, type = \u0026quot;response\u0026quot;) table(test2$GenSolarBinary, predTest2WithMod \u0026gt; 0.5)  FALSE TRUE 0 39 7 1 16 18 (39 + 18) / nrow(test2) [1] 0.7125  Problem 12 - Evaluating the Performance of the Cluster-the-Predict Algorithm To compute the overall test-set accuracy of the cluster-the-predict approach, I can combine all the test-set predictions into a single vector “AllPredictions” and all the true outcomes into a single vector “AllOutcomes”.\nAllPredictions \u0026lt;- c(predTest1, predTest2) AllOutcomes \u0026lt;- c(test1$GenSolarBinary, test2$GenSolarBinary) What is the overall accuracy on the test-set, using the cluster-then-predict approach, again using a threshold of 0.5?\ntable(AllOutcomes, AllPredictions \u0026gt; 0.5)  AllOutcomes FALSE TRUE 0 154 7 1 25 24 length(AllOutcomes) [1] 210 length(AllPredictions) [1] 210 (154 + 24) / length(AllOutcomes) [1] 0.847619  ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"60264bdb1439568f19dea01c11783261","permalink":"/project/energy_patterns/energy_patterns/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/energy_patterns/energy_patterns/","section":"project","summary":"Exploring renewable energies","tags":["R","Data Analytics","Machine Learning"],"title":"Patterns In Renewable Energy Generation","type":"project"},{"authors":null,"categories":null,"content":" Clustering can be used for market segmentation, the idea of dividing airline passengers into small, more similar groups, and then designing a marketing strategy specifically for each group. In this analysis, I’ll see how this idea can be applied to retail consumer data.\nI’ll use the dataset Households.csv.xz, which contains data collected over two years for a group of 2,500 households. Each row (observation) in our dataset represents a unique household. The dataset contains the following variables:\n NumVisits = the number of times the household visited the retailer AvgProdCount = the average number of products purchased per transaction AvgDiscount = the average discount per transaction from coupon usage (in %) - NOTE: Do not divide this value by 100! AvgSalesValue = the average sales value per transaction MorningPct = the percentage of visits in the morning (8am - 1:59pm) AfternoonPct = the percentage of visits in the afternoon (2pm - 7:59pm)  Note that some visits can occur outside of morning and afternoon hours. That is, visits from 8pm - 7:59am are possible.\nThis dataset was derived from source files provided by dunnhumby, a customer science company based in the UK.\nProblem 1 - Reading in the data Read the dataset Households.csv.xz into R.\nHouseHolds \u0026lt;- read.csv(\u0026quot;Households.csv.xz\u0026quot;) str(HouseHolds) \u0026#39;data.frame\u0026#39;: 2500 obs. of 6 variables: $ NumVisits : int 86 45 47 30 40 250 59 113 20 9 ... $ AvgProdCount : num 20.08 15.87 19.62 10.03 5.55 ... $ AvgDiscount : num 8.11 7.44 14.37 3.85 2.96 ... $ AvgSalesValue: num 50.4 43.4 56.5 40 19.5 ... $ MorningPct : num 46.51 8.89 14.89 13.33 2.5 ... $ AfternoonPct : num 51.2 60 76.6 56.7 67.5 ... summary(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. : 1.0 Min. : 1.186 Min. : 0.089 Min. : 2.388 1st Qu.: 39.0 1st Qu.: 6.123 1st Qu.: 3.006 1st Qu.: 18.329 Median : 79.0 Median : 8.979 Median : 4.865 Median : 27.417 Mean : 110.6 Mean :10.291 Mean : 5.713 Mean : 31.621 3rd Qu.: 142.2 3rd Qu.:13.116 3rd Qu.: 7.327 3rd Qu.: 40.546 Max. :1300.0 Max. :56.600 Max. :47.176 Max. :165.829 MorningPct AfternoonPct Min. : 0.00 Min. : 0.00 1st Qu.: 16.67 1st Qu.: 42.20 Median : 26.09 Median : 52.00 Mean : 28.73 Mean : 51.45 3rd Qu.: 37.17 3rd Qu.: 61.29 Max. :100.00 Max. :100.00  head(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct AfternoonPct 1 86 20.08140 8.105116 50.35070 46.511628 51.16279 2 45 15.86667 7.444222 43.42978 8.888889 60.00000 3 47 19.61702 14.365106 56.45128 14.893617 76.59574 4 30 10.03333 3.855000 40.00367 13.333333 56.66667 5 40 5.55000 2.958250 19.47650 2.500000 67.50000 6 250 7.16400 3.313360 23.98464 25.600000 61.20000 How many households have logged transactions at the retailer only in the morning?\nnrow(subset(HouseHolds, MorningPct == 100)) [1] 4 How many households have logged transactions at the retailer only in the afternoon?\nnrow(subset(HouseHolds, AfternoonPct == 100)) [1] 13  Problem 2 - Descriptive statistics Of the households that spend more than $150 per transaction on average, what is the minimum average discount per transaction?\nmin(subset(HouseHolds, AvgSalesValue \u0026gt; 150)$AvgDiscount) [1] 15.64607 Of the households who have an average discount per transaction greater than 25%, what is the minimum average sales value per transaction?\nmin(subset(HouseHolds, AvgDiscount \u0026gt; 25)$AvgSalesValue) [1] 50.1175 What proportion of households visited the retailer at least 300 times?\nnrow(subset(HouseHolds, NumVisits \u0026gt;= 300)) / nrow(HouseHolds) [1] 0.0592  Problem 3 - Importance of Normalizing When clustering data, its often important to normalize the variables so that they are all on the same scale.\nIf you clustered this dataset without normalizing, which variable would you expect to dominate in the distance calculations?\nsummary(HouseHolds)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. : 1.0 Min. : 1.186 Min. : 0.089 Min. : 2.388 1st Qu.: 39.0 1st Qu.: 6.123 1st Qu.: 3.006 1st Qu.: 18.329 Median : 79.0 Median : 8.979 Median : 4.865 Median : 27.417 Mean : 110.6 Mean :10.291 Mean : 5.713 Mean : 31.621 3rd Qu.: 142.2 3rd Qu.:13.116 3rd Qu.: 7.327 3rd Qu.: 40.546 Max. :1300.0 Max. :56.600 Max. :47.176 Max. :165.829 MorningPct AfternoonPct Min. : 0.00 Min. : 0.00 1st Qu.: 16.67 1st Qu.: 42.20 Median : 26.09 Median : 52.00 Mean : 28.73 Mean : 51.45 3rd Qu.: 37.17 3rd Qu.: 61.29 Max. :100.00 Max. :100.00  NumVisits   Problem 4 - Normalizing the Data Normalize all of the variables in the dataset (Note that these codes assume that our dataset is called “Households”, and create the normalized dataset “HouseholdsNorm”. You can change the names to anything you want by editing the codes.)\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 preproc \u0026lt;- preProcess(HouseHolds) HouseHoldsNorm \u0026lt;- predict(preproc, HouseHolds) (Remember that for each variable, the normalization process subtracts the mean and divides by the standard deviation. In our normalized dataset, all of the variables should have mean 0 and standard deviation 1.\nsummary(HouseHoldsNorm)  NumVisits AvgProdCount AvgDiscount AvgSalesValue Min. :-0.9475 Min. :-1.5239 Min. :-1.4010 Min. :-1.5342 1st Qu.:-0.6190 1st Qu.:-0.6976 1st Qu.:-0.6743 1st Qu.:-0.6976 Median :-0.2731 Median :-0.2197 Median :-0.2112 Median :-0.2206 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 3rd Qu.: 0.2737 3rd Qu.: 0.4728 3rd Qu.: 0.4021 3rd Qu.: 0.4684 Max. :10.2828 Max. : 7.7500 Max. :10.3293 Max. : 7.0432 MorningPct AfternoonPct Min. :-1.6779 Min. :-3.22843 1st Qu.:-0.7047 1st Qu.:-0.58006 Median :-0.1546 Median : 0.03458 Mean : 0.0000 Mean : 0.00000 3rd Qu.: 0.4926 3rd Qu.: 0.61755 Max. : 4.1617 Max. : 3.04658  What is the maximum value of NumVisits in the normalized dataset? #### 10.2828\nWhat is the minimum value of AfternoonPct in the normalized dataset? #### -3.22843\nCreate a dendrogram of our data:\nset.seed(200) distances \u0026lt;- dist(HouseHoldsNorm, method = \u0026quot;euclidean\u0026quot;) ClusterShoppers \u0026lt;- hclust(distances, method = \u0026quot;ward.D\u0026quot;) plot(ClusterShoppers, labels = FALSE)  Problem 5 - Interpreting the Dendrogram Based on the dendrogram, how many clusters do you think would be appropriate for this problem? #### 2, 3, 5\n Problem 6 - K-means Clustering Run the k-means clustering algorithm on our normalized dataset, selecting 10 clusters. Right before using the kmeans function, Remember “set.seed(200)”.\nset.seed(200) k \u0026lt;- 10 KMC \u0026lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000) str(KMC) List of 9 $ cluster : int [1:2500] 7 3 1 3 5 6 1 1 3 8 ... $ centers : num [1:10, 1:6] -0.248 -0.483 -0.234 -0.18 -0.246 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:10] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:6] \u0026quot;NumVisits\u0026quot; \u0026quot;AvgProdCount\u0026quot; \u0026quot;AvgDiscount\u0026quot; \u0026quot;AvgSalesValue\u0026quot; ... $ totss : num 14994 $ withinss : num [1:10] 628 449 700 282 580 ... $ tot.withinss: num 4828 $ betweenss : num 10166 $ size : int [1:10] 246 51 490 118 504 226 141 284 52 388 $ iter : int 5 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; kmeansGroups \u0026lt;- KMC$cluster table(kmeansGroups) kmeansGroups 1 2 3 4 5 6 7 8 9 10 246 51 490 118 504 226 141 284 52 388  How many observations are in the smallest cluster?\nmin(table(kmeansGroups)) [1] 51 51 How many observations are in the largest cluster?\nmax(table(kmeansGroups)) [1] 504  504   Problem 7 - Understanding the Clusters Now, use the cluster assignments from k-means clustering together with the cluster centroids to explore the next few questions.\nkmeans1 \u0026lt;- subset(HouseHolds, kmeansGroups == 1) kmeans2 \u0026lt;- subset(HouseHolds, kmeansGroups == 2) kmeans3 \u0026lt;- subset(HouseHolds, kmeansGroups == 3) kmeans4 \u0026lt;- subset(HouseHolds, kmeansGroups == 4) kmeans5 \u0026lt;- subset(HouseHolds, kmeansGroups == 5) kmeans6 \u0026lt;- subset(HouseHolds, kmeansGroups == 6) kmeans7 \u0026lt;- subset(HouseHolds, kmeansGroups == 7) kmeans8 \u0026lt;- subset(HouseHolds, kmeansGroups == 8) kmeans9 \u0026lt;- subset(HouseHolds, kmeansGroups == 9) kmeans10 \u0026lt;- subset(HouseHolds, kmeansGroups == 10) colMeans(kmeans1)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 81.89431 19.11594 10.92924 59.49868 22.76746 AfternoonPct 61.93939  colMeans(kmeans2)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 54.70588 32.62351 19.65784 99.73684 32.15593 AfternoonPct 49.41508  colMeans(kmeans3)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 83.508163 12.081068 6.881078 37.391552 25.609449 AfternoonPct 51.185788  colMeans(kmeans4)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 89.788136 7.053082 3.877403 21.175564 71.391580 AfternoonPct 22.584436  colMeans(kmeans5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 82.18254 5.89608 2.90764 17.51880 19.36659 AfternoonPct 55.03936  colMeans(kmeans6)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 281.796460 8.117065 4.297144 25.446228 29.851517 AfternoonPct 51.583481  colMeans(kmeans7)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 99.886525 15.469831 9.343551 50.447122 53.462000 AfternoonPct 35.836861  colMeans(kmeans8)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 80.288732 9.992614 5.288399 29.327693 13.350751 AfternoonPct 74.066827  colMeans(kmeans9)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 626.903846 5.203533 2.632325 16.278150 24.641085 AfternoonPct 48.731981  colMeans(kmeans10)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 70.729381 6.479314 3.527893 19.688575 37.377204 AfternoonPct 38.916046  Which cluster best fits the description “morning shoppers stopping in to make a quick purchase”? #### Cluster 4\n Problem 8 - Understanding the Clusters Which cluster best fits the description “shoppers with high average product count and high average value per visit”? #### Cluster 2\n Problem 9 - Understanding the Clusters Which cluster best fits the description “frequent shoppers with low value per visit”? #### Cluster 9\n Problem 10 - Random Behavior If we ran hierarchical clustering a second time without making any additional calls to set.seed, we would expect: #### Identical results to the first hierarchical clustering\nIf we ran k-means clustering a second time without making any additional calls to set.seed, we would expect: #### Different results from the first k-means clustering\nIf we ran k-means clustering a second time, again running the code set.seed(200) right before doing the clustering, we would expect: #### Identical results to the first k-means clustering\nIf we ran k-means clustering a second time, running the code set.seed(100) right before doing the clustering, we would expect: #### Different results from the first k-means clustering\n Problem 11 - The Number of Clusters Suppose the marketing department at the retail store decided that the 10 clusters were too specific, and they wanted more general clusters to describe the consumer base.\nWould they want to increase or decrease the number of clusters? #### Decrease the number of clusters\n Problem 12 - Increasing the Number of Clusters Run the k-means clustering algorithm again, this time selecting 5 clusters. Right before the “kmeans” function, set the random seed to 5000.\nset.seed(5000) k \u0026lt;- 5 KMC.5 \u0026lt;- kmeans(HouseHoldsNorm, centers = k, iter.max = 1000) str(KMC.5) List of 9 $ cluster : int [1:2500] 5 5 1 5 2 2 5 5 5 2 ... $ centers : num [1:5, 1:6] -0.398 -0.193 -0.169 2.695 -0.176 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:5] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:6] \u0026quot;NumVisits\u0026quot; \u0026quot;AvgProdCount\u0026quot; \u0026quot;AvgDiscount\u0026quot; \u0026quot;AvgSalesValue\u0026quot; ... $ totss : num 14994 $ withinss : num [1:5] 1264 1838 1336 754 1604 $ tot.withinss: num 6795 $ betweenss : num 8199 $ size : int [1:5] 182 994 428 172 724 $ iter : int 7 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; kmeansGroups.5 \u0026lt;- KMC.5$cluster table(kmeansGroups.5) kmeansGroups.5 1 2 3 4 5 182 994 428 172 724  How many observations are in the smallest cluster?\nmin(table(kmeansGroups.5)) [1] 172 172 How many observations are in the largest cluster?\nmax(table(kmeansGroups.5)) [1] 994  994   Problem 13 - Describing the Clusters Use the cluster assignments from k-means clustering with 5 clusters, which cluster best fits the description “frequent shoppers with low value per visit”?\nkmeans1.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 1) kmeans2.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 2) kmeans3.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 3) kmeans4.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 4) kmeans5.5 \u0026lt;- subset(HouseHolds, kmeansGroups.5 == 5) colMeans(kmeans1.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 64.60989 24.47666 14.69849 76.09221 31.49236 AfternoonPct 53.37519  colMeans(kmeans2.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 88.274648 6.573304 3.355126 19.679329 21.516913 AfternoonPct 53.900113  colMeans(kmeans3.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 91.095794 8.541218 4.910884 26.956499 54.594773 AfternoonPct 32.958057  colMeans(kmeans4.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 422.273256 7.074807 3.710112 22.317813 27.386172 AfternoonPct 51.043708  colMeans(kmeans5.5)  NumVisits AvgProdCount AvgDiscount AvgSalesValue MorningPct 90.276243 13.628410 7.640804 41.803608 22.980428 AfternoonPct 58.626883  Cluster 4   Problem 14 - Understanding Centroids Why do we typically use cluster centroids to describe the clusters? #### The cluster centroid captures the average behavior in the cluster, and can be used to summarize the general pattern in the cluster.\n Problem 15 - Using a Visualization Which of the following visualizations could be used to observe the distribution of NumVisits, broken down by cluster? #### - A box plot of the variable NumVisits, subdivided by cluster #### - ggplot with NumVisits on the x-axis and the cluster number on the y-axis, plotting with geom_point()\n ","date":1554940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554940800,"objectID":"4f02b10dc027954958aa0c2b14e68c0f","permalink":"/project/retail_consumers/retail_consumers/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/project/retail_consumers/retail_consumers/","section":"project","summary":"Understanding consumer behaviour","tags":["R","Data Analytics","Machine Learning"],"title":"Understanding Retail Consumers Behaviour","type":"project"},{"authors":null,"categories":null,"content":" The cliche goes that the world is an increasingly interconnected place, and the connections between different entities are often best represented with a graph. Graphs are comprised of vertices (also often called “nodes”) and edges connecting those nodes. In this analysis, I’ll explore how to visualize networks using the igraph package in R.\nI’ll visualize social networking data using anonymized data from Facebook; this data was originally curated in a recent paper about computing social circles in social networks. In our visualizations, the vertices in our network will represent Facebook users and the edges will represent these users being Facebook friends with each other.\nThe first file I’ll use, edges.csv, contains variables V1 and V2, which label the endpoints of edges in our network. Each row represents a pair of users in our graph who are Facebook friends. For a pair of friends A and B, edges.csv will only contain a single row – the smaller identifier will be listed first in this row. From this row, I’ll know that A is friends with B and B is friends with A.\nThe second file, users.csv, contains information about the Facebook users, who are the vertices in our network. This file contains the following variables:\n id: A unique identifier for this user; this is the value that appears in the rows of edges.csv gender: An identifier for the gender of a user taking the values A and B. Because the data is anonymized, we don’t know which value refers to males and which value refers to females. school: An identifier for the school the user attended taking the values A and AB (users with AB attended school A as well as another school B). Because the data is anonymized, we don’t know the schools represented by A and B. locale: An identifier for the locale of the user taking the values A and B. Because the data is anonymized, we don’t know which value refers to what locale.  Problem 1.1 - Summarizing the Data Load the data from edges.csv into a dataframe called edges, and load the data from users.csv into a dataframe called users.\nedges \u0026lt;- read.csv(\u0026quot;edges.csv\u0026quot;) users \u0026lt;- read.csv(\u0026quot;users.csv\u0026quot;) How many Facebook users are there in our dataset?\nstr(users) \u0026#39;data.frame\u0026#39;: 59 obs. of 4 variables: $ id : int 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 ... $ gender: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 2 3 3 3 3 3 2 3 3 2 ... $ school: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;AB\u0026quot;: 2 1 1 1 1 2 1 1 2 1 ... $ locale: Factor w/ 3 levels \u0026quot;\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 3 3 3 3 3 3 2 3 3 2 ... In our dataset, what is the average number of friends per user? Hint: this question is tricky, and it might help to start by thinking about a small example with two users who are friends.\nhead(edges)  V1 V2 1 4019 4026 2 4023 4031 3 4023 4030 4 4027 4032 5 3988 4021 6 3982 3986 edges[1,] # users 4019 and 4026 are friends  V1 V2 1 4019 4026 str(subset(edges, V1 == 4019)) # user 4019 has 2 connections as V1 \u0026#39;data.frame\u0026#39;: 2 obs. of 2 variables: $ V1: int 4019 4019 $ V2: int 4026 4030 str(subset(edges, V2 == 4019)) # user 4019 has 5 connections as V2 \u0026#39;data.frame\u0026#39;: 5 obs. of 2 variables: $ V1: int 3997 3994 3998 4009 3981 $ V2: int 4019 4019 4019 4019 4019 str(subset(edges, V1 == 4026)) # user 4026 has 1 connections as V1 \u0026#39;data.frame\u0026#39;: 1 obs. of 2 variables: $ V1: int 4026 $ V2: int 4030 str(subset(edges, V2 == 4026)) # user 4026 has 7 connections as V2 \u0026#39;data.frame\u0026#39;: 7 obs. of 2 variables: $ V1: int 4019 4000 3995 4017 3986 3982 4021 $ V2: int 4026 4026 4026 4026 4026 4026 4026 edges2 \u0026lt;- edges edges2$PK \u0026lt;- row.names(edges2) edges2  V1 V2 PK 1 4019 4026 1 2 4023 4031 2 3 4023 4030 3 4 4027 4032 4 5 3988 4021 5 6 3982 3986 6 7 3994 3998 7 8 3998 3999 8 9 3993 3995 9 10 3982 4021 10 11 3982 4037 11 12 3997 4019 12 13 3994 4019 13 14 3992 4017 14 15 3981 3998 15 16 3997 4018 16 17 4009 4030 17 18 3994 4018 18 19 3995 4000 19 20 4000 4026 20 21 4027 4038 21 22 4031 4038 22 23 4000 4021 23 24 3986 4030 24 25 3985 4014 25 26 3994 4030 26 27 3998 4021 27 28 3994 4009 28 29 3982 4023 29 30 3998 4019 30 31 4020 4031 31 32 4009 4023 32 33 3994 3997 33 34 3981 4023 34 35 3997 4030 35 36 3997 4021 36 37 4023 4034 37 38 3993 4004 38 39 3994 3996 39 40 4000 4030 40 41 3998 4014 41 42 4004 4013 42 43 4016 4025 43 44 3990 4016 44 45 3999 4005 45 46 4004 4023 46 47 4002 4020 47 48 3998 4018 48 49 3985 3995 49 50 3989 3991 50 51 4000 4017 51 52 4003 4009 52 53 3982 4030 53 54 3982 3994 54 55 3998 4005 55 56 3995 4014 56 57 4021 4030 57 58 594 4011 58 59 3993 4030 59 60 4020 4030 60 61 3989 4038 61 62 3989 4011 62 63 4009 4019 63 64 4004 4020 64 65 3995 4026 65 66 4017 4026 66 67 3989 4013 67 68 4020 4037 68 69 3998 4002 69 70 3995 4023 70 71 3983 4017 71 72 3999 4036 72 73 3982 3997 73 74 3990 4007 74 75 3985 3988 75 76 4018 4030 76 77 4026 4030 77 78 3997 4023 78 79 3996 4028 79 80 3982 3988 80 81 3988 4030 81 82 4013 4023 82 83 4014 4021 83 84 4014 4037 84 85 3986 4021 85 86 4017 4021 86 87 3982 4009 87 88 3998 4023 88 89 3998 4009 89 90 594 3989 90 91 3992 4000 91 92 4011 4031 92 93 4019 4030 93 94 4020 4038 94 95 3997 3998 95 96 4023 4038 96 97 4004 4031 97 98 4027 4031 98 99 4014 4038 99 100 3986 4000 100 101 3982 4003 101 102 3986 4033 102 103 3981 3994 103 104 4004 4038 104 105 3985 3993 105 106 4000 4033 106 107 4013 4038 107 108 4018 4023 108 109 4003 4030 109 110 3990 4025 110 111 3986 4026 111 112 3996 4002 112 113 4001 4029 113 114 4014 4030 114 115 4020 4027 115 116 3982 3998 116 117 3988 3993 117 118 4002 4031 118 119 3988 3995 119 120 3986 4014 120 121 4003 4023 121 122 3981 4019 122 123 3997 4009 123 124 4014 4023 124 125 4004 4030 125 126 4006 4027 126 127 594 4031 127 128 4007 4025 128 129 3981 4018 129 130 3981 3997 130 131 3982 4026 131 132 4014 4017 132 133 3991 4031 133 134 3987 4012 134 135 4007 4016 135 136 3995 4004 136 137 4017 4030 137 138 4002 4023 138 139 3994 4023 139 140 3982 4014 140 141 3981 4009 141 142 4021 4026 142 143 4013 4031 143 144 3986 4017 144 145 4002 4027 145 146 3985 4004 146  Problem 1.2 - Summarizing the Data Out of all the students who listed a school, what was the most common locale?\nsummary(users)  id gender school locale Min. : 594 : 2 :40 : 3 1st Qu.:3994 A:15 A :17 A: 6 Median :4009 B:42 AB: 2 B:50 Mean :3952 3rd Qu.:4024 Max. :4038  table(users$school, users$locale)  A B 3 6 31 A 0 0 17 AB 0 0 2 Locale B   Problem 1.3 - Summarizing the Data Is it possible that either school A or B is an all-girls or all-boys school?\ntable(users$gender, users$school)  A AB 1 1 0 A 11 3 1 B 28 13 1 No   Problem 2.1 - Creating a Network We can create a new graph object using the graph.data.frame() function. Based on ?graph.data.frame, using the following code we will create a graph g describing our social network, with the attributes of each user correctly loaded.\n?graph.data.frame g \u0026lt;- graph.data.frame(edges, FALSE, users) g IGRAPH 097ec57 UN-- 59 146 -- + attr: name (v/c), gender (v/c), school (v/c), locale (v/c) + edges from 097ec57 (vertex names): [1] 4019--4026 4023--4031 4023--4030 4027--4032 3988--4021 3982--3986 [7] 3994--3998 3998--3999 3993--3995 3982--4021 3982--4037 3997--4019 [13] 3994--4019 3992--4017 3981--3998 3997--4018 4009--4030 3994--4018 [19] 3995--4000 4000--4026 4027--4038 4031--4038 4000--4021 3986--4030 [25] 3985--4014 3994--4030 3998--4021 3994--4009 3982--4023 3998--4019 [31] 4020--4031 4009--4023 3994--3997 3981--4023 3997--4030 3997--4021 [37] 4023--4034 3993--4004 3994--3996 4000--4030 3998--4014 4004--4013 [43] 4016--4025 3990--4016 3999--4005 4004--4023 4002--4020 3998--4018 + ... omitted several edges Note: A directed graph is one where the edges only go one way – they point from one vertex to another. The other option is an undirected graph, which means that the relations between the vertices are symmetric.\nNow, we want to plot our graph. By default, the vertices are large and have text labels of a user’s identifier, this would clutter the output.\nWe will plot with no text labels and smaller vertices:\nplot(g, vertex.size=5, vertex.label=NA) In this graph, there are a number of groups of nodes where all the nodes in each group are connected but, the groups are disjoint from one another, forming “islands” in the graph. Such groups are called “connected components,” or “components” for short.\nHow many connected components with at least 2 nodes are there in the graph? #### 4\nHow many users are there with no friends in the network? #### 7\n Problem 2.3 - Creating a Network In our graph, the “degree” of a node is its number of friends. We have already seen that some nodes in our graph have degree 0 (these are the nodes with no friends), while others have much higher degree. We can use degree(g) to compute the degree of all the nodes in our graph g.\ndegree(g) 3981 3982 3983 3984 3985 3986 3987 3988 3989 3990 3991 3992 3993 3994 3995 7 13 1 0 5 8 1 6 5 3 2 2 5 10 8 594 3996 3997 3998 3999 4000 4001 4002 4003 4004 4005 4006 4007 4008 4009 3 3 10 13 3 8 1 6 4 9 2 1 3 0 9 4010 4011 4012 4013 4014 4015 4016 4017 4018 4019 4020 4021 4022 4023 4024 0 3 1 5 11 0 3 8 6 7 7 10 0 17 0 4025 4026 4027 4028 4029 4030 4031 4032 4033 4034 4035 4036 4037 4038 3 8 6 1 1 18 10 1 2 1 0 1 3 8  How many users are friends with 10 or more other Facebook users in this network?\nsum(degree(g) \u0026gt;= 10) [1] 9  Problem 2.4 - Creating a Network In a network, it’s often visually useful to draw attention to “important” nodes in the network. While this might mean different things in different contexts, in a social network we might consider a user with a large number of friends to be an important user. From the previous problem, we know this is the same as saying that nodes with a high degree are important users.\nTo visually draw attention to these nodes, we will change the size of the vertices so the vertices with high degrees are larger. To do this, we will change the “size” attribute of the vertices of our graph to be an increasing function of their degrees:\nV(g)$size \u0026lt;- degree(g)/2+2 Now, that we have specified the vertex size of each vertex, we will no longer use the vertex.size parameter when we plot our graph:\nplot(g, vertex.label=NA) What is the largest size we assigned to any node in our graph?\nmax(V(g)$size) [1] 11 What is the smallest size we assigned to any node in our graph?\nmin(V(g)$size) [1] 2  Problem 3.1 - Coloring Vertices Thus far, we have changed the “size” attributes of our vertices. However, we can also change the colors of vertices to capture additional information about the Facebook users we are depicting.\nWhen changing the size of nodes, we first obtained the vertices of our graph with V(g) and then accessed the the size attribute with V(g)\\(size. To change the color, we will update the attribute V(g)\\)color.\nTo color the vertices based on the gender of the user, we will need access to that variable. When we created our graph g, we provided it with the dataframe users, which had variables gender, school, and locale. These are now stored as attributes V(g)\\(gender, V(g)\\)school, and V(g)$locale.\nWe can update the colors by setting the color to black for all vertices, than setting it to red for the vertices with gender A and setting it to gray for the vertices with gender B:\nV(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$gender == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$gender == \u0026quot;B\u0026quot;] = \u0026quot;gray\u0026quot; Ploting the resulting graph.\nWhat is the gender of the users with the highest degree in the graph?\nplot(g, vertex.label=NA) Gender B\n Problem 3.2 - Coloring Vertices Now, color the vertices based on the school that each user in our network attended.\ntable(V(g)$school)  A AB 40 17 2  V(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$school == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$school == \u0026quot;AB\u0026quot;] = \u0026quot;gray\u0026quot; plot(g, vertex.label=NA) Are the two users who attended both schools A and B Facebook friends with each other? #### Yes\nWhat best describes the users with highest degree? #### Some, but not all, of the high-degree users attended school A\n Problem 3.3 - Coloring Vertices Now, color the vertices based on the locale of the user.\ntable(V(g)$locale)  A B 3 6 50  V(g)$color = \u0026quot;black\u0026quot; V(g)$color[V(g)$locale == \u0026quot;A\u0026quot;] = \u0026quot;red\u0026quot; V(g)$color[V(g)$locale == \u0026quot;B\u0026quot;] = \u0026quot;gray\u0026quot; plot(g, vertex.label=NA) The large connected component is most associated with which locale? #### Locale B\nThe 4-user connected component is most associated with which locale? #### Locale A\n Problem 4 - Other Plotting Options The help page is a helpful tool when making visualizations. The following questions with the help of ?igraph.plotting and experimentation in our R console.\n?igraph.plotting Which igraph plotting function would enable us to plot our graph in 3-D?\n?rglplot rglplot\nWhat parameter to the plot() function would we use to change the edge width when plotting g?\n?plot.igraph edge.width\n ","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"53743b1e3bee51610022fc5a424544d2","permalink":"/project/visualizing_network/visualizing_network/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/project/visualizing_network/visualizing_network/","section":"project","summary":"Visualize social networking data from Facebook","tags":["R","Data Analytics","Machine Learning"],"title":"Network Data Visualizing","type":"project"},{"authors":null,"categories":null,"content":" Prevously, we used logistic regression on polling data in order to construct US presidential election predictions. We separated our data into a training set, containing data from 2004 and 2008 polls, and a test-set, containing the data from 2012 polls. We then proceeded to develop a logistic regression model to forecast the 2012 US presidential election.\nIn this problem, we’ll revisit our logistic regression model, and learn how to plot the output on a map of the United States. Unlike what we did prevously, this time we’ll be plotting predictions rather than data!\nFirst, load the ggplot2, maps, and ggmap packages using the library function. All three packages should be installed on your computer from lecture, but if not, you may need to install them too using the install.packages function.\nThen, load the US map and save it to the variable statesMap:\nstatesMap = map_data(“state”)\nThe maps package contains other built-in maps, including a US county map, a world map, and maps for France and Italy.\n# Load states map statesMap = map_data(\u0026quot;state\u0026quot;) Problem 1.1 - Drawing a Map of the US If you look at the structure of the statesMap dataframe using the str function, you should see that there are 6 variables. One of the variables, group, defines the different shapes or polygons on the map. Sometimes a state may have multiple groups, for example, if it includes islands.\nHow many different groups are there?\nstr(statesMap) \u0026#39;data.frame\u0026#39;: 15537 obs. of 6 variables: $ long : num -87.5 -87.5 -87.5 -87.5 -87.6 ... $ lat : num 30.4 30.4 30.4 30.3 30.3 ... $ group : num 1 1 1 1 1 1 1 1 1 1 ... $ order : int 1 2 3 4 5 6 7 8 9 10 ... $ region : chr \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; ... $ subregion: chr NA NA NA NA ... length(table(statesMap$group)) [1] 63 The variable “order” defines the order to connect the points within each group, and the variable “region” gives the name of the state.\n Problem 1.2 - Drawing a Map of the US You can draw a map of the US by typing the following code:\nggplot(statesMap, aes(x = long, y = lat, group = group)) + geom_polygon(fill = \u0026quot;white\u0026quot;, color = \u0026quot;black\u0026quot;) We specified two colors in geom_polygon – fill and color. Which one defined the color of the outline of the states? #### color\n Problem 2.1 - Coloring the States by Predictions Now, let’s color the map of the US according to our 2012 US presidential election predictions from the Unit 3 Recitation. We’ll rebuild the model here, using the dataset PollingImputed.csv. Be sure to use this file so that you don’t have to redo the imputation to fill in the missing values, like we did in the Unit 3 Recitation.\nLoad the data using the read.csv function, and call it “polling”. Then split the data using the subset function into a training set called “Train” that has observations from 2004 and 2008, and a testing set called “Test” that has observations from 2012.\npolling \u0026lt;- read.csv(\u0026quot;PollingImputed.csv\u0026quot;) str(polling) \u0026#39;data.frame\u0026#39;: 145 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 1 1 2 2 3 3 3 4 4 4 ... $ Year : int 2004 2008 2004 2008 2004 2008 2012 2004 2008 2012 ... $ Rasmussen : int 11 21 19 16 5 5 8 7 10 13 ... $ SurveyUSA : int 18 25 21 18 15 3 5 5 7 21 ... $ DiffCount : int 5 5 1 6 8 9 4 8 5 2 ... $ PropR : num 1 1 1 1 1 ... $ Republican: int 1 1 1 1 1 1 1 1 1 1 ... table(polling$Year)  2004 2008 2012 50 50 45  Train \u0026lt;- subset(polling, Year \u0026lt;= 2008) Test \u0026lt;- subset(polling, Year \u0026gt; 2008) str(Train) \u0026#39;data.frame\u0026#39;: 100 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 1 1 2 2 3 3 4 4 5 5 ... $ Year : int 2004 2008 2004 2008 2004 2008 2004 2008 2004 2008 ... $ Rasmussen : int 11 21 19 16 5 5 7 10 -11 -27 ... $ SurveyUSA : int 18 25 21 18 15 3 5 7 -11 -24 ... $ DiffCount : int 5 5 1 6 8 9 8 5 -8 -5 ... $ PropR : num 1 1 1 1 1 1 1 1 0 0 ... $ Republican: int 1 1 1 1 1 1 1 1 0 0 ... str(Test) \u0026#39;data.frame\u0026#39;: 45 obs. of 7 variables: $ State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 3 4 5 6 7 9 10 11 12 13 ... $ Year : int 2012 2012 2012 2012 2012 2012 2012 2012 2012 2012 ... $ Rasmussen : int 8 13 -12 3 -7 2 5 -22 31 -22 ... $ SurveyUSA : int 5 21 -14 -2 -13 0 8 -24 24 -16 ... $ DiffCount : int 4 2 -6 -5 -8 6 4 -2 1 -5 ... $ PropR : num 0.833 1 0 0.308 0 ... $ Republican: int 1 1 0 0 0 0 1 0 1 0 ... Note that we only have 45 states in our testing set, since we are missing observations for Alaska, Delaware, Alabama, Wyoming, and Vermont, so these states will not appear colored in our map.\nThen, create a logistic regression model and make predictions on the test-set using the following code:\nmod2 \u0026lt;- glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = \u0026quot;binomial\u0026quot;) TestPrediction \u0026lt;- predict(mod2, newdata = Test, type = \u0026quot;response\u0026quot;) TestPrediction gives the predicted probabilities for each state, but let’s also create a vector of Republican/Democrat predictions by using the following code:\nTestPredictionBinary \u0026lt;- as.numeric(TestPrediction \u0026gt; 0.5) Now, put the predictions and state labels in a data.frame so that we can use ggplot:\npredictionDataFrame \u0026lt;- data.frame(TestPrediction, TestPredictionBinary, Test$State) To make sure everything went smoothly, answer the following.\nFor how many states is our binary prediction 1 (for 2012), corresponding to Republican?\nstr(TestPredictionBinary)  num [1:45] 1 1 0 0 0 1 1 0 1 0 ... table(TestPredictionBinary) TestPredictionBinary 0 1 23 22  22\nWhat is the average predicted probability of our model (on the Test set, for 2012)?\nmean(TestPrediction) [1] 0.4852626  Problem 2.2 - Coloring the States by Predictions Now, we need to merge “predictionDataFrame” with the map data “statesMap”. Before doing so, we need to convert the Test.State variable to lowercase, so that it matches the region variable in statesMap.\npredictionDataFrame$region \u0026lt;- tolower(predictionDataFrame$Test.State) # Now, merge the two data frames using the following command: predictionMap \u0026lt;- merge(statesMap, predictionDataFrame, by = \u0026quot;region\u0026quot;) # Lastly, we need to make sure the observations are in order so that the map is # drawn properly, by typing the following: predictionMap \u0026lt;- predictionMap[order(predictionMap$order),] How many observations are there in predictionMap?\nstr(predictionMap) \u0026#39;data.frame\u0026#39;: 15034 obs. of 9 variables: $ region : chr \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; \u0026quot;arizona\u0026quot; ... $ long : num -115 -115 -115 -115 -115 ... $ lat : num 35 35.1 35.1 35.2 35.2 ... $ group : num 2 2 2 2 2 2 2 2 2 2 ... $ order : int 204 205 206 207 208 209 210 211 212 213 ... $ subregion : chr NA NA NA NA ... $ TestPrediction : num 0.974 0.974 0.974 0.974 0.974 ... $ TestPredictionBinary: num 1 1 1 1 1 1 1 1 1 1 ... $ Test.State : Factor w/ 50 levels \u0026quot;Alabama\u0026quot;,\u0026quot;Alaska\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... How many observations are there in statesMap?\nstr(statesMap) \u0026#39;data.frame\u0026#39;: 15537 obs. of 6 variables: $ long : num -87.5 -87.5 -87.5 -87.5 -87.6 ... $ lat : num 30.4 30.4 30.4 30.3 30.3 ... $ group : num 1 1 1 1 1 1 1 1 1 1 ... $ order : int 1 2 3 4 5 6 7 8 9 10 ... $ region : chr \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; \u0026quot;alabama\u0026quot; ... $ subregion: chr NA NA NA NA ...  Problem 2.3 - Coloring the States by Predictions When we merged the data in the previous problem, it caused the number of observations to change. Why? Check out the help page for merge by typing ?merge to help us answer this question. #### Because we only make predictions for 45 states, we no longer have observations for some of the states. These observations were removed in the merging process.\n Problem 2.4 - Coloring the States by Predictions Now we are ready to color the US map with our predictions! You can color the states according to our binary predictions by typing the following code:\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = \u0026quot;black\u0026quot;) The states appear light blue and dark blue in this map. Which color represents a Republican prediction? #### Light blue\n Problem 2.5 - Coloring the States by Predictions We see that the legend displays a blue gradient for outcomes between 0 and 1. However, when plotting the binary predictions there are only two possible outcomes: 0 or 1.\nLet’s replot the map with discrete outcomes. We can also change the color scheme to blue and red, to match the blue color associated with the Democratic Party in the US and the red color associated with the Republican Party in the US.\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPredictionBinary)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) Alternatively, we could plot the probabilities instead of the binary predictions. Change the plot code above to instead color the states by the variable TestPrediction. You should see a gradient of colors ranging from red to blue.\nDo the colors of the states in the map for TestPrediction look different from the colors of the states in the map with TestPredictionBinary? Why or why not?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) round(TestPrediction, 2)  7 10 13 16 19 24 27 30 33 36 39 42 45 48 51 0.97 1.00 0.00 0.01 0.00 0.96 0.99 0.00 1.00 0.00 1.00 0.06 0.95 0.99 1.00 54 57 60 63 66 69 72 75 78 81 84 87 90 93 96 0.00 0.00 0.00 0.00 0.00 0.93 1.00 1.00 1.00 0.00 0.00 0.00 0.00 0.00 0.95 99 102 105 108 111 114 117 120 123 126 129 134 137 140 143 1.00 0.00 1.00 0.00 0.00 0.00 1.00 0.99 1.00 1.00 1.00 0.02 0.00 1.00 0.00  The two maps look very similar. This is because most of our predicted probabilities are close to 0 or close to 1. NOTE: If you have a hard time seeing the red/blue gradient, feel free to change the color scheme, by changing the arguments low = “blue” and high = “red” to colors of your choice (to see all of the color options in R, type colors() in your R console). You can even change it to a gray scale, by changing the low and high colors to “gray” and “black”.\n  Problem 3.1 - Understanding the Predictions In the 2012 election, the state of Florida ended up being a very close race. It was ultimately won by the Democratic party. Did we predict this state correctly or incorrectly? To see the names and locations of the different states, take a look at the World Atlas map here. #### We incorrectly predicted this state by predicting that it would be won by the Republican party.\n Problem 3.2 - Understanding the Predictions What was our predicted probability for the state of Florida?\npredictionDataFrame[predictionDataFrame$region == \u0026quot;florida\u0026quot;, ]  TestPrediction TestPredictionBinary Test.State region 24 0.9640395 1 Florida florida What does this imply? #### Our prediction model did not do a very good job of correctly predicting the state of Florida, and we were very confident in our incorrect prediction.\n PROBLEM 4 - PARAMETER SETTINGS In this part, we’ll explore what the different parameter settings of geom_polygon do. Throughout the problem, use the help page for geom_polygon, which can be accessed by ?geom_polygon. To see more information about a certain parameter, just type a question mark and then the parameter name to get the help page for that parameter. Experiment with different parameter settings to try and replicate the plots!\nWe’ll be asking questions about the following three plots:\n Problem 4.1 - Parameter Settings Plots (1) and (2) were created by setting different parameters of geom_polygon to the value 3.\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) ?geom_polygon What is the name of the parameter we set to have value 3 to create plot (1)?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, linetype = 3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) linetype\nWhat is the name of the parameter we set to have value 3 to create plot (2)?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, size = 3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) size\n Problem 4.2 - Parameter Settings Plot (3) was created by changing the value of a different geom_polygon parameter to have value 0.3. Which parameter did we use?\nggplot(predictionMap, aes(x = long, y = lat, group = group, fill = TestPrediction)) + geom_polygon(color = \u0026quot;black\u0026quot;, alpha = 0.3) + scale_fill_gradient(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, guide = \u0026quot;legend\u0026quot;, breaks= c(0,1), labels = c(\u0026quot;Democrat\u0026quot;, \u0026quot;Republican\u0026quot;), name = \u0026quot;Prediction 2012\u0026quot;) alpha\n ","date":1554854400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554854400,"objectID":"b734c087b7c1ea845e4e1ed28554edef","permalink":"/project/election_forecasting/election_forecasting/","publishdate":"2019-04-10T00:00:00Z","relpermalink":"/project/election_forecasting/election_forecasting/","section":"project","summary":"How to plot the output on a map of the United States","tags":["R","Data Analytics","Machine Learning"],"title":"Revisiting Election Forecasting","type":"project"},{"authors":null,"categories":null,"content":" Market segmentation is a strategy that divides a broad target market of customers into smaller, more similar groups, and then designs a marketing strategy specifically for each group. Clustering is a common technique for market segmentation since it automatically finds similar groups given a dataset.\nIn this analysis, I’ll see how clustering can be used to find similar groups of customers who belong to an airline’s frequent flyer program. The airline is trying to learn more about its customers so that it can target different customer segments with different types of mileage offers.\nThe file AirlinesCluster.csv contains information on 3,999 members of the frequent flyer program. This data comes from the textbook Data Mining for Business Intelligence, by Galit Shmueli, Nitin R. Patel, and Peter C. Bruce. For more information, see the website for the book.\nThere are seven different variables in the dataset, described below:\n Balance = number of miles eligible for award travel QualMiles = number of miles qualifying for TopFlight status BonusMiles = number of miles earned from non-flight bonus transactions in the past 12 months BonusTrans = number of non-flight bonus transactions in the past 12 months FlightMiles = number of flight miles in the past 12 months FlightTrans = number of flight transactions in the past 12 months DaysSinceEnroll = number of days since enrolled in the frequent flyer program  Problem 1.1 - Normalizing the Data Read the dataset AirlinesCluster.csv into R and call it “airlines”.\nairlines \u0026lt;- read.csv(\u0026quot;AirlinesCluster.csv\u0026quot;) Looking at the summary of airlines, which TWO variables have (on average) the smallest values?\nsummary(airlines)  Balance QualMiles BonusMiles BonusTrans Min. : 0 Min. : 0.0 Min. : 0 Min. : 0.0 1st Qu.: 18528 1st Qu.: 0.0 1st Qu.: 1250 1st Qu.: 3.0 Median : 43097 Median : 0.0 Median : 7171 Median :12.0 Mean : 73601 Mean : 144.1 Mean : 17145 Mean :11.6 3rd Qu.: 92404 3rd Qu.: 0.0 3rd Qu.: 23800 3rd Qu.:17.0 Max. :1704838 Max. :11148.0 Max. :263685 Max. :86.0 FlightMiles FlightTrans DaysSinceEnroll Min. : 0.0 Min. : 0.000 Min. : 2 1st Qu.: 0.0 1st Qu.: 0.000 1st Qu.:2330 Median : 0.0 Median : 0.000 Median :4096 Mean : 460.1 Mean : 1.374 Mean :4119 3rd Qu.: 311.0 3rd Qu.: 1.000 3rd Qu.:5790 Max. :30817.0 Max. :53.000 Max. :8296  BonusTrans and FlightTrans\n Problem 1.2 - Normalizing the Data In this problem, we will normalize our data before we run the clustering algorithms.\nWhy is it important to normalize the data before clustering? #### If we don’t normalize the data, the clustering will be dominated by the variables that are on a larger scale.\n Problem 1.3 - Normalizing the Data Let’s go ahead and normalize our data. You can normalize the variables in a dataframe by using the preProcess function in the “caret” package.\nNow, create a normalized dataframe called “airlinesNorm” by running the following code:\npreproc \u0026lt;- preProcess(airlines) airlinesNorm \u0026lt;- predict(preproc, airlines) The first code pre-processes the data, and the second code performs the normalization. If you look at the summary of airlinesNorm, you should see that all of the variables now have mean zero. You can also see that each of the variables has standard deviation 1 by using the sd() function.\nsummary(airlinesNorm)  Balance QualMiles BonusMiles BonusTrans Min. :-0.7303 Min. :-0.1863 Min. :-0.7099 Min. :-1.20805 1st Qu.:-0.5465 1st Qu.:-0.1863 1st Qu.:-0.6581 1st Qu.:-0.89568 Median :-0.3027 Median :-0.1863 Median :-0.4130 Median : 0.04145 Mean : 0.0000 Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 3rd Qu.: 0.1866 3rd Qu.:-0.1863 3rd Qu.: 0.2756 3rd Qu.: 0.56208 Max. :16.1868 Max. :14.2231 Max. :10.2083 Max. : 7.74673 FlightMiles FlightTrans DaysSinceEnroll Min. :-0.3286 Min. :-0.36212 Min. :-1.99336 1st Qu.:-0.3286 1st Qu.:-0.36212 1st Qu.:-0.86607 Median :-0.3286 Median :-0.36212 Median :-0.01092 Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 3rd Qu.:-0.1065 3rd Qu.:-0.09849 3rd Qu.: 0.80960 Max. :21.6803 Max. :13.61035 Max. : 2.02284  sd(airlinesNorm$Balance) [1] 1 sd(airlinesNorm$FlightTrans) [1] 1 In the normalized data, which variable has the largest maximum value? #### FlightMiles\nIn the normalized data, which variable has the smallest minimum value? #### DaysSinceEnroll\n Problem 2.1 - Hierarchical Clustering Compute the distances between data points (using euclidean distance) and then run the Hierarchical clustering algorithm (using method=“ward.D”) on the normalized data. It may take a few minutes for the code to finish since the dataset has a large number of observations for hierarchical clustering.\ndistAirlines \u0026lt;- dist(airlinesNorm, method = \u0026quot;euclidean\u0026quot;) hclustAirlines \u0026lt;- hclust(distAirlines, method = \u0026quot;ward.D\u0026quot;) Then, plot the dendrogram of the hierarchical clustering process. Suppose the airline is looking for somewhere between 2 and 10 clusters. According to the dendrogram, which of the following is NOT a good choice for the number of clusters?\nplot(hclustAirlines) 6\n Problem 2.2 - Hierarchical Clustering Suppose that after looking at the dendrogram and discussing with the marketing department, the airline decides to proceed with 5 clusters. Divide the data points into 5 clusters by using the cutree function.\nHow many data points are in Cluster 1?\nhclustGroups \u0026lt;- cutree(hclustAirlines, k = 5) table(hclustGroups) hclustGroups 1 2 3 4 5 776 519 494 868 1342  776\n Problem 2.3 - Hierarchical Clustering Now, use tapply to compare the average values in each of the variables for the 5 clusters (the centroids of the clusters). You may want to compute the average values of the unnormalized data so that it is easier to interpret. You can do this for the variable “Balance”.\nstr(airlines) \u0026#39;data.frame\u0026#39;: 3999 obs. of 7 variables: $ Balance : int 28143 19244 41354 14776 97752 16420 84914 20856 443003 104860 ... $ QualMiles : int 0 0 0 0 0 0 0 0 0 0 ... $ BonusMiles : int 174 215 4123 500 43300 0 27482 5250 1753 28426 ... $ BonusTrans : int 1 2 4 1 26 0 25 4 43 28 ... $ FlightMiles : int 0 0 0 0 2077 0 0 250 3850 1150 ... $ FlightTrans : int 0 0 0 0 4 0 0 1 12 3 ... $ DaysSinceEnroll: int 7000 6968 7034 6952 6935 6942 6994 6938 6948 6931 ... tapply(airlines$Balance, hclustGroups, mean)  1 2 3 4 5 57866.90 110669.27 198191.57 52335.91 36255.91  tapply(airlines$QualMiles, hclustGroups, mean)  1 2 3 4 5 0.6443299 1065.9826590 30.3461538 4.8479263 2.5111773  tapply(airlines$BonusMiles, hclustGroups, mean)  1 2 3 4 5 10360.124 22881.763 55795.860 20788.766 2264.788  tapply(airlines$BonusTrans, hclustGroups, mean)  1 2 3 4 5 10.823454 18.229287 19.663968 17.087558 2.973174  tapply(airlines$FlightMiles, hclustGroups, mean)  1 2 3 4 5 83.18428 2613.41811 327.67611 111.57373 119.32191  tapply(airlines$FlightTrans, hclustGroups, mean)  1 2 3 4 5 0.3028351 7.4026975 1.0688259 0.3444700 0.4388972  tapply(airlines$DaysSinceEnroll, hclustGroups, mean)  1 2 3 4 5 6235.365 4402.414 5615.709 2840.823 3060.081  Compared to the other clusters, Cluster 1 has the largest average values in which variables (if any)? #### DaysSinceEnroll\nHow would you describe the customers in Cluster 1? #### Infrequent but loyal customers.\n Problem 2.4 - Hierarchical Clustering Compared to the other clusters, Cluster 2 has the largest average values in which variables? #### QualMiles, FlightMiles, FlightTrans\nHow would you describe the customers in Cluster 2? #### Customers who have accumulated a large amount of miles, and the ones with the largest number of flight transactions.\n Problem 2.5 - Hierarchical Clustering Compared to the other clusters, Cluster 3 has the largest average values in which variables? #### Balance, BonusMiles, BonusTrans\nHow would you describe the customers in Cluster 3? #### Customers who have accumulated a large amount of miles, mostly through non-flight transactions.\n Problem 2.6 - Hierarchical Clustering Compared to the other clusters, Cluster 4 has the largest average values in which variables? #### None\nHow would you describe the customers in Cluster 4? #### Relatively new customers who seem to be accumulating miles, mostly through non-flight transactions.\n Problem 2.7 - Hierarchical Clustering Compared to the other clusters, Cluster 5 has the largest average values in which variables? #### None\nHow would you describe the customers in Cluster 5? #### Relatively new customers who don’t use the airline very often.\n Problem 3.1 - K-Means Clustering Now run the k-means clustering algorithm on the normalized data, again creating 5 clusters. Set the seed to 88 right before running the clustering algorithm, and set the argument iter.max to 1000.\nset.seed(88) kmeansAirlines \u0026lt;- kmeans(airlinesNorm, centers = 5, iter.max = 1000) str(kmeansAirlines) List of 9 $ cluster : int [1:3999] 4 4 4 4 1 4 3 4 2 3 ... $ centers : num [1:5, 1:7] 1.4444 1.0005 -0.0558 -0.1333 -0.4058 ... ..- attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 .. ..$ : chr [1:5] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; ... .. ..$ : chr [1:7] \u0026quot;Balance\u0026quot; \u0026quot;QualMiles\u0026quot; \u0026quot;BonusMiles\u0026quot; \u0026quot;BonusTrans\u0026quot; ... $ totss : num 27986 $ withinss : num [1:5] 4948 3624 2054 2040 2321 $ tot.withinss: num 14987 $ betweenss : num 12999 $ size : int [1:5] 408 141 993 1182 1275 $ iter : int 4 $ ifault : int 0 - attr(*, \u0026quot;class\u0026quot;)= chr \u0026quot;kmeans\u0026quot; How many clusters have more than 1,000 observations?\ntable(kmeansAirlines$cluster)  1 2 3 4 5 408 141 993 1182 1275  2\n Problem 3.2 - K-Means Clustering Now, compare the cluster centroids to each other either by dividing the data points into groups and then using tapply, or by looking at the output of kmeansClust\\(centers, where \u0026quot;kmeansClust\u0026quot; is the name of the output of the kmeans function. (Note that the output of kmeansClust\\)centers will be for the normalized data. If you want to look at the average values for the unnormalized data, you need to use tapply like we did for hierarchical clustering.)\nkmeansGroups \u0026lt;- kmeansAirlines$cluster tapply(airlines$Balance, kmeansGroups, mean)  1 2 3 4 5 219161.40 174431.51 67977.44 60166.18 32706.67  tapply(airlines$QualMiles, kmeansGroups, mean)  1 2 3 4 5 539.57843 673.16312 34.99396 55.20812 126.46667  tapply(airlines$BonusMiles, kmeansGroups, mean)  1 2 3 4 5 62474.483 31985.085 24490.019 8709.712 3097.478  tapply(airlines$BonusTrans, kmeansGroups, mean)  1 2 3 4 5 21.524510 28.134752 18.429003 8.362098 4.284706  tapply(airlines$FlightMiles, kmeansGroups, mean)  1 2 3 4 5 623.8725 5859.2340 289.4713 203.2589 181.4698  tapply(airlines$FlightTrans, kmeansGroups, mean)  1 2 3 4 5 1.9215686 17.0000000 0.8851964 0.6294416 0.5403922  tapply(airlines$DaysSinceEnroll, kmeansGroups, mean)  1 2 3 4 5 5605.051 4684.901 3416.783 6109.540 2281.055  Do you expect Cluster 1 of the K-Means clustering output to necessarily be similar to Cluster 1 of the Hierarchical clustering output? #### No, because cluster ordering is not meaningful in either k-means clustering or hierarchical clustering.\n ","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"6fc6cd4924828f5b2c77b4d0866cb8fe","permalink":"/project/market_segmentation_for_airlines/market/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/project/market_segmentation_for_airlines/market/","section":"project","summary":"Finding similar groups of travellers who belong to an airline's frequent flyer program","tags":["R","Data Analytics","Machine Learning"],"title":"Market Segmentation For Airlines","type":"project"},{"authors":null,"categories":null,"content":" About cluster-then-predict, a methodology in which you first cluster observations and then build cluster-specific prediction models. In this problem, I’ll use cluster-then-predict to predict future stock prices using historical stock data.\nWhen selecting which stocks to invest in, investors seek to obtain good future returns. In this analysis, I’ll first use clustering to identify clusters of stocks that have similar returns over time. Then, use logistic regression to predict whether or not the stocks will have positive future returns.\nFor this problem, I’ll use StocksCluster.csv, which contains monthly stock returns from the NASDAQ stock exchange. The NASDAQ is the second-largest stock exchange in the world, and it lists many tech companies. The stock price data used in this analysis was obtained from infochimps, a website providing access to many datasets.\nEach observation in the dataset is the monthly returns of a particular company in a particular year. The years included are 2000-2009. The companies are limited to tickers that were listed on the exchange for the entire period 2000-2009, and whose stock price never fell below $1. So, for example, one observation is for Yahoo in 2000, and another observation is for Yahoo in 2001. Our goal will be to predict whether or not the stock return in December will be positive, using the stock returns for the first 11 months of the year.\nThis dataset contains the following variables:\n ReturnJan = the return for the company’s stock during January (in the year of the observation). ReturnFeb = the return for the company’s stock during February (in the year of the observation). ReturnMar = the return for the company’s stock during March (in the year of the observation). ReturnApr = the return for the company’s stock during April (in the year of the observation). ReturnMay = the return for the company’s stock during May (in the year of the observation). ReturnJune = the return for the company’s stock during June (in the year of the observation). ReturnJuly = the return for the company’s stock during July (in the year of the observation). ReturnAug = the return for the company’s stock during August (in the year of the observation). ReturnSep = the return for the company’s stock during September (in the year of the observation). ReturnOct = the return for the company’s stock during October (in the year of the observation). ReturnNov = the return for the company’s stock during November (in the year of the observation). PositiveDec = whether or not the company’s stock had a positive return in December (in the year of the observation). This variable takes value 1 if the return was positive, and value 0 if the return was not positive.  For the first 11 variables, the value stored is a proportional change in stock value during that month. For instance, a value of 0.05 means the stock increased in value 5% during the month, while a value of -0.02 means the stock decreased in value 2% during the month.\nProblem 1.1 - Exploring the Dataset Load StocksCluster.csv into a dataframe called “stocks”.\nHow many observations are in the dataset?\nstocks \u0026lt;- read.csv(\u0026quot;StocksCluster.csv\u0026quot;) str(stocks) \u0026#39;data.frame\u0026#39;: 11580 obs. of 12 variables: $ ReturnJan : num 0.0807 -0.0107 0.0477 -0.074 -0.031 ... $ ReturnFeb : num 0.0663 0.1021 0.036 -0.0482 -0.2127 ... $ ReturnMar : num 0.0329 0.1455 0.0397 0.0182 0.0915 ... $ ReturnApr : num 0.1831 -0.0844 -0.1624 -0.0247 0.1893 ... $ ReturnMay : num 0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ... $ ReturnJune : num -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ... $ ReturnJuly : num -0.0205 -0.0253 -0.1354 -0.094 0.3553 ... $ ReturnAug : num 0.0247 0.2113 0.0334 0.0953 0.0568 ... $ ReturnSep : num -0.0204 -0.58 0 0.0567 0.0336 ... $ ReturnOct : num -0.1733 -0.2671 0.0917 -0.0963 0.0363 ... $ ReturnNov : num -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ... $ PositiveDec: int 0 0 0 1 1 1 1 0 0 0 ... 11580\n Problem 1.2 - Exploring the Dataset What proportion of the observations have positive returns in December?\ntable(stocks$PositiveDec)  0 1 5256 6324  6324 / (5256 + 6324) [1] 0.546114  Problem 1.3 - Exploring the Dataset What is the maximum correlation between any two return variables in the dataset? You should look at the pairwise correlations between ReturnJan, ReturnFeb, ReturnMar, ReturnApr, ReturnMay, ReturnJune, ReturnJuly, ReturnAug, ReturnSep, ReturnOct, and ReturnNov.\ncor(stocks[, 1:11])  ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay ReturnJan 1.00000000 0.06677458 -0.090496798 -0.037678006 -0.044411417 ReturnFeb 0.06677458 1.00000000 -0.155983263 -0.191351924 -0.095520920 ReturnMar -0.09049680 -0.15598326 1.000000000 0.009726288 -0.003892789 ReturnApr -0.03767801 -0.19135192 0.009726288 1.000000000 0.063822504 ReturnMay -0.04441142 -0.09552092 -0.003892789 0.063822504 1.000000000 ReturnJune 0.09223831 0.16999448 -0.085905486 -0.011027752 -0.021074539 ReturnJuly -0.08142976 -0.06177851 0.003374160 0.080631932 0.090850264 ReturnAug -0.02279202 0.13155979 -0.022005400 -0.051756051 -0.033125658 ReturnSep -0.02643715 0.04350177 0.076518327 -0.028920972 0.021962862 ReturnOct 0.14297723 -0.08732427 -0.011923758 0.048540025 0.017166728 ReturnNov 0.06763233 -0.15465828 0.037323535 0.031761837 0.048046590 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnJan 0.09223831 -0.0814297650 -0.0227920187 -0.0264371526 ReturnFeb 0.16999448 -0.0617785094 0.1315597863 0.0435017706 ReturnMar -0.08590549 0.0033741597 -0.0220053995 0.0765183267 ReturnApr -0.01102775 0.0806319317 -0.0517560510 -0.0289209718 ReturnMay -0.02107454 0.0908502642 -0.0331256580 0.0219628623 ReturnJune 1.00000000 -0.0291525996 0.0107105260 0.0447472692 ReturnJuly -0.02915260 1.0000000000 0.0007137558 0.0689478037 ReturnAug 0.01071053 0.0007137558 1.0000000000 0.0007407139 ReturnSep 0.04474727 0.0689478037 0.0007407139 1.0000000000 ReturnOct -0.02263599 -0.0547089088 -0.0755945614 -0.0580792362 ReturnNov -0.06527054 -0.0483738369 -0.1164890345 -0.0197197998 ReturnOct ReturnNov ReturnJan 0.14297723 0.06763233 ReturnFeb -0.08732427 -0.15465828 ReturnMar -0.01192376 0.03732353 ReturnApr 0.04854003 0.03176184 ReturnMay 0.01716673 0.04804659 ReturnJune -0.02263599 -0.06527054 ReturnJuly -0.05470891 -0.04837384 ReturnAug -0.07559456 -0.11648903 ReturnSep -0.05807924 -0.01971980 ReturnOct 1.00000000 0.19167279 ReturnNov 0.19167279 1.00000000 ReturnOct vs ReturnNov = 0.19167279\n Problem 1.4 - Exploring the Dataset Which month (from January through November) has the largest mean return across all observations in the dataset?\nstr(stocks) \u0026#39;data.frame\u0026#39;: 11580 obs. of 12 variables: $ ReturnJan : num 0.0807 -0.0107 0.0477 -0.074 -0.031 ... $ ReturnFeb : num 0.0663 0.1021 0.036 -0.0482 -0.2127 ... $ ReturnMar : num 0.0329 0.1455 0.0397 0.0182 0.0915 ... $ ReturnApr : num 0.1831 -0.0844 -0.1624 -0.0247 0.1893 ... $ ReturnMay : num 0.13033 -0.3273 -0.14743 -0.00604 -0.15385 ... $ ReturnJune : num -0.0176 -0.3593 0.0486 -0.0253 -0.1061 ... $ ReturnJuly : num -0.0205 -0.0253 -0.1354 -0.094 0.3553 ... $ ReturnAug : num 0.0247 0.2113 0.0334 0.0953 0.0568 ... $ ReturnSep : num -0.0204 -0.58 0 0.0567 0.0336 ... $ ReturnOct : num -0.1733 -0.2671 0.0917 -0.0963 0.0363 ... $ ReturnNov : num -0.0254 -0.1512 -0.0596 -0.0405 -0.0853 ... $ PositiveDec: int 0 0 0 1 1 1 1 0 0 0 ... colMeans(stocks[, 1:11])  ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.012631602 -0.007604784 0.019402336 0.026308147 0.024736591 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct 0.005937902 0.003050863 0.016198265 -0.014720768 0.005650844 ReturnNov 0.011387440  ReturnApr = 0.026308147\nWhich month (from January through November) has the smallest mean return across all observations in the dataset? #### ReturnSep = -0.014720768\n Problem 2.1 - Initial Logistic Regression Model Split the data into a training set and testing set, putting 70% of the data in the training set and 30% of the data in the testing set:\nlibrary(caTools) set.seed(144) spl \u0026lt;- sample.split(stocks$PositiveDec, SplitRatio = 0.7) stocksTrain \u0026lt;- subset(stocks, spl == TRUE) stocksTest \u0026lt;- subset(stocks, spl == FALSE) Then, use the stocksTrain dataframe to train a logistic regression model (name it StocksModel) to predict PositiveDec using all the other variables as independent variables.\nNot forgetting to add the argument family=binomial to our glm code.\nStocksModel \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain, family = binomial) What is the overall accuracy on the training set, using a threshold of 0.5?\nStocksModelTrainPred \u0026lt;- predict(StocksModel, type = \u0026quot;response\u0026quot;) head(StocksModelTrainPred)  1 2 4 6 7 8 0.6333193 0.3804326 0.5432996 0.6485711 0.5991750 0.4372892  table(stocksTrain$PositiveDec, StocksModelTrainPred \u0026gt;= 0.5)  FALSE TRUE 0 990 2689 1 787 3640 (990 + 3640) / nrow(stocksTrain) [1] 0.5711818  Problem 2.2 - Initial Logistic Regression Model Now obtain test-set predictions from StocksModel.\nWhat is the overall accuracy of the model on the test, again using a threshold of 0.5?\nStocksModelTestPred \u0026lt;- predict(StocksModel, newdata = stocksTest, type = \u0026quot;response\u0026quot;) head(StocksModelTestPred)  3 5 15 17 23 26 0.4506152 0.6470609 0.6089785 0.5708036 0.4758428 0.3631213  table(stocksTest$PositiveDec, StocksModelTestPred \u0026gt;= 0.5)  FALSE TRUE 0 417 1160 1 344 1553 (417 + 1553) / nrow(stocksTest) [1] 0.5670697  Problem 2.3 - Initial Logistic Regression Model What is the accuracy on the test-set of a baseline model that always predicts the most common outcome (PositiveDec = 1)?\ntable(stocksTest$PositiveDec)  0 1 1577 1897  1897 / (1577 + 1897) [1] 0.5460564  Problem 3.1 - Clustering Stocks Now, let’s cluster the stocks. The first step in this process is to remove the dependent variable.\nlimitedTrain \u0026lt;- stocksTrain limitedTrain$PositiveDec \u0026lt;- NULL limitedTest \u0026lt;- stocksTest limitedTest$PositiveDec \u0026lt;- NULL Why do we need to remove the dependent variable in the clustering phase of the cluster-then-predict methodology? #### Needing to know the dependent variable value to assign an observation to a cluster defeats the purpose of the methodology\n Problem 3.2 - Clustering Stocks preProcess code from the caret package, which normalizes variables by subtracting by the mean and dividing by the standard deviation.\nIn cases where we have a training and testing set, we’ll want to normalize by the mean and standard deviation of the variables in the training set. We can do this by passing just the training set to the preProcess function:\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2 preproc \u0026lt;- preProcess(limitedTrain) normTrain \u0026lt;- predict(preproc, limitedTrain) normTest \u0026lt;- predict(preproc, limitedTest) What is the mean of the ReturnJan variable in normTrain?\nsummary(normTrain)  ReturnJan ReturnFeb ReturnMar Min. :-4.57682 Min. :-3.43004 Min. :-4.54609 1st Qu.:-0.48271 1st Qu.:-0.35589 1st Qu.:-0.40758 Median :-0.07055 Median :-0.01875 Median :-0.05778 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.35898 3rd Qu.: 0.25337 3rd Qu.: 0.36106 Max. :18.06234 Max. :34.92751 Max. :24.77296 ReturnApr ReturnMay ReturnJune Min. :-5.0227 Min. :-4.96759 Min. :-4.82957 1st Qu.:-0.4757 1st Qu.:-0.43045 1st Qu.:-0.45602 Median :-0.1104 Median :-0.06983 Median :-0.04354 Mean : 0.0000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.3400 3rd Qu.: 0.35906 3rd Qu.: 0.37273 Max. :14.6959 Max. :42.69158 Max. :10.84515 ReturnJuly ReturnAug ReturnSep Min. :-5.19139 Min. :-5.60378 Min. :-5.47078 1st Qu.:-0.51832 1st Qu.:-0.47163 1st Qu.:-0.39604 Median :-0.02372 Median :-0.07393 Median : 0.04767 Mean : 0.00000 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.47735 3rd Qu.: 0.39967 3rd Qu.: 0.42287 Max. :17.33975 Max. :27.14273 Max. :39.05435 ReturnOct ReturnNov Min. :-3.53719 Min. :-4.31684 1st Qu.:-0.42176 1st Qu.:-0.43564 Median :-0.01891 Median :-0.01878 Mean : 0.00000 Mean : 0.00000 3rd Qu.: 0.37451 3rd Qu.: 0.42560 Max. :31.25996 Max. :17.18255  What is the mean of the ReturnJan variable in normTest?\nsummary(normTest)  ReturnJan ReturnFeb ReturnMar Min. :-3.743836 Min. :-3.251044 Min. :-4.07731 1st Qu.:-0.485690 1st Qu.:-0.348951 1st Qu.:-0.40662 Median :-0.066856 Median :-0.006860 Median :-0.05674 Mean :-0.000419 Mean :-0.003862 Mean : 0.00583 3rd Qu.: 0.357729 3rd Qu.: 0.264647 3rd Qu.: 0.35653 Max. : 8.412973 Max. : 9.552365 Max. : 9.00982 ReturnApr ReturnMay ReturnJune Min. :-4.47865 Min. :-5.84445 Min. :-4.73628 1st Qu.:-0.51121 1st Qu.:-0.43819 1st Qu.:-0.44968 Median :-0.11414 Median :-0.05346 Median :-0.02678 Mean :-0.03638 Mean : 0.02651 Mean : 0.04315 3rd Qu.: 0.32742 3rd Qu.: 0.42290 3rd Qu.: 0.43010 Max. : 6.84589 Max. : 7.21362 Max. :29.00534 ReturnJuly ReturnAug ReturnSep Min. :-5.201454 Min. :-4.62097 Min. :-3.57222 1st Qu.:-0.512039 1st Qu.:-0.51546 1st Qu.:-0.38067 Median :-0.026576 Median :-0.10277 Median : 0.08215 Mean : 0.006016 Mean :-0.04973 Mean : 0.02939 3rd Qu.: 0.457193 3rd Qu.: 0.38781 3rd Qu.: 0.45847 Max. :12.790901 Max. : 6.66889 Max. : 7.09106 ReturnOct ReturnNov Min. :-3.807577 Min. :-4.881463 1st Qu.:-0.393856 1st Qu.:-0.396764 Median : 0.006783 Median :-0.002337 Mean : 0.029672 Mean : 0.017128 3rd Qu.: 0.419005 3rd Qu.: 0.424617 Max. : 7.428466 Max. :21.007786   Problem 3.3 - Clustering Stocks Why is the mean ReturnJan variable much closer to 0 in normTrain than in normTest? #### The distribution of the ReturnJan variable is different in the training and testing set\n Problem 3.4 - Clustering Stocks Set the random seed to 144 (it is important to do this again, even though we did it earlier). Run k-means clustering with 3 clusters on normTrain, storing the result in an object called km.\nset.seed(144) km \u0026lt;- kmeans(normTrain, centers = 3) Which cluster has the largest number of observations?\ntable(km$cluster)  1 2 3 3157 4696 253  Cluster 2\n Problem 3.5 - Clustering Stocks Recall from the recitation that we can use the flexclust package to obtain training set and testing set cluster assignments for our observations (note that the call to as.kcca may take a while to complete):\nlibrary(flexclust) Loading required package: grid Loading required package: modeltools Loading required package: stats4 km.kcca \u0026lt;- as.kcca(km, normTrain) clusterTrain = predict(km.kcca) clusterTest = predict(km.kcca, newdata=normTest) How many test-set observations were assigned to Cluster 2?\ntable(clusterTest) clusterTest 1 2 3 1298 2080 96  2080\n Problem 4.1 - Cluster-Specific Predictions Using the subset function, build dataframes stocksTrain1, stocksTrain2, and stocksTrain3, containing the elements in the stocksTrain dataframe assigned to clusters 1, 2, and 3, respectively (be careful to take subsets of stocksTrain, not of normTrain). Similarly build stocksTest1, stocksTest2, and stocksTest3 from the stocksTest dataframe.\nstocksTrain1 \u0026lt;- subset(stocksTrain, clusterTrain == 1) stocksTrain2 \u0026lt;- subset(stocksTrain, clusterTrain == 2) stocksTrain3 \u0026lt;- subset(stocksTrain, clusterTrain == 3) stocksTest1 \u0026lt;- subset(stocksTest, clusterTest == 1) stocksTest2 \u0026lt;- subset(stocksTest, clusterTest == 2) stocksTest3 \u0026lt;- subset(stocksTest, clusterTest == 3) Which training set dataframe has the highest average value of the dependent variable?\nmean(stocksTrain1$PositiveDec) [1] 0.6024707 mean(stocksTrain2$PositiveDec) [1] 0.5140545 mean(stocksTrain3$PositiveDec) [1] 0.4387352 stocksTrain1\n Problem 4.2 - Cluster-Specific Predictions Build logistic regression models StocksModel1, StocksModel2, and StocksModel3, which predict PositiveDec using all the other variables as independent variables. StocksModel1 should be trained on stocksTrain1, StocksModel2 should be trained on stocksTrain2, and StocksModel3 should be trained on stocksTrain3.\nStocksModel1 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain1, family = binomial) StocksModel2 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain2, family = binomial) StocksModel3 \u0026lt;- glm(PositiveDec ~ ., data = stocksTrain3, family = binomial) Which variables have a positive sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3 and a negative sign for the coefficient in at least one of StocksModel1, StocksModel2, and StocksModel3? Select all that apply.\nStocksModel1$coefficients (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.17223985 0.02498357 -0.37207369 0.59554957 1.19047752 0.30420906 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov -0.01165375 0.19769226 0.51272941 0.58832685 -1.02253506 -0.74847186  StocksModel2$coefficients (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay 0.1029318 0.8845148 0.3176221 -0.3797811 0.4929105 0.8965492 ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov 1.5008787 0.7831487 -0.2448602 0.7368522 -0.2775631 -0.7874737  StocksModel3$coefficients  (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr -0.181895809 -0.009789345 -0.046883260 0.674179495 1.281466189 ReturnMay ReturnJune ReturnJuly ReturnAug ReturnSep 0.762511555 0.329433917 0.774164370 0.982605385 0.363806823 ReturnOct ReturnNov 0.782242086 -0.873752144  rbind(StocksModel1$coefficients \u0026gt; 0, StocksModel2$coefficients \u0026gt; 0, StocksModel3$coefficients \u0026gt; 0)  (Intercept) ReturnJan ReturnFeb ReturnMar ReturnApr ReturnMay [1,] TRUE TRUE FALSE TRUE TRUE TRUE [2,] TRUE TRUE TRUE FALSE TRUE TRUE [3,] FALSE FALSE FALSE TRUE TRUE TRUE ReturnJune ReturnJuly ReturnAug ReturnSep ReturnOct ReturnNov [1,] FALSE TRUE TRUE TRUE FALSE FALSE [2,] TRUE TRUE FALSE TRUE FALSE FALSE [3,] TRUE TRUE TRUE TRUE TRUE FALSE ReturnJan, ReturnFeb, ReturnMar, ReturnJune, ReturnAug, ReturnOct\n Problem 4.3 - Cluster-Specific Predictions Using StocksModel1, make test-set predictions called PredictTest1 on the dataframe stocksTest1. Using StocksModel2, make test-set predictions called PredictTest2 on the dataframe stocksTest2. Using StocksModel3, make test-set predictions called PredictTest3 on the dataframe stocksTest3.\nPredictTest1 \u0026lt;- predict(StocksModel1, newdata = stocksTest1, type = \u0026quot;response\u0026quot;) PredictTest2 \u0026lt;- predict(StocksModel2, newdata = stocksTest2, type = \u0026quot;response\u0026quot;) PredictTest3 \u0026lt;- predict(StocksModel3, newdata = stocksTest3, type = \u0026quot;response\u0026quot;) What is the overall accuracy of StocksModel1 on the test-set stocksTest1, using a threshold of 0.5?\ntable(stocksTest1$PositiveDec, PredictTest1 \u0026gt;= 0.5)  FALSE TRUE 0 30 471 1 23 774 (30 + 774) / nrow(stocksTest1) [1] 0.6194145 What is the overall accuracy of StocksModel2 on the test-set stocksTest2, using a threshold of 0.5?\ntable(stocksTest2$PositiveDec, PredictTest2 \u0026gt;= 0.5)  FALSE TRUE 0 388 626 1 309 757 (388 + 757) / nrow(stocksTest2) [1] 0.5504808 What is the overall accuracy of StocksModel3 on the test-set stocksTest3, using a threshold of 0.5?\ntable(stocksTest3$PositiveDec, PredictTest3 \u0026gt;= 0.5)  FALSE TRUE 0 49 13 1 21 13 (49 + 13) / nrow(stocksTest3) [1] 0.6458333  Problem 4.4 - Cluster-Specific Predictions To compute the overall test-set accuracy of the cluster-then-predict approach, we can combine all the test-set predictions into a single vector and all the true outcomes into a single vector:\nAllPredictions \u0026lt;- c(PredictTest1, PredictTest2, PredictTest3) AllOutcomes \u0026lt;- c(stocksTest1$PositiveDec, stocksTest2$PositiveDec, stocksTest3$PositiveDec) What is the overall test-set accuracy of the cluster-then-predict approach, again using a threshold of 0.5?\ntable(AllOutcomes, AllPredictions \u0026gt;= 0.5)  AllOutcomes FALSE TRUE 0 467 1110 1 353 1544 length(AllOutcomes) [1] 3474 (467 + 1544) / length(AllOutcomes) [1] 0.5788716  Conclusion We see a modest improvement over the original logistic regression model. Since predicting stock returns is a notoriously hard problem, this is a good increase in accuracy. By investing in stocks for which we are more confident that they will have positive returns (by selecting the ones with higher predicted probabilities), this cluster-then-predict model can give us an edge over the original logistic regression model.\n ","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"5a590c082ca3588c01aea5bfd1ad85e0","permalink":"/project/stock_returns/stock/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/project/stock_returns/stock/","section":"project","summary":"Selecting which stocks to invest-in?","tags":["R","Data Analytics","Machine Learning"],"title":"Stock Returns Prediction","type":"project"},{"authors":null,"categories":null,"content":" The medical literature is enormous! Pubmed, a database of medical publications maintained by the U.S. National Library of Medicine, has indexed over 23 million medical publications. Further, the rate of medical publication has increased over time, and now there are nearly 1 million new publications in the field each year, or more than one per minute.\nThe large size and fast-changing nature of the medical literature has increased the need for reviews, which search databases like Pubmed for papers on a particular topic and then report results from the papers found. While such reviews are often performed manually, with multiple people reviewing each search result, this is tedious and time consuming. In this analysis, I’ll see how text analytics can be used to automate the process of information retrieval.\nThe dataset consists of the titles (variable title) and abstracts (variable abstract) of papers retrieved in a Pubmed search. Each search result is labeled with whether the paper is a clinical trial testing a drug therapy for cancer (variable trial). These labels were obtained by two people reviewing each search result and accessing the actual paper if necessary, as part of a literature review of clinical trials testing drug therapies for advanced and metastatic breast cancer.\nLoading the packages  Problem 1.1 - Loading the Data Load clinical_trial.csv into a dataframe called trials (remembering to add the argument stringsAsFactors=FALSE when working with text analytics, so that the text is read in properply), and investigate the dataframe.\ntrials \u0026lt;- read.csv(\u0026quot;clinical_trial.csv\u0026quot;, stringsAsFactors = FALSE) str(trials) \u0026#39;data.frame\u0026#39;: 1860 obs. of 3 variables: $ title : chr \u0026quot;Treatment of Hodgkin\u0026#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).\u0026quot; \u0026quot;Cell mediated immune status in malignancy--pretherapy and post-therapy assessment.\u0026quot; \u0026quot;Neoadjuvant vinorelbine-capecitabine versus docetaxel-doxorubicin-cyclophosphamide in early nonresponsive breas\u0026quot;| __truncated__ \u0026quot;Randomized phase 3 trial of fluorouracil, epirubicin, and cyclophosphamide alone or followed by Paclitaxel for \u0026quot;| __truncated__ ... $ abstract: chr \u0026quot;\u0026quot; \u0026quot;Twenty-eight cases of malignancies of different kinds were studied to assess T-cell activity and population bef\u0026quot;| __truncated__ \u0026quot;BACKGROUND: Among breast cancer patients, nonresponse to initial neoadjuvant chemotherapy is associated with un\u0026quot;| __truncated__ \u0026quot;BACKGROUND: Taxanes are among the most active drugs for the treatment of metastatic breast cancer, and, as a co\u0026quot;| __truncated__ ... $ trial : int 1 0 1 1 1 0 1 0 0 0 ... summary(trials)  title abstract trial Length:1860 Length:1860 Min. :0.0000 Class :character Class :character 1st Qu.:0.0000 Mode :character Mode :character Median :0.0000 Mean :0.4392 3rd Qu.:1.0000 Max. :1.0000  IMPORTANT NOTE: Should you get an error like “invalid multibyte string” when performing certain parts of this analysis, use the argument fileEncoding=“latin1” when reading in the file with read.csv. This should cause those errors to go away. We can use R’s string functions to learn more about the titles and abstracts of the located papers. The nchar() function counts the number of characters in a piece of text.\nUsing the nchar() function on the variables in the dataframe. How many characters are there in the longest abstract? (Longest here is defined as the abstract with the largest number of characters.)\nmax(nchar(trials$abstract)) [1] 3708 which.max(nchar(trials$abstract)) [1] 664 trials[664, ]  title 664 Five versus more than five years of tamoxifen therapy for breast cancer patients with negative lymph nodes and estrogen receptor-positive tumors. abstract 664 BACKGROUND: In 1982, the National Surgical Adjuvant Breast and Bowel Project initiated a randomized, double-blinded, placebo-controlled trial (B-14) to determine the effectiveness of adjuvant tamoxifen therapy in patients with primary operable breast cancer who had estrogen receptor-positive tumors and no axillary lymph node involvement. The findings indicated that tamoxifen therapy provided substantial benefit to patients with early stage disease. However, questions arose about how long the observed benefit would persist, about the duration of therapy necessary to maintain maximum benefit, and about the nature and severity of adverse effects from prolonged treatment.PURPOSE: We evaluated the outcome of patients in the B-14 trial through 10 years of follow-up. In addition, the effects of 5 years versus more than 5 years of tamoxifen therapy were compared.METHODS: In the trial, patients were initially assigned to receive either tamoxifen at 20 mg/day (n = 1404) or placebo (n = 1414). Tamoxifen-treated patients who remained disease free after 5 years of therapy were then reassigned to receive either another 5 years of tamoxifen (n = 322) or 5 years of placebo (n = 321). After the study began, another group of patients who met the same protocol eligibility requirements as the randomly assigned patients were registered to receive tamoxifen (n = 1211). Registered patients who were disease free after 5 years of treatment were also randomly assigned to another 5 years of tamoxifen (n = 261) or to 5 years of placebo (n = 249). To compare 5 years with more than 5 years of tamoxifen therapy, data relating to all patients reassigned to an additional 5 years of the drug were combined. Patients who were not reassigned to either tamoxifen or placebo continued to be followed in the study. Survival, disease-free survival, and distant disease-free survival (relating to failure at distant sites) were estimated by use of the Kaplan-Meier method; differences between the treatment groups were assessed by use of the logrank test. The relative risks of failure (with 95% confidence intervals [CIs]) were determined by use of the Cox proportional hazards model. Reported P values are two-sided.RESULTS: Through 10 years of follow-up, a significant advantage in disease-free survival (69% versus 57%, P \u0026lt; .0001; relative risk = 0.66; 95% CI = 0.58-0.74), distant disease-free survival (76% versus 67%, P \u0026lt; .0001; relative risk = 0.70; 95% CI = 0.61-0.81), and survival (80% versus 76%, P = .02; relative risk = 0.84; 95% CI = 0.71-0.99) was found for patients in the group first assigned to receive tamoxifen. The survival benefit extended to those 49 years of age or younger and to those 50 years of age or older. Tamoxifen therapy was associated with a 37% reduction in the incidence of contralateral (opposite) breast cancer (P = .007). Through 4 years after the reassignment of tamoxifen-treated patients to either continued-therapy or placebo groups, advantages in disease-free survival (92% versus 86%, P = .003) and distant disease-free survival (96% versus 90%, P = .01) were found for those who discontinued tamoxifen treatment. Survival was 96% for those who discontinued tamoxifen compared with 94% for those who continued tamoxifen treatment (P = .08). A higher incidence of thromboembolic events was seen in tamoxifen-treated patients (through 5 years, 1.7% versus 0.4%). Except for endometrial cancer, the incidence of second cancers was not increased with tamoxifen therapy.CONCLUSIONS AND IMPLICATIONS: The benefit from 5 years of tamoxifen therapy persists through 10 years of follow-up. No additional advantage is obtained from continuing tamoxifen therapy for more than 5 years. trial 664 1 nchar(trials[664, ]$abstract) [1] 3708 3708\n Problem 1.2 - Loading the Data How many search results provided no abstract? (HINT: A search result provided no abstract if the number of characters in the abstract field is zero.)\nsum(nchar(trials$abstract) == 0) [1] 112  Problem 1.3 - Loading the Data Find the observation with the minimum number of characters in the title (the variable “title”) out of all of the observations in this dataset. What is the text of the title of this article?\nInclude capitalization and punctuation in our response, but don’t include the quotes.\nmin(nchar(trials$title)) [1] 28 which.min(nchar(trials$title)) [1] 1258 trials[1258,]  title 1258 A decade of letrozole: FACE. abstract 1258 Third-generation nonsteroidal aromatase inhibitors (AIs), letrozole and anastrozole, are superior to tamoxifen as initial therapy for early breast cancer but have not been directly compared in a head-to-head adjuvant trial. Cumulative evidence suggests that AIs are not equivalent in terms of potency of estrogen suppression and that there may be differences in clinical efficacy. Thus, with no data from head-to-head comparisons of the AIs as adjuvant therapy yet available, the question of whether there are efficacy differences between the AIs remains. To help answer this question, the Femara versus Anastrozole Clinical Evaluation (FACE) is a phase IIIb open-label, randomized, multicenter trial designed to test whether letrozole or anastrozole has superior efficacy as adjuvant treatment of postmenopausal women with hormone receptor (HR)- and lymph node-positive breast cancer. Eligible patients (target accrual, N=4,000) are randomized to receive either letrozole 2.5 mg or anastrozole 1 mg daily for up to 5 years. The primary objective is to compare disease-free survival at 5 years. Secondary end points include safety, overall survival, time to distant metastases, and time to contralateral breast cancer. The FACE trial will determine whether or not letrozole offers a greater clinical benefit to postmenopausal women with HR+ early breast cancer at increased risk of early recurrence compared with anastrozole. trial 1258 0 trials[1258,]$title [1] \u0026quot;A decade of letrozole: FACE.\u0026quot;  Problem 2.1 - Preparing the Corpus Because we have both title and abstract information for trials, we need to build two corpera instead of one. Naming them corpusTitle and corpusAbstract.\nThe code performs the following tasks (you might need to load the “tm” package first if it isn’t already loaded). Making sure to perform them in this order.\n# 1) Convert the title variable to corpusTitle and the abstract variable to corpusAbstract corpusTitle \u0026lt;- Corpus(VectorSource(trials$title)) corpusTitle[[1]]$content [1] \u0026quot;Treatment of Hodgkin\u0026#39;s disease and other cancers with 1,3-bis(2-chloroethyl)-1-nitrosourea (BCNU; NSC-409962).\u0026quot; corpusAbstract \u0026lt;- Corpus(VectorSource(trials$abstract)) corpusAbstract[[1]]$content [1] \u0026quot;\u0026quot; # 2) Convert corpusTitle and corpusAbstract to lowercase corpusTitle = tm_map(corpusTitle, content_transformer(tolower)) Warning in tm_map.SimpleCorpus(corpusTitle, content_transformer(tolower)): transformation drops documents corpusAbstract = tm_map(corpusAbstract, content_transformer(tolower)) Warning in tm_map.SimpleCorpus(corpusAbstract, content_transformer(tolower)): transformation drops documents #corpusTitle = tm_map(corpusTitle, PlainTextDocument) #corpusAbstract = tm_map(corpusAbstract, PlainTextDocument) # 3) Remove the punctuation in corpusTitle and corpusAbstract corpusTitle = tm_map(corpusTitle, removePunctuation) Warning in tm_map.SimpleCorpus(corpusTitle, removePunctuation): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediated immune status in malignancypretherapy and posttherapy assessment\u0026quot; corpusAbstract = tm_map(corpusAbstract, removePunctuation) Warning in tm_map.SimpleCorpus(corpusAbstract, removePunctuation): transformation drops documents # 4) Remove the English language stop words from corpusTitle and corpusAbstract corpusTitle \u0026lt;- tm_map(corpusTitle, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusTitle, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediated immune status malignancypretherapy posttherapy assessment\u0026quot; corpusAbstract \u0026lt;- tm_map(corpusAbstract, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusAbstract, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusAbstract[[2]]$content [1] \u0026quot;twentyeight cases malignancies different kinds studied assess tcell activity population institution therapy fifteen cases diagnosed nonmetastasising squamous cell carcinoma larynx pharynx laryngopharynx hypopharynx tonsils seven cases nonmetastasising infiltrating duct carcinoma breast 6 cases nonhodgkins lymphoma nhl observed 3 15 cases 20 squamous cell carcinoma cases mantoux test mt negative tcell population less 40 2 7 cases 286 infiltrating duct carcinoma breast mt negative tcell population less 40 3 6 cases 50 nhl mt negative tcell population less 40 normal controls consisting apparently normal healthy adults tcell population 40 mt positive patients showed negative skin test tcell population less 40 subjected assessment tcell population activity appropriate therapy clinical cure disease observed 2 3 cases 6666 squamous cell carcinomas 2 2 cases 100 adenocarcinomas one 3 cases 3333 nhl showed positive conversion tcell population 40\u0026quot; # 5) Stem the words in corpusTitle and corpusAbstract (each stemming might take a few minutes) corpusTitle = tm_map(corpusTitle, stemDocument) Warning in tm_map.SimpleCorpus(corpusTitle, stemDocument): transformation drops documents corpusTitle[[2]]$content [1] \u0026quot;cell mediat immun status malignancypretherapi posttherapi assess\u0026quot; corpusAbstract = tm_map(corpusAbstract, stemDocument) Warning in tm_map.SimpleCorpus(corpusAbstract, stemDocument): transformation drops documents corpusAbstract[[2]]$content [1] \u0026quot;twentyeight case malign differ kind studi assess tcell activ popul institut therapi fifteen case diagnos nonmetastasis squamous cell carcinoma larynx pharynx laryngopharynx hypopharynx tonsil seven case nonmetastasis infiltr duct carcinoma breast 6 case nonhodgkin lymphoma nhl observ 3 15 case 20 squamous cell carcinoma case mantoux test mt negat tcell popul less 40 2 7 case 286 infiltr duct carcinoma breast mt negat tcell popul less 40 3 6 case 50 nhl mt negat tcell popul less 40 normal control consist appar normal healthi adult tcell popul 40 mt posit patient show negat skin test tcell popul less 40 subject assess tcell popul activ appropri therapi clinic cure diseas observ 2 3 case 6666 squamous cell carcinoma 2 2 case 100 adenocarcinoma one 3 case 3333 nhl show posit convers tcell popul 40\u0026quot; # 6) Build a document term matrix called dtmTitle from corpusTitle and dtmAbstract from corpusAbstract dtmTitle = DocumentTermMatrix(corpusTitle) dtmTitle \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 2836)\u0026gt;\u0026gt; Non-/sparse entries: 23416/5251544 Sparsity : 100% Maximal term length: 49 Weighting : term frequency (tf) dtmAbstract = DocumentTermMatrix(corpusAbstract) dtmAbstract \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 12451)\u0026gt;\u0026gt; Non-/sparse entries: 153290/23005570 Sparsity : 99% Maximal term length: 67 Weighting : term frequency (tf) # 7) Limit dtmTitle and dtmAbstract to terms with sparseness of at most 95% (aka terms that appear in at least 5% of documents) dtmTitle \u0026lt;- removeSparseTerms(dtmTitle, 0.95) dtmTitle \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 31)\u0026gt;\u0026gt; Non-/sparse entries: 10683/46977 Sparsity : 81% Maximal term length: 15 Weighting : term frequency (tf) dtmAbstract \u0026lt;- removeSparseTerms(dtmAbstract, 0.95) dtmAbstract \u0026lt;\u0026lt;DocumentTermMatrix (documents: 1860, terms: 335)\u0026gt;\u0026gt; Non-/sparse entries: 91969/531131 Sparsity : 85% Maximal term length: 15 Weighting : term frequency (tf) # 8) Convert dtmTitle and dtmAbstract to data frames (keep the names dtmTitle and dtmAbstract) dtmTitle \u0026lt;- as.data.frame(as.matrix(dtmTitle)) dtmAbstract \u0026lt;- as.data.frame(as.matrix(dtmAbstract)) When removing stop words, use tm_map(corpusTitle, removeWords, sw) and tm_map(corpusAbstract, removeWords, sw) instead of tm_map(corpusTitle, removeWords, stopwords(“english”)) and tm_map(corpusAbstract, removeWords, stopwords(“english”)).\nlength(stopwords(\u0026quot;english\u0026quot;)) [1] 174 How many terms remain in dtmTitle after removing sparse terms (aka how many columns does it have)?\nstr(dtmTitle) \u0026#39;data.frame\u0026#39;: 1860 obs. of 31 variables: $ cancer : num 1 0 1 1 1 1 0 1 1 2 ... $ treatment : num 1 0 0 0 1 0 0 0 0 1 ... $ breast : num 0 0 1 1 1 1 0 1 1 1 ... $ earli : num 0 0 1 1 0 0 0 1 0 0 ... $ iii : num 0 0 1 0 0 0 0 0 0 1 ... $ phase : num 0 0 1 1 0 0 0 0 0 1 ... $ random : num 0 0 1 1 1 0 0 0 0 1 ... $ trial : num 0 0 1 1 1 0 0 1 1 1 ... $ versus : num 0 0 1 0 0 0 0 1 0 0 ... $ cyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ chemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ combin : num 0 0 0 0 1 0 1 0 0 0 ... $ effect : num 0 0 0 0 1 0 0 1 0 1 ... $ metastat : num 0 0 0 0 1 0 0 0 0 0 ... $ patient : num 0 0 0 0 1 0 1 0 1 1 ... $ respons : num 0 0 0 0 0 1 0 0 0 0 ... $ advanc : num 0 0 0 0 0 0 1 0 0 0 ... $ postmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ randomis : num 0 0 0 0 0 0 0 1 1 0 ... $ studi : num 0 0 0 0 0 0 0 1 0 0 ... $ tamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ women : num 0 0 0 0 0 0 0 1 0 0 ... $ adjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ group : num 0 0 0 0 0 0 0 0 1 1 ... $ therapi : num 0 0 0 0 0 0 0 0 0 0 ... $ compar : num 0 0 0 0 0 0 0 0 0 0 ... $ doxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ docetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ result : num 0 0 0 0 0 0 0 0 0 0 ... $ plus : num 0 0 0 0 0 0 0 0 0 0 ... $ clinic : num 0 0 0 0 0 0 0 0 0 0 ... 31 How many terms remain in dtmAbstract?\nstr(dtmAbstract) \u0026#39;data.frame\u0026#39;: 1860 obs. of 335 variables: $ 100 : num 0 1 0 0 0 0 0 0 0 0 ... $ activ : num 0 2 0 1 0 0 1 0 0 0 ... $ assess : num 0 2 1 2 0 1 0 0 0 3 ... $ breast : num 0 2 3 3 3 4 2 2 2 3 ... $ carcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ case : num 0 11 0 0 1 0 0 0 0 0 ... $ cell : num 0 3 0 0 0 1 0 0 0 0 ... $ clinic : num 0 1 0 1 0 0 0 0 0 0 ... $ consist : num 0 1 0 0 0 0 0 0 0 0 ... $ control : num 0 1 0 0 0 0 0 0 1 0 ... $ differ : num 0 1 2 1 3 0 0 1 0 1 ... $ diseas : num 0 1 0 1 3 0 0 1 0 0 ... $ less : num 0 4 1 0 0 0 0 0 0 6 ... $ negat : num 0 4 0 0 0 3 0 0 0 0 ... $ observ : num 0 2 1 0 1 0 0 0 0 0 ... $ one : num 0 1 0 0 0 0 0 0 0 0 ... $ patient : num 0 1 9 5 5 6 8 3 2 5 ... $ popul : num 0 8 0 0 0 0 0 0 0 0 ... $ posit : num 0 2 0 1 0 5 0 0 0 0 ... $ seven : num 0 1 0 0 0 0 0 0 0 0 ... $ show : num 0 2 0 0 1 0 1 0 0 3 ... $ studi : num 0 1 1 1 0 1 3 2 0 1 ... $ test : num 0 2 1 1 0 0 0 0 0 0 ... $ therapi : num 0 2 0 1 0 0 0 0 0 0 ... $ 500 : num 0 0 1 0 2 0 0 0 0 0 ... $ addit : num 0 0 2 0 0 0 0 0 0 0 ... $ among : num 0 0 2 4 0 0 0 0 1 0 ... $ arm : num 0 0 7 4 2 0 0 1 0 1 ... $ assign : num 0 0 2 1 0 0 0 1 1 1 ... $ associ : num 0 0 1 3 0 2 0 1 2 0 ... $ background : num 0 0 1 1 1 0 0 1 1 0 ... $ better : num 0 0 1 0 1 1 0 0 0 0 ... $ cancer : num 0 0 2 3 3 3 2 2 3 0 ... $ chang : num 0 0 1 0 0 0 0 0 0 0 ... $ chemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ compar : num 0 0 1 2 0 0 0 1 1 1 ... $ complet : num 0 0 3 0 1 1 1 2 0 0 ... $ conclus : num 0 0 1 0 1 0 0 0 0 0 ... $ confid : num 0 0 1 1 0 0 0 0 0 0 ... $ continu : num 0 0 2 0 1 0 0 2 0 0 ... $ cycl : num 0 0 6 0 2 0 1 0 0 0 ... $ cyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ day : num 0 0 1 0 0 0 3 0 0 0 ... $ decreas : num 0 0 1 0 0 0 0 0 1 0 ... $ defin : num 0 0 2 0 0 0 0 0 0 0 ... $ demonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ docetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ doxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ effect : num 0 0 2 0 0 1 0 2 0 1 ... $ efficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ enrol : num 0 0 1 0 0 0 1 0 0 1 ... $ four : num 0 0 4 0 0 0 0 0 0 0 ... $ hematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ initi : num 0 0 3 0 0 0 0 1 0 0 ... $ interv : num 0 0 1 1 0 0 0 0 0 0 ... $ least : num 0 0 2 0 0 0 0 0 0 0 ... $ lymph : num 0 0 1 4 0 3 0 0 1 0 ... $ method : num 0 0 1 0 1 0 0 1 1 0 ... $ mgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ neoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ node : num 0 0 1 3 0 0 0 0 1 0 ... $ number : num 0 0 1 1 0 2 1 0 0 0 ... $ outcom : num 0 0 2 0 0 0 0 0 0 2 ... $ patholog : num 0 0 3 0 0 1 0 0 0 0 ... $ per : num 0 0 1 0 0 0 0 0 0 0 ... $ previous : num 0 0 1 0 1 0 2 0 0 0 ... $ random : num 0 0 2 1 1 0 0 1 1 2 ... $ rate : num 0 0 1 1 2 0 1 0 1 0 ... $ receiv : num 0 0 2 0 1 1 3 0 0 2 ... $ reduct : num 0 0 1 2 0 0 0 1 0 0 ... $ regimen : num 0 0 2 0 0 0 1 0 0 0 ... $ respond : num 0 0 2 0 0 0 0 0 0 0 ... $ respons : num 0 0 7 0 4 2 2 0 0 0 ... $ result : num 0 0 1 0 1 0 1 0 0 0 ... $ similar : num 0 0 2 0 0 0 0 0 1 0 ... $ size : num 0 0 1 3 0 1 0 0 0 0 ... $ statist : num 0 0 1 3 0 0 0 0 0 0 ... $ surgeri : num 0 0 1 1 0 0 0 0 0 0 ... $ toler : num 0 0 1 0 0 0 1 0 0 0 ... $ toxic : num 0 0 2 0 1 0 1 0 0 5 ... $ treatment : num 0 0 3 6 14 0 0 4 1 1 ... $ tumor : num 0 0 2 4 0 0 2 0 0 0 ... $ two : num 0 0 3 0 1 0 0 1 0 0 ... $ 001 : num 0 0 0 1 0 0 0 1 0 0 ... $ adjuv : num 0 0 0 2 0 1 0 2 4 0 ... $ age : num 0 0 0 1 0 0 0 0 0 0 ... $ also : num 0 0 0 1 0 0 0 0 2 0 ... $ analysi : num 0 0 0 2 0 0 0 1 0 0 ... $ analyz : num 0 0 0 1 0 0 0 0 0 0 ... $ axillari : num 0 0 0 1 0 0 0 0 1 0 ... $ death : num 0 0 0 2 0 0 0 0 1 0 ... $ dfs : num 0 0 0 3 0 0 0 0 0 0 ... $ diseasefre : num 0 0 0 1 0 2 0 0 3 0 ... $ drug : num 0 0 0 1 0 0 0 0 0 0 ... $ elig : num 0 0 0 1 0 0 0 0 0 0 ... $ endpoint : num 0 0 0 2 0 0 0 0 1 3 ... $ epirubicin : num 0 0 0 1 1 0 0 0 0 0 ... $ estim : num 0 0 0 1 1 0 0 0 0 0 ... $ fluorouracil : num 0 0 0 1 1 0 0 0 0 0 ... [list output truncated]  335   Problem 2.2 - Preparing the Corpus What is the most likely reason why dtmAbstract has so many more terms than dtmTitle? #### Abstracts tend to have many more words than titles\n Problem 2.3 - Preparing the Corpus What is the most frequent word stem across all the abstracts? Hint: you can use colSums() to compute the frequency of a word across all the abstracts.\n?colSums colSums(dtmAbstract)  100 activ assess breast 225 509 668 3859 carcinoma case cell clinic 251 233 359 944 consist control differ diseas 200 621 1176 950 less negat observ one 351 258 700 570 patient popul posit seven 8381 162 511 108 show studi test therapi 516 1965 282 1564 500 addit among arm 169 420 365 1038 assign associ background better 435 604 397 186 cancer chang chemotherapi compar 3726 431 2344 1359 complet conclus confid continu 628 842 241 281 cycl cyclophosphamid day decreas 962 632 1245 350 defin demonstr docetaxel doxorubicin 123 251 514 486 effect efficaci enrol four 1340 591 221 369 hematolog initi interv least 117 275 349 177 lymph method mgm2 neoadjuv 249 892 1093 293 node number outcom patholog 477 296 335 254 per previous random rate 218 355 1520 1253 receiv reduct regimen respond 1908 301 807 200 respons result similar size 2051 1485 438 177 statist surgeri toler toxic 384 407 373 1065 treatment tumor two 001 2894 1122 889 162 adjuv age also analysi 1162 429 364 587 analyz axillari death dfs 124 292 215 310 diseasefre drug elig endpoint 364 332 196 213 epirubicin estim fluorouracil follow 339 139 215 675 found hazard her2 hormon 238 301 314 428 includ involv marker metastat 529 180 189 755 model multivari nodeposit oper 180 154 199 193 overal paclitaxel predict primari 962 397 369 718 prognost proport ratio receptor 242 125 344 573 reduc relaps respect risk 400 254 758 635 sampl secondari signific status 172 158 2043 538 surviv type valu week 1927 126 256 1074 women year agent benefit 1484 1335 240 551 combin detect determin durat 926 148 352 344 either evalu everi evid 532 926 487 150 firstlin howev life may 182 339 178 413 measur object partial perform 411 400 295 342 plus potenti progress prolong 622 156 622 125 qualiti score singl stabl 189 254 149 154 subgroup term time total 192 153 881 397 use vomit whether 0001 1053 174 235 249 5fluorouracil aim correl express 208 185 203 356 factor followup grade group 552 494 580 2668 high independ larg level 378 149 108 743 longer low median month 193 196 1180 1575 premenopaus progesteron randomis remain 303 114 264 158 trend tumour wherea achiev 115 320 173 245 administ advanc daili dose 322 556 412 1123 indic infus intraven neutropenia 269 237 192 234 phase prior support treat 481 305 183 893 trial aromatas caus due 1417 171 115 154 earli end estrogen increas 325 221 421 729 inhibitor lower particip point 182 236 144 202 postmenopaus receptorposit tamoxifen versus 590 152 1632 570 within alon can distant 172 472 191 149 find first higher iii 177 421 415 266 improv limit mastectomi metastas 562 127 165 352 occur postop radiotherapi recurr 312 177 244 465 site stage system advers 183 286 193 256 baselin common doubleblind event 340 191 149 409 greater mean placebo purpos 279 310 475 434 obtain oral present relat 147 422 218 351 suggest bone administr cours 274 514 218 283 given safeti schedul seen 374 265 215 199 standard without although new 305 306 191 171 accord anthracyclin base eight 182 207 124 124 histolog investig local set 127 295 300 191 analys sever three import 177 288 564 138 shown 005 avail data 117 124 108 405 incid period profil report 300 170 158 357 endocrin hundr multicent design 266 195 126 219 growth human pretreat well 208 144 175 328 examin six appear identifi 190 261 164 148 nausea provid regress five 239 155 120 173 conduct prospect develop sequenti 177 239 259 168 serum comparison frequent cmf 315 116 153 586 consid rang select possibl 131 248 124 130 inform major need function 124 122 128 188 failur confirm requir experienc 262 178 168 167 characterist methotrex progressionfre general 119 265 158 126 prevent andor mbc main 143 128 276 131 side superior start tissu 168 161 131 197 second regard enter 138 105 117  max(colSums(dtmAbstract)) [1] 8381 which.max(colSums(dtmAbstract)) patient 17  dtmAbstract$patient  [1] 0 1 9 5 5 6 8 3 2 5 2 4 2 1 1 3 0 8 6 0 3 7 7 [24] 4 2 5 2 5 0 0 3 2 3 5 12 3 5 4 7 2 0 1 2 3 6 5 [47] 1 4 7 6 2 5 6 5 5 6 9 5 1 5 10 6 3 3 6 1 4 4 0 [70] 6 9 5 9 1 11 5 0 3 5 6 3 8 8 2 6 9 3 7 4 1 6 13 [93] 3 12 0 5 5 3 0 1 8 16 5 13 0 7 2 7 0 7 3 2 5 1 6 [116] 13 4 5 4 4 4 6 5 6 5 3 1 3 3 0 5 1 3 9 8 2 3 7 [139] 5 3 4 4 11 2 9 1 1 6 8 1 2 1 1 7 0 2 4 6 16 0 3 [162] 0 3 8 3 5 2 11 5 0 0 2 8 8 1 4 7 11 6 5 6 1 7 2 [185] 14 13 4 23 7 3 6 3 3 2 3 4 11 2 4 12 3 6 4 3 6 9 4 [208] 8 2 4 2 6 1 7 6 5 5 14 5 13 0 11 1 7 6 3 5 4 3 7 [231] 0 0 7 2 4 5 7 0 4 4 16 0 8 3 0 3 7 6 6 2 2 1 5 [254] 4 7 8 1 1 5 4 2 3 16 2 1 5 7 3 4 0 6 3 2 1 5 6 [277] 6 2 9 6 2 6 5 7 9 10 9 5 2 4 5 0 0 3 3 6 4 6 3 [300] 9 6 3 3 8 5 0 7 9 4 2 7 3 5 7 4 0 2 6 8 1 4 5 [323] 2 1 2 2 4 6 5 13 2 0 10 4 4 4 9 7 2 5 3 4 4 4 4 [346] 3 9 2 5 0 9 0 4 5 2 7 5 1 4 5 5 5 4 2 0 2 9 9 [369] 1 0 6 3 7 8 0 2 6 5 2 1 2 1 8 3 6 6 7 17 3 7 6 [392] 3 8 9 1 8 5 10 0 7 4 6 8 3 8 0 9 8 3 1 0 0 7 4 [415] 8 5 4 4 5 5 4 4 0 3 3 5 1 9 3 2 3 3 8 4 9 7 0 [438] 5 1 0 10 7 6 5 7 4 5 2 13 1 6 7 3 4 5 7 0 5 4 4 [461] 2 6 4 3 2 2 5 5 5 0 1 0 8 2 0 2 0 2 8 11 0 2 3 [484] 5 7 6 4 3 6 4 7 10 12 7 11 7 2 8 6 2 4 4 5 5 3 8 [507] 3 0 0 5 4 5 1 4 0 0 6 3 3 3 6 2 3 2 3 4 11 3 5 [530] 7 2 6 6 2 4 6 1 13 5 3 7 4 3 3 4 3 8 3 5 1 5 3 [553] 0 11 5 7 2 1 0 6 4 5 5 4 3 8 4 4 1 4 2 7 9 14 10 [576] 0 6 2 1 8 0 2 5 2 2 1 5 0 10 6 5 0 1 1 4 4 4 0 [599] 0 4 8 3 3 8 7 5 7 1 2 3 3 9 5 11 8 8 6 4 3 2 0 [622] 2 6 9 1 0 12 3 2 12 9 6 3 6 0 4 5 4 1 13 3 0 2 5 [645] 3 1 8 5 9 5 5 9 5 5 7 6 5 6 6 3 0 6 7 13 7 9 5 [668] 6 7 5 2 6 1 3 0 3 4 3 3 0 10 0 7 0 6 3 9 4 7 12 [691] 0 0 7 9 0 7 6 6 3 2 5 2 0 5 5 7 4 5 5 3 6 2 7 [714] 1 2 1 6 4 11 9 2 0 8 10 2 7 2 6 1 8 0 0 4 3 9 5 [737] 2 3 1 7 4 0 0 0 6 3 13 3 5 5 0 4 2 3 6 4 5 0 1 [760] 4 9 12 4 0 10 5 9 4 3 10 6 2 3 5 3 9 3 6 1 5 7 0 [783] 4 5 3 4 1 0 1 3 10 17 0 7 5 2 0 9 0 1 5 8 0 6 9 [806] 2 0 9 10 0 3 3 4 6 5 5 1 1 12 10 4 0 5 4 6 3 7 2 [829] 1 0 0 4 0 10 5 8 9 8 1 3 2 6 1 12 2 0 5 2 9 3 4 [852] 7 5 3 10 6 6 9 0 4 7 4 1 6 0 6 5 8 3 3 0 4 9 8 [875] 2 8 3 11 7 1 5 4 7 6 8 3 1 5 6 6 0 5 20 5 4 13 2 [898] 1 0 2 1 3 4 10 4 5 5 1 9 0 1 0 0 11 3 12 4 6 7 4 [921] 0 3 3 5 6 11 0 1 4 10 2 5 8 1 0 7 6 2 5 4 0 2 1 [944] 6 5 10 3 0 0 3 12 3 4 10 0 5 3 3 9 0 1 3 17 1 8 14 [967] 6 3 3 2 8 0 6 8 1 0 11 10 10 18 0 7 9 2 5 1 7 5 5 [990] 5 6 4 3 7 8 2 10 5 0 10 5 5 0 5 3 1 4 4 3 2 6 1 [1013] 2 6 3 6 14 2 0 6 5 3 4 9 2 5 8 4 5 3 4 7 5 6 2 [1036] 3 4 7 8 11 8 6 9 4 7 5 14 7 2 12 0 5 3 6 1 4 0 1 [1059] 6 5 4 3 0 2 2 1 6 5 2 1 1 9 6 11 4 1 5 5 0 5 10 [1082] 0 5 2 2 4 5 7 1 4 5 2 10 0 0 5 0 2 3 3 6 2 8 3 [1105] 3 3 1 5 6 0 0 2 4 5 2 8 10 7 9 5 10 4 7 10 6 10 6 [1128] 3 0 0 7 6 8 4 4 4 0 8 7 7 1 6 6 10 8 4 4 5 11 7 [1151] 5 7 4 6 4 4 7 3 12 12 0 3 0 4 2 7 4 4 6 0 6 6 1 [1174] 9 5 6 11 6 15 3 2 3 6 4 5 4 1 3 0 3 0 2 6 0 1 7 [1197] 2 8 0 0 0 4 11 9 2 10 3 6 5 2 1 11 0 6 1 5 4 3 2 [1220] 2 6 7 1 11 6 10 5 4 7 10 4 0 4 4 0 2 8 7 5 4 6 4 [1243] 0 1 7 6 5 0 12 7 7 7 5 7 0 11 5 1 8 11 3 1 8 4 3 [1266] 3 7 2 8 2 4 0 7 5 0 1 0 9 7 1 7 4 7 8 3 1 6 3 [1289] 2 4 3 2 3 9 6 2 3 6 5 11 5 7 8 0 6 4 5 4 1 1 3 [1312] 5 2 0 6 3 3 0 6 6 0 7 11 0 3 3 7 5 2 0 3 5 2 0 [1335] 4 4 3 6 4 3 4 5 0 7 1 6 6 6 5 5 13 15 2 7 12 3 2 [1358] 6 5 5 5 9 7 0 5 7 4 8 2 0 2 2 7 8 2 4 0 7 9 4 [1381] 4 6 2 8 4 0 1 10 4 4 5 3 4 6 4 8 2 6 12 5 5 5 4 [1404] 5 11 4 4 7 5 6 7 2 3 6 3 3 2 5 2 4 1 2 2 7 7 4 [1427] 13 4 12 4 7 7 4 2 0 7 6 7 6 7 4 5 5 8 5 2 5 1 3 [1450] 0 10 2 0 5 3 3 11 1 0 3 1 5 0 2 0 3 2 0 6 0 3 2 [1473] 4 11 0 0 1 4 1 3 2 1 5 0 2 3 2 3 10 3 4 2 0 0 9 [1496] 4 8 4 0 2 5 5 0 0 2 0 8 3 0 9 4 3 2 2 4 1 0 6 [1519] 2 2 2 0 3 0 0 6 0 0 4 2 4 12 2 8 6 2 1 0 6 3 4 [1542] 5 0 6 1 1 2 5 1 3 4 11 9 0 6 2 7 5 3 7 5 0 2 7 [1565] 10 2 3 3 5 3 4 2 3 4 3 1 6 2 1 3 1 3 2 3 5 4 3 [1588] 0 1 0 5 1 3 9 1 6 4 1 1 7 2 0 1 7 3 0 5 2 5 2 [1611] 2 2 0 6 0 5 1 4 0 0 1 7 1 4 6 2 0 1 8 2 3 5 0 [1634] 7 7 7 4 7 7 3 5 6 1 6 4 16 5 8 7 3 1 7 3 10 4 7 [1657] 10 4 9 1 10 4 6 9 0 4 5 2 9 5 1 8 4 12 8 6 3 6 10 [1680] 2 3 1 1 13 4 2 5 7 10 7 10 5 4 3 8 4 3 6 5 2 9 6 [1703] 7 4 4 7 3 6 9 5 3 4 4 6 8 4 6 1 8 7 10 6 7 7 1 [1726] 9 6 15 10 10 9 8 6 5 8 5 6 7 4 8 9 8 8 9 10 6 5 6 [1749] 6 9 11 4 4 11 4 5 6 7 4 6 3 7 4 7 9 8 4 5 10 6 2 [1772] 0 8 5 5 6 3 7 4 0 2 4 1 0 3 6 10 3 14 3 10 0 5 0 [1795] 8 0 5 0 9 0 3 0 0 8 3 5 5 2 0 0 4 16 2 7 3 0 0 [1818] 0 7 6 0 2 5 5 0 4 5 10 9 3 5 9 4 1 4 4 2 0 3 2 [1841] 4 4 5 0 9 8 4 0 0 0 8 2 4 4 0 0 0 0 0 0  Problem 3.1 - Building a model We want to combine dtmTitle and dtmAbstract into a single dataframe to make predictions. However, some of the variables in these dataframes have the same names. To fix this issue, run the following code:\ncolnames(dtmTitle) \u0026lt;- paste0(\u0026quot;T\u0026quot;, colnames(dtmTitle)) str(dtmTitle) \u0026#39;data.frame\u0026#39;: 1860 obs. of 31 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... colnames(dtmAbstract) \u0026lt;- paste0(\u0026quot;A\u0026quot;, colnames(dtmAbstract)) str(dtmAbstract) \u0026#39;data.frame\u0026#39;: 1860 obs. of 335 variables: $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... $ Areceiv : num 0 0 2 0 1 1 3 0 0 2 ... $ Areduct : num 0 0 1 2 0 0 0 1 0 0 ... $ Aregimen : num 0 0 2 0 0 0 1 0 0 0 ... $ Arespond : num 0 0 2 0 0 0 0 0 0 0 ... $ Arespons : num 0 0 7 0 4 2 2 0 0 0 ... $ Aresult : num 0 0 1 0 1 0 1 0 0 0 ... $ Asimilar : num 0 0 2 0 0 0 0 0 1 0 ... $ Asize : num 0 0 1 3 0 1 0 0 0 0 ... $ Astatist : num 0 0 1 3 0 0 0 0 0 0 ... $ Asurgeri : num 0 0 1 1 0 0 0 0 0 0 ... $ Atoler : num 0 0 1 0 0 0 1 0 0 0 ... $ Atoxic : num 0 0 2 0 1 0 1 0 0 5 ... $ Atreatment : num 0 0 3 6 14 0 0 4 1 1 ... $ Atumor : num 0 0 2 4 0 0 2 0 0 0 ... $ Atwo : num 0 0 3 0 1 0 0 1 0 0 ... $ A001 : num 0 0 0 1 0 0 0 1 0 0 ... $ Aadjuv : num 0 0 0 2 0 1 0 2 4 0 ... $ Aage : num 0 0 0 1 0 0 0 0 0 0 ... $ Aalso : num 0 0 0 1 0 0 0 0 2 0 ... $ Aanalysi : num 0 0 0 2 0 0 0 1 0 0 ... $ Aanalyz : num 0 0 0 1 0 0 0 0 0 0 ... $ Aaxillari : num 0 0 0 1 0 0 0 0 1 0 ... $ Adeath : num 0 0 0 2 0 0 0 0 1 0 ... $ Adfs : num 0 0 0 3 0 0 0 0 0 0 ... $ Adiseasefre : num 0 0 0 1 0 2 0 0 3 0 ... $ Adrug : num 0 0 0 1 0 0 0 0 0 0 ... $ Aelig : num 0 0 0 1 0 0 0 0 0 0 ... $ Aendpoint : num 0 0 0 2 0 0 0 0 1 3 ... $ Aepirubicin : num 0 0 0 1 1 0 0 0 0 0 ... $ Aestim : num 0 0 0 1 1 0 0 0 0 0 ... $ Afluorouracil : num 0 0 0 1 1 0 0 0 0 0 ... [list output truncated] What was the effect of these functions? #### Adding the letter T in front of all the title variable names and adding the letter A in front of all the abstract variable names.\n Problem 3.2 - Building a Model Using cbind(), combine dtmTitle and dtmAbstract into a single dataframe called dtm:\ndtm \u0026lt;- cbind(dtmTitle, dtmAbstract) str(dtm) \u0026#39;data.frame\u0026#39;: 1860 obs. of 366 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... [list output truncated] Now, add the dependent variable “trial” to dtm, copying it from the original dataframe called trials.\nHow many columns are in this combined dataframe?\ndtm$trial \u0026lt;- trials$trial str(dtm) \u0026#39;data.frame\u0026#39;: 1860 obs. of 367 variables: $ Tcancer : num 1 0 1 1 1 1 0 1 1 2 ... $ Ttreatment : num 1 0 0 0 1 0 0 0 0 1 ... $ Tbreast : num 0 0 1 1 1 1 0 1 1 1 ... $ Tearli : num 0 0 1 1 0 0 0 1 0 0 ... $ Tiii : num 0 0 1 0 0 0 0 0 0 1 ... $ Tphase : num 0 0 1 1 0 0 0 0 0 1 ... $ Trandom : num 0 0 1 1 1 0 0 0 0 1 ... $ Ttrial : num 0 0 1 1 1 0 0 1 1 1 ... $ Tversus : num 0 0 1 0 0 0 0 1 0 0 ... $ Tcyclophosphamid: num 0 0 0 1 0 0 0 0 0 0 ... $ Tchemotherapi : num 0 0 0 0 1 1 0 0 0 0 ... $ Tcombin : num 0 0 0 0 1 0 1 0 0 0 ... $ Teffect : num 0 0 0 0 1 0 0 1 0 1 ... $ Tmetastat : num 0 0 0 0 1 0 0 0 0 0 ... $ Tpatient : num 0 0 0 0 1 0 1 0 1 1 ... $ Trespons : num 0 0 0 0 0 1 0 0 0 0 ... $ Tadvanc : num 0 0 0 0 0 0 1 0 0 0 ... $ Tpostmenopaus : num 0 0 0 0 0 0 0 1 1 0 ... $ Trandomis : num 0 0 0 0 0 0 0 1 1 0 ... $ Tstudi : num 0 0 0 0 0 0 0 1 0 0 ... $ Ttamoxifen : num 0 0 0 0 0 0 0 2 1 0 ... $ Twomen : num 0 0 0 0 0 0 0 1 0 0 ... $ Tadjuv : num 0 0 0 0 0 0 0 0 1 0 ... $ Tgroup : num 0 0 0 0 0 0 0 0 1 1 ... $ Ttherapi : num 0 0 0 0 0 0 0 0 0 0 ... $ Tcompar : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdoxorubicin : num 0 0 0 0 0 0 0 0 0 0 ... $ Tdocetaxel : num 0 0 0 0 0 0 0 0 0 0 ... $ Tresult : num 0 0 0 0 0 0 0 0 0 0 ... $ Tplus : num 0 0 0 0 0 0 0 0 0 0 ... $ Tclinic : num 0 0 0 0 0 0 0 0 0 0 ... $ A100 : num 0 1 0 0 0 0 0 0 0 0 ... $ Aactiv : num 0 2 0 1 0 0 1 0 0 0 ... $ Aassess : num 0 2 1 2 0 1 0 0 0 3 ... $ Abreast : num 0 2 3 3 3 4 2 2 2 3 ... $ Acarcinoma : num 0 5 0 0 0 0 0 0 0 2 ... $ Acase : num 0 11 0 0 1 0 0 0 0 0 ... $ Acell : num 0 3 0 0 0 1 0 0 0 0 ... $ Aclinic : num 0 1 0 1 0 0 0 0 0 0 ... $ Aconsist : num 0 1 0 0 0 0 0 0 0 0 ... $ Acontrol : num 0 1 0 0 0 0 0 0 1 0 ... $ Adiffer : num 0 1 2 1 3 0 0 1 0 1 ... $ Adiseas : num 0 1 0 1 3 0 0 1 0 0 ... $ Aless : num 0 4 1 0 0 0 0 0 0 6 ... $ Anegat : num 0 4 0 0 0 3 0 0 0 0 ... $ Aobserv : num 0 2 1 0 1 0 0 0 0 0 ... $ Aone : num 0 1 0 0 0 0 0 0 0 0 ... $ Apatient : num 0 1 9 5 5 6 8 3 2 5 ... $ Apopul : num 0 8 0 0 0 0 0 0 0 0 ... $ Aposit : num 0 2 0 1 0 5 0 0 0 0 ... $ Aseven : num 0 1 0 0 0 0 0 0 0 0 ... $ Ashow : num 0 2 0 0 1 0 1 0 0 3 ... $ Astudi : num 0 1 1 1 0 1 3 2 0 1 ... $ Atest : num 0 2 1 1 0 0 0 0 0 0 ... $ Atherapi : num 0 2 0 1 0 0 0 0 0 0 ... $ A500 : num 0 0 1 0 2 0 0 0 0 0 ... $ Aaddit : num 0 0 2 0 0 0 0 0 0 0 ... $ Aamong : num 0 0 2 4 0 0 0 0 1 0 ... $ Aarm : num 0 0 7 4 2 0 0 1 0 1 ... $ Aassign : num 0 0 2 1 0 0 0 1 1 1 ... $ Aassoci : num 0 0 1 3 0 2 0 1 2 0 ... $ Abackground : num 0 0 1 1 1 0 0 1 1 0 ... $ Abetter : num 0 0 1 0 1 1 0 0 0 0 ... $ Acancer : num 0 0 2 3 3 3 2 2 3 0 ... $ Achang : num 0 0 1 0 0 0 0 0 0 0 ... $ Achemotherapi : num 0 0 1 2 3 5 2 0 1 0 ... $ Acompar : num 0 0 1 2 0 0 0 1 1 1 ... $ Acomplet : num 0 0 3 0 1 1 1 2 0 0 ... $ Aconclus : num 0 0 1 0 1 0 0 0 0 0 ... $ Aconfid : num 0 0 1 1 0 0 0 0 0 0 ... $ Acontinu : num 0 0 2 0 1 0 0 2 0 0 ... $ Acycl : num 0 0 6 0 2 0 1 0 0 0 ... $ Acyclophosphamid: num 0 0 1 1 1 1 3 0 0 0 ... $ Aday : num 0 0 1 0 0 0 3 0 0 0 ... $ Adecreas : num 0 0 1 0 0 0 0 0 1 0 ... $ Adefin : num 0 0 2 0 0 0 0 0 0 0 ... $ Ademonstr : num 0 0 1 0 0 0 0 0 0 0 ... $ Adocetaxel : num 0 0 1 0 0 0 3 0 0 0 ... $ Adoxorubicin : num 0 0 1 0 0 1 0 0 0 0 ... $ Aeffect : num 0 0 2 0 0 1 0 2 0 1 ... $ Aefficaci : num 0 0 1 0 0 0 0 0 0 0 ... $ Aenrol : num 0 0 1 0 0 0 1 0 0 1 ... $ Afour : num 0 0 4 0 0 0 0 0 0 0 ... $ Ahematolog : num 0 0 1 0 0 0 0 0 0 0 ... $ Ainiti : num 0 0 3 0 0 0 0 1 0 0 ... $ Ainterv : num 0 0 1 1 0 0 0 0 0 0 ... $ Aleast : num 0 0 2 0 0 0 0 0 0 0 ... $ Alymph : num 0 0 1 4 0 3 0 0 1 0 ... $ Amethod : num 0 0 1 0 1 0 0 1 1 0 ... $ Amgm2 : num 0 0 5 0 4 0 9 0 0 0 ... $ Aneoadjuv : num 0 0 2 0 0 0 0 0 0 0 ... $ Anode : num 0 0 1 3 0 0 0 0 1 0 ... $ Anumber : num 0 0 1 1 0 2 1 0 0 0 ... $ Aoutcom : num 0 0 2 0 0 0 0 0 0 2 ... $ Apatholog : num 0 0 3 0 0 1 0 0 0 0 ... $ Aper : num 0 0 1 0 0 0 0 0 0 0 ... $ Aprevious : num 0 0 1 0 1 0 2 0 0 0 ... $ Arandom : num 0 0 2 1 1 0 0 1 1 2 ... $ Arate : num 0 0 1 1 2 0 1 0 1 0 ... [list output truncated] 367   Problem 3.3 - Building a Model Now that we have prepared our dataframe, it’s time to split it into a training and testing set and to build regression models. Set the random seed to 144 and use the sample.split function from the caTools package to split dtm into dataframes named “train” and “test”, putting 70% of the data in the training set.\nset.seed(144) trialSplit \u0026lt;- sample.split(dtm$trial, 0.7) train \u0026lt;- subset(dtm, trialSplit == TRUE) test \u0026lt;- subset(dtm, trialSplit == FALSE) What is the accuracy of the baseline model on the training set? (Remember that the baseline model predicts the most frequent outcome in the training set for all observations.)\ntable(train$trial)  0 1 730 572  730 / (730 + 572) [1] 0.5606759  Problem 3.4 - Building a Model Build a CART model called trialCART, using all the independent variables in the training set to train the model, and then plot the CART model. Just use the default parameters to build the model (don’t add a minbucket or cp value). Remember to add the method=“class” argument, since this is a classification problem.\ntrialCART \u0026lt;- rpart(trial ~ ., data = train, method = \u0026quot;class\u0026quot;) What is the name of the first variable the model split on?\nprp(trialCART) Tphase   Problem 3.5 - Building a Model Obtain the training set predictions for the model (do not yet predict on the test-set). Extract the predicted probability of a result being a trial (recall that this involves not setting a type argument, and keeping only the second column of the predict output).\nWhat is the maximum predicted probability for any result?\npredTrain \u0026lt;- predict(trialCART) predTrain[1:10,]  0 1 1 0.8636364 0.13636364 2 0.8636364 0.13636364 3 0.1281139 0.87188612 5 0.2176871 0.78231293 6 0.9454545 0.05454545 7 0.2176871 0.78231293 10 0.1281139 0.87188612 12 0.7125000 0.28750000 13 0.1281139 0.87188612 14 0.7125000 0.28750000 predTrainProb \u0026lt;- predTrain[, 2] max(predTrainProb) [1] 0.8718861 summary(predTrainProb)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.05455 0.13636 0.28750 0.43932 0.78231 0.87189  Problem 3.6 - Building a Model Without running the analysis, how do you expect the maximum predicted probability to differ in the testing set? #### The maximum predicted probability will likely be exactly the same in the testing set. Because the CART tree assigns the same predicted probability to each leaf node and there are a small number of leaf nodes compared to data points, we expect exactly the same maximum predicted probability.\n  Problem 3.7 - Building a Model For these questions, use a threshold probability of 0.5 to predict that an observation is a clinical trial.\ntable(train$trial, predTrainProb \u0026gt;= 0.5)  FALSE TRUE 0 631 99 1 131 441 What is the training set accuracy of the CART model?\n(631 + 441) / nrow(train) [1] 0.8233487 What is the training set sensitivity of the CART model?\n441 / (441 + 131) [1] 0.770979 What is the training set specificity of the CART model?\n631 / (631 + 99) [1] 0.8643836  Problem 4.1 - Evaluating the model on the testing set Evaluate the CART model on the testing set using the predict function and creating a vector of predicted probabilities predTest.\npred \u0026lt;- predict(trialCART, newdata = test) pred[1:10,]  0 1 4 0.1281139 0.8718861 8 0.8636364 0.1363636 9 0.7125000 0.2875000 11 0.7125000 0.2875000 19 0.7125000 0.2875000 31 0.8636364 0.1363636 40 0.8636364 0.1363636 42 0.8636364 0.1363636 43 0.8636364 0.1363636 48 0.7125000 0.2875000 predTest \u0026lt;- pred[, 2] What is the testing set accuracy, assuming a probability threshold of 0.5 for predicting that a result is a clinical trial?\ntable(test$trial, predTest \u0026gt;= 0.5)  FALSE TRUE 0 261 52 1 83 162 (261 + 162) / nrow(test) [1] 0.7580645  Problem 4.2 - Evaluating the Model on the Testing Set Using the ROCR package, what is the testing set AUC of the prediction model?\npredROCR \u0026lt;- prediction(predTest, test$trial) performance(predROCR, \u0026quot;auc\u0026quot;)@y.values [[1]] [1] 0.8371063  Problem 5.1 - Decision-Maker Tradeoffs What is the cost associated with the model in Step 1 making a false negative prediction? #### A paper that should have been included in Set A will be missed, affecting the quality of the results of Step 3.\n Problem 5.2 - Decision-Maker Tradeoffs What is the cost associated with the model in Step 1 making a false positive prediction? #### A paper will be mistakenly added to Set A, yielding additional work in Step 2 of the process but not affecting the quality of the results of Step 3.\n Problem 5.3 - Decision-Maker Tradeoffs Given the costs associated with false positives and false negatives, which of the following is most accurate? #### A false negative is more costly than a false positive; the decision maker should use a probability threshold less than 0.5 for the machine learning model.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"538e0fdefd2e3a8dcae36f46dbac43e1","permalink":"/project/medicine/medicine/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/medicine/medicine/","section":"project","summary":"Automate the process of information retrieval","tags":["R","Data Analytics","Machine Learning"],"title":"Automating Reviews In Medicine","type":"project"},{"authors":null,"categories":null,"content":" The United States government periodically collects demographic information by conducting a census.\nIn this analysis, I am are going to use census information about individuals to predict how much a person earns – in particular, whether the person earns more than $50,000 per year. This data comes from the UCI Machine Learning Repository.\nThe file census.csv contains 1994 census data for 31,978 individuals in the U.S. The dataset includes the following 13 variables:\n age = the age of the individual in years workclass = the classification of the individual’s working status (does the person work for the federal government, work for the local government, work without pay, and so on) education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on) maritalstatus = the marital status of the individual occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on) relationship = relationship of individual to his/her household race = the individual’s race sex = the individual’s sex capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price) capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price) hoursperweek = the number of hours the individual works per week nativecountry = the native country of the individual over50k = whether or not the individual earned more than $50,000 in 1994  Problem 1.1 - A Logistic Regression Model Let’s begin by building a logistic regression model to predict whether an individual’s earnings are above $50,000 (the variable “over50k”) using all of the other variables as independent variables.\nFirst, read the dataset census.csv into R.\ncensus \u0026lt;- read.csv(\u0026quot;census.csv\u0026quot;) str(census) \u0026#39;data.frame\u0026#39;: 31978 obs. of 13 variables: $ age : int 39 50 38 53 28 37 49 52 31 42 ... $ workclass : Factor w/ 9 levels \u0026quot; ?\u0026quot;,\u0026quot; Federal-gov\u0026quot;,..: 8 7 5 5 5 5 5 7 5 5 ... $ education : Factor w/ 16 levels \u0026quot; 10th\u0026quot;,\u0026quot; 11th\u0026quot;,..: 10 10 12 2 10 13 7 12 13 10 ... $ maritalstatus: Factor w/ 7 levels \u0026quot; Divorced\u0026quot;,\u0026quot; Married-AF-spouse\u0026quot;,..: 5 3 1 3 3 3 4 3 5 3 ... $ occupation : Factor w/ 15 levels \u0026quot; ?\u0026quot;,\u0026quot; Adm-clerical\u0026quot;,..: 2 5 7 7 11 5 9 5 11 5 ... $ relationship : Factor w/ 6 levels \u0026quot; Husband\u0026quot;,\u0026quot; Not-in-family\u0026quot;,..: 2 1 2 1 6 6 2 1 2 1 ... $ race : Factor w/ 5 levels \u0026quot; Amer-Indian-Eskimo\u0026quot;,..: 5 5 5 3 3 5 3 5 5 5 ... $ sex : Factor w/ 2 levels \u0026quot; Female\u0026quot;,\u0026quot; Male\u0026quot;: 2 2 2 2 1 1 1 2 1 2 ... $ capitalgain : int 2174 0 0 0 0 0 0 0 14084 5178 ... $ capitalloss : int 0 0 0 0 0 0 0 0 0 0 ... $ hoursperweek : int 40 13 40 40 40 40 16 45 50 40 ... $ nativecountry: Factor w/ 41 levels \u0026quot; Cambodia\u0026quot;,\u0026quot; Canada\u0026quot;,..: 39 39 39 39 5 39 23 39 39 39 ... $ over50k : Factor w/ 2 levels \u0026quot; \u0026lt;=50K\u0026quot;,\u0026quot; \u0026gt;50K\u0026quot;: 1 1 1 1 1 1 1 2 2 2 ... summary(census)  age workclass education Min. :17.00 Private :22286 HS-grad :10368 1st Qu.:28.00 Self-emp-not-inc: 2499 Some-college: 7187 Median :37.00 Local-gov : 2067 Bachelors : 5210 Mean :38.58 ? : 1809 Masters : 1674 3rd Qu.:48.00 State-gov : 1279 Assoc-voc : 1366 Max. :90.00 Self-emp-inc : 1074 11th : 1167 (Other) : 964 (Other) : 5006 maritalstatus occupation Divorced : 4394 Prof-specialty :4038 Married-AF-spouse : 23 Craft-repair :4030 Married-civ-spouse :14692 Exec-managerial:3992 Married-spouse-absent: 397 Adm-clerical :3721 Never-married :10488 Sales :3584 Separated : 1005 Other-service :3212 Widowed : 979 (Other) :9401 relationship race sex Husband :12947 Amer-Indian-Eskimo: 311 Female:10608 Not-in-family : 8156 Asian-Pac-Islander: 956 Male :21370 Other-relative: 952 Black : 3028 Own-child : 5005 Other : 253 Unmarried : 3384 White :27430 Wife : 1534 capitalgain capitalloss hoursperweek nativecountry Min. : 0 Min. : 0.00 Min. : 1.00 United-States:29170 1st Qu.: 0 1st Qu.: 0.00 1st Qu.:40.00 Mexico : 643 Median : 0 Median : 0.00 Median :40.00 Philippines : 198 Mean : 1064 Mean : 86.74 Mean :40.42 Germany : 137 3rd Qu.: 0 3rd Qu.: 0.00 3rd Qu.:45.00 Canada : 121 Max. :99999 Max. :4356.00 Max. :99.00 Puerto-Rico : 114 (Other) : 1595 over50k \u0026lt;=50K:24283 \u0026gt;50K : 7695  Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60% of the observations, while the testing set contains 40% of the observations.\nlibrary(caTools) set.seed(2000) censusSplit = sample.split(census$over50k, SplitRatio = 0.6) censusTrain = subset(census, censusSplit == TRUE) censusTest = subset(census, censusSplit == FALSE) nrow(censusTrain) [1] 19187 nrow(censusTest) [1] 12791 Next, build a logistic regression model to predict the dependent variable “over50k”, using all of the other variables in the dataset as independent variables. Use the training set to build the model.\nWhich variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.)\nCensusLog \u0026lt;- glm(over50k ~ ., data = censusTrain, family = binomial) Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(CensusLog)  Call: glm(formula = over50k ~ ., family = binomial, data = censusTrain) Deviance Residuals: Min 1Q Median 3Q Max -5.1065 -0.5037 -0.1804 -0.0008 3.3383 Coefficients: (1 not defined because of singularities) Estimate Std. Error z value (Intercept) -8.658e+00 1.379e+00 -6.279 age 2.548e-02 2.139e-03 11.916 workclass Federal-gov 1.105e+00 2.014e-01 5.489 workclass Local-gov 3.675e-01 1.821e-01 2.018 workclass Never-worked -1.283e+01 8.453e+02 -0.015 workclass Private 6.012e-01 1.626e-01 3.698 workclass Self-emp-inc 7.575e-01 1.950e-01 3.884 workclass Self-emp-not-inc 1.855e-01 1.774e-01 1.046 workclass State-gov 4.012e-01 1.961e-01 2.046 workclass Without-pay -1.395e+01 6.597e+02 -0.021 education 11th 2.225e-01 2.867e-01 0.776 education 12th 6.380e-01 3.597e-01 1.774 education 1st-4th -7.075e-01 7.760e-01 -0.912 education 5th-6th -3.170e-01 4.880e-01 -0.650 education 7th-8th -3.498e-01 3.126e-01 -1.119 education 9th -1.258e-01 3.539e-01 -0.355 education Assoc-acdm 1.602e+00 2.427e-01 6.601 education Assoc-voc 1.541e+00 2.368e-01 6.506 education Bachelors 2.177e+00 2.218e-01 9.817 education Doctorate 2.761e+00 2.893e-01 9.544 education HS-grad 1.006e+00 2.169e-01 4.638 education Masters 2.421e+00 2.353e-01 10.289 education Preschool -2.237e+01 6.864e+02 -0.033 education Prof-school 2.938e+00 2.753e-01 10.672 education Some-college 1.365e+00 2.195e-01 6.219 maritalstatus Married-AF-spouse 2.540e+00 7.145e-01 3.555 maritalstatus Married-civ-spouse 2.458e+00 3.573e-01 6.880 maritalstatus Married-spouse-absent -9.486e-02 3.204e-01 -0.296 maritalstatus Never-married -4.515e-01 1.139e-01 -3.962 maritalstatus Separated 3.609e-02 1.984e-01 0.182 maritalstatus Widowed 1.858e-01 1.962e-01 0.947 occupation Adm-clerical 9.470e-02 1.288e-01 0.735 occupation Armed-Forces -1.008e+00 1.487e+00 -0.677 occupation Craft-repair 2.174e-01 1.109e-01 1.960 occupation Exec-managerial 9.400e-01 1.138e-01 8.257 occupation Farming-fishing -1.068e+00 1.908e-01 -5.599 occupation Handlers-cleaners -6.237e-01 1.946e-01 -3.204 occupation Machine-op-inspct -1.862e-01 1.376e-01 -1.353 occupation Other-service -8.183e-01 1.641e-01 -4.987 occupation Priv-house-serv -1.297e+01 2.267e+02 -0.057 occupation Prof-specialty 6.331e-01 1.222e-01 5.180 occupation Protective-serv 6.267e-01 1.710e-01 3.664 occupation Sales 3.276e-01 1.175e-01 2.789 occupation Tech-support 6.173e-01 1.533e-01 4.028 occupation Transport-moving NA NA NA relationship Not-in-family 7.881e-01 3.530e-01 2.233 relationship Other-relative -2.194e-01 3.137e-01 -0.699 relationship Own-child -7.489e-01 3.507e-01 -2.136 relationship Unmarried 7.041e-01 3.720e-01 1.893 relationship Wife 1.324e+00 1.331e-01 9.942 race Asian-Pac-Islander 4.830e-01 3.548e-01 1.361 race Black 3.644e-01 2.882e-01 1.265 race Other 2.204e-01 4.513e-01 0.488 race White 4.108e-01 2.737e-01 1.501 sex Male 7.729e-01 1.024e-01 7.545 capitalgain 3.280e-04 1.372e-05 23.904 capitalloss 6.445e-04 4.854e-05 13.277 hoursperweek 2.897e-02 2.101e-03 13.791 nativecountry Canada 2.593e-01 1.308e+00 0.198 nativecountry China -9.695e-01 1.327e+00 -0.730 nativecountry Columbia -1.954e+00 1.526e+00 -1.280 nativecountry Cuba 5.735e-02 1.323e+00 0.043 nativecountry Dominican-Republic -1.435e+01 3.092e+02 -0.046 nativecountry Ecuador -3.550e-02 1.477e+00 -0.024 nativecountry El-Salvador -6.095e-01 1.395e+00 -0.437 nativecountry England -6.707e-02 1.327e+00 -0.051 nativecountry France 5.301e-01 1.419e+00 0.374 nativecountry Germany 5.474e-02 1.306e+00 0.042 nativecountry Greece -2.646e+00 1.714e+00 -1.544 nativecountry Guatemala -1.293e+01 3.345e+02 -0.039 nativecountry Haiti -9.221e-01 1.615e+00 -0.571 nativecountry Holand-Netherlands -1.282e+01 2.400e+03 -0.005 nativecountry Honduras -9.584e-01 3.412e+00 -0.281 nativecountry Hong -2.362e-01 1.492e+00 -0.158 nativecountry Hungary 1.412e-01 1.555e+00 0.091 nativecountry India -8.218e-01 1.314e+00 -0.625 nativecountry Iran -3.299e-02 1.366e+00 -0.024 nativecountry Ireland 1.579e-01 1.473e+00 0.107 nativecountry Italy 6.100e-01 1.333e+00 0.458 nativecountry Jamaica -2.279e-01 1.387e+00 -0.164 nativecountry Japan 5.072e-01 1.375e+00 0.369 nativecountry Laos -6.831e-01 1.661e+00 -0.411 nativecountry Mexico -9.182e-01 1.303e+00 -0.705 nativecountry Nicaragua -1.987e-01 1.507e+00 -0.132 nativecountry Outlying-US(Guam-USVI-etc) -1.373e+01 8.502e+02 -0.016 nativecountry Peru -9.660e-01 1.678e+00 -0.576 nativecountry Philippines 4.393e-02 1.281e+00 0.034 nativecountry Poland 2.410e-01 1.383e+00 0.174 nativecountry Portugal 7.276e-01 1.477e+00 0.493 nativecountry Puerto-Rico -5.769e-01 1.357e+00 -0.425 nativecountry Scotland -1.188e+00 1.719e+00 -0.691 nativecountry South -8.183e-01 1.341e+00 -0.610 nativecountry Taiwan -2.590e-01 1.350e+00 -0.192 nativecountry Thailand -1.693e+00 1.737e+00 -0.975 nativecountry Trinadad\u0026amp;Tobago -1.346e+00 1.721e+00 -0.782 nativecountry United-States -8.594e-02 1.269e+00 -0.068 nativecountry Vietnam -1.008e+00 1.523e+00 -0.662 nativecountry Yugoslavia 1.402e+00 1.648e+00 0.851 Pr(\u0026gt;|z|) (Intercept) 3.41e-10 *** age \u0026lt; 2e-16 *** workclass Federal-gov 4.03e-08 *** workclass Local-gov 0.043641 * workclass Never-worked 0.987885 workclass Private 0.000218 *** workclass Self-emp-inc 0.000103 *** workclass Self-emp-not-inc 0.295646 workclass State-gov 0.040728 * workclass Without-pay 0.983134 education 11th 0.437738 education 12th 0.076064 . education 1st-4th 0.361897 education 5th-6th 0.516008 education 7th-8th 0.263152 education 9th 0.722228 education Assoc-acdm 4.10e-11 *** education Assoc-voc 7.74e-11 *** education Bachelors \u0026lt; 2e-16 *** education Doctorate \u0026lt; 2e-16 *** education HS-grad 3.52e-06 *** education Masters \u0026lt; 2e-16 *** education Preschool 0.973996 education Prof-school \u0026lt; 2e-16 *** education Some-college 5.00e-10 *** maritalstatus Married-AF-spouse 0.000378 *** maritalstatus Married-civ-spouse 6.00e-12 *** maritalstatus Married-spouse-absent 0.767155 maritalstatus Never-married 7.42e-05 *** maritalstatus Separated 0.855672 maritalstatus Widowed 0.343449 occupation Adm-clerical 0.462064 occupation Armed-Forces 0.498170 occupation Craft-repair 0.049972 * occupation Exec-managerial \u0026lt; 2e-16 *** occupation Farming-fishing 2.15e-08 *** occupation Handlers-cleaners 0.001353 ** occupation Machine-op-inspct 0.176061 occupation Other-service 6.14e-07 *** occupation Priv-house-serv 0.954385 occupation Prof-specialty 2.22e-07 *** occupation Protective-serv 0.000248 *** occupation Sales 0.005282 ** occupation Tech-support 5.63e-05 *** occupation Transport-moving NA relationship Not-in-family 0.025562 * relationship Other-relative 0.484263 relationship Own-child 0.032716 * relationship Unmarried 0.058392 . relationship Wife \u0026lt; 2e-16 *** race Asian-Pac-Islander 0.173504 race Black 0.206001 race Other 0.625263 race White 0.133356 sex Male 4.52e-14 *** capitalgain \u0026lt; 2e-16 *** capitalloss \u0026lt; 2e-16 *** hoursperweek \u0026lt; 2e-16 *** nativecountry Canada 0.842879 nativecountry China 0.465157 nativecountry Columbia 0.200470 nativecountry Cuba 0.965432 nativecountry Dominican-Republic 0.962972 nativecountry Ecuador 0.980829 nativecountry El-Salvador 0.662181 nativecountry England 0.959686 nativecountry France 0.708642 nativecountry Germany 0.966572 nativecountry Greece 0.122527 nativecountry Guatemala 0.969180 nativecountry Haiti 0.568105 nativecountry Holand-Netherlands 0.995736 nativecountry Honduras 0.778775 nativecountry Hong 0.874155 nativecountry Hungary 0.927653 nativecountry India 0.531661 nativecountry Iran 0.980736 nativecountry Ireland 0.914628 nativecountry Italy 0.647194 nativecountry Jamaica 0.869467 nativecountry Japan 0.712179 nativecountry Laos 0.680866 nativecountry Mexico 0.481103 nativecountry Nicaragua 0.895132 nativecountry Outlying-US(Guam-USVI-etc) 0.987115 nativecountry Peru 0.564797 nativecountry Philippines 0.972640 nativecountry Poland 0.861624 nativecountry Portugal 0.622327 nativecountry Puerto-Rico 0.670837 nativecountry Scotland 0.489616 nativecountry South 0.541809 nativecountry Taiwan 0.847878 nativecountry Thailand 0.329678 nativecountry Trinadad\u0026amp;Tobago 0.434105 nativecountry United-States 0.946020 nativecountry Vietnam 0.507799 nativecountry Yugoslavia 0.394874 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 21175 on 19186 degrees of freedom Residual deviance: 12104 on 19090 degrees of freedom AIC: 12298 Number of Fisher Scoring iterations: 15 age, workclass, education, maritalstatus, occupation, relationship, sex, capitalgain, capitalloss, housperweek   Problem 1.2 - A Logistic Regression Model What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)\npredictLog \u0026lt;- predict(CensusLog, newdata = censusTest, type = \u0026quot;response\u0026quot;) Warning in predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type == : prediction from a rank-deficient fit may be misleading table(censusTest$over50k, predictLog \u0026gt; 0.5)  FALSE TRUE \u0026lt;=50K 9051 662 \u0026gt;50K 1190 1888 (9051 + 1888) / nrow(censusTest) [1] 0.8552107  Problem 1.3 - A Logistic Regression Model What is the baseline accuracy for the testing set? table(censusTest$over50k)\ntable(censusTest$over50k)  \u0026lt;=50K \u0026gt;50K 9713 3078  9713 / nrow(censusTest) [1] 0.7593621 Problem 1.4 - A Logistic Regression Model What is the area-under-the-curve (AUC) for this model on the test-set?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess ROCRpredLog = prediction(predictLog, censusTest$over50k) as.numeric(performance(ROCRpredLog, \u0026quot;auc\u0026quot;)@y.values) [1] 0.9061598   Problem 2.1 - A CART Model I have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.\nLet’s now build a classification tree to predict “over50k”. Using the training set to build the model, and all of the other variables as independent variables.\nUsing the default parameters, so don’t set a value for minbucket or cp. Remember to specify method=“class” as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.\nHow many splits does the tree have in total?\nlibrary(rpart) library(rpart.plot) censusTree \u0026lt;- rpart(over50k ~ ., data = censusTrain, method=\u0026quot;class\u0026quot;) prp(censusTree) 4   Problem 2.2 - A CART Model Which variable does the tree split on at the first level (the very first split of the tree)? #### relationship\n Problem 2.3 - A CART Model Which variables does the tree split on at the second level (immediately after the first split of the tree)? #### capitalgain, education\n Problem 2.4 - A CART Model What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type=“class”, or generate probabilities and use a threshold of 0.5 like in logistic regression.)\npredictTree \u0026lt;- as.vector(predict(censusTree, newdata = censusTest, type = \u0026quot;class\u0026quot;)) head(predictTree) [1] \u0026quot; \u0026gt;50K\u0026quot; \u0026quot; \u0026gt;50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026lt;=50K\u0026quot; \u0026quot; \u0026gt;50K\u0026quot;  table(censusTest$over50k, predictTree)  predictTree \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 9243 470 \u0026gt;50K 1482 1596 (9243 + 1596) / nrow(censusTest) [1] 0.8473927 This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.\n Problem 2.5 - A CART Model Let’s now consider the ROC curve and AUC for the CART model on the test-set. I will need to get predicted probabilities for the observations in the test-set to build the ROC curve and compute the AUC. Remember that you can do this by removing the type=“class” argument when making predictions, and taking the second column of the resulting object.\npredictTreeProb \u0026lt;- predict(censusTree, newdata = censusTest) head(predictTreeProb)  \u0026lt;=50K \u0026gt;50K 2 0.2794982 0.72050176 5 0.2794982 0.72050176 7 0.9490143 0.05098572 8 0.6972807 0.30271934 11 0.6972807 0.30271934 12 0.2794982 0.72050176 head(predictTreeProb[, 2])  2 5 7 8 11 12 0.72050176 0.72050176 0.05098572 0.30271934 0.30271934 0.72050176  Plot the ROC curve for the CART model you have estimated. Observe that compared to the logistic regression ROC curve, the CART ROC curve is less smooth than the logistic regression ROC curve.\nWhich of the following explanations for this behavior is most correct? (HINT: Think about what the ROC curve is plotting and what changing the threshold does.)\nROCRpredTree = prediction(predictTreeProb[, 2], censusTest$over50k) ROCRperfTree = performance(ROCRpredTree, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;) plot(ROCRperfTree) The probabilities from the CART model take only a handful of values (five, one for each end bucket/leaf of the tree); the changes in the ROC curve correspond to setting the threshold to one of those values.   Problem 2.6 - A CART Model What is the AUC of the CART model on the test-set?\nas.numeric(performance(ROCRpredTree, \u0026quot;auc\u0026quot;)@y.values) [1] 0.8470256  Problem 3.1 - A Random Forest Model Before building a random forest model, I’ll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression.\nFor this reason, before continuing I’ll define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set.\nset.seed(1) trainSmall \u0026lt;- censusTrain[sample(nrow(censusTrain), 2000), ] Let’s now build a random forest model to predict “over50k”, using the dataset “trainSmall” as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest “cannot handle categorical predictors with more than 32 categories”, re-build the model without the nativecountry variable as one of the independent variables.)\nlibrary(randomForest) randomForest 4.6-14 Type rfNews() to see new features/changes/bug fixes. set.seed(1) CensusForest \u0026lt;- randomForest(over50k ~ ., data = trainSmall) Then, make predictions using this model on the entire test-set.\nWhat is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that you don’t need a “type” argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that our accuracy might be different from the one reported here, since random forest models can still differ depending on our operating system, even when the random seed is set.)\npredictForest \u0026lt;- predict(CensusForest, newdata = censusTest) table(censusTest$over50k, predictForest)  predictForest \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 8843 870 \u0026gt;50K 1029 2049 (9586 + 1093) / nrow(censusTest) [1] 0.8348839  Problem 3.2 - A Random Forest Model As we discussed, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important.\nHowever, we can still compute metrics that give us insight into which variables are important. One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split.\nTo view this metric, run the following lines of code (replace “MODEL” with the name of your random forest model):\nvu \u0026lt;- varUsed(CensusForest, count=TRUE) vusorted \u0026lt;- sort(vu, decreasing = FALSE, index.return = TRUE) dotchart(vusorted$x, names(CensusForest$forest$xlevels[vusorted$ix])) This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis).\nWhich of the following variables is the most important in terms of the number of splits? #### age\n Problem 3.3 - A Random Forest Model A different metric we can look at is related to “impurity”, which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased.\nTherefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest.\nTo compute this metric, run the following code (replace “MODEL” with the name of your random forest model):\nvarImpPlot(CensusForest) Which one of the following variables is the most important in terms of mean reduction in impurity? #### occupation\n Problem 4.1 - Selecting cp by Cross-Validation We now conclude our analysis of this dataset by looking at how CART behaves with different choices of its parameters. Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments.\nAlso, remember we using the entire training set “train” when building this model. The train function might take some time to run.\nWhich value of cp does the train function recommend?\nlibrary(caret) Loading required package: lattice Loading required package: ggplot2  Attaching package: \u0026#39;ggplot2\u0026#39; The following object is masked from \u0026#39;package:randomForest\u0026#39;: margin library(e1071) # number of folds tr.control = trainControl(method = \u0026quot;cv\u0026quot;, number = 10) # cp values cartGrid \u0026lt;- expand.grid( .cp = seq(0.002,0.1,0.002)) cartGrid  .cp 1 0.002 2 0.004 3 0.006 4 0.008 5 0.010 6 0.012 7 0.014 8 0.016 9 0.018 10 0.020 11 0.022 12 0.024 13 0.026 14 0.028 15 0.030 16 0.032 17 0.034 18 0.036 19 0.038 20 0.040 21 0.042 22 0.044 23 0.046 24 0.048 25 0.050 26 0.052 27 0.054 28 0.056 29 0.058 30 0.060 31 0.062 32 0.064 33 0.066 34 0.068 35 0.070 36 0.072 37 0.074 38 0.076 39 0.078 40 0.080 41 0.082 42 0.084 43 0.086 44 0.088 45 0.090 46 0.092 47 0.094 48 0.096 49 0.098 50 0.100 # Cross-validation set.seed(2) tr = train(over50k ~ ., data = censusTrain, method = \u0026quot;rpart\u0026quot;, trControl = tr.control, tuneGrid = cartGrid) tr CART 19187 samples 12 predictor 2 classes: \u0026#39; \u0026lt;=50K\u0026#39;, \u0026#39; \u0026gt;50K\u0026#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 17268, 17268, 17269, 17269, 17269, 17268, ... Resampling results across tuning parameters: cp Accuracy Kappa 0.002 0.8510972 0.55404931 0.004 0.8482829 0.55537475 0.006 0.8452078 0.53914084 0.008 0.8442176 0.53817486 0.010 0.8433317 0.53305978 0.012 0.8433317 0.53305978 0.014 0.8433317 0.53305978 0.016 0.8413510 0.52349296 0.018 0.8400480 0.51528594 0.020 0.8381193 0.50351272 0.022 0.8381193 0.50351272 0.024 0.8381193 0.50351272 0.026 0.8381193 0.50351272 0.028 0.8381193 0.50351272 0.030 0.8381193 0.50351272 0.032 0.8381193 0.50351272 0.034 0.8352011 0.48749911 0.036 0.8326470 0.47340390 0.038 0.8267570 0.44688035 0.040 0.8248289 0.43893150 0.042 0.8248289 0.43893150 0.044 0.8248289 0.43893150 0.046 0.8248289 0.43893150 0.048 0.8248289 0.43893150 0.050 0.8231084 0.42467058 0.052 0.8174798 0.37478096 0.054 0.8138837 0.33679015 0.056 0.8118514 0.30751485 0.058 0.8118514 0.30751485 0.060 0.8118514 0.30751485 0.062 0.8118514 0.30751485 0.064 0.8118514 0.30751485 0.066 0.8099233 0.29697206 0.068 0.7971025 0.22226318 0.070 0.7958512 0.21465656 0.072 0.7958512 0.21465656 0.074 0.7958512 0.21465656 0.076 0.7689601 0.05701508 0.078 0.7593684 0.00000000 0.080 0.7593684 0.00000000 0.082 0.7593684 0.00000000 0.084 0.7593684 0.00000000 0.086 0.7593684 0.00000000 0.088 0.7593684 0.00000000 0.090 0.7593684 0.00000000 0.092 0.7593684 0.00000000 0.094 0.7593684 0.00000000 0.096 0.7593684 0.00000000 0.098 0.7593684 0.00000000 0.100 0.7593684 0.00000000 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.002.  Problem 4.2 - Selecting cp by Cross-Validation Fit a CART model to the training data using this value of cp. What is the prediction accuracy on the test-set?\ncensusTree2 \u0026lt;- rpart(over50k ~ ., data = censusTrain, method=\u0026quot;class\u0026quot;, cp = 0.002) predictTree2 \u0026lt;- as.vector(predict(censusTree2, newdata = censusTest, type = \u0026quot;class\u0026quot;)) table(censusTest$over50k, predictTree2)  predictTree2 \u0026lt;=50K \u0026gt;50K \u0026lt;=50K 9178 535 \u0026gt;50K 1240 1838 (9178 + 1838) / nrow(censusTest) [1] 0.8612306  Problem 4.3 - Selecting cp by Cross-Validation Compared to the original accuracy using the default value of cp, this new CART model is an improvement, and so we should clearly favor this new model over the old one – or should we?\nPlot the CART tree for this model. How many splits are there?\nprp(censusTree2) 18   Conclusion This highlights one important trade-off in building predictive models. By tuning cp, we improved our accuracy by over 1%, but our tree became significantly more complicated. In some applications, such an improvement in accuracy would be worth the loss in interpretability. In others, we may prefer a less accurate model that is simpler to understand and describe over a more accurate – but more complicated – model.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"0071f1f170734f4498f4ebc12f22bda6","permalink":"/project/earnings/earnings/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/earnings/earnings/","section":"project","summary":"Predict how much a person earns","tags":["R","Data Analytics","Machine Learning"],"title":"Earnings Prediction From Census Data","type":"project"},{"authors":null,"categories":null,"content":" One of the earliest applications of predictive analytics methods applied so far was to automatically recognize letters, which post office machines use to sort mail. In this analysis, I’ll build a model that uses statistics of images of four letters in the Roman alphabet – A, B, P, and R – to predict which letter a particular image corresponds to.\nNote, that this is a multiclass classification problem. We have mostly focused on binary classification problems (e.g., predicting whether an individual voted or not, whether the Supreme Court will affirm or reverse a case, whether or not a person is at risk for a certain disease, etc.). In this problem, we have more than two classifications that are possible for each observation.\nThe file letters_ABPR.csv contains 3116 observations, each of which corresponds to a certain image of one of the four letters A, B, P and R. The images came from 20 different fonts, which were then randomly distorted to produce the final images; each such distorted image is represented as a collection of pixels, each of which is “on” or “off”.\nFor each such distorted image, we have available certain statistics of the image in terms of these pixels, as well as which of the four letters the image is. This data comes from the UCI ML Repository.\nThis dataset contains the following 17 variables:\n letter = the letter that the image corresponds to (A, B, P or R) xbox = the horizontal position of where the smallest box covering the letter shape begins. ybox = the vertical position of where the smallest box covering the letter shape begins. width = the width of this smallest box. height = the height of this smallest box. onpix = the total number of “on” pixels in the character image xbar = the mean horizontal position of all of the “on” pixels ybar = the mean vertical position of all of the “on” pixels x2bar = the mean squared horizontal position of all of the “on” pixels in the image y2bar = the mean squared vertical position of all of the “on” pixels in the image xybar = the mean of the product of the horizontal and vertical position of all of the “on” pixels in the image x2ybar = the mean of the product of the squared horizontal position and the vertical position of all of the “on” pixels xy2bar = the mean of the product of the horizontal position and the squared vertical position of all of the “on” pixels xedge = the mean number of edges (the number of times an “off” pixel is followed by an “on” pixel, or the image boundary is hit) as the image is scanned from left to right, along the whole vertical length of the image xedgeycor = the mean of the product of the number of horizontal edges at each vertical position and the vertical position yedge = the mean number of edges as the images is scanned from top to bottom, along the whole horizontal length of the image yedgexcor = the mean of the product of the number of vertical edges at each horizontal position and the horizontal position  Load the dataset letters \u0026lt;- read.csv(\u0026quot;letters_ABPR.csv\u0026quot;) summary(letters)  letter xbox ybox width A:789 Min. : 0.000 Min. : 0.000 Min. : 1.000 B:766 1st Qu.: 3.000 1st Qu.: 5.000 1st Qu.: 4.000 P:803 Median : 4.000 Median : 7.000 Median : 5.000 R:758 Mean : 3.915 Mean : 7.051 Mean : 5.186 3rd Qu.: 5.000 3rd Qu.: 9.000 3rd Qu.: 6.000 Max. :13.000 Max. :15.000 Max. :11.000 height onpix xbar ybar Min. : 0.000 Min. : 0.000 Min. : 3.000 Min. : 0.000 1st Qu.: 4.000 1st Qu.: 2.000 1st Qu.: 6.000 1st Qu.: 6.000 Median : 6.000 Median : 4.000 Median : 7.000 Median : 7.000 Mean : 5.276 Mean : 3.869 Mean : 7.469 Mean : 7.197 3rd Qu.: 7.000 3rd Qu.: 5.000 3rd Qu.: 8.000 3rd Qu.: 9.000 Max. :12.000 Max. :12.000 Max. :14.000 Max. :15.000 x2bar y2bar xybar x2ybar Min. : 0.000 Min. :0.000 Min. : 3.000 Min. : 0.00 1st Qu.: 3.000 1st Qu.:2.000 1st Qu.: 7.000 1st Qu.: 3.00 Median : 4.000 Median :4.000 Median : 8.000 Median : 5.00 Mean : 4.706 Mean :3.903 Mean : 8.491 Mean : 4.52 3rd Qu.: 6.000 3rd Qu.:5.000 3rd Qu.:10.000 3rd Qu.: 6.00 Max. :11.000 Max. :8.000 Max. :14.000 Max. :10.00 xy2bar xedge xedgeycor yedge Min. : 0.000 Min. : 0.000 Min. : 1.000 Min. : 0.0 1st Qu.: 6.000 1st Qu.: 2.000 1st Qu.: 7.000 1st Qu.: 3.0 Median : 7.000 Median : 2.000 Median : 8.000 Median : 4.0 Mean : 6.711 Mean : 2.913 Mean : 7.763 Mean : 4.6 3rd Qu.: 8.000 3rd Qu.: 4.000 3rd Qu.: 9.000 3rd Qu.: 6.0 Max. :14.000 Max. :10.000 Max. :13.000 Max. :12.0 yedgexcor Min. : 1.000 1st Qu.: 7.000 Median : 8.000 Mean : 8.418 3rd Qu.:10.000 Max. :13.000  str(letters) \u0026#39;data.frame\u0026#39;: 3116 obs. of 17 variables: $ letter : Factor w/ 4 levels \u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,\u0026quot;P\u0026quot;,\u0026quot;R\u0026quot;: 2 1 4 2 3 4 4 1 3 3 ... $ xbox : int 4 1 5 5 3 8 2 3 8 6 ... $ ybox : int 2 1 9 9 6 10 6 7 14 10 ... $ width : int 5 3 5 7 4 8 4 5 7 8 ... $ height : int 4 2 7 7 4 6 4 5 8 8 ... $ onpix : int 4 1 6 10 2 6 3 3 4 7 ... $ xbar : int 8 8 6 9 4 7 6 12 5 8 ... $ ybar : int 7 2 11 8 14 7 7 2 10 5 ... $ x2bar : int 6 2 7 4 8 3 5 3 6 7 ... $ y2bar : int 6 2 3 4 1 5 5 2 3 5 ... $ xybar : int 7 8 7 6 11 8 6 10 12 7 ... $ x2ybar : int 6 2 3 8 6 4 5 2 5 6 ... $ xy2bar : int 6 8 9 6 3 8 7 9 4 6 ... $ xedge : int 2 1 2 6 0 6 3 2 4 3 ... $ xedgeycor: int 8 6 7 11 10 6 7 6 10 9 ... $ yedge : int 7 2 5 8 4 7 5 3 4 8 ... $ yedgexcor: int 10 7 11 7 8 7 8 8 8 9 ...  Problem 1.1 - Predicting B or not B Let’s warm up by attempting to predict just whether a letter is B or not. To begin, load the file letters_ABPR.csv into R, and call it letters. Then, create a new variable isB in the dataframe, which takes the value “TRUE” if the observation corresponds to the letter B, and “FALSE” if it does not.\nletters$isB \u0026lt;- as.factor(letters$letter == \u0026quot;B\u0026quot;) Now, split the dataset into a training and testing set, putting 50% of the data in the training set. Set the seed to 1000 before making the split. The first argument to sample.split should be the dependent variable “letters$isB”. Remember that TRUE values from sample.split should go in the training set.\nlibrary(caTools) set.seed(1000) lettersSplit = sample.split(letters$isB, SplitRatio = 0.5) lettersTrain = subset(letters, lettersSplit == TRUE) lettersTest = subset(letters, lettersSplit == FALSE) Before building models, let’s consider a baseline method that always predicts the most frequent outcome, which is “not B”.\nWhat is the accuracy of this baseline method on the test set?\ntable(letters$isB)  FALSE TRUE 2350 766  table(lettersTest$isB)  FALSE TRUE 1175 383  1175 / (1175 + 383) [1] 0.754172  Problem 1.2 - Predicting B or not B Now, build a classification tree to predict whether a letter is a B or not, using the training set to build the model. Remember to remove the variable “letter” out of the model, as this is related to what we are trying to predict!\nlibrary(rpart) library(rpart.plot) CARTb \u0026lt;- rpart(isB ~ . - letter, data = lettersTrain, method=\u0026quot;class\u0026quot;) We are just using the default parameters in our CART model, so we don’t need to add the minbucket or cp arguments at all. We also added the argument method=“class” since this is a classification problem.\nWhat is the accuracy of the CART model on the test-set? (Use type=“class” when making predictions on the test set.)\nbPredict \u0026lt;- predict(CARTb, newdata = lettersTest, type = \u0026quot;class\u0026quot;) table(lettersTest$isB, bPredict)  bPredict FALSE TRUE FALSE 1118 57 TRUE 43 340 (1126 + 342) / nrow(lettersTest) [1] 0.9422336  Problem 1.3 - Predicting B or Not B Now, build a random forest model to predict whether the letter is a B or not (the isB variable) using the training set. Using all of the other variables as independent variables, except letter (since it helped us define what we are trying to predict!). Using the default settings for ntree and nodesize (don’t include these arguments at all). Right before building the model, set the seed to 1000. (NOTE: You might get a slightly different answer on this problem, even if you set the random seed. This has to do with your operating system and the implementation of the random forest algorithm.)\nWhat is the accuracy of the model on the test set?\nbForestPredict \u0026lt;- predict(bForest, newdata = lettersTest, type = \u0026quot;class\u0026quot;) table(lettersTest$isB, bForestPredict)  bForestPredict FALSE TRUE FALSE 1163 12 TRUE 9 374 (1164 + 372) / nrow(lettersTest) [1] 0.9858793 Random forests tends to improve on CART in terms of predictive accuracy. Sometimes, this improvement can be quite significant, as it is here.\n Problem 2.1 - Predicting the letters A, B, P, R Let us now move on to the problem that we were originally interested in, which is to predict whether or not a letter is one of the four letters A, B, P or R.\nAs we saw earlier, building a multiclass classification CART model is no harder than building the models for binary classification problems. Fortunately, building a random forest model is just as easy. The variable in our dataframe which we will be trying to predict is “letter”.\nStart by converting letter in the original dataset (letters) to a factor by running the following code:\nletters$letter \u0026lt;- as.factor(letters$letter) Now, generate new training and testing sets of the letters dataframe using letters$letter as the first input to the sample.split function. Before splitting, set your seed to 2000. Again put 50% of the data in the training set. (Why do we need to split the data again? Remember that sample.split balances the outcome variable in the training and testing sets. With a new outcome variable, we want to re-generate our split.)\nset.seed(2000) lettersAllSplit \u0026lt;- sample.split(letters$letter, SplitRatio = 0.5) lettersAllTrain \u0026lt;- subset(letters, lettersAllSplit == TRUE) lettersAllTest \u0026lt;- subset(letters, lettersAllSplit == FALSE) In a multiclass classification problem, a simple baseline model is to predict the most frequent class of all of the options.\nWhat is the baseline accuracy on the testing set?\ntable(letters$letter)  A B P R 789 766 803 758  table(lettersAllTest$letter)  A B P R 395 383 401 379  401 / nrow(lettersAllTest) [1] 0.2573813 P is the most frequent class in the test set   Problem 2.2 - Predicting the letters A, B, P, R Now build a classification tree to predict “letter”, using the training set to build our model. You should use all of the other variables as independent variables, except “isB”, since it is related to what we are trying to predict!\nJust use the default parameters in your CART model. Add the argument method=“class” since this is a classification problem. Even though we have multiple classes here, nothing changes in how we build the model from the binary case.\nCARTletters \u0026lt;- rpart(letter ~ . - isB, data = lettersAllTrain, method=\u0026quot;class\u0026quot;) summary(CARTletters) Call: rpart(formula = letter ~ . - isB, data = lettersAllTrain, method = \u0026quot;class\u0026quot;) n= 1558 CP nsplit rel error xerror xstd 1 0.31920415 0 1.0000000 1.0346021 0.01442043 2 0.25865052 1 0.6807958 0.6323529 0.01704001 3 0.18685121 2 0.4221453 0.4238754 0.01585412 4 0.02595156 3 0.2352941 0.2370242 0.01299919 5 0.02076125 4 0.2093426 0.2162630 0.01253234 6 0.01730104 5 0.1885813 0.1980969 0.01209034 7 0.01384083 6 0.1712803 0.1894464 0.01186782 8 0.01211073 7 0.1574394 0.1678201 0.01127370 9 0.01000000 8 0.1453287 0.1608997 0.01107113 Variable importance ybar xedgeycor x2ybar xy2bar yedge y2bar xedge 17 16 14 12 11 8 7 xybar x2bar xbar 5 5 3 Node number 1: 1558 observations, complexity param=0.3192042 predicted class=P expected loss=0.7419769 P(node) =1 class counts: 394 383 402 379 probabilities: 0.253 0.246 0.258 0.243 left son=2 (1088 obs) right son=3 (470 obs) Primary splits: xedgeycor \u0026lt; 8.5 to the left, improve=293.2010, (0 missing) ybar \u0026lt; 5.5 to the left, improve=287.8322, (0 missing) xy2bar \u0026lt; 5.5 to the right, improve=278.1742, (0 missing) x2ybar \u0026lt; 2.5 to the left, improve=262.6356, (0 missing) yedge \u0026lt; 4.5 to the left, improve=177.0582, (0 missing) Surrogate splits: xy2bar \u0026lt; 5.5 to the right, agree=0.892, adj=0.643, (0 split) ybar \u0026lt; 8.5 to the left, agree=0.821, adj=0.406, (0 split) xedge \u0026lt; 1.5 to the right, agree=0.816, adj=0.391, (0 split) xybar \u0026lt; 10.5 to the left, agree=0.785, adj=0.287, (0 split) x2ybar \u0026lt; 6.5 to the left, agree=0.777, adj=0.262, (0 split) Node number 2: 1088 observations, complexity param=0.2586505 predicted class=A expected loss=0.6488971 P(node) =0.6983312 class counts: 382 338 13 355 probabilities: 0.351 0.311 0.012 0.326 left son=4 (344 obs) right son=5 (744 obs) Primary splits: ybar \u0026lt; 5.5 to the left, improve=275.7625, (0 missing) x2ybar \u0026lt; 2.5 to the left, improve=240.6702, (0 missing) y2bar \u0026lt; 2.5 to the left, improve=226.4519, (0 missing) yedge \u0026lt; 3.5 to the left, improve=215.2610, (0 missing) xedgeycor \u0026lt; 7.5 to the right, improve=171.4917, (0 missing) Surrogate splits: x2ybar \u0026lt; 2.5 to the left, agree=0.904, adj=0.698, (0 split) y2bar \u0026lt; 2.5 to the left, agree=0.892, adj=0.657, (0 split) yedge \u0026lt; 3.5 to the left, agree=0.881, adj=0.625, (0 split) x2bar \u0026lt; 2.5 to the left, agree=0.820, adj=0.430, (0 split) xbar \u0026lt; 9.5 to the right, agree=0.779, adj=0.302, (0 split) Node number 3: 470 observations, complexity param=0.01730104 predicted class=P expected loss=0.1723404 P(node) =0.3016688 class counts: 12 45 389 24 probabilities: 0.026 0.096 0.828 0.051 left son=6 (91 obs) right son=7 (379 obs) Primary splits: xybar \u0026lt; 7.5 to the left, improve=59.48719, (0 missing) xy2bar \u0026lt; 6.5 to the right, improve=54.86112, (0 missing) ybar \u0026lt; 7.5 to the left, improve=49.49367, (0 missing) yedge \u0026lt; 6.5 to the right, improve=48.42295, (0 missing) xedge \u0026lt; 5.5 to the left, improve=30.83057, (0 missing) Surrogate splits: xy2bar \u0026lt; 6.5 to the right, agree=0.936, adj=0.670, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.902, adj=0.495, (0 split) xedge \u0026lt; 5.5 to the right, agree=0.889, adj=0.429, (0 split) yedge \u0026lt; 6.5 to the right, agree=0.885, adj=0.407, (0 split) onpix \u0026lt; 6.5 to the right, agree=0.838, adj=0.165, (0 split) Node number 4: 344 observations predicted class=A expected loss=0.04360465 P(node) =0.2207959 class counts: 329 9 3 3 probabilities: 0.956 0.026 0.009 0.009 Node number 5: 744 observations, complexity param=0.1868512 predicted class=R expected loss=0.5268817 P(node) =0.4775353 class counts: 53 329 10 352 probabilities: 0.071 0.442 0.013 0.473 left son=10 (342 obs) right son=11 (402 obs) Primary splits: xedgeycor \u0026lt; 7.5 to the right, improve=139.70670, (0 missing) xy2bar \u0026lt; 7.5 to the left, improve= 92.43059, (0 missing) x2ybar \u0026lt; 5.5 to the right, improve= 81.07422, (0 missing) y2bar \u0026lt; 4.5 to the right, improve= 56.45671, (0 missing) yedgexcor \u0026lt; 10.5 to the left, improve= 52.58754, (0 missing) Surrogate splits: x2ybar \u0026lt; 5.5 to the right, agree=0.738, adj=0.430, (0 split) xy2bar \u0026lt; 6.5 to the left, agree=0.675, adj=0.292, (0 split) xedge \u0026lt; 2.5 to the left, agree=0.675, adj=0.292, (0 split) yedge \u0026lt; 5.5 to the right, agree=0.644, adj=0.225, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.625, adj=0.184, (0 split) Node number 6: 91 observations, complexity param=0.01384083 predicted class=B expected loss=0.5604396 P(node) =0.05840822 class counts: 10 40 20 21 probabilities: 0.110 0.440 0.220 0.231 left son=12 (55 obs) right son=13 (36 obs) Primary splits: x2bar \u0026lt; 3.5 to the right, improve=14.308240, (0 missing) xy2bar \u0026lt; 7.5 to the left, improve= 9.472092, (0 missing) yedge \u0026lt; 4.5 to the left, improve= 9.449763, (0 missing) x2ybar \u0026lt; 7.5 to the right, improve= 8.053076, (0 missing) yedgexcor \u0026lt; 6.5 to the right, improve= 7.478284, (0 missing) Surrogate splits: yedgexcor \u0026lt; 5.5 to the right, agree=0.736, adj=0.333, (0 split) x2ybar \u0026lt; 7.5 to the left, agree=0.725, adj=0.306, (0 split) yedge \u0026lt; 5.5 to the right, agree=0.725, adj=0.306, (0 split) xy2bar \u0026lt; 8.5 to the left, agree=0.714, adj=0.278, (0 split) ybar \u0026lt; 7.5 to the left, agree=0.681, adj=0.194, (0 split) Node number 7: 379 observations predicted class=P expected loss=0.02638522 P(node) =0.2432606 class counts: 2 5 369 3 probabilities: 0.005 0.013 0.974 0.008 Node number 10: 342 observations, complexity param=0.02595156 predicted class=B expected loss=0.2192982 P(node) =0.2195122 class counts: 14 267 10 51 probabilities: 0.041 0.781 0.029 0.149 left son=20 (283 obs) right son=21 (59 obs) Primary splits: xy2bar \u0026lt; 7.5 to the left, improve=48.65030, (0 missing) xedge \u0026lt; 2.5 to the left, improve=33.98799, (0 missing) y2bar \u0026lt; 4.5 to the right, improve=27.13499, (0 missing) yedgexcor \u0026lt; 6.5 to the left, improve=15.49245, (0 missing) ybar \u0026lt; 8.5 to the left, improve=15.03303, (0 missing) Surrogate splits: xedge \u0026lt; 5.5 to the left, agree=0.871, adj=0.254, (0 split) yedgexcor \u0026lt; 4.5 to the right, agree=0.854, adj=0.153, (0 split) ybar \u0026lt; 9.5 to the left, agree=0.848, adj=0.119, (0 split) xbox \u0026lt; 6.5 to the left, agree=0.842, adj=0.085, (0 split) ybox \u0026lt; 11.5 to the left, agree=0.842, adj=0.085, (0 split) Node number 11: 402 observations, complexity param=0.02076125 predicted class=R expected loss=0.2512438 P(node) =0.2580231 class counts: 39 62 0 301 probabilities: 0.097 0.154 0.000 0.749 left son=22 (26 obs) right son=23 (376 obs) Primary splits: yedge \u0026lt; 2.5 to the left, improve=35.46191, (0 missing) x2ybar \u0026lt; 0.5 to the left, improve=34.14932, (0 missing) y2bar \u0026lt; 1.5 to the left, improve=33.87850, (0 missing) x2bar \u0026lt; 3.5 to the left, improve=19.57685, (0 missing) yedgexcor \u0026lt; 8.5 to the left, improve=19.07812, (0 missing) Surrogate splits: y2bar \u0026lt; 1.5 to the left, agree=0.993, adj=0.885, (0 split) x2ybar \u0026lt; 0.5 to the left, agree=0.993, adj=0.885, (0 split) Node number 12: 55 observations predicted class=B expected loss=0.3090909 P(node) =0.03530167 class counts: 1 38 13 3 probabilities: 0.018 0.691 0.236 0.055 Node number 13: 36 observations predicted class=R expected loss=0.5 P(node) =0.02310655 class counts: 9 2 7 18 probabilities: 0.250 0.056 0.194 0.500 Node number 20: 283 observations predicted class=B expected loss=0.08480565 P(node) =0.1816431 class counts: 3 259 8 13 probabilities: 0.011 0.915 0.028 0.046 Node number 21: 59 observations predicted class=R expected loss=0.3559322 P(node) =0.03786906 class counts: 11 8 2 38 probabilities: 0.186 0.136 0.034 0.644 Node number 22: 26 observations predicted class=A expected loss=0.03846154 P(node) =0.01668806 class counts: 25 0 0 1 probabilities: 0.962 0.000 0.000 0.038 Node number 23: 376 observations, complexity param=0.01211073 predicted class=R expected loss=0.2021277 P(node) =0.241335 class counts: 14 62 0 300 probabilities: 0.037 0.165 0.000 0.798 left son=46 (26 obs) right son=47 (350 obs) Primary splits: yedge \u0026lt; 7.5 to the right, improve=19.73450, (0 missing) x2ybar \u0026lt; 5.5 to the right, improve=16.32647, (0 missing) xybar \u0026lt; 8.5 to the right, improve=15.20779, (0 missing) xedge \u0026lt; 3.5 to the right, improve=14.35240, (0 missing) onpix \u0026lt; 4.5 to the right, improve=12.94437, (0 missing) Surrogate splits: xedgeycor \u0026lt; 4.5 to the left, agree=0.939, adj=0.115, (0 split) Node number 46: 26 observations predicted class=B expected loss=0.3076923 P(node) =0.01668806 class counts: 4 18 0 4 probabilities: 0.154 0.692 0.000 0.154 Node number 47: 350 observations predicted class=R expected loss=0.1542857 P(node) =0.224647 class counts: 10 44 0 296 probabilities: 0.029 0.126 0.000 0.846  prp(CARTletters) What is the test-set accuracy of our CART model? Use the argument type=“class” when making predictions. (HINT: When you are computing the test-set accuracy using the confusion matrix, you want to add everything on the main diagonal and divide by the total number of observations in the test-set, which can be computed with nrow(test), where test is the name of our test-set).\nlettersPredict \u0026lt;- as.vector(predict(CARTletters, newdata = lettersAllTest, type = \u0026quot;class\u0026quot;)) length(lettersPredict) [1] 1558 lettersPredict  [1] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [18] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [35] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [52] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [69] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [86] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [103] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [120] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [137] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [154] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [171] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [188] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [205] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [222] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [239] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [256] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [273] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [290] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [307] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [324] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [341] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [358] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [375] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [392] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [409] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [426] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [443] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [460] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [477] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [494] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [511] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [528] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [545] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [562] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [579] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [596] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [613] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [630] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [647] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [664] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [681] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [698] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [715] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [732] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [749] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [766] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [783] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [800] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [817] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [834] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [851] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [868] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [885] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [902] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [919] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [936] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [953] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [970] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [987] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1004] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1021] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1038] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1055] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1072] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1089] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1106] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1123] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1140] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1157] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1174] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1191] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1208] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1225] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1242] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1259] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1276] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1293] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1310] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1327] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [1344] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1361] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1378] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1395] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1412] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1429] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1446] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1463] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1480] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1497] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1514] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [1531] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1548] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; nrow(lettersAllTest) [1] 1558 table(lettersAllTest$letter, lettersPredict)  lettersPredict A B P R A 348 4 0 43 B 8 318 12 45 P 2 21 363 15 R 10 24 5 340 (348 + 318 + 363 + 340) / nrow(lettersAllTest) [1] 0.8786906  Problem 2.3 - Predicting the letters A, B, P, R Now build a random forest model on the training data, using the same independent variables as in the previous problem – again, don’t forget to remove the isB variable.\nJust use the default parameter values for ntree and node size (you don’t need to include these arguments at all). Set the seed to 1000 right before building our model. (Remember that you might get a slightly different result even if you set the random seed.)\nset.seed(1000) lettersForest \u0026lt;- randomForest(letter ~ . - isB, data = lettersAllTrain) What is the test-set accuracy of your random forest model?\nlettersForestPredict \u0026lt;- as.vector(predict(lettersForest, newdata = lettersAllTest, type = \u0026quot;class\u0026quot;)) lettersForestPredict  [1] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [18] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [35] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [52] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [69] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [86] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [103] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [120] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [137] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [154] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [171] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [188] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [205] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [222] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [239] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [256] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [273] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [290] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [307] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [324] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [341] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [358] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [375] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [392] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [409] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [426] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [443] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [460] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [477] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [494] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [511] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [528] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [545] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [562] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [579] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [596] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [613] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [630] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [647] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [664] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [681] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [698] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [715] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [732] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [749] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [766] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [783] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [800] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [817] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [834] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; [851] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [868] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [885] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [902] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [919] \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [936] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [953] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [970] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [987] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1004] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1021] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1038] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1055] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1072] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1089] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1106] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1123] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1140] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1157] \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1174] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1191] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; [1208] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; [1225] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1242] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1259] \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; [1276] \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; [1293] \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1310] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; [1327] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; [1344] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1361] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1378] \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; [1395] \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; [1412] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1429] \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; [1446] \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; [1463] \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; [1480] \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; [1497] \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; [1514] \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; [1531] \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;B\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;B\u0026quot; [1548] \u0026quot;P\u0026quot; \u0026quot;P\u0026quot; \u0026quot;A\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; \u0026quot;A\u0026quot; \u0026quot;B\u0026quot; \u0026quot;A\u0026quot; \u0026quot;P\u0026quot; \u0026quot;R\u0026quot; \u0026quot;A\u0026quot; table(lettersAllTest$letter, lettersForestPredict)  lettersForestPredict A B P R A 391 0 3 1 B 0 380 1 2 P 0 6 394 1 R 3 14 0 362 (390 + 380 + 393 + 364) / nrow(lettersAllTest) [1] 0.9801027  Conclusion You should find this value rather striking, for several reasons. The first is that it is significantly higher than the value for CART, highlighting the gain in accuracy that is possible from using random forest models.\nThe second is that while the accuracy of CART decreased significantly as we transitioned from the problem of predicting B or not B (a relatively simple problem) to the problem of predicting the four letters (certainly a harder problem), the accuracy of the random forest model decreased by a tiny amount.\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"4066379bd08c6aafc0d70b3f118f9193","permalink":"/project/letter_recognition/letters/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/letter_recognition/letters/","section":"project","summary":"Predict which letter a particular image corresponds to","tags":["R","Data Analytics","Machine Learning"],"title":"Letter Reecognition","type":"project"},{"authors":null,"categories":null,"content":" In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is social, or extrinsic, pressure.\nTo quote the first paragraph of their 2008 research paper:\n “Among the most striking features of a democratic political system is the participation of millions of voters in elections. Why do large numbers of people vote, despite the fact that …”the casting of a single vote is of no significance where there is a multitude of electors“? One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail to participate in elections. Voters’ sense of civic duty has long been a leading explanation of voters turnout…”  In this analysis, I’ll use both logistic regression and classification trees to analyze the data they collected.\nThe data The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a “control” group, and the rest were categorized into one of four “treatment” groups. These five groups correspond to five binary variables in the dataset.\n“Civic Duty” (variable civicduty) group members were sent a letter that simply said “DO YOUR CIVIC DUTY - VOTE!” “Hawthorne Effect” (variable hawthorne) group members were sent a letter that had the “Civic Duty” message plus the additional message “YOU ARE BEING STUDIED” and they were informed that their voting behavior would be examined by means of public records. “Self” (variable self) group members received the “Civic Duty” message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records. “Neighbors” (variable neighbors) group members were given the same message as that for the “Self” group, except the message not only had the household voting records but, also that of neighbors - maximizing social pressure. “Control” (variable control) group members were not sent anything, and represented the typical voting situation.  Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).\n Problem 1.1 - Exploration and Logistic Regression We will first get familiar with the data.\nWhat proportion of people in this dataset voted in this election?\ngerber \u0026lt;- read.csv(\u0026quot;gerber.csv\u0026quot;) str(gerber) \u0026#39;data.frame\u0026#39;: 344084 obs. of 8 variables: $ sex : int 0 1 1 1 0 1 0 0 1 0 ... $ yob : int 1941 1947 1982 1950 1951 1959 1956 1981 1968 1967 ... $ voting : int 0 0 1 1 1 1 1 0 0 0 ... $ hawthorne: int 0 0 1 1 1 0 0 0 0 0 ... $ civicduty: int 1 1 0 0 0 0 0 0 0 0 ... $ neighbors: int 0 0 0 0 0 0 0 0 0 0 ... $ self : int 0 0 0 0 0 0 0 0 0 0 ... $ control : int 0 0 0 0 0 1 1 1 1 1 ... table(gerber$voting)  0 1 235388 108696  108696 / (235388 + 108696) [1] 0.3158996  Problem 1.2 - Exploration and Logistic Regression Which of the four “treatment groups” had the largest percentage of people who actually voted (voting = 1)?\n# howthorne table(gerber$voting, gerber$hawthorne)  0 1 0 209500 25888 1 96380 12316 12316 / (25888 + 12316) [1] 0.3223746 # civicduty table(gerber$voting, gerber$civicduty)  0 1 0 209191 26197 1 96675 12021 12021 / (26197 + 12021) [1] 0.3145377 # neighbors table(gerber$voting, gerber$neighbors)  0 1 0 211625 23763 1 94258 14438 14438 / (23763 + 14438) [1] 0.3779482 # self table(gerber$voting, gerber$self)  0 1 0 210361 25027 1 95505 13191 13191 / (25027 + 13191) [1] 0.3451515 Neighbors   Problem 1.3 - Exploration and Logistic Regression Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Using all the data to build the model (NOT spliting the data into a training set and testing set).\nWhich of the following coefficients are significant in the logistic regression model?\nVotingLog \u0026lt;- glm(voting ~ civicduty + hawthorne + self + neighbors, data = gerber, family = binomial) summary(VotingLog)  Call: glm(formula = voting ~ civicduty + hawthorne + self + neighbors, family = binomial, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9744 -0.8691 -0.8389 1.4586 1.5590 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.863358 0.005006 -172.459 \u0026lt; 2e-16 *** civicduty 0.084368 0.012100 6.972 3.12e-12 *** hawthorne 0.120477 0.012037 10.009 \u0026lt; 2e-16 *** self 0.222937 0.011867 18.786 \u0026lt; 2e-16 *** neighbors 0.365092 0.011679 31.260 \u0026lt; 2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428090 on 344079 degrees of freedom AIC: 428100 Number of Fisher Scoring iterations: 4 All coefficients are significant   Problem 1.4 - Exploration and Logistic Regression Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don’t need to use the new data argument since we didn’t split our data.)\npredictVoting \u0026lt;- predict(VotingLog, type = \u0026quot;response\u0026quot;) table(gerber$voting, predictVoting \u0026gt; 0.3)  FALSE TRUE 0 134513 100875 1 56730 51966 (134513 + 51966) / nrow(gerber) [1] 0.5419578  Problem 1.5 - Exploration and Logistic Regression Using a threshold of 0.5, what is the accuracy of the logistic regression model?\ntable(gerber$voting, predictVoting \u0026gt; 0.5)  FALSE 0 235388 1 108696 (235388) / nrow(gerber) [1] 0.6841004 table(gerber$voting)  0 1 235388 108696  235388 / (235388 + 108696) [1] 0.6841004 0.6841004 =\u0026gt; equal to accuracy of threshold of 0.5   Problem 1.6 - Exploration and Logistic Regression Compare our previous two answers to the percentage of people who did not vote (the baseline accuracy) and computing the AUC of the model. What is happening here?\nlibrary(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess ROCRpred = prediction(predictVoting, gerber$voting) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.5308461 Even though all of the variables are significant, this is a weak predictive model.   Problem 2.1 - Trees I’ll now try out trees! Building a CART tree for voting using all data and the same four treatment variables we used before. Don’t set the option method=“class” - we are actually going to create a regression tree here.\nWe are interested in building a tree to explore the fraction of people who vote, or the probability of voting.\nI’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different).\nHowever, with regression trees, CART will split even if both groups have probability less than 50%. Leave all the parameters at their default values.\nlibrary(rpart) library(rpart.plot) CARTmodel \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors, data = gerber) # plot the tree. What happens, and if relevant, why? prp(CARTmodel) No variables are used (the tree is only a root node) - none of the variables make a big enough effect to be split on.   Problem 2.2 - Trees Now build the tree:\nCARTmodel2 \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0) prp(CARTmodel2) What do we observe about the order of the splits? #### Neighbor is the first split, civic duty is the last.\n Problem 2.3 - Trees Using only the CART tree plot, we note that the fraction (a number between 0 and 1) of “Civic Duty” people voted amounted to: #### 31%\n Problem 2.4 - Trees Building a new tree that includes the “sex” variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.\nCARTmodel3 \u0026lt;- rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0) prp(CARTmodel3) In the control group, which gender is more likely to vote? #### Men (0)\nIn the “Civic Duty” group, which gender is more likely to vote? #### Men (0)\n Problem 3.1 - Interaction Terms We know trees can handle “nonlinear” relationships, e.g. “in the ‘Civic Duty’ group and female”, but as we will see in the next few questions, it is possible to do the same for logistic regression.\nFirstly, let’s explore what trees can tell us. Let’s just focus on the “Control” treatment group. Creating a regression tree using just the “control” variable, then creating another tree with the “control” and “sex” variables, both with cp=0.0.\nCARTcontrol \u0026lt;- rpart(voting ~ control, data = gerber, cp = 0.0) CARTcontrolAndSex \u0026lt;- rpart(voting ~ control + sex, data = gerber, cp = 0.0) In the “control” only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group?\nUsing the absolute value function to get an answer, i.e. abs(Control Prediction - Non-Control Prediction). I’ll add the argument “digits = 6” to the prp code to get a more accurate estimate.\nprp(CARTcontrol, digits = 6) abs(0.296638 - 0.34) [1] 0.043362 0.043362   Problem 3.2 - Interaction Terms Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group (being in any of the four treatment groups):\nprp(CARTcontrolAndSex, digits = 6) They are affected about the same (change in probability within 0.001 of each other).   Problem 3.3 - Interaction Terms Going back to logistic regression now, I’ll build a model using “sex” and “control”. Interpreting the coefficient for “sex”:\nVotingControlAndSexLog \u0026lt;- glm(voting ~ control + sex, data = gerber, family = binomial) summary(VotingControlAndSexLog)  Call: glm(formula = voting ~ control + sex, family = binomial, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9220 -0.9012 -0.8290 1.4564 1.5717 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.635538 0.006511 -97.616 \u0026lt; 2e-16 *** control -0.200142 0.007364 -27.179 \u0026lt; 2e-16 *** sex -0.055791 0.007343 -7.597 3.02e-14 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428443 on 344081 degrees of freedom AIC: 428449 Number of Fisher Scoring iterations: 4 Coefficient is negative, reflecting that women are less likely to vote!   Problem 3.4 - Interaction Terms The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control).\nLogistic regression has attempted to do the same, although it wasn’t able to do as well because it can’t consider exactly the joint possibility of being a women and in the control group.\nI can quantify this precisely. By creating the following dataframe (this contains all of the possible values of sex and control), and evaluating our logistic regression using the predict function (where “LogModelSex” is the name of our logistic regression model that uses both control and sex):\nPossibilities \u0026lt;- data.frame(sex=c(0,0,1,1), control=c(0,1,0,1)) predict(VotingControlAndSexLog, newdata=Possibilities, type=\u0026quot;response\u0026quot;)  1 2 3 4 0.3462559 0.3024455 0.3337375 0.2908065  The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ).\nWhat is the absolute difference between the tree and the logistic regression for the (Woman, Control) case?\nThe answer contains five numbers after the decimal point.\nabs(0.290456 - 0.2908065) [1] 0.0003505 round(0.0003505, digits = 5) [1] 0.00035  Problem 3.5 - Interaction Terms So the difference is not too big for this dataset, but it’s there. I’m going to add a new term to our logistic regression now, that is the combination of the “sex” and “control” variables - so if this new variable is 1, that means the person is a woman AND in the control group.\nLogModel2 \u0026lt;- glm(voting ~ sex + control + sex:control, data = gerber, family = \u0026quot;binomial\u0026quot;) How do I interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?\nsummary(LogModel2)  Call: glm(formula = voting ~ sex + control + sex:control, family = \u0026quot;binomial\u0026quot;, data = gerber) Deviance Residuals: Min 1Q Median 3Q Max -0.9213 -0.9019 -0.8284 1.4573 1.5724 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -0.637471 0.007603 -83.843 \u0026lt; 2e-16 *** sex -0.051888 0.010801 -4.804 1.55e-06 *** control -0.196553 0.010356 -18.980 \u0026lt; 2e-16 *** sex:control -0.007259 0.014729 -0.493 0.622 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 429238 on 344083 degrees of freedom Residual deviance: 428442 on 344080 degrees of freedom AIC: 428450 Number of Fisher Scoring iterations: 4 If a person is a woman and in the control group, the chance that she voted goes down.   Problem 3.6 - Interaction Terms Run the same code as before to calculate the average for each group:\npredict(LogModel2, newdata=Possibilities, type=\u0026quot;response\u0026quot;)  1 2 3 4 0.3458183 0.3027947 0.3341757 0.2904558  Now, what is the difference between the logistic regression model and the CART model for the (Woman, Control) case?\nAgain, our answer has five numbers after the decimal point.\nabs(0.290456 - 0.2904558) [1] 2e-07 round(2e-07, digits = 5) [1] 0  Conclusion - Interaction Terms This example has shown that trees can capture nonlinear relationships that logistic regression cannot, but that we can get around this sometimes by using variables that are the combination of two variables.\nShould we always include all possible interaction terms of the independent variables when building a logistic regression model? #### No (because of overfitting)\n ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"bbef8a24f98dd4e6abfb460d2c2fbcdd","permalink":"/project/understanding_votes/votes/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/understanding_votes/votes/","section":"project","summary":"Why do people vote?","tags":["R","Data Analytics","Machine Learning"],"title":"Understanding Why People Vote?","type":"project"},{"authors":null,"categories":null,"content":" Wikipedia is a free online encyclopedia that anyone can edit and contribute to. It’s available in many languages and is growing all the time. On the English language version of Wikipedia:\n There are currently 4.7 million pages. There have been a total over 760 million edits (also called revisions) over its lifetime. There are approximately 130,000 edits per day.  One of the consequences of being editable by anyone is that some people vandalize pages.\nThis can take the form of removing content, adding promotional or inappropriate content, or more subtle shifts that change the meaning of the article.\nWith these many articles and edits per day it is difficult for humans to detect all instances of vandalism and revert (undo) them. As a result, Wikipedia uses bots - computer programs that automatically revert edits that look like vandalism. In this analysis I’ll attempt to develop a vandalism detector that uses ML to distinguish between a valid edit and vandalism.\nThe data for this analysis is based on the revision history of the page Language. Wikipedia provides a history for each page that consists of the state of the page at each revision. Rather than manually considering each revision, a script was run that checked whether edits stayed or were reverted. If a change was eventually reverted then that revision is marked as vandalism. This may result in some misclassifications, but the script performs well enough for our needs.\nAs a result of this preprocessing, some common processing tasks have already been done, including lower-casing and punctuation removal. The columns in the dataset are:\n Vandal = 1 if this edit was vandalism, 0 if not. Minor = 1 if the user marked this edit as a “minor edit”, 0 if not. Loggedin = 1 if the user made this edit while using a Wikipedia account, 0 if they did not. Added = The unique words added. Removed = The unique words removed.  Notice the repeated use of unique. The data we have available is not the traditional bag of words - rather it is the set of words that were removed or added. For example, if a word was removed multiple times in a revision it will only appear one time in the “Removed” column.\nLoading the packages # install packages if necessary list.of.packages \u0026lt;- c(\u0026quot;SnowballC\u0026quot;) new.packages \u0026lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\u0026quot;Package\u0026quot;])] if(length(new.packages)) install.packages(new.packages) ## load packages library(tm) Loading required package: NLP library(SnowballC) library(caTools) # to use sample.split function. library(rpart) library(rpart.plot) library(ROCR) Loading required package: gplots  Attaching package: \u0026#39;gplots\u0026#39; The following object is masked from \u0026#39;package:stats\u0026#39;: lowess Problem 1.1 - Bags of Words Load the data wiki.csv with the option stringsAsFactors=FALSE, calling the dataframe “wiki”. Convert the “Vandal” column to a factor using the code wiki\\(Vandal = as.factor(wiki\\)Vandal).\nwiki \u0026lt;- read.csv(\u0026quot;wiki.csv\u0026quot;, stringsAsFactors = FALSE) wiki$Vandal \u0026lt;- as.factor(wiki$Vandal) str(wiki) \u0026#39;data.frame\u0026#39;: 3876 obs. of 7 variables: $ X.1 : int 1 2 3 4 5 6 7 8 9 10 ... $ X : int 1 2 3 4 5 6 7 8 9 10 ... $ Vandal : Factor w/ 2 levels \u0026quot;0\u0026quot;,\u0026quot;1\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ Minor : int 1 1 0 1 1 0 0 0 1 0 ... $ Loggedin: int 1 1 1 0 1 1 1 1 1 0 ... $ Added : chr \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethno\u0026quot;| __truncated__ \u0026quot; website external links\u0026quot; \u0026quot; \u0026quot; \u0026quot; afghanistan used iran mostly that farsiis is countries some xmlspacepreservepersian parts tajikestan region\u0026quot; ... $ Removed : chr \u0026quot; \u0026quot; \u0026quot; talklanguagetalk\u0026quot; \u0026quot; regarded as technologytechnologies human first\u0026quot; \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help all actions through ethnologue relat\u0026quot;| __truncated__ ... How many cases of vandalism were detected in the history of this page?\ntable(wiki$Vandal)  0 1 2061 1815  1815\n  Problem 1.2 - Bags of Words We will now use the bag of words approach to build a model. We have two columns of textual data, with different meanings. For example, adding rude words has a different meaning to removing rude words. The text already is lowercase and stripped of punctuation. So to pre-process the data, just complete the following four steps:\n# 1) Create the corpus for the Added column, and call it \u0026quot;corpusAdded\u0026quot;. corpusAdded \u0026lt;- Corpus(VectorSource(wiki$Added)) corpusAdded[[1]]$content [1] \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text all actions through human ethnologue relationships linguistics regarded writing languages to other listing xmlspacepreservelanguages metaverse formal term philology common each including phonologyphonology often ten list humans affiliation see computer are speechpathologyspeech our what for ways dialects please artificial written body be of quite hypothesis found alone refers by about language profanity study programming priorities rosenfelders technologytechnologies makes or first among useful languagephilosophy one sounds use area create phrases mark their genetic basic families complete but sapirwhorfhypothesissapirwhorf with talklanguagetalk population animals this science up vocal can concepts called at and topics locations as numbers have in pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy most important meaningful a historicallinguisticsorphilologyhistorical semanticssemantics patterns the oral\u0026quot; # 2) Remove the English-language stopwords. corpusAdded \u0026lt;- tm_map(corpusAdded, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusAdded, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusAdded[[1]]$content [1] \u0026quot; represent psycholinguisticspsycholinguistics orthographyorthography help text actions human ethnologue relationships linguistics regarded writing languages listing xmlspacepreservelanguages metaverse formal term philology common including phonologyphonology often ten list humans affiliation see computer speechpathologyspeech ways dialects please artificial written body quite hypothesis found alone refers language profanity study programming priorities rosenfelders technologytechnologies makes first among useful languagephilosophy one sounds use area create phrases mark genetic basic families complete sapirwhorfhypothesissapirwhorf talklanguagetalk population animals science vocal can concepts called topics locations numbers pathology different develop 4000 things ideas grouped complex animal mathematics fairly literature httpwwwzompistcom philosophy important meaningful historicallinguisticsorphilologyhistorical semanticssemantics patterns oral\u0026quot; # 3) Stem the words. corpusAdded \u0026lt;- tm_map(corpusAdded, stemDocument) Warning in tm_map.SimpleCorpus(corpusAdded, stemDocument): transformation drops documents corpusAdded[[1]]$content [1] \u0026quot;repres psycholinguisticspsycholinguist orthographyorthographi help text action human ethnologu relationship linguist regard write languag list xmlspacepreservelanguag metavers formal term philolog common includ phonologyphonolog often ten list human affili see comput speechpathologyspeech way dialect pleas artifici written bodi quit hypothesi found alon refer languag profan studi program prioriti rosenfeld technologytechnolog make first among use languagephilosophi one sound use area creat phrase mark genet basic famili complet sapirwhorfhypothesissapirwhorf talklanguagetalk popul anim scienc vocal can concept call topic locat number patholog differ develop 4000 thing idea group complex anim mathemat fair literatur httpwwwzompistcom philosophi import meaning historicallinguisticsorphilologyhistor semanticssemant pattern oral\u0026quot; # 4) Build the DocumentTermMatrix, and call it dtmAdded. dtmAdded = DocumentTermMatrix(corpusAdded) dtmAdded \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 6675)\u0026gt;\u0026gt; Non-/sparse entries: 15368/25856932 Sparsity : 100% Maximal term length: 784 Weighting : term frequency (tf) How many terms appear in dtmAdded? #### 6675\nIf the code length(stopwords(“english”)) does not return 174 for you, then please run the line of code in this file, which will store the standard stop words in a variable called sw. When removing stop words, use tm_map(corpusAdded, removeWords, sw) instead of tm_map(corpusAdded, removeWords, stopwords(“english”)).\nlength(stopwords(\u0026quot;english\u0026quot;)) [1] 174  Problem 1.3 - Bags of Words Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded.\nsparseAdded \u0026lt;- removeSparseTerms(dtmAdded, 0.997) sparseAdded \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 166)\u0026gt;\u0026gt; Non-/sparse entries: 2681/640735 Sparsity : 100% Maximal term length: 28 Weighting : term frequency (tf) How many terms appear in sparseAdded? #### 166\n Problem 1.4 - Bags of Words Convert sparseAdded to a dataframe called wordsAdded, and then prepend all the words with the letter A, by using the code:\nwordsAdded \u0026lt;- as.data.frame(as.matrix(sparseAdded)) colnames(wordsAdded) \u0026lt;- paste(\u0026quot;A\u0026quot;, colnames(wordsAdded)) str(wordsAdded) \u0026#39;data.frame\u0026#39;: 3876 obs. of 166 variables: $ A bodi : num 1 0 0 0 1 0 0 0 0 0 ... $ A call : num 1 0 0 0 1 0 0 0 0 0 ... $ A complet : num 1 0 0 0 1 0 0 0 0 0 ... $ A concept : num 1 0 0 0 1 0 0 0 0 0 ... $ A creat : num 1 0 0 0 1 0 0 0 0 0 ... $ A develop : num 1 0 0 0 1 0 0 0 0 0 ... $ A differ : num 1 0 0 0 1 0 0 0 0 0 ... $ A famili : num 1 0 0 0 1 0 0 0 0 0 ... $ A first : num 1 0 0 0 0 0 0 0 0 0 ... $ A group : num 1 0 0 0 1 0 0 0 0 0 ... $ A help : num 1 0 0 0 1 0 0 0 0 0 ... $ A idea : num 1 0 0 0 1 0 0 0 0 0 ... $ A import : num 1 0 0 0 1 0 0 0 0 0 ... $ A includ : num 1 0 0 0 1 0 0 0 0 0 ... $ A linguist : num 1 0 0 0 1 0 0 0 0 0 ... $ A make : num 1 0 0 0 1 0 0 0 0 0 ... $ A number : num 1 0 0 0 1 0 0 0 0 0 ... $ A pattern : num 1 0 0 0 1 0 0 0 0 0 ... $ A popul : num 1 0 0 0 1 0 0 0 0 0 ... $ A refer : num 1 0 0 0 1 0 0 0 0 0 ... $ A relationship : num 1 0 0 0 1 0 0 0 0 0 ... $ A repres : num 1 0 0 0 1 0 0 0 0 0 ... $ A sound : num 1 0 0 0 1 0 0 0 0 0 ... $ A studi : num 1 0 0 0 1 0 0 0 0 0 ... $ A ten : num 1 0 0 0 1 0 0 0 0 0 ... $ A use : num 2 0 0 1 2 0 0 0 0 0 ... $ A write : num 1 0 0 0 1 0 0 0 0 0 ... $ A part : num 0 0 0 1 0 0 0 0 0 0 ... $ A name : num 0 0 0 0 0 0 0 0 0 0 ... $ A communic : num 0 0 0 0 0 0 0 0 0 0 ... $ A defin : num 0 0 0 0 0 0 0 0 0 0 ... $ A express : num 0 0 0 0 0 0 0 0 0 0 ... $ A mean : num 0 0 0 0 0 0 0 0 0 0 ... $ A must : num 0 0 0 0 0 0 0 0 0 0 ... $ A set : num 0 0 0 0 0 0 0 0 0 0 ... $ A featur : num 0 0 0 0 0 0 0 0 0 0 ... $ A form : num 0 0 0 0 0 0 0 0 0 0 ... $ A symbol : num 0 0 0 0 0 0 0 0 0 0 ... $ A talk : num 0 0 0 0 0 0 0 0 0 0 ... $ A type : num 0 0 0 0 0 0 0 0 0 0 ... $ A learn : num 0 0 0 0 0 0 0 0 0 0 ... $ A general : num 0 0 0 0 0 0 0 0 0 0 ... $ A exampl : num 0 0 0 0 0 0 0 0 0 0 ... $ A provid : num 0 0 0 0 0 0 0 0 0 0 ... $ A speak : num 0 0 0 0 0 0 0 0 0 0 ... $ A thought : num 0 0 0 0 0 0 0 0 0 0 ... $ A cours : num 0 0 0 0 0 0 0 0 0 0 ... $ A german : num 0 0 0 0 0 0 0 0 0 0 ... $ A onlin : num 0 0 0 0 0 0 0 0 0 0 ... $ A biolog : num 0 0 0 0 0 0 0 0 0 0 ... $ A close : num 0 0 0 0 0 0 0 0 0 0 ... $ A communiti : num 0 0 0 0 0 0 0 0 0 0 ... $ A distinct : num 0 0 0 0 0 0 0 0 0 0 ... $ A dutch : num 0 0 0 0 0 0 0 0 0 0 ... $ A intellig : num 0 0 0 0 0 0 0 0 0 0 ... $ A least : num 0 0 0 0 0 0 0 0 0 0 ... $ A note : num 0 0 0 0 0 0 0 0 0 0 ... $ A parallel : num 0 0 0 0 0 0 0 0 0 0 ... $ A possibl : num 0 0 0 0 0 0 0 0 0 0 ... $ A principl : num 0 0 0 0 0 0 0 0 0 0 ... $ A sentenc : num 0 0 0 0 0 0 0 0 0 0 ... $ A standard : num 0 0 0 0 0 0 0 0 0 0 ... $ A stem : num 0 0 0 0 0 0 0 0 0 0 ... $ A understood : num 0 0 0 0 0 0 0 0 0 0 ... $ A utter : num 0 0 0 0 0 0 0 0 0 0 ... $ A arbitrari : num 0 0 0 0 0 0 0 0 0 0 ... $ A combin : num 0 0 0 0 0 0 0 0 0 0 ... $ A cultur : num 0 0 0 0 0 0 0 0 0 0 ... $ A discuss : num 0 0 0 0 0 0 0 0 0 0 ... $ A invent : num 0 0 0 0 0 0 0 0 0 0 ... $ A properti : num 0 0 0 0 0 0 0 0 0 0 ... $ A research : num 0 0 0 0 0 0 0 0 0 0 ... $ A unit : num 0 0 0 0 0 0 0 0 0 0 ... $ A work : num 0 0 0 0 0 0 0 0 0 0 ... $ A direct : num 0 0 0 0 0 0 0 0 0 0 ... $ A relat : num 0 0 0 0 0 0 0 0 0 0 ... $ A agre : num 0 0 0 0 0 0 0 0 0 0 ... $ A exist : num 0 0 0 0 0 0 0 0 0 0 ... $ A wide : num 0 0 0 0 0 0 0 0 0 0 ... $ A describ : num 0 0 0 0 0 0 0 0 0 0 ... $ A literari : num 0 0 0 0 0 0 0 0 0 0 ... $ A purpos : num 0 0 0 0 0 0 0 0 0 0 ... $ A fantasi : num 0 0 0 0 0 0 0 0 0 0 ... $ A promin : num 0 0 0 0 0 0 0 0 0 0 ... $ A manipul : num 0 0 0 0 0 0 0 0 0 0 ... $ A tolkien : num 0 0 0 0 0 0 0 0 0 0 ... $ A follow : num 0 0 0 0 0 0 0 0 0 0 ... $ A regular : num 0 0 0 0 0 0 0 0 0 0 ... $ A gestur : num 0 0 0 0 0 0 0 0 0 0 ... $ A similar : num 0 0 0 0 0 0 0 0 0 0 ... $ A imposs : num 0 0 0 0 0 0 0 0 0 0 ... $ A interact : num 0 0 0 0 0 0 0 0 0 0 ... $ A person : num 0 0 0 0 0 0 0 0 0 0 ... $ A reason : num 0 0 0 0 0 0 0 0 0 0 ... $ A writer : num 0 0 0 0 0 0 0 0 0 0 ... $ A phenomenon : num 0 0 0 0 0 0 0 0 0 0 ... $ A accord : num 0 0 0 0 0 0 0 0 0 0 ... $ A individu : num 0 0 0 0 0 0 0 0 0 0 ... $ A object : num 0 0 0 0 0 0 0 0 0 0 ... [list output truncated] Now repeat all of the steps we’ve done so far (create a corpus, remove stop words, stem the document, create a sparse document term matrix, and convert it to a dataframe) to create a Removed bag-of-words dataframe, called wordsRemoved, except this time, prepend all of the words with the letter R:\n# 1) Create the corpus for the Removed column, and call it \u0026quot;corpusRemoved\u0026quot;. corpusRemoved \u0026lt;- Corpus(VectorSource(wiki$Removed)) corpusRemoved[[3]]$content [1] \u0026quot; regarded as technologytechnologies human first\u0026quot; # 2) Remove the English-language stopwords. corpusRemoved \u0026lt;- tm_map(corpusRemoved, removeWords, stopwords(\u0026quot;english\u0026quot;)) Warning in tm_map.SimpleCorpus(corpusRemoved, removeWords, stopwords(\u0026quot;english\u0026quot;)): transformation drops documents corpusRemoved[[3]]$content [1] \u0026quot; regarded technologytechnologies human first\u0026quot; # 3) Stem the words. corpusRemoved \u0026lt;- tm_map(corpusRemoved, stemDocument) Warning in tm_map.SimpleCorpus(corpusRemoved, stemDocument): transformation drops documents corpusRemoved[[3]]$content [1] \u0026quot;regard technologytechnolog human first\u0026quot; # 4) Build the DocumentTermMatrix, and call it dtmRemoved. dtmRemoved \u0026lt;- DocumentTermMatrix(corpusRemoved) dtmRemoved \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 5403)\u0026gt;\u0026gt; Non-/sparse entries: 13293/20928735 Sparsity : 100% Maximal term length: 784 Weighting : term frequency (tf) # 5) Sparse document term matrix sparseRemoved \u0026lt;- removeSparseTerms(dtmRemoved, 0.997) sparseRemoved \u0026lt;\u0026lt;DocumentTermMatrix (documents: 3876, terms: 162)\u0026gt;\u0026gt; Non-/sparse entries: 2552/625360 Sparsity : 100% Maximal term length: 28 Weighting : term frequency (tf) # 6) Create dataframe and preppend the colnames with the letter \u0026quot;R\u0026quot; wordsRemoved \u0026lt;- as.data.frame(as.matrix(sparseRemoved)) colnames(wordsRemoved) \u0026lt;- paste(\u0026quot;R\u0026quot;, colnames(wordsRemoved)) str(wordsRemoved) \u0026#39;data.frame\u0026#39;: 3876 obs. of 162 variables: $ R first : num 0 0 1 0 0 0 0 0 0 0 ... $ R bodi : num 0 0 0 1 0 0 0 0 0 0 ... $ R call : num 0 0 0 1 0 0 0 0 0 0 ... $ R complet : num 0 0 0 1 0 0 0 0 0 0 ... $ R concept : num 0 0 0 1 0 0 0 0 0 0 ... $ R creat : num 0 0 0 1 0 0 0 0 0 0 ... $ R develop : num 0 0 0 1 0 0 0 0 0 0 ... $ R differ : num 0 0 0 1 0 0 0 0 0 0 ... $ R famili : num 0 0 0 1 0 0 0 0 0 0 ... $ R group : num 0 0 0 1 0 0 0 0 0 0 ... $ R idea : num 0 0 0 1 0 0 0 0 0 0 ... $ R includ : num 0 0 0 1 0 0 0 0 0 0 ... $ R linguist : num 0 0 0 1 0 0 0 0 0 0 ... $ R make : num 0 0 0 1 0 0 0 0 0 0 ... $ R pattern : num 0 0 0 1 0 0 0 0 0 0 ... $ R popul : num 0 0 0 1 0 0 0 0 0 0 ... $ R quit : num 0 0 0 1 0 0 0 0 0 0 ... $ R refer : num 0 0 0 1 0 0 0 0 0 0 ... $ R relationship : num 0 0 0 1 0 0 0 0 0 0 ... $ R repres : num 0 0 0 1 0 0 0 0 0 0 ... $ R sound : num 0 0 0 1 0 0 0 0 0 0 ... $ R studi : num 0 0 0 1 0 0 0 0 0 0 ... $ R use : num 0 0 0 2 1 0 0 0 0 0 ... $ R part : num 0 0 0 0 1 0 0 0 0 0 ... $ R communic : num 0 0 0 0 0 0 0 0 0 0 ... $ R defin : num 0 0 0 0 0 0 0 0 0 0 ... $ R express : num 0 0 0 0 0 0 0 0 0 0 ... $ R mean : num 0 0 0 0 0 0 0 0 0 0 ... $ R must : num 0 0 0 0 0 0 0 0 0 0 ... $ R name : num 0 0 0 0 0 0 0 0 0 0 ... $ R featur : num 0 0 0 0 0 0 0 0 0 0 ... $ R set : num 0 0 0 0 0 0 0 0 0 0 ... $ R symbol : num 0 0 0 0 0 0 0 0 0 0 ... $ R type : num 0 0 0 0 0 0 0 0 0 0 ... $ R know : num 0 0 0 0 0 0 0 0 0 0 ... $ R method : num 0 0 0 0 0 0 0 0 0 0 ... $ R quot : num 0 0 0 0 0 0 0 0 0 0 ... $ R suggest : num 0 0 0 0 0 0 0 0 0 0 ... $ R want : num 0 0 0 0 0 0 0 0 0 0 ... $ R speak : num 0 0 0 0 0 0 0 0 0 0 ... $ R agre : num 0 0 0 0 0 0 0 0 0 0 ... $ R regular : num 0 0 0 0 0 0 0 0 0 0 ... $ R actual : num 0 0 0 0 0 0 0 0 0 0 ... $ R spanish : num 0 0 0 0 0 0 0 0 0 0 ... $ R cours : num 0 0 0 0 0 0 0 0 0 0 ... $ R onlin : num 0 0 0 0 0 0 0 0 0 0 ... $ R fuck : num 0 0 0 0 0 0 0 0 0 0 ... $ R person : num 0 0 0 0 0 0 0 0 0 0 ... $ R believ : num 0 0 0 0 0 0 0 0 0 0 ... $ R direct : num 0 0 0 0 0 0 0 0 0 0 ... $ R experiment : num 0 0 0 0 0 0 0 0 0 0 ... $ R will : num 0 0 0 0 0 0 0 0 0 0 ... $ R deriv : num 0 0 0 0 0 0 0 0 0 0 ... $ R gestur : num 0 0 0 0 0 0 0 0 0 0 ... $ R least : num 0 0 0 0 0 0 0 0 0 0 ... $ R logic : num 0 0 0 0 0 0 0 0 0 0 ... $ R mainlinguist : num 0 0 0 0 0 0 0 0 0 0 ... $ R possibl : num 0 0 0 0 0 0 0 0 0 0 ... $ R reason : num 0 0 0 0 0 0 0 0 0 0 ... $ R result : num 0 0 0 0 0 0 0 0 0 0 ... $ R standard : num 0 0 0 0 0 0 0 0 0 0 ... $ R thought : num 0 0 0 0 0 0 0 0 0 0 ... $ R tri : num 0 0 0 0 0 0 0 0 0 0 ... $ R say : num 0 0 0 0 0 0 0 0 0 0 ... $ R origin : num 0 0 0 0 0 0 0 0 0 0 ... $ R process : num 0 0 0 0 0 0 0 0 0 0 ... $ R languageenglish : num 0 0 0 0 0 0 0 0 0 0 ... $ R analog : num 0 0 0 0 0 0 0 0 0 0 ... $ R subject : num 0 0 0 0 0 0 0 0 0 0 ... $ R learn : num 0 0 0 0 0 0 0 0 0 0 ... $ R peopl : num 0 0 0 0 0 0 0 0 0 0 ... $ R discuss : num 0 0 0 0 0 0 0 0 0 0 ... $ R biologyanalog : num 0 0 0 0 0 0 0 0 0 0 ... $ R govern : num 0 0 0 0 0 0 0 0 0 0 ... $ R linguisticssent : num 0 0 0 0 0 0 0 0 0 0 ... $ R object : num 0 0 0 0 0 0 0 0 0 0 ... $ R sentenc : num 0 0 0 0 0 0 0 0 0 0 ... $ R verb : num 0 0 0 0 0 0 0 0 0 0 ... $ R compar : num 0 0 0 0 0 0 0 0 0 0 ... $ R get : num 0 0 0 0 0 0 0 0 0 0 ... $ R provid : num 0 0 0 0 0 0 0 0 0 0 ... $ R serv : num 0 0 0 0 0 0 0 0 0 0 ... $ R clear : num 0 0 0 0 0 0 0 0 0 0 ... $ R intern : num 0 0 0 0 0 0 0 0 0 0 ... $ R combin : num 0 0 0 0 0 0 0 0 0 0 ... $ R distinct : num 0 0 0 0 0 0 0 0 0 0 ... $ R relat : num 0 0 0 0 0 0 0 0 0 0 ... $ R map : num 0 0 0 0 0 0 0 0 0 0 ... $ R nation : num 0 0 0 0 0 0 0 0 0 0 ... $ R care : num 0 0 0 0 0 0 0 0 0 0 ... $ R geograph : num 0 0 0 0 0 0 0 0 0 0 ... $ R notion : num 0 0 0 0 0 0 0 0 0 0 ... $ R present : num 0 0 0 0 0 0 0 0 0 0 ... $ R appar : num 0 0 0 0 0 0 0 0 0 0 ... $ R close : num 0 0 0 0 0 0 0 0 0 0 ... $ R wide : num 0 0 0 0 0 0 0 0 0 0 ... $ R bigger : num 0 0 0 0 0 0 0 0 0 0 ... $ R follow : num 0 0 0 0 0 0 0 0 0 0 ... $ R recent : num 0 0 0 0 0 0 0 0 0 0 ... [list output truncated] How many words are in the wordsRemoved dataframe? #### 162\n Problem 1.5 - Bags of Words Combine the two dataframes into a dataframe called wikiWords with the following:\nwikiWords \u0026lt;- cbind(wordsAdded, wordsRemoved) The cbind function combines two sets of variables for the same observations into one dataframe. Then add the Vandal column. Set the random seed to 123 and then split the dataset using sample.split from the “caTools” package to put 70% in the training set.\nwikiWords$Vandal \u0026lt;- wiki$Vandal set.seed(123) wikiSplit \u0026lt;- sample.split(wikiWords$Vandal, 0.7) wikiTrain \u0026lt;- subset(wikiWords, wikiSplit == TRUE) wikiTest \u0026lt;- subset(wikiWords, wikiSplit == FALSE) table(wikiTest$Vandal)  0 1 618 545  618 / (618 + 545) [1] 0.5313844  Problem 1.6 - Bags of Words Build a CART model to predict Vandal, using all of the other variables as independent variables. Use the training set to build the model and the default parameters (don’t set values for minbucket or cp).\nwikiTree \u0026lt;- rpart(Vandal ~ ., data = wikiTrain, method = \u0026quot;class\u0026quot;) What is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that if you add the argument type=“class” when making predictions, the output of predict will automatically use a threshold of 0.5.)\nwikiPred \u0026lt;- as.vector(predict(wikiTree, newdata = wikiTest, type=\u0026quot;class\u0026quot;)) head(wikiPred) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest$Vandal, wikiPred)  wikiPred 0 1 0 614 4 1 526 19 (618 + 12) / nrow(wikiTest) [1] 0.5417025  Problem 1.7 - Bags of Words Plot the CART tree.\nHow many word stems does the CART model use?\nprp(wikiTree) 2   Problem 1.8 - Bags of Words Given the performance of the CART model relative to the baseline, what is the best explanation for these results? #### Although it beats the baseline, bag of words is not very predictive for this problem.\n Problem 2.1 - Problem-specific Knowledge We weren’t able to improve on the baseline using the raw textual information. More specifically, the words themselves were not useful. There are other options though, and in this section we will try two techniques - identifying a key class of words, and counting words. The key class of words we will use are website addresses. “Website addresses” (also known as URLs - Uniform Resource Locators) are comprised of two main parts. An example would be “http://www.google.com”. The first part is the protocol, which is usually “http” (HyperText Transfer Protocol). The second part is the address of the site, e.g. “www.google.com”.\nWe have stripped all punctuation so links to websites appear in the data as one word, e.g. “httpwwwgooglecom”. We hypothesize that given, given that a lot of vandalism seems to be adding links to promotional or irrelevant websites. The presence of a web address is a sign of vandalism. We can search for the presence of a web address in the words added by searching for “http” in the Added column. The grepl function returns TRUE if a string is found in another string, e.g. * grepl(“cat”,“dogs and cats”,fixed=TRUE) # TRUE * grepl(“cat”,“dogs and rats”,fixed=TRUE) # FALSE\nCreate a copy of your dataframe from the previous question:\nwikiWords2 \u0026lt;- wikiWords # make a new column in wikiWords2 that is 1 if \u0026quot;http\u0026quot; was in Added: wikiWords2$HTTP \u0026lt;- ifelse(grepl(\u0026quot;http\u0026quot;, wiki$Added, fixed = TRUE), 1, 0) # based on this new column, how many revisions added a link? table(wikiWords2$HTTP)  0 1 3659 217  217   Problem 2.2 - Problem-Specific Knowledge In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets:\nwikiTrain2 \u0026lt;- subset(wikiWords2, wikiSplit == TRUE) wikiTest2 \u0026lt;- subset(wikiWords2, wikiSplit == FALSE) Then create a new CART model using this new variable as one of the independent variables.\nwikiTree2 \u0026lt;- rpart(Vandal ~ ., data = wikiTrain2, method = \u0026quot;class\u0026quot;) What is the new accuracy of the CART model on the test-set, using a threshold of 0.5?\nwikiPred2 \u0026lt;- as.vector(predict(wikiTree2, newdata = wikiTest2, type=\u0026quot;class\u0026quot;)) head(wikiPred2) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest2$Vandal, wikiPred2)  wikiPred2 0 1 0 605 13 1 481 64 (609 + 57) / nrow(wikiTest2) [1] 0.5726569  Problem 2.3 - Problem-Specific Knowledge Another possibility is that the number of words added and removed is predictive, perhaps more so than the actual words themselves. We already have a word count available in the form of the document-term matrices (DTMs). Sum the rows of dtmAdded and dtmRemoved and add them as new variables in our dataframe wikiWords2 (called NumWordsAdded and NumWordsRemoved):\nwikiWords2$NumWordsAdded \u0026lt;- rowSums(as.matrix(dtmAdded)) wikiWords2$NumWordsRemoved \u0026lt;- rowSums(as.matrix(dtmRemoved)) What is the average number of words added?\nmean(wikiWords2$NumWordsAdded) [1] 4.050052  Problem 2.4 - Problem-Specific Knowledge In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords2.\nCreate the CART model again (using the training set and the default parameters).\nwikiTrain2b \u0026lt;- subset(wikiWords2, wikiSplit == TRUE) wikiTest2b \u0026lt;- subset(wikiWords2, wikiSplit == FALSE) Then create a new CART model using this new variable as one of the independent variables.\nwikiTree2b \u0026lt;- rpart(Vandal ~ ., data = wikiTrain2b, method = \u0026quot;class\u0026quot;) What is the new accuracy of the CART model on the test-set?\nwikiPred2b \u0026lt;- as.vector(predict(wikiTree2b, newdata = wikiTest2b, type=\u0026quot;class\u0026quot;)) head(wikiPred2b) [1] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest2b$Vandal, wikiPred2b)  wikiPred2b 0 1 0 514 104 1 297 248 (514 + 248) / nrow(wikiTest2b) [1] 0.6552021  Problem 3.1 - Using Non-Textual Data We have two pieces of “metadata” (data about data) that we haven’t yet used.\nMake a copy of wikiWords2, and call it wikiWords3:\nwikiWords3 \u0026lt;- wikiWords2 Then add the two original variables Minor and Loggedin to this new dataframe:\nwikiWords3$Minor \u0026lt;- wiki$Minor wikiWords3$Loggedin \u0026lt;- wiki$Loggedin In problem 1.5, you computed a vector called “spl” that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords3.\nwikiTrain3 \u0026lt;- subset(wikiWords3, wikiSplit == TRUE) wikiTest3 \u0026lt;- subset(wikiWords3, wikiSplit == FALSE) Build a CART model using all the training data.\nWhat is the accuracy of the model on the test-set?\nThen create a new CART model using this new variable as one of the independentvariables.\nwikiTree3 \u0026lt;- rpart(Vandal ~ ., data = wikiTrain3, method = \u0026quot;class\u0026quot;) wikiPred3 \u0026lt;- as.vector(predict(wikiTree3, newdata = wikiTest3, type=\u0026quot;class\u0026quot;)) head(wikiPred3) [1] \u0026quot;0\u0026quot; \u0026quot;1\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; table(wikiTest3$Vandal, wikiPred3)  wikiPred3 0 1 0 595 23 1 304 241 (595 + 241) / nrow(wikiTest3) [1] 0.7188306  Problem 3.2 - Using Non-Textual Data There is a substantial difference in the accuracy of the model using the meta data. Is this because we made a more complicated model?\nPlot the CART tree. How many splits are there in the tree?\nprp(wikiTree3) 3   ","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"ba2981075c05658c51efdfdb05674083","permalink":"/project/vandalism/vandalism/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/project/vandalism/vandalism/","section":"project","summary":"Developing a vandalism detector","tags":["R","Data Analytics","Machine Learning"],"title":"Vandalism Detection On Wikipedia","type":"project"},{"authors":null,"categories":null,"content":" In the lending industry, investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, if the borrower is unable to repay the loan, then the lender loses money. Therefore, lenders face the problem of predicting the risk of a borrower being unable to repay a loan.\nTo address this analysis, I’ll use publicly available data from LendingClub.com, a website that connects borrowers and investors over the Internet. This dataset represents 9,578 3-year loans that were funded through the LendingClub.com platform between May 2007 and February 2010.\nThe binary dependent variable not.fully.paid indicates that the loan was not paid back in full (the borrower either defaulted or the loan was “charged off,” meaning the borrower was deemed unlikely to ever pay it back).\nTo predict this dependent variable, I’ll use the following independent variables available to the investor when deciding whether to fund a loan:\n credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise. purpose: The purpose of the loan (takes values “credit_card”, “debt_consolidation”, “educational”, “major_purchase”, “small_business”, and “all_other”). int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates. installment: The monthly installments ($) owed by the borrower if the loan is funded. log.annual.inc: The natural log of the self-reported annual income of the borrower. dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income). fico: The FICO credit score of the borrower. days.with.cr.line: The number of days the borrower has had a credit line. revol.bal: The borrower’s revolving balance (amount unpaid at the end of the credit card billing cycle). revol.util: The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available). inq.last.6mths: The borrower’s number of inquiries by creditors in the last 6 months. delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years. pub.rec: The borrower’s number of derogatory public records (bankruptcy filings, tax liens, or judgments).  Problem 1.1 - Preparing the Dataset Load the dataset loans.csv into a dataframe called loans, and explore it using the str() and summary() functions.\nloans \u0026lt;- read.csv(\u0026quot;loans.csv\u0026quot;) str(loans) \u0026#39;data.frame\u0026#39;: 9578 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 3 2 3 3 2 2 3 1 5 3 ... $ int.rate : num 0.119 0.107 0.136 0.101 0.143 ... $ installment : num 829 228 367 162 103 ... $ log.annual.inc : num 11.4 11.1 10.4 11.4 11.3 ... $ dti : num 19.5 14.3 11.6 8.1 15 ... $ fico : int 737 707 682 712 667 727 667 722 682 707 ... $ days.with.cr.line: num 5640 2760 4710 2700 4066 ... $ revol.bal : int 28854 33623 3511 33667 4740 50807 3839 24220 69909 5630 ... $ revol.util : num 52.1 76.7 25.6 73.2 39.5 51 76.8 68.6 51.1 23 ... $ inq.last.6mths : int 0 0 1 1 0 0 0 0 1 1 ... $ delinq.2yrs : int 0 0 0 0 1 0 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 1 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 1 1 0 0 ... summary(loans)  credit.policy purpose int.rate Min. :0.000 all_other :2331 Min. :0.0600 1st Qu.:1.000 credit_card :1262 1st Qu.:0.1039 Median :1.000 debt_consolidation:3957 Median :0.1221 Mean :0.805 educational : 343 Mean :0.1226 3rd Qu.:1.000 home_improvement : 629 3rd Qu.:0.1407 Max. :1.000 major_purchase : 437 Max. :0.2164 small_business : 619 installment log.annual.inc dti fico Min. : 15.67 Min. : 7.548 Min. : 0.000 Min. :612.0 1st Qu.:163.77 1st Qu.:10.558 1st Qu.: 7.213 1st Qu.:682.0 Median :268.95 Median :10.928 Median :12.665 Median :707.0 Mean :319.09 Mean :10.932 Mean :12.607 Mean :710.8 3rd Qu.:432.76 3rd Qu.:11.290 3rd Qu.:17.950 3rd Qu.:737.0 Max. :940.14 Max. :14.528 Max. :29.960 Max. :827.0 NA\u0026#39;s :4 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : 0.00 Min. : 0.000 1st Qu.: 2820 1st Qu.: 3187 1st Qu.: 22.70 1st Qu.: 0.000 Median : 4140 Median : 8596 Median : 46.40 Median : 1.000 Mean : 4562 Mean : 16914 Mean : 46.87 Mean : 1.572 3rd Qu.: 5730 3rd Qu.: 18250 3rd Qu.: 71.00 3rd Qu.: 2.000 Max. :17640 Max. :1207359 Max. :119.00 Max. :33.000 NA\u0026#39;s :29 NA\u0026#39;s :62 NA\u0026#39;s :29 delinq.2yrs pub.rec not.fully.paid Min. : 0.0000 Min. :0.0000 Min. :0.0000 1st Qu.: 0.0000 1st Qu.:0.0000 1st Qu.:0.0000 Median : 0.0000 Median :0.0000 Median :0.0000 Mean : 0.1638 Mean :0.0621 Mean :0.1601 3rd Qu.: 0.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 Max. :13.0000 Max. :5.0000 Max. :1.0000 NA\u0026#39;s :29 NA\u0026#39;s :29  What proportion of the loans in the dataset were not paid in full?\ntable(loans$not.fully.paid)  0 1 8045 1533  1533 / (8045 + 1533) [1] 0.1600543  Problem 1.2 - Preparing the Dataset Which of the following variables has at least one missing observation?\n log.annual.inc days.with.cr.line revol.util inq.last.6mths delinq.2yrs pub.rec   Problem 1.3 - Preparing the Dataset Which of the following is the best reason to fill in the missing values for these variables instead of removing observations with missing data? (Hint: you can use the subset() function to build a dataframe with the observations missing at least one value. To test if a variable, for example pub.rec, is missing a value, use is.na(pub.rec).)\nloansNA \u0026lt;- subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec)) str(loansNA) \u0026#39;data.frame\u0026#39;: 62 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 1 4 3 3 6 2 1 4 1 1 ... $ int.rate : num 0.113 0.11 0.113 0.123 0.106 ... $ installment : num 98.7 52.4 263.2 23.4 182.4 ... $ log.annual.inc : num 10.53 10.53 10.71 9.85 11.26 ... $ dti : num 7.72 15.84 8.75 12.38 4.26 ... $ fico : int 677 682 682 662 697 667 687 687 722 752 ... $ days.with.cr.line: num 1680 1830 2490 1200 4141 ... $ revol.bal : int 0 0 0 0 0 0 0 0 0 0 ... $ revol.util : num NA NA NA NA NA NA NA NA NA NA ... $ inq.last.6mths : int 1 0 1 1 0 0 1 0 1 0 ... $ delinq.2yrs : int 0 0 1 0 0 0 0 0 0 0 ... $ pub.rec : int 0 0 0 0 1 0 0 0 0 0 ... $ not.fully.paid : int 1 0 1 0 0 1 0 0 0 0 ... summary(loansNA)  credit.policy purpose int.rate Min. :0.0000 all_other :41 Min. :0.0712 1st Qu.:0.0000 credit_card : 3 1st Qu.:0.0933 Median :0.0000 debt_consolidation: 8 Median :0.1122 Mean :0.3871 educational : 3 Mean :0.1187 3rd Qu.:1.0000 home_improvement : 1 3rd Qu.:0.1456 Max. :1.0000 major_purchase : 5 Max. :0.1913 small_business : 1 installment log.annual.inc dti fico Min. : 23.35 Min. : 8.294 Min. : 0.000 Min. :642.0 1st Qu.: 78.44 1st Qu.:10.096 1st Qu.: 5.147 1st Qu.:682.0 Median :145.91 Median :10.639 Median :10.000 Median :707.0 Mean :159.19 Mean :10.558 Mean : 9.184 Mean :711.5 3rd Qu.:192.73 3rd Qu.:11.248 3rd Qu.:11.540 3rd Qu.:740.8 Max. :859.57 Max. :13.004 Max. :22.720 Max. :802.0 NA\u0026#39;s :4 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : NA Min. :0.000 1st Qu.:1830 1st Qu.: 0 1st Qu.: NA 1st Qu.:0.000 Median :2580 Median : 0 Median : NA Median :1.000 Mean :3158 Mean : 5476 Mean :NaN Mean :1.182 3rd Qu.:4621 3rd Qu.: 0 3rd Qu.: NA 3rd Qu.:2.000 Max. :7890 Max. :290291 Max. : NA Max. :6.000 NA\u0026#39;s :29 NA\u0026#39;s :62 NA\u0026#39;s :29 delinq.2yrs pub.rec not.fully.paid Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 Median :0.0000 Median :0.0000 Median :0.0000 Mean :0.2121 Mean :0.0303 Mean :0.1935 3rd Qu.:0.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 Max. :4.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :29 NA\u0026#39;s :29  table(loansNA$not.fully.paid)  0 1 50 12  12 / (50 + 12) [1] 0.1935484 We want to be able to predict risk for all borrowers, instead of just the ones with all data reported.   Problem 1.4 - Preparing the Dataset For the rest of this problem, I’ll be using a revised version of the dataset that has the missing values filled in with multiple imputation. To ensure everybody has the same dataframe going forward, you can either run the code below in your R console (if you haven’t already, run the code install.packages(“mice”) first), or you can download and load into R the dataset we created after running the imputation: loans_imputed.csv.\nIMPORTANT NOTE: On certain operating systems, the imputation results are not the same even if you set the random seed. If you decide to do the imputation yourself, please still read the provided imputed dataset (loans_imputed.csv) into R and compare your results, using the summary function. If the results are different, please make sure to use the data in loans_imputed.csv for the rest of the problem.\n library(mice) set.seed(144) vars.for.imputation = setdiff(names(loans), “not.fully.paid”) imputed = complete(mice(loans[vars.for.imputation])) loans[vars.for.imputation] = imputed  loans \u0026lt;- read.csv(\u0026quot;loans_imputed.csv\u0026quot;) Note, that to do this imputation, we set vars.for.imputation to all variables in the dataframe except for not.fully.paid, to impute the values using all of the other independent variables.\nWhat best describes the process we just used to handle missing values? #### We predicted missing variable values using the available independent variables for each observation.\n Problem 2.1 - Prediction Models Now that we have prepared the dataset, we need to split it into a training and testing set. To ensure everybody obtains the same split, set the random seed to 144 (even though you already did so earlier in the problem) and use the sample.split function to select the 70% of observations for the training set (the dependent variable for sample.split is not.fully.paid). Name the dataframes train and test.\nset.seed(144) split = sample.split(loans$not.fully.paid, SplitRatio = 0.7) train = subset(loans, split == TRUE) test = subset(loans, split == FALSE) Now, use logistic regression trained on the training set to predict the dependent variable not.fully.paid using all the independent variables.\nLoansLog \u0026lt;- glm(not.fully.paid ~ ., data = train, family = binomial) Which independent variables are significant in our model? (Significant variables have at least one star, or a Pr(\u0026gt;|z|) value less than 0.05.)\nsummary(LoansLog)  Call: glm(formula = not.fully.paid ~ ., family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -2.2049 -0.6205 -0.4951 -0.3606 2.6397 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 9.187e+00 1.554e+00 5.910 3.42e-09 *** credit.policy -3.368e-01 1.011e-01 -3.332 0.000861 *** purposecredit_card -6.141e-01 1.344e-01 -4.568 4.93e-06 *** purposedebt_consolidation -3.212e-01 9.183e-02 -3.498 0.000469 *** purposeeducational 1.347e-01 1.753e-01 0.768 0.442201 purposehome_improvement 1.727e-01 1.480e-01 1.167 0.243135 purposemajor_purchase -4.830e-01 2.009e-01 -2.404 0.016203 * purposesmall_business 4.120e-01 1.419e-01 2.905 0.003678 ** int.rate 6.110e-01 2.085e+00 0.293 0.769446 installment 1.275e-03 2.092e-04 6.093 1.11e-09 *** log.annual.inc -4.337e-01 7.148e-02 -6.067 1.30e-09 *** dti 4.638e-03 5.502e-03 0.843 0.399288 fico -9.317e-03 1.710e-03 -5.448 5.08e-08 *** days.with.cr.line 2.371e-06 1.588e-05 0.149 0.881343 revol.bal 3.085e-06 1.168e-06 2.641 0.008273 ** revol.util 1.839e-03 1.535e-03 1.199 0.230722 inq.last.6mths 8.437e-02 1.600e-02 5.275 1.33e-07 *** delinq.2yrs -8.320e-02 6.561e-02 -1.268 0.204762 pub.rec 3.300e-01 1.139e-01 2.898 0.003756 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 5896.6 on 6704 degrees of freedom Residual deviance: 5485.2 on 6686 degrees of freedom AIC: 5523.2 Number of Fisher Scoring iterations: 5  credit.policy purposecredit_card purposedebt_consolidation purposemajor_purchase purposesmall_business installment log.annual.inc fico revol.bal inq.last.6mths pub.rec   Problem 2.2 - Prediction Models Consider two loan applications, which are identical other than the fact that the borrower in Application A has FICO credit score 700 while the borrower in Application B has FICO credit score 710.\nLet’s Logit(A) be the log odds of loan A not being paid back in full, according to our logistic regression model, and define Logit(B) similarly for loan B.\nWhat is the value of Logit(A) - Logit(B)?\napplicationA \u0026lt;- train[1, ] applicationB \u0026lt;- applicationA applicationA$fico = 700 applicationB$fico = 710 applicationA  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 700 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 applicationB  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 710 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 applications \u0026lt;- rbind(applicationA, applicationB) applications  credit.policy purpose int.rate installment log.annual.inc 1 1 debt_consolidation 0.1189 829.1 11.35041 2 1 debt_consolidation 0.1189 829.1 11.35041 dti fico days.with.cr.line revol.bal revol.util inq.last.6mths 1 19.48 700 5639.958 28854 52.1 0 2 19.48 710 5639.958 28854 52.1 0 delinq.2yrs pub.rec not.fully.paid 1 0 0 0 2 0 0 0 PredApplications \u0026lt;- predict(LoansLog, type = \u0026quot;response\u0026quot;, newdata = applications) PredApplications  1 2 0.1828795 0.1693660  PredApplications[1] - PredApplications[2]  1 0.01351347  CalcApplicationA \u0026lt;- 1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 700))) CalcApplicationB \u0026lt;- 1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 710))) CalcApplicationA - CalcApplicationB [1] 0.005902546 Now, let O(A) be the odds of loan A not being paid back in full, according to our logistic regression model, and define O(B) similarly for loan B.\nWhat is the value of O(A)/O(B)? (HINT: Use the mathematical rule that exp(A + B + C) = exp(A)exp(B)exp(C). Also, remember that exp() is the exponential function in R.)\nOddsApplicationA \u0026lt;- PredApplications[1] / (1 - PredApplications[1]) OddsApplicationA  1 0.2238097  OddsApplicationB \u0026lt;- PredApplications[2] / (1 - PredApplications[2]) OddsApplicationB  2 0.2038997  OddsApplicationA / OddsApplicationB  1 1.097646  1.097646 =\u0026gt; OK! After computing the logs, try log(odds) for previous problem\nlog(OddsApplicationA)  1 -1.496959  log(OddsApplicationB)  2 -1.590127  log(OddsApplicationA) - log(OddsApplicationB)  1 0.0931679   0.0931679 =\u0026gt; OK!   Problem 2.3 - Prediction Models Predict the probability of the test-set loans not being paid back in full (remember type=“response” for the predict function). Store these predicted probabilities in a variable named predicted.risk and add it to our test-set (we will use this variable in later parts of the problem). Compute the confusion matrix using a threshold of 0.5.\npredicted.risk \u0026lt;- predict(LoansLog, type = \u0026quot;response\u0026quot;, newdata = test) str(predicted.risk)  Named num [1:2873] 0.0771 0.1728 0.1087 0.1016 0.0681 ... - attr(*, \u0026quot;names\u0026quot;)= chr [1:2873] \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;10\u0026quot; \u0026quot;12\u0026quot; ... str(test) \u0026#39;data.frame\u0026#39;: 2873 obs. of 14 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 2 3 3 3 1 3 1 2 7 2 ... $ int.rate : num 0.107 0.136 0.122 0.132 0.08 ... $ installment : num 228.2 366.9 84.1 253.6 188 ... $ log.annual.inc : num 11.1 10.4 10.2 11.8 11.2 ... $ dti : num 14.29 11.63 10 9.16 16.08 ... $ fico : int 707 682 707 662 772 662 772 797 712 682 ... $ days.with.cr.line: num 2760 4710 2730 4298 4889 ... $ revol.bal : int 33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ... $ revol.util : num 76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ... $ inq.last.6mths : int 0 1 1 2 1 0 0 0 0 3 ... $ delinq.2yrs : int 0 0 0 1 0 1 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 0 0 0 0 ... test$predicted.risk \u0026lt;- predicted.risk summary(test)  credit.policy purpose int.rate Min. :0.0000 all_other : 688 Min. :0.0600 1st Qu.:1.0000 credit_card : 411 1st Qu.:0.1028 Median :1.0000 debt_consolidation:1206 Median :0.1221 Mean :0.8047 educational : 93 Mean :0.1225 3rd Qu.:1.0000 home_improvement : 186 3rd Qu.:0.1393 Max. :1.0000 major_purchase : 105 Max. :0.2121 small_business : 184 installment log.annual.inc dti fico Min. : 15.67 Min. : 8.102 Min. : 0.00 Min. :612.0 1st Qu.:163.57 1st Qu.:10.560 1st Qu.: 7.16 1st Qu.:682.0 Median :267.74 Median :10.933 Median :12.85 Median :707.0 Mean :316.99 Mean :10.928 Mean :12.75 Mean :710.8 3rd Qu.:421.89 3rd Qu.:11.290 3rd Qu.:18.30 3rd Qu.:737.0 Max. :926.83 Max. :13.459 Max. :29.96 Max. :822.0 days.with.cr.line revol.bal revol.util inq.last.6mths Min. : 179 Min. : 0 Min. : 0.00 Min. : 0.000 1st Qu.: 2795 1st Qu.: 3362 1st Qu.: 23.40 1st Qu.: 0.000 Median : 4140 Median : 8712 Median : 46.90 Median : 1.000 Mean : 4494 Mean : 17198 Mean : 47.02 Mean : 1.576 3rd Qu.: 5670 3rd Qu.: 18728 3rd Qu.: 70.40 3rd Qu.: 2.000 Max. :17640 Max. :1207359 Max. :108.80 Max. :24.000 delinq.2yrs pub.rec not.fully.paid predicted.risk Min. : 0.0000 Min. :0.00000 Min. :0.0000 Min. :0.02114 1st Qu.: 0.0000 1st Qu.:0.00000 1st Qu.:0.0000 1st Qu.:0.09461 Median : 0.0000 Median :0.00000 Median :0.0000 Median :0.13697 Mean : 0.1605 Mean :0.05743 Mean :0.1601 Mean :0.15785 3rd Qu.: 0.0000 3rd Qu.:0.00000 3rd Qu.:0.0000 3rd Qu.:0.19658 Max. :13.0000 Max. :3.00000 Max. :1.0000 Max. :0.95373  table(test$not.fully.paid, test$predicted.risk \u0026gt; 0.5)  FALSE TRUE 0 2400 13 1 457 3 What is the accuracy of the logistic regression model?\n(2400 + 3) / nrow(test) [1] 0.8364079 What is the accuracy of the baseline model?\ntable(test$not.fully.paid)  0 1 2413 460  2413 / (2413 + 460) [1] 0.8398886  Problem 2.4 - Prediction Models Use the ROCR package to compute the test-set AUC.\nROCRpred = prediction(test$predicted.risk, test$not.fully.paid) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.6720995 The model has poor accuracy at the threshold 0.5. But, despite the poor accuracy, we will see later how an investor can still leverage this logistic regression model to make profitable investments.\n Problem 3.1 - A “Smart Baseline” In the previous problem, we built a logistic regression model that has an AUC significantly higher than the AUC of 0.5 that would be obtained by randomly ordering observations. However, LendingClub.com assigns the interest rate to a loan based on their estimate of that loan’s risk.\nThis variable, int.rate, is an independent variable in our dataset. In this part, we will investigate using the loan’s interest rate as a “smart baseline” to order the loans according to risk.\nUsing the training set, build a bivariate logistic regression model (aka a logistic regression model with a single independent variable) that predicts the dependent variable not.fully.paid using only the variable int.rate.\nLoansLog2 \u0026lt;- glm(not.fully.paid ~ int.rate, data = train, family = binomial) summary(LoansLog2)  Call: glm(formula = not.fully.paid ~ int.rate, family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -1.0547 -0.6271 -0.5442 -0.4361 2.2914 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -3.6726 0.1688 -21.76 \u0026lt;2e-16 *** int.rate 15.9214 1.2702 12.54 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 5896.6 on 6704 degrees of freedom Residual deviance: 5734.8 on 6703 degrees of freedom AIC: 5738.8 Number of Fisher Scoring iterations: 4 The variable int.rate is highly significant in the bivariate model, but it is not significant at the 0.05 level in the model trained with all the independent variables.\nWhat is the most likely explanation for this difference? #### int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.\n Problem 3.2 - A “Smart Baseline” Make test-set predictions for the bivariate model.\nWhat is the highest predicted probability of a loan not being paid in full on the testing set?\npredicted.risk2 \u0026lt;- predict(LoansLog2, type = \u0026quot;response\u0026quot;, newdata = test) max(predicted.risk2) [1] 0.426624 0.426624 With a logistic regression cut-off of 0.5, how many loans would be predicted as not being paid in full on the testing set?\ntable(test$not.fully.paid, predicted.risk2 \u0026gt; 0.5)  FALSE 0 2413 1 460 str(test) \u0026#39;data.frame\u0026#39;: 2873 obs. of 15 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 2 3 3 3 1 3 1 2 7 2 ... $ int.rate : num 0.107 0.136 0.122 0.132 0.08 ... $ installment : num 228.2 366.9 84.1 253.6 188 ... $ log.annual.inc : num 11.1 10.4 10.2 11.8 11.2 ... $ dti : num 14.29 11.63 10 9.16 16.08 ... $ fico : int 707 682 707 662 772 662 772 797 712 682 ... $ days.with.cr.line: num 2760 4710 2730 4298 4889 ... $ revol.bal : int 33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ... $ revol.util : num 76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ... $ inq.last.6mths : int 0 1 1 2 1 0 0 0 0 3 ... $ delinq.2yrs : int 0 0 0 1 0 1 0 0 0 0 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 0 0 0 0 0 0 0 0 0 0 ... $ predicted.risk : num 0.0771 0.1728 0.1087 0.1016 0.0681 ...  0   Problem 3.3 - A “Smart Baseline” What is the test-set AUC of the bivariate model?\nROCRpred2 = prediction(predicted.risk2, test$not.fully.paid) as.numeric(performance(ROCRpred2, \u0026quot;auc\u0026quot;)@y.values) [1] 0.6239081  Problem 4.1 - Computing the Profitability of an Investment While thus far we have predicted if a loan will be paid back or not, an investor needs to identify loans that are expected to be profitable.\nIf the loan is paid back in full, then the investor makes interest on the loan. However, if the loan is not paid back, the investor loses the money invested. Therefore, the investor should seek loans that best balance this risk and reward.\nTo compute interest revenue, consider a $c investment in a loan that has an annual interest rate r over a period of t years.\nUsing continuous compounding of interest, this investment pays back c * exp(rt) dollars by the end of the t years, where exp(rt) is e raised to the r*t power.\nHow much does a $10 investment with an annual interest rate of 6% pay back after 3 years, using continuous compounding of interest? Hint: remember to convert the percentage to a proportion before doing the math.\n10 * exp(0.06 * 3) [1] 11.97217  Problem 4.2 - Computing the Profitability of an Investment While the investment has value c * exp(rt) dollars after collecting interest, the investor had to pay $c for the investment.\nWhat is the profit to the investor if the investment is paid back in full?\n10 * exp(0.06 * 3) - 10 [1] 1.972174  Problem 4.3 - Computing the Profitability of an Investment Now, consider the case where the investor made a $c investment, but it was not paid back in full. Assume, conservatively, that no money was received from the borrower (often a lender will receive some but not all of the value of the loan, making this a pessimistic assumption of how much is received).\nWhat is the loss to the investor in this scenario? #### -10\n Problem 5.1 - A Simple Investment Strategy In the previous subproblem, we concluded that an investor who invested c dollars in a loan with interest rate r for t years makes c * (exp(rt) - 1) dollars of profit if the loan is paid back in full and -c dollars of profit if the loan is not paid back in full (pessimistically).\nIn order to evaluate the quality of an investment strategy, we need to compute this profit for each loan in the test-set.\nFor this variable, we will assume a $1 investment (aka c=1). To create the variable, we first assign to the profit for a fully paid loan, exp(rt)-1, to every observation, and we then replace this value with -1 in the cases where the loan was not paid in full.\nAll the loans in our dataset are 3-year loans, meaning t=3 in our calculations.\ntest$profit = exp(test$int.rate*3) - 1 test$profit[test$not.fully.paid == 1] = -1 What is the maximum profit of a $10 investment in any loan in the testing set?\nmax(test$profit) * 10 [1] 8.894769  Problem 6.1 - An Investment Strategy Based on Risk A simple investment strategy of equally investing in all the loans would yield profit $20.94 for a $100 investment. But this simple investment strategy does not leverage the prediction model we built earlier in this problem.\nAs stated earlier, investors seek loans that balance reward with risk, in that they simultaneously have high interest rates and a low risk of not being paid back.\nTo meet this objective, I’ll analyze an investment strategy in which the investor only purchases loans with a high interest rate (a rate of at least 15%), but amongst these loans selects the ones with the lowest predicted risk of not being paid back in full.\nWe will model an investor who invests $1 in each of the most promising 100 loans.\nFirst, use the subset() function to build a dataframe called highInterest consisting of the test-set loans with an interest rate of at least 15%.\nhighInterest \u0026lt;- subset(test, int.rate \u0026gt;= 0.15) What is the average profit of a $1 investment in one of these high-interest loans?\nmean(highInterest$profit) [1] 0.2251015 What proportion of the high-interest loans were not paid back in full?\ntable(highInterest$not.fully.paid)  0 1 327 110  110 / (327 + 110) [1] 0.2517162  Problem 6.2 - An Investment Strategy Based on Risk Next, I’ll determine the 100th smallest predicted probability of not paying in full by sorting the predicted risks in increasing order and selecting the 100th element of this sorted list.\ncutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100] cutoff [1] 0.1763305 Use the subset() function to build a dataframe called selectedLoans consisting of the high-interest loans with predicted risk not exceeding the cut-off we just computed. Check to make sure you have selected 100 loans for investment.\nselectedLoans \u0026lt;- subset(highInterest, highInterest$predicted.risk \u0026lt;= cutoff) summary(selectedLoans)  credit.policy purpose int.rate installment Min. :0.00 all_other : 8 Min. :0.1501 Min. : 48.79 1st Qu.:1.00 credit_card :17 1st Qu.:0.1533 1st Qu.:176.16 Median :1.00 debt_consolidation:60 Median :0.1570 Median :309.37 Mean :0.93 educational : 1 Mean :0.1610 Mean :358.30 3rd Qu.:1.00 home_improvement : 1 3rd Qu.:0.1645 3rd Qu.:473.10 Max. :1.00 major_purchase : 7 Max. :0.2052 Max. :907.60 small_business : 6 log.annual.inc dti fico days.with.cr.line Min. : 9.575 Min. : 0.00 Min. :642.0 Min. : 1140 1st Qu.:10.776 1st Qu.: 6.05 1st Qu.:662.0 1st Qu.: 2162 Median :11.127 Median :12.35 Median :672.0 Median : 3630 Mean :11.203 Mean :12.19 Mean :680.5 Mean : 3911 3rd Qu.:11.670 3rd Qu.:18.23 3rd Qu.:692.0 3rd Qu.: 5010 Max. :13.305 Max. :28.15 Max. :782.0 Max. :13170 revol.bal revol.util inq.last.6mths delinq.2yrs Min. : 0 Min. : 0.00 Min. : 0.00 Min. :0.00 1st Qu.: 3768 1st Qu.:45.92 1st Qu.: 0.00 1st Qu.:0.00 Median : 9691 Median :71.65 Median : 0.00 Median :0.00 Mean : 19923 Mean :65.79 Mean : 0.89 Mean :0.33 3rd Qu.: 24534 3rd Qu.:93.80 3rd Qu.: 1.00 3rd Qu.:0.00 Max. :168496 Max. :99.70 Max. :10.00 Max. :4.00 pub.rec not.fully.paid predicted.risk profit Min. :0.00 Min. :0.00 Min. :0.06871 Min. :-1.0000 1st Qu.:0.00 1st Qu.:0.00 1st Qu.:0.13596 1st Qu.: 0.5823 Median :0.00 Median :0.00 Median :0.15327 Median : 0.5992 Mean :0.03 Mean :0.19 Mean :0.14794 Mean : 0.3128 3rd Qu.:0.00 3rd Qu.:0.00 3rd Qu.:0.16514 3rd Qu.: 0.6317 Max. :1.00 Max. :1.00 Max. :0.17633 Max. : 0.8508  str(selectedLoans) \u0026#39;data.frame\u0026#39;: 100 obs. of 16 variables: $ credit.policy : int 1 1 1 1 1 1 1 1 1 1 ... $ purpose : Factor w/ 7 levels \u0026quot;all_other\u0026quot;,\u0026quot;credit_card\u0026quot;,..: 7 2 3 1 3 5 2 3 2 3 ... $ int.rate : num 0.15 0.153 0.158 0.159 0.156 ... $ installment : num 225 444 420 246 245 ... $ log.annual.inc : num 12.3 11 11.5 11.5 10.8 ... $ dti : num 6.45 19.52 18.55 24.19 2.72 ... $ fico : int 677 667 667 667 672 702 667 672 662 682 ... $ days.with.cr.line: num 6240 2701 4560 5376 3010 ... $ revol.bal : int 56411 33074 34841 590 3273 4980 15977 16473 22783 87502 ... $ revol.util : num 75.3 68.8 89.6 84.3 69.6 55.3 83.6 94.1 93.7 96.4 ... $ inq.last.6mths : int 0 2 0 0 1 1 0 2 3 0 ... $ delinq.2yrs : int 0 0 0 0 0 0 0 2 1 1 ... $ pub.rec : int 0 0 0 0 0 0 0 0 0 0 ... $ not.fully.paid : int 1 0 0 0 1 0 0 0 0 0 ... $ predicted.risk : num 0.164 0.169 0.158 0.162 0.147 ... $ profit : num -1 0.584 0.604 0.61 -1 ... What is the profit of the investor, who invested $1 in each of these 100 loans?\nsum(selectedLoans$profit) [1] 31.27825 How many of 100 selected loans were not paid back in full?\ntable(selectedLoans$not.fully.paid)  0 1 81 19  19   Conclusion We have now seen how analytics can be used to select a subset of the high-interest loans that were paid back at only a slightly lower rate than average, resulting in a significant increase in the profit from our investor’s $100 investment. Although the logistic regression models developed in this analysis did not have large AUC values, we see that they still provided the edge needed to improve the profitability of an investment portfolio.\nWe conclude with a note of warning. Throughout this analysis I’ve assume that the loans we invest in will perform in the same way as the loans we used to train our model, even though our training set covers a relatively short period of time. If there is an economic shock like a large financial downturn, default rates might be significantly higher than those observed in the training set and we might end up losing money instead of profiting. Investors must pay careful attention to such risk when making investment decisions.\n ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"71ca679d3bab7188324ce724fa8dc839","permalink":"/project/loan_repayment/loan_repayments/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/loan_repayment/loan_repayments/","section":"project","summary":"Predict the risk of a borrower being unable to repay a loan","tags":["R","Data Analytics","Machine Learning"],"title":"Predict Loan Repayments","type":"project"},{"authors":null,"categories":null,"content":" In many criminal justice systems around the world, inmates deemed not to be a threat to society are released from prison under the parole system prior to completing their sentence. They are still considered to be serving their sentence while on parole, and they can be returned to prison if they violate the terms of their parole.\nParole boards are charged with identifying which inmates are good candidates for release on parole. They seek to release inmates who will not commit additional crimes after release. In this analysis, I’ll build and validate a model that predicts if an inmate will violate the terms of his or her parole.\nSuch a model could be useful to a parole board when deciding to approve or deny an application for parole.\nFor this prediction task, I’ll use data from the U.S 2004 National Corrections Reporting Program, a nationwide census of parole releases that occurred during 2004.\nI’ve limited my focus to parolees who served no more than 6 months in prison and whose maximum sentence for all charges did not exceed 18 months.\nThe dataset contains all such parolees who either successfully completed their term of parole during 2004 or those who violated the terms of their parole during that year. The dataset contains the following variables:\n male: 1 if the parolee is male, 0 if female race: 1 if the parolee is white, 2 otherwise age: the parolee’s age (in years) when he or she was released from prison state: a code for the parolee’s state. 2 is Kentucky, 3 is Louisiana, 4 is Virginia, and 1 is any other state. The three states were selected due to having a high representation in the dataset. time.served: the number of months the parolee served in prison (limited by the inclusion criteria to not exceed 6 months). max.sentence: the maximum sentence length for all charges, in months (limited by the inclusion criteria to not exceed 18 months). multiple.offenses: 1 if the parolee was incarcerated for multiple offenses, 0 otherwise. crime: a code for the parolee’s main crime leading to incarceration. 2 is larceny, 3 is drug-related crime, 4 is driving-related crime, and 1 is any other crime. violator: 1 if the parolee violated the parole, and 0 if the parolee completed the parole without violation.  Loading the Dataset \u0026amp; EDA parole \u0026lt;- read.csv(\u0026quot;parole.csv\u0026quot;) str(parole) \u0026#39;data.frame\u0026#39;: 675 obs. of 9 variables: $ male : int 1 0 1 1 1 1 1 0 0 1 ... $ race : int 1 1 2 1 2 2 1 1 1 2 ... $ age : num 33.2 39.7 29.5 22.4 21.6 46.7 31 24.6 32.6 29.1 ... $ state : int 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.5 5.4 5.6 5.7 5.4 6 6 4.8 4.5 4.7 ... $ max.sentence : int 18 12 12 18 12 18 18 12 13 12 ... $ multiple.offenses: int 0 0 0 0 0 0 0 0 0 0 ... $ crime : int 4 3 3 1 1 4 3 1 3 2 ... $ violator : int 0 0 0 0 0 0 0 0 0 0 ... summary(parole)  male race age state Min. :0.0000 Min. :1.000 Min. :18.40 Min. :1.000 1st Qu.:1.0000 1st Qu.:1.000 1st Qu.:25.35 1st Qu.:2.000 Median :1.0000 Median :1.000 Median :33.70 Median :3.000 Mean :0.8074 Mean :1.424 Mean :34.51 Mean :2.887 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:42.55 3rd Qu.:4.000 Max. :1.0000 Max. :2.000 Max. :67.00 Max. :4.000 time.served max.sentence multiple.offenses crime Min. :0.000 Min. : 1.00 Min. :0.0000 Min. :1.000 1st Qu.:3.250 1st Qu.:12.00 1st Qu.:0.0000 1st Qu.:1.000 Median :4.400 Median :12.00 Median :1.0000 Median :2.000 Mean :4.198 Mean :13.06 Mean :0.5363 Mean :2.059 3rd Qu.:5.200 3rd Qu.:15.00 3rd Qu.:1.0000 3rd Qu.:3.000 Max. :6.000 Max. :18.00 Max. :1.0000 Max. :4.000 violator Min. :0.0000 1st Qu.:0.0000 Median :0.0000 Mean :0.1156 3rd Qu.:0.0000 Max. :1.0000  How many parolees are contained in the dataset? #### 675\n Problem 1.1 - Preparing the Dataset Which variables in this dataset are unordered factors with at least three levels? #### state, crime\n Problem 1.2 - Preparing the Dataset In the last subproblem, we identified variables that are unordered factors with at least 3 levels, so we need to convert them to factors for our prediction problem.\nUsing the as.factor() function, we convert these variables to factors. Keep in mind that we are not changing the values, just the way R understands them (the values are still numbers).\nparole$state \u0026lt;- as.factor(parole$state) parole$crime \u0026lt;- as.factor(parole$crime) How does the output of summary() change for a factor variable as compared to a numerical variable?\nsummary(parole)  male race age state time.served Min. :0.0000 Min. :1.000 Min. :18.40 1:143 Min. :0.000 1st Qu.:1.0000 1st Qu.:1.000 1st Qu.:25.35 2:120 1st Qu.:3.250 Median :1.0000 Median :1.000 Median :33.70 3: 82 Median :4.400 Mean :0.8074 Mean :1.424 Mean :34.51 4:330 Mean :4.198 3rd Qu.:1.0000 3rd Qu.:2.000 3rd Qu.:42.55 3rd Qu.:5.200 Max. :1.0000 Max. :2.000 Max. :67.00 Max. :6.000 max.sentence multiple.offenses crime violator Min. : 1.00 Min. :0.0000 1:315 Min. :0.0000 1st Qu.:12.00 1st Qu.:0.0000 2:106 1st Qu.:0.0000 Median :12.00 Median :1.0000 3:153 Median :0.0000 Mean :13.06 Mean :0.5363 4:101 Mean :0.1156 3rd Qu.:15.00 3rd Qu.:1.0000 3rd Qu.:0.0000 Max. :18.00 Max. :1.0000 Max. :1.0000  The output becomes similar to that of the table() function applied to that variable   Problem 2.1 - Splitting into a Training and Testing Set To ensure consistent training/testing set splits, run the following 5 lines of code (do not include the line numbers at the beginning):\nset.seed(144) # 70% to the training set, 30% to the testing set split = sample.split(parole$violator, SplitRatio = 0.7) train = subset(parole, split == TRUE) test = subset(parole, split == FALSE) Roughly what proportion of parolees have been allocated to the training and testing sets?\nstr(train) \u0026#39;data.frame\u0026#39;: 473 obs. of 9 variables: $ male : int 1 1 1 1 1 0 0 1 1 1 ... $ race : int 1 1 2 2 1 1 2 1 1 1 ... $ age : num 33.2 22.4 21.6 46.7 31 32.6 28.4 20.5 30.1 37.8 ... $ state : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.5 5.7 5.4 6 6 4.5 4.5 5.9 5.3 5.3 ... $ max.sentence : int 18 18 12 18 18 13 12 12 16 8 ... $ multiple.offenses: int 0 0 0 0 0 0 1 0 0 0 ... $ crime : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 4 1 1 4 3 3 1 1 3 3 ... $ violator : int 0 0 0 0 0 0 0 0 0 0 ... 473 / 675 [1] 0.7007407 str(test) \u0026#39;data.frame\u0026#39;: 202 obs. of 9 variables: $ male : int 0 1 0 1 1 1 1 1 1 1 ... $ race : int 1 2 1 2 2 1 1 2 1 1 ... $ age : num 39.7 29.5 24.6 29.1 24.5 32.8 36.7 36.5 33.5 37.3 ... $ state : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time.served : num 5.4 5.6 4.8 4.7 6 5.9 0.9 3.9 4.2 4.6 ... $ max.sentence : int 12 12 12 12 16 16 16 12 12 12 ... $ multiple.offenses: int 0 0 0 0 0 0 0 1 1 1 ... $ crime : Factor w/ 4 levels \u0026quot;1\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;4\u0026quot;: 3 3 1 2 3 3 3 4 1 1 ... $ violator : int 0 0 0 0 0 0 0 1 1 1 ... 202 / 675  [1] 0.2992593  Problem 2.2 - Splitting into a Training and Testing Set Now, suppose you re-ran lines [1]-[5] of Problem 3.1. What would you expect? #### The exact same training/testing set split as the first execution of [1]-[5]\nIf you instead ONLY re-ran lines [3]-[5], what would you expect? #### A different training/testing set split from the first execution of [1]-[5]\nIf you instead called set.seed() with a different number and then re-ran lines [3]-[5] of Problem 3.1, what would you expect? #### A different training/testing set split from the first execution of [1]-[5]\n?sample.split  Problem 3.1 - Building a Logistic Regression Model If you tested other training/testing set splits in the previous section, please re-run the original 5 lines of code to obtain the original split. Using glm (and remembering the parameter family=“binomial”), train a logistic regression model on the training set. Your dependent variable is “violator”, and you should use all of the other variables as independent variables.\nWhat variables are significant in this model? Significant variables should have a least one star, or should have a probability less than 0.05 (the column Pr(\u0026gt;|z|) in the summary output).\nParoleViolatorLog \u0026lt;- glm(violator ~ ., data = train, family = binomial) summary(ParoleViolatorLog)  Call: glm(formula = violator ~ ., family = binomial, data = train) Deviance Residuals: Min 1Q Median 3Q Max -1.7041 -0.4236 -0.2719 -0.1690 2.8375 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -4.2411574 1.2938852 -3.278 0.00105 ** male 0.3869904 0.4379613 0.884 0.37690 race 0.8867192 0.3950660 2.244 0.02480 * age -0.0001756 0.0160852 -0.011 0.99129 state2 0.4433007 0.4816619 0.920 0.35739 state3 0.8349797 0.5562704 1.501 0.13335 state4 -3.3967878 0.6115860 -5.554 2.79e-08 *** time.served -0.1238867 0.1204230 -1.029 0.30359 max.sentence 0.0802954 0.0553747 1.450 0.14705 multiple.offenses 1.6119919 0.3853050 4.184 2.87e-05 *** crime2 0.6837143 0.5003550 1.366 0.17180 crime3 -0.2781054 0.4328356 -0.643 0.52054 crime4 -0.0117627 0.5713035 -0.021 0.98357 --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 340.04 on 472 degrees of freedom Residual deviance: 251.48 on 460 degrees of freedom AIC: 277.48 Number of Fisher Scoring iterations: 6 race, state4, multiple.offenses   Problem 3.2 - Building a Logistic Regression Model What can we say based on the coefficient of the multiple.offenses variable? The following two properties might be useful to you when exploring this question:\nIf we have a coefficient c for a variable, then that means the log odds (or Logit) are increased by c for a unit increase in the variable. If we have a coefficient c for a variable, then that means the odds are multiplied by e^c for a unit increase in the variable.  exp(1.6119919) [1] 5.012786 Our model predicts that a parolee who committed multiple offenses has 5.01 times higher odds of being a violator than a parolee who did not commit multiple offenses but, is otherwise identical.\n Problem 3.3 - Building a Logistic Regression Model Consider a parolee who is male, of white race, aged 50 years at prison release, from the state of Maryland, served 3 months, had a maximum sentence of 12 months, did not commit multiple offenses, and committed a larceny.\nExplore the following questions based on the model’s predictions for this individual. (HINT: We should use the coefficients of our model, the Logistic Response Function, and the Odds equation to solve this problem.) According to the model, what are the odds this individual is a violator?\nexp(-4.2411574 + # intercept 0.3869904 * 1 + # male 0.8867192 * 1 + # white race -0.0001756 * 50 + # aged 50 0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland -0.1238867 * 3 + # served 3 months 0.0802954 * 12 + # max sentence of 12 months 1.6119919 * 0 + # did not commit multiple offenses 0.6837143*1 + -0.2781054*0 + -0.0117627*0 ) [1] 0.1825687 ## 0.1825687 # according to the model, what is the probability this individual is a violator? 1 / (1 + exp(-1 * (-4.2411574 + # intercept 0.3869904 * 1 + # male 0.8867192 * 1 + # white race -0.0001756 * 50 + # aged 50 0.4433007*0 + 0.8349797*0 + -3.3967878*0 + # Maryland -0.1238867 * 3 + # served 3 months 0.0802954 * 12 + # max sentence of 12 months 1.6119919 * 0 + # did not commit multiple offenses 0.6837143*1 + -0.2781054*0 + -0.0117627*0 ))) [1] 0.1543832 ## Logistic Response Function -\u0026gt; P(y = 1) = 0.1543832  Problem 4.1 - Evaluating the Model on the Testing Set Use the predict() function to obtain the model’s predicted probabilities for parolees in the testing set, remembering to pass type=“response”.\nWhat is the maximum predicted probability of a violation?\nParolePredTest \u0026lt;- predict(ParoleViolatorLog, type = \u0026quot;response\u0026quot;, newdata = test) max(ParolePredTest) [1] 0.9072791  Problem 4.2 - Evaluating the Model on the Testing Set In the following questions, evaluate the model’s predictions on the test-set using a threshold of 0.5.\ntable(test$violator, ParolePredTest \u0026gt; 0.5)  FALSE TRUE 0 167 12 1 11 12 # what is the model\u0026#39;s sensitivity? 12 / (11 + 12) # TP / (TP + FN) [1] 0.5217391 # what is the model\u0026#39;s specificity? 167 / (167 + 12) # TN / (TN + FP) [1] 0.9329609 # what is the model\u0026#39;s accuracy? (167 + 12) / nrow(test) # (TN + TP) / N [1] 0.8861386  Problem 4.3 - Evaluating the Model on the Testing Set What is the accuracy of a simple model that predicts that every parolee is a non-violator?\ntable(test$violator)  0 1 179 23  179 / (179 + 23) [1] 0.8861386  Problem 4.4 - Evaluating the Model on the Testing Set Consider a parole board using the model to predict whether parolees will be violators or not.\nThe job of a parole board is to make sure that a prisoner is ready to be released into free society, and therefore parole boards tend to be particularily concerned about releasing prisoners who will violate their parole.\nWhich of the following most likely describes their preferences and best course of action? #### The board assigns more cost to a false negative than a false positive, and should therefore use a logistic regression cut-off less than 0.5.\n Problem 4.5 - Evaluating the Model on the Testing Set Which of the following is the most accurate assessment of the value of the logistic regression model with a cut-off 0.5 to a parole board, based on the model’s accuracy as compared to the simple baseline model? #### The model is likely of value to the board, and using a different logistic regression cut-off is likely to improve the model’s value.\n Problem 4.6 - Evaluating the Model on the Testing Set Using the ROCR package, what is the AUC value for the model?\nROCRpred = prediction(ParolePredTest, test$violator) as.numeric(performance(ROCRpred, \u0026quot;auc\u0026quot;)@y.values) [1] 0.8945834  Problem 4.7 - Evaluating the Model on the Testing Set Describe the meaning of AUC in this context. #### The probability the model can correctly differentiate between a randomly selected parole violator and a randomly selected parole non-violator.\n Problem 5.1 - Identifying Bias in Observational Data Our goal has been to predict the outcome of a parole decision, and we used a publicly available dataset of parole releases for predictions.\nIn this final problem, we’ll evaluate a potential source of bias associated with our analysis. It is always important to evaluate a dataset for possible sources of bias.\nThe dataset contains all individuals released from parole in 2004, either due to completing their parole term or violating the terms of their parole. However, it does not contain parolees who neither violated their parole nor completed their term in 2004, causing non-violators to be underrepresented.\nThis is called “selection bias” or “selecting on the dependent variable,” because only a subset of all relevant parolees were included in our analysis, based on our dependent variable in this analysis (parole violation).\nHow could we improve our dataset to best address selection bias? #### We should use a dataset tracking a group of parolees from the start of their parole until either they violated parole or they completed their term.\n ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"a08232a5d5fe79172edf0f14ceff9d3c","permalink":"/project/parole_violators/parole_violators/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/parole_violators/parole_violators/","section":"project","summary":"Predict if an inmate will violate his or her terms of parole","tags":["R","Data Analytics","Machine Learning"],"title":"Predict Parole Violators","type":"project"},{"authors":null,"categories":null,"content":" Popularity of music records\nThe music industry has a well-developed market with a global annual revenue around $15 billion. The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales.\nArtists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale. A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.\nUnfortunately, the success of an artist’s release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads, while another single may turn out quite unpopular, and therefore unprofitable.\nKnowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success.\nHow can we use analytics to predict the popularity of a song? In this project, we challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.\nTaking an analytics approach, we aim to use information about a song’s properties to predict its popularity. The dataset songs.csv consists of all songs which made it to the Top 10 of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn’t make the Top 10. This data comes from three sources: Wikipedia, Billboard.com, and EchoNest.\nThe variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes: time signature, loudness, key, pitch, tempo, and timbre.\nHere’s a detailed description of the variables:\n year = the year the song was released songtitle = the title of the song artistname = the name of the artist of the song songID and artistID = identifying variables for the song and artist timesignature and timesignature_confidence = a variable estimating the time signature of the song, and the confidence in the estimate loudness = a continuous variable indicating the average amplitude of the audio in decibels tempo and tempo_confidence = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate key and key_confidence = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate energy = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness pitch = a continuous variable that indicates the pitch of the song timbre_0_min, timbre_0_max, timbre_1_min, timbre_1_max, . . . , timbre_11_min, and timbre_11_max = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables) Top10 = a binary variable indicating whether or not the song made it to the Top 10 of the Billboard Hot 100 Chart (1 if it was in the top 10, and 0 if it was not)  Problem 1.1 - Understanding the Data Use the read.csv function to load the dataset “songs.csv” into R. How many observations (songs) are from the year 2010?\nsongs \u0026lt;- read.csv(\u0026quot;songs.csv\u0026quot;) str(songs) \u0026#39;data.frame\u0026#39;: 7574 obs. of 39 variables: $ year : int 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ... $ songtitle : Factor w/ 7141 levels \u0026quot;̈́ l\u0026#39;or_e des bois\u0026quot;,..: 6204 5522 241 3115 48 608 255 4419 2886 6756 ... $ artistname : Factor w/ 1032 levels \u0026quot;50 Cent\u0026quot;,\u0026quot;98 Degrees\u0026quot;,..: 3 3 3 3 3 3 3 3 3 12 ... $ songID : Factor w/ 7549 levels \u0026quot;SOAACNI1315CD4AC42\u0026quot;,..: 595 5439 5252 1716 3431 1020 1831 3964 6904 2473 ... $ artistID : Factor w/ 1047 levels \u0026quot;AR00B1I1187FB433EB\u0026quot;,..: 671 671 671 671 671 671 671 671 671 507 ... $ timesignature : int 3 4 4 4 4 4 4 4 4 4 ... $ timesignature_confidence: num 0.853 1 1 1 0.788 1 0.968 0.861 0.622 0.938 ... $ loudness : num -4.26 -4.05 -3.57 -3.81 -4.71 ... $ tempo : num 91.5 140 160.5 97.5 140.1 ... $ tempo_confidence : num 0.953 0.921 0.489 0.794 0.286 0.347 0.273 0.83 0.018 0.929 ... $ key : int 11 10 2 1 6 4 10 5 9 11 ... $ key_confidence : num 0.453 0.469 0.209 0.632 0.483 0.627 0.715 0.423 0.751 0.602 ... $ energy : num 0.967 0.985 0.99 0.939 0.988 ... $ pitch : num 0.024 0.025 0.026 0.013 0.063 0.038 0.026 0.033 0.027 0.004 ... $ timbre_0_min : num 0.002 0 0.003 0 0 ... $ timbre_0_max : num 57.3 57.4 57.4 57.8 56.9 ... $ timbre_1_min : num -6.5 -37.4 -17.2 -32.1 -223.9 ... $ timbre_1_max : num 171 171 171 221 171 ... $ timbre_2_min : num -81.7 -149.6 -72.9 -138.6 -147.2 ... $ timbre_2_max : num 95.1 180.3 157.9 173.4 166 ... $ timbre_3_min : num -285 -380.1 -204 -73.5 -128.1 ... $ timbre_3_max : num 259 384 251 373 389 ... $ timbre_4_min : num -40.4 -48.7 -66 -55.6 -43.9 ... $ timbre_4_max : num 73.6 100.4 152.1 119.2 99.3 ... $ timbre_5_min : num -104.7 -87.3 -98.7 -77.5 -96.1 ... $ timbre_5_max : num 183.1 42.8 141.4 141.2 38.3 ... $ timbre_6_min : num -88.8 -86.9 -88.9 -70.8 -110.8 ... $ timbre_6_max : num 73.5 75.5 66.5 64.5 72.4 ... $ timbre_7_min : num -71.1 -65.8 -67.4 -63.7 -55.9 ... $ timbre_7_max : num 82.5 106.9 80.6 96.7 110.3 ... $ timbre_8_min : num -52 -61.3 -59.8 -78.7 -56.5 ... $ timbre_8_max : num 39.1 35.4 46 41.1 37.6 ... $ timbre_9_min : num -35.4 -81.9 -46.3 -49.2 -48.6 ... $ timbre_9_max : num 71.6 74.6 59.9 95.4 67.6 ... $ timbre_10_min : num -126.4 -103.8 -108.3 -102.7 -52.8 ... $ timbre_10_max : num 18.7 121.9 33.3 46.4 22.9 ... $ timbre_11_min : num -44.8 -38.9 -43.7 -59.4 -50.4 ... $ timbre_11_max : num 26 22.5 25.7 37.1 32.8 ... $ Top10 : int 0 0 0 0 0 0 0 0 0 1 ... summary(songs)  year songtitle artistname Min. :1990 Intro : 15 Various artists: 162 1st Qu.:1997 Forever : 8 Anal Cunt : 49 Median :2002 Home : 7 Various Artists: 44 Mean :2001 Goodbye : 6 Tori Amos : 41 3rd Qu.:2006 Again : 5 Eels : 37 Max. :2010 Beautiful: 5 Napalm Death : 37 (Other) :7528 (Other) :7204 songID artistID timesignature SOALSZJ1370F1A7C75: 2 ARAGWS81187FB3F768: 222 Min. :0.000 SOANPAC13936E0B640: 2 ARL14X91187FB4CF14: 49 1st Qu.:4.000 SOBDGMX12B0B80808E: 2 AR4KS8C1187FB4CF3D: 41 Median :4.000 SOBUDCZ12A58A80013: 2 AR0JZZ01187B9B2C99: 37 Mean :3.894 SODFRLK13134387FB5: 2 ARZGTK71187B9AC7F5: 37 3rd Qu.:4.000 SOEJPOK12A6D4FAFE4: 2 AR95XYH1187FB53951: 31 Max. :7.000 (Other) :7562 (Other) :7157 timesignature_confidence loudness tempo Min. :0.0000 Min. :-42.451 Min. : 0.00 1st Qu.:0.8193 1st Qu.:-10.847 1st Qu.: 88.86 Median :0.9790 Median : -7.649 Median :103.27 Mean :0.8533 Mean : -8.817 Mean :107.35 3rd Qu.:1.0000 3rd Qu.: -5.640 3rd Qu.:124.80 Max. :1.0000 Max. : 1.305 Max. :244.31 tempo_confidence key key_confidence energy Min. :0.0000 Min. : 0.000 Min. :0.0000 Min. :0.00002 1st Qu.:0.3720 1st Qu.: 2.000 1st Qu.:0.2040 1st Qu.:0.50014 Median :0.7015 Median : 6.000 Median :0.4515 Median :0.71816 Mean :0.6229 Mean : 5.385 Mean :0.4338 Mean :0.67547 3rd Qu.:0.8920 3rd Qu.: 9.000 3rd Qu.:0.6460 3rd Qu.:0.88740 Max. :1.0000 Max. :11.000 Max. :1.0000 Max. :0.99849 pitch timbre_0_min timbre_0_max timbre_1_min Min. :0.00000 Min. : 0.000 Min. :12.58 Min. :-333.72 1st Qu.:0.00300 1st Qu.: 0.000 1st Qu.:53.12 1st Qu.:-160.12 Median :0.00700 Median : 0.027 Median :55.53 Median :-107.75 Mean :0.01082 Mean : 4.123 Mean :54.46 Mean :-110.79 3rd Qu.:0.01400 3rd Qu.: 2.772 3rd Qu.:57.08 3rd Qu.: -59.71 Max. :0.54100 Max. :48.353 Max. :64.01 Max. : 123.73 timbre_1_max timbre_2_min timbre_2_max timbre_3_min Min. :-74.37 Min. :-324.86 Min. : -0.832 Min. :-495.36 1st Qu.:171.13 1st Qu.:-167.64 1st Qu.:100.519 1st Qu.:-226.87 Median :194.40 Median :-136.60 Median :129.908 Median :-170.61 Mean :212.34 Mean :-136.89 Mean :136.673 Mean :-186.11 3rd Qu.:239.24 3rd Qu.:-106.51 3rd Qu.:166.121 3rd Qu.:-131.56 Max. :549.97 Max. : 34.57 Max. :397.095 Max. : -21.55 timbre_3_max timbre_4_min timbre_4_max timbre_5_min Min. : 12.85 Min. :-207.07 Min. : -0.651 Min. :-262.48 1st Qu.:127.14 1st Qu.: -77.69 1st Qu.: 83.966 1st Qu.:-113.58 Median :189.50 Median : -63.83 Median :107.422 Median : -95.47 Mean :211.81 Mean : -65.28 Mean :108.227 Mean :-104.00 3rd Qu.:290.72 3rd Qu.: -51.34 3rd Qu.:130.286 3rd Qu.: -81.02 Max. :499.62 Max. : 51.43 Max. :257.801 Max. : -42.17 timbre_5_max timbre_6_min timbre_6_max timbre_7_min Min. :-22.41 Min. :-152.170 Min. : 12.70 Min. :-214.791 1st Qu.: 84.64 1st Qu.: -94.792 1st Qu.: 59.04 1st Qu.:-101.171 Median :119.90 Median : -80.418 Median : 70.47 Median : -81.797 Mean :127.04 Mean : -80.944 Mean : 72.17 Mean : -84.313 3rd Qu.:162.34 3rd Qu.: -66.521 3rd Qu.: 83.19 3rd Qu.: -64.301 Max. :350.94 Max. : 4.503 Max. :208.39 Max. : 5.153 timbre_7_max timbre_8_min timbre_8_max timbre_9_min Min. : 15.70 Min. :-158.756 Min. :-25.95 Min. :-149.51 1st Qu.: 76.50 1st Qu.: -73.051 1st Qu.: 40.58 1st Qu.: -70.28 Median : 94.63 Median : -62.661 Median : 49.22 Median : -58.65 Mean : 95.65 Mean : -63.704 Mean : 50.06 Mean : -59.52 3rd Qu.:112.71 3rd Qu.: -52.983 3rd Qu.: 58.46 3rd Qu.: -47.70 Max. :214.82 Max. : -2.382 Max. :144.99 Max. : 1.14 timbre_9_max timbre_10_min timbre_10_max timbre_11_min Min. : 8.415 Min. :-208.82 Min. : -6.359 Min. :-145.599 1st Qu.: 53.037 1st Qu.:-105.13 1st Qu.: 39.196 1st Qu.: -58.058 Median : 65.935 Median : -83.07 Median : 50.895 Median : -50.892 Mean : 68.028 Mean : -87.34 Mean : 55.521 Mean : -50.868 3rd Qu.: 81.267 3rd Qu.: -64.52 3rd Qu.: 66.593 3rd Qu.: -43.292 Max. :161.518 Max. : -10.64 Max. :192.417 Max. : -6.497 timbre_11_max Top10 Min. : 7.20 Min. :0.0000 1st Qu.: 38.98 1st Qu.:0.0000 Median : 46.44 Median :0.0000 Mean : 47.49 Mean :0.1477 3rd Qu.: 55.03 3rd Qu.:0.0000 Max. :110.27 Max. :1.0000  table(songs$year)  1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 328 196 186 324 198 258 178 329 380 357 363 282 518 434 479 2005 2006 2007 2008 2009 2010 392 479 622 415 483 373  373\n Problem 1.2 - Understanding the Data How many songs does the dataset include for which the artist name is “Michael Jackson”?\nnrow(subset(songs, artistname == \u0026quot;Michael Jackson\u0026quot;)) [1] 18  Problem 1.3 - Understanding the Data Which of these songs by Michael Jackson made it to the Top 10? Select all that apply.\nsubset(songs, artistname == \u0026quot;Michael Jackson\u0026quot; \u0026amp; Top10 == 1, select = c(artistname, songtitle))  artistname songtitle 4329 Michael Jackson You Rock My World 6207 Michael Jackson You Are Not Alone 6210 Michael Jackson Black or White 6218 Michael Jackson Remember the Time 6915 Michael Jackson In The Closet You Rock My World, You Are Not Alone\n Problem 1.4 - Understanding the Data The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). What are the values of this variable that occur in our dataset?\nsummary(songs$timesignature)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.000 4.000 4.000 3.894 4.000 7.000  table(songs$timesignature)  0 1 3 4 5 7 10 143 503 6787 112 19  Which timesignature value is the most frequent among songs in our dataset? #### 4\n Problem 1.5 - Understanding the Data Out of all of the songs in our dataset, the song with the highest tempo is one of the following songs.\nWhich one is it?\nsummary(songs$tempo)  Min. 1st Qu. Median Mean 3rd Qu. Max. 0.00 88.86 103.27 107.35 124.80 244.31  which.max(songs$tempo) [1] 6206 songs$tempo[6206] [1] 244.307 nrow(subset(songs, tempo == 244.307)) [1] 1 songs$songtitle[6206] [1] Wanna Be Startin\u0026#39; Somethin\u0026#39; 7141 Levels: ̈́ l\u0026#39;or_e des bois _\\x84_ _\\x84\\x8d ... Zumbi Wanna Be Startin’ Somethin’\n Problem 2.1 - Creating Our Prediction Model We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set “SongsTrain” consisting of all the observations up to and including 2009 song releases, and a testing set “SongsTest”, consisting of the 2010 song releases.\nHow many observations (songs) are in the training set?\nSongsTrain \u0026lt;- subset(songs, year \u0026lt;= 2009) SongsTest \u0026lt;- subset(songs, year == 2010) nrow(songs) [1] 7574 nrow(SongsTrain) + nrow(SongsTest) [1] 7574  Problem 2.2 - Creating our Prediction Model In this problem, our outcome variable is “Top10” - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart.\nSince the outcome variable is binary, we will build a logistic regression model. We’ll start by using all song attributes as our independent variables, which we’ll call Model 1. We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model.\nSo we won’t use the variables “year”, “songtitle”, “artistname”, “songID” or “artistID”. We have seen in the lecture that, to build the logistic regression model, we would normally explicitly input the formula including all the independent variables in R. However, in this case, this is a tedious amount of work since we have a large number of independent variables. There is a nice trick to avoid doing so. Let’s suppose that, except for the outcome variable Top10, all other variables in the training set are inputs to Model 1. Then, we can use the formula SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial) to build our model. Notice that the “.” is used in place of enumerating all the independent variables. (Also, keep in mind that you can choose to put quotes around binomial, or leave out the quotes. R can understand this argument either way.) However, in our case, we want to exclude some of the variables in our dataset from being used as independent variables (“year”, “songtitle”, “artistname”, “songID”, and “artistID”).\nTo do this, we can use the following trick. First define a vector of variable names called nonvars - these are the variables that we won’t use in our model.\nnonvars = c(\u0026quot;year\u0026quot;, \u0026quot;songtitle\u0026quot;, \u0026quot;artistname\u0026quot;, \u0026quot;songID\u0026quot;, \u0026quot;artistID\u0026quot;) To remove these variables from our training and testing sets.\nSongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ] SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ] Now, use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. You should use SongsTrain to build the model.\nLooking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?\nSongsLog1 \u0026lt;- glm(Top10 ~ ., data = SongsTrain, family=binomial) summary(SongsLog1)  Call: glm(formula = Top10 ~ ., family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -1.9220 -0.5399 -0.3459 -0.1845 3.0770 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 1.470e+01 1.806e+00 8.138 4.03e-16 *** timesignature 1.264e-01 8.674e-02 1.457 0.145050 timesignature_confidence 7.450e-01 1.953e-01 3.815 0.000136 *** loudness 2.999e-01 2.917e-02 10.282 \u0026lt; 2e-16 *** tempo 3.634e-04 1.691e-03 0.215 0.829889 tempo_confidence 4.732e-01 1.422e-01 3.329 0.000873 *** key 1.588e-02 1.039e-02 1.529 0.126349 key_confidence 3.087e-01 1.412e-01 2.187 0.028760 * energy -1.502e+00 3.099e-01 -4.847 1.25e-06 *** pitch -4.491e+01 6.835e+00 -6.570 5.02e-11 *** timbre_0_min 2.316e-02 4.256e-03 5.441 5.29e-08 *** timbre_0_max -3.310e-01 2.569e-02 -12.882 \u0026lt; 2e-16 *** timbre_1_min 5.881e-03 7.798e-04 7.542 4.64e-14 *** timbre_1_max -2.449e-04 7.152e-04 -0.342 0.732087 timbre_2_min -2.127e-03 1.126e-03 -1.889 0.058843 . timbre_2_max 6.586e-04 9.066e-04 0.726 0.467571 timbre_3_min 6.920e-04 5.985e-04 1.156 0.247583 timbre_3_max -2.967e-03 5.815e-04 -5.103 3.34e-07 *** timbre_4_min 1.040e-02 1.985e-03 5.237 1.63e-07 *** timbre_4_max 6.110e-03 1.550e-03 3.942 8.10e-05 *** timbre_5_min -5.598e-03 1.277e-03 -4.385 1.16e-05 *** timbre_5_max 7.736e-05 7.935e-04 0.097 0.922337 timbre_6_min -1.686e-02 2.264e-03 -7.445 9.66e-14 *** timbre_6_max 3.668e-03 2.190e-03 1.675 0.093875 . timbre_7_min -4.549e-03 1.781e-03 -2.554 0.010661 * timbre_7_max -3.774e-03 1.832e-03 -2.060 0.039408 * timbre_8_min 3.911e-03 2.851e-03 1.372 0.170123 timbre_8_max 4.011e-03 3.003e-03 1.336 0.181620 timbre_9_min 1.367e-03 2.998e-03 0.456 0.648356 timbre_9_max 1.603e-03 2.434e-03 0.659 0.510188 timbre_10_min 4.126e-03 1.839e-03 2.244 0.024852 * timbre_10_max 5.825e-03 1.769e-03 3.292 0.000995 *** timbre_11_min -2.625e-02 3.693e-03 -7.108 1.18e-12 *** timbre_11_max 1.967e-02 3.385e-03 5.811 6.21e-09 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4759.2 on 7167 degrees of freedom AIC: 4827.2 Number of Fisher Scoring iterations: 6 AIC: 4827.2\n Problem 2.3 - Creating Our Prediction Model Let’s now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). What does the model suggest? #### The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10\n Problem 2.4 - Creating Our Prediction Model In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex.\nWhat does Model 1 suggest in terms of complexity? #### Mainstream listeners tend to prefer less complex songs\n Problem 2.5 - Creating Our Prediction Model Songs with heavier instrumentation tend to be louder (have higher values in the variable “loudness”) and more energetic (have higher values in the variable “energy”). By inspecting the coefficient of the variable “loudness”, what does Model 1 suggest? #### Mainstream listeners prefer songs with heavy instrumentation\nBy inspecting the coefficient of the variable “energy”, do we draw the same conclusions as above? #### No\n Problem 3.1 - Beware of Multicollinearity Issues! What is the correlation between the variables “loudness” and “energy” in the training set?\ncor(SongsTrain$loudness, SongsTrain$energy) [1] 0.7399067 Given that these two variables are highly correlated, Model 1 suffers from multicollinearity. To avoid this issue, we will omit one of these two variables and re-run the logistic regression.\nIn the rest of this problem, we’ll build two variations of our original model: Model 2, in which we keep “energy” and omit “loudness”, and Model 3, in which we keep “loudness” and omit “energy”.\n Problem 3.2 - Beware of Multicollinearity Issues! Create Model 2, which is Model 1 without the independent variable “loudness”.\nSongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial) We just subtracted the variable loudness. We couldn’t do this with the variables “songtitle” and “artistname”, because they are not numeric variables, and we might get different values in the test-set that the training set has never seen. But this approach (subtracting the variable from the model formula) will always work when you want to remove numeric variables.\nLook at the summary of SongsLog2, and inspect the coefficient of the variable “energy”. What do you observe?\nsummary(SongsLog2)  Call: glm(formula = Top10 ~ . - loudness, family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -2.0983 -0.5607 -0.3602 -0.1902 3.3107 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) -2.241e+00 7.465e-01 -3.002 0.002686 ** timesignature 1.625e-01 8.734e-02 1.860 0.062873 . timesignature_confidence 6.885e-01 1.924e-01 3.578 0.000346 *** tempo 5.521e-04 1.665e-03 0.332 0.740226 tempo_confidence 5.497e-01 1.407e-01 3.906 9.40e-05 *** key 1.740e-02 1.026e-02 1.697 0.089740 . key_confidence 2.954e-01 1.394e-01 2.118 0.034163 * energy 1.813e-01 2.608e-01 0.695 0.486991 pitch -5.150e+01 6.857e+00 -7.511 5.87e-14 *** timbre_0_min 2.479e-02 4.240e-03 5.847 5.01e-09 *** timbre_0_max -1.007e-01 1.178e-02 -8.551 \u0026lt; 2e-16 *** timbre_1_min 7.143e-03 7.710e-04 9.265 \u0026lt; 2e-16 *** timbre_1_max -7.830e-04 7.064e-04 -1.108 0.267650 timbre_2_min -1.579e-03 1.109e-03 -1.424 0.154531 timbre_2_max 3.889e-04 8.964e-04 0.434 0.664427 timbre_3_min 6.500e-04 5.949e-04 1.093 0.274524 timbre_3_max -2.462e-03 5.674e-04 -4.339 1.43e-05 *** timbre_4_min 9.115e-03 1.952e-03 4.670 3.02e-06 *** timbre_4_max 6.306e-03 1.532e-03 4.115 3.87e-05 *** timbre_5_min -5.641e-03 1.255e-03 -4.495 6.95e-06 *** timbre_5_max 6.937e-04 7.807e-04 0.889 0.374256 timbre_6_min -1.612e-02 2.235e-03 -7.214 5.45e-13 *** timbre_6_max 3.814e-03 2.157e-03 1.768 0.076982 . timbre_7_min -5.102e-03 1.755e-03 -2.907 0.003644 ** timbre_7_max -3.158e-03 1.811e-03 -1.744 0.081090 . timbre_8_min 4.488e-03 2.810e-03 1.597 0.110254 timbre_8_max 6.423e-03 2.950e-03 2.177 0.029497 * timbre_9_min -4.282e-04 2.955e-03 -0.145 0.884792 timbre_9_max 3.525e-03 2.377e-03 1.483 0.138017 timbre_10_min 2.993e-03 1.804e-03 1.660 0.097004 . timbre_10_max 7.367e-03 1.731e-03 4.255 2.09e-05 *** timbre_11_min -2.837e-02 3.630e-03 -7.815 5.48e-15 *** timbre_11_max 1.829e-02 3.341e-03 5.476 4.34e-08 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4871.8 on 7168 degrees of freedom AIC: 4937.8 Number of Fisher Scoring iterations: 6 Model 2 suggests that songs with high energy levels tend to be more popular. This contradicts our observation in Model 1.\n Problem 3.3 - Beware of Multicollinearity Issues! Now, create Model 3, which should be exactly like Model 1, but without the variable “energy”.\nSongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial) Look at the summary of Model 3 and inspect the coefficient of the variable “loudness”. Remembering that higher loudness and energy both occur in songs with heavier instrumentation, do we make the same observation about the popularity of heavy instrumentation as we did with Model 2?\nsummary(SongsLog3)  Call: glm(formula = Top10 ~ . - energy, family = binomial, data = SongsTrain) Deviance Residuals: Min 1Q Median 3Q Max -1.9182 -0.5417 -0.3481 -0.1874 3.4171 Coefficients: Estimate Std. Error z value Pr(\u0026gt;|z|) (Intercept) 1.196e+01 1.714e+00 6.977 3.01e-12 *** timesignature 1.151e-01 8.726e-02 1.319 0.187183 timesignature_confidence 7.143e-01 1.946e-01 3.670 0.000242 *** loudness 2.306e-01 2.528e-02 9.120 \u0026lt; 2e-16 *** tempo -6.460e-04 1.665e-03 -0.388 0.698107 tempo_confidence 3.841e-01 1.398e-01 2.747 0.006019 ** key 1.649e-02 1.035e-02 1.593 0.111056 key_confidence 3.394e-01 1.409e-01 2.409 0.015984 * pitch -5.328e+01 6.733e+00 -7.914 2.49e-15 *** timbre_0_min 2.205e-02 4.239e-03 5.200 1.99e-07 *** timbre_0_max -3.105e-01 2.537e-02 -12.240 \u0026lt; 2e-16 *** timbre_1_min 5.416e-03 7.643e-04 7.086 1.38e-12 *** timbre_1_max -5.115e-04 7.110e-04 -0.719 0.471928 timbre_2_min -2.254e-03 1.120e-03 -2.012 0.044190 * timbre_2_max 4.119e-04 9.020e-04 0.457 0.647915 timbre_3_min 3.179e-04 5.869e-04 0.542 0.588083 timbre_3_max -2.964e-03 5.758e-04 -5.147 2.64e-07 *** timbre_4_min 1.105e-02 1.978e-03 5.585 2.34e-08 *** timbre_4_max 6.467e-03 1.541e-03 4.196 2.72e-05 *** timbre_5_min -5.135e-03 1.269e-03 -4.046 5.21e-05 *** timbre_5_max 2.979e-04 7.855e-04 0.379 0.704526 timbre_6_min -1.784e-02 2.246e-03 -7.945 1.94e-15 *** timbre_6_max 3.447e-03 2.182e-03 1.580 0.114203 timbre_7_min -5.128e-03 1.768e-03 -2.900 0.003733 ** timbre_7_max -3.394e-03 1.820e-03 -1.865 0.062208 . timbre_8_min 3.686e-03 2.833e-03 1.301 0.193229 timbre_8_max 4.658e-03 2.988e-03 1.559 0.119022 timbre_9_min -9.318e-05 2.957e-03 -0.032 0.974859 timbre_9_max 1.342e-03 2.424e-03 0.554 0.579900 timbre_10_min 4.050e-03 1.827e-03 2.217 0.026637 * timbre_10_max 5.793e-03 1.759e-03 3.294 0.000988 *** timbre_11_min -2.638e-02 3.683e-03 -7.162 7.96e-13 *** timbre_11_max 1.984e-02 3.365e-03 5.896 3.74e-09 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 6017.5 on 7200 degrees of freedom Residual deviance: 4782.7 on 7168 degrees of freedom AIC: 4848.7 Number of Fisher Scoring iterations: 6 In the remainder of this problem, we’ll just use Model 3.\n Problem 4.1 - Validating Our Model Make predictions on the test-set using Model 3. What is the accuracy of Model 3 on the test-set, using a threshold of 0.45? (Compute the accuracy as a number between 0 and 1.)\npredSongsTest = predict(SongsLog3, type=\u0026quot;response\u0026quot;, newdata = SongsTest) table(SongsTest$Top10, predSongsTest \u0026gt; 0.45)  FALSE TRUE 0 309 5 1 40 19 (309 + 19) / nrow(SongsTest) [1] 0.8793566  Problem 4.2 - Validating Our Model Let’s check if there’s any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs.\nWhat would the accuracy of the baseline model be on the test-set?\ntable(SongsTest$Top10)  0 1 314 59  314/(314 + 59) [1] 0.8418231  Problem 4.3 - Validating Our Model It seems that Model 3 gives us a small improvement over the baseline model. Still, does it create an edge? Let’s view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. The company’s objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular.\nA competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer.\nHow many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?\ntable(SongsTest$Top10, predSongsTest \u0026gt; 0.45)  FALSE TRUE 0 309 5 1 40 19 19\nHow many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45? #### 5\n Problem 4.4 - Validating Our Model # what is the sensitivity of Model 3 on the test set, using a threshold of 0.45? 19 / (40 + 19) [1] 0.3220339 # what is the specificity of Model 3 on the test set, using a threshold of 0.45? 309 / (309 + 5) [1] 0.9840764  Conclusions  Model 3 favors specificity over sensitivity. Model 3 provides conservative predictions, and predicts that a song will make it to the Top 10 very rarely. So while it detects less than half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.   ","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"64f145759eb69d7479b4cbb5964be4cc","permalink":"/project/music/music/","publishdate":"2019-04-07T00:00:00Z","relpermalink":"/project/music/music/","section":"project","summary":"Predict whether a song will reach a spot in the Top 10, of the Billboard Hot 100 Chart","tags":["regression","R","Machine Learning"],"title":"Predict Popular Songs","type":"project"},{"authors":null,"categories":null,"content":" Flu epidemics constitute a major public health concern causing respiratory illnesses, hospitalizations, and deaths. According to the National Vital Statistics Reports published in October 2012, influenza ranked as the eighth leading cause of death in 2011 in the U.S. Each year, 250,000 to 500,000 deaths are attributed to influenza related diseases throughout the world.\nThe U.S. Centers for Disease Control and Prevention (CDC) and the European Influenza Surveillance Scheme (EISS) detect influenza activity through virologic and clinical data, including Influenza-like Illness (ILI) physician visits. Reporting national and regional data, however, are published with a 1-2 week lag.\nThe Google Flu Trends project was initiated to see if faster reporting can be made possible by considering flu-related online search queries – data that is available almost immediately.\nI would like to estimate influenza-like illness (ILI) activity using Google web search logs. Fortunately, one can easily access this data online:\n ILI Data - The CDC publishes on its website the official regional and state-level percentage of patient visits to healthcare providers for ILI purposes on a weekly basis. Google Search Queries - Google Trends allows public retrieval of weekly counts for every query searched by users around the world.  For each location, the counts are normalized by dividing the count for each query in a particular week by the total number of online search queries submitted in that location during the week. Then, the values are adjusted to be between 0 and 1.\nThe csv file FluTrain.csv aggregates this data from January 1, 2004 until December 31, 2011 as follows:\n “Week” - The range of dates represented by this observation, in year/month/day format. “ILI” - This column lists the percentage of ILI-related physician visits for the corresponding week. “Queries” - This column lists the fraction of queries that are ILI-related for the corresponding week, adjusted to be between 0 and 1 (higher values correspond to more ILI-related search queries).  Before applying analytics tools on the training set, we first need to understand the data at hand. Looking at the time period 2004-2011, which week corresponds to the highest percentage of ILI-related physician visits?\nLoading the data FluTrain \u0026lt;- read.csv(\u0026quot;FluTrain.csv\u0026quot;) summary(FluTrain)  Week ILI Queries 2004-01-04 - 2004-01-10: 1 Min. :0.5341 Min. :0.04117 2004-01-11 - 2004-01-17: 1 1st Qu.:0.9025 1st Qu.:0.15671 2004-01-18 - 2004-01-24: 1 Median :1.2526 Median :0.28154 2004-01-25 - 2004-01-31: 1 Mean :1.6769 Mean :0.28603 2004-02-01 - 2004-02-07: 1 3rd Qu.:2.0587 3rd Qu.:0.37849 2004-02-08 - 2004-02-14: 1 Max. :7.6189 Max. :1.00000 (Other) :411  str(FluTrain) \u0026#39;data.frame\u0026#39;: 417 obs. of 3 variables: $ Week : Factor w/ 417 levels \u0026quot;2004-01-04 - 2004-01-10\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ ILI : num 2.42 1.81 1.71 1.54 1.44 ... $ Queries: num 0.238 0.22 0.226 0.238 0.224 ...  Problem 1.1 - EDA Select the day of the month corresponding to the start of this week?\nFluTrain[which.max(FluTrain$ILI),]  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 Which week corresponds to the highest percentage of ILI-related query fraction?\nFluTrain[which.max(FluTrain$Queries),]  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 subset(FluTrain, Queries == 1)  Week ILI Queries 303 2009-10-18 - 2009-10-24 7.618892 1 October 18, 2009\n Problem 1.2 - EDA Let’s now understand the data at a high level. Plot the histogram of the dependent variable, ILI.\nWhat best describes the distribution of values of ILI?\nhist(FluTrain$ILI) Most of the ILI values are small, with a relatively small number of much larger values (in statistics, this sort of data is called “skew right”).   Problem 1.3 - EDA When handling a skewed dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself – this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models.\nIn this problem, I’ll predict the natural log of the ILI variable, which can be computed using the log() function. Plot the natural logarithm of ILI versus Queries.\nplot(log(FluTrain$ILI), FluTrain$Queries) plot(FluTrain$Queries, log(FluTrain$ILI)) What does the plot suggest? #### There is a positive, linear relationship between log(ILI) and Queries.\n Problem 2.1 - Linear Regression Model Based on the plot we just made, it seems that a linear regression model could be a good modeling choice. Based on our understanding of the data from the previous subproblem, which model best describes our estimation problem? #### log(ILI) = intercept + coefficient x Queries, where the coefficient is positive.\n Problem 2.2 - Linear Regression Model Let’s call the regression model from the previous problem (Problem 2.1). FluTrend1 and run it. Hint: to take the logarithm of a variable Var in a regression equation, you simply use log(Var) when specifying the formula to the lm() function.\nFluTrend1 \u0026lt;- lm(log(ILI) ~ Queries, data = FluTrain) What is the training set R-squared value for FluTrend1 model (the “Multiple R-squared”)?\nsummary(FluTrend1)  Call: lm(formula = log(ILI) ~ Queries, data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.76003 -0.19696 -0.01657 0.18685 1.06450 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.49934 0.03041 -16.42 \u0026lt;2e-16 *** Queries 2.96129 0.09312 31.80 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.2995 on 415 degrees of freedom Multiple R-squared: 0.709, Adjusted R-squared: 0.7083 F-statistic: 1011 on 1 and 415 DF, p-value: \u0026lt; 2.2e-16 0.709\n Problem 2.3 - Linear Regression Model For a single variable linear regression model, there is a direct relationship between the R-squared and the correlation between the independent and the dependent variables.\nWhat is the relationship we infer from our problem? (Don’t forget that you can use the cor function to compute the correlation between two variables.)\ncorILIQueries \u0026lt;- cor(log(FluTrain$ILI), FluTrain$Queries) cor(FluTrain$ILI, FluTrain$Queries) [1] 0.8142115 corILIQueries^2 [1] 0.7090201 log(1/corILIQueries) [1] 0.1719357 exp(-0.5 * corILIQueries) [1] 0.6563792 Note = R-squared = Correlation^2\nNote that the “exp” function stands for the exponential function. The exponential can be computed in R using the function exp().\n Problem 3.1 - Performance on the Test Set The file provides the 2012 weekly data of the ILI-related search queries and the observed weekly percentage of ILI-related physician visits.\nLoad this data into a dataframe called FluTest.\nFluTest \u0026lt;- read.csv(\u0026quot;FluTest.csv\u0026quot;) Normally, we would obtain test-set predictions from the model FluTrend1 using the code PredTest1 = predict(FluTrend1, newdata=FluTest) However, the dependent variable in our model is log(ILI), so PredTest1 would contain predictions of the log(ILI) value.\nWe are instead interested in obtaining predictions of the ILI value. We can convert from predictions of log(ILI) to predictions of ILI via exponentiation, or the exp() function. The new code, which predicts the ILI value.\nPredTest1 = exp(predict(FluTrend1, newdata=FluTest)) What is our estimate for the percentage of ILI-related physician visits for the week of March 11, 2012? (HINT: You can either just output FluTest$Week to find which element corresponds to March 11, 2012, or you can use the “which” function in R. To learn more about the which function, type ?which in your R console.)\nFluTest$Week  [1] 2012-01-01 - 2012-01-07 2012-01-08 - 2012-01-14 [3] 2012-01-15 - 2012-01-21 2012-01-22 - 2012-01-28 [5] 2012-01-29 - 2012-02-04 2012-02-05 - 2012-02-11 [7] 2012-02-12 - 2012-02-18 2012-02-19 - 2012-02-25 [9] 2012-02-26 - 2012-03-03 2012-03-04 - 2012-03-10 [11] 2012-03-11 - 2012-03-17 2012-03-18 - 2012-03-24 [13] 2012-03-25 - 2012-03-31 2012-04-01 - 2012-04-07 [15] 2012-04-08 - 2012-04-14 2012-04-15 - 2012-04-21 [17] 2012-04-22 - 2012-04-28 2012-04-29 - 2012-05-05 [19] 2012-05-06 - 2012-05-12 2012-05-13 - 2012-05-19 [21] 2012-05-20 - 2012-05-26 2012-05-27 - 2012-06-02 [23] 2012-06-03 - 2012-06-09 2012-06-10 - 2012-06-16 [25] 2012-06-17 - 2012-06-23 2012-06-24 - 2012-06-30 [27] 2012-07-01 - 2012-07-07 2012-07-08 - 2012-07-14 [29] 2012-07-15 - 2012-07-21 2012-07-22 - 2012-07-28 [31] 2012-07-29 - 2012-08-04 2012-08-05 - 2012-08-11 [33] 2012-08-12 - 2012-08-18 2012-08-19 - 2012-08-25 [35] 2012-08-26 - 2012-09-01 2012-09-02 - 2012-09-08 [37] 2012-09-09 - 2012-09-15 2012-09-16 - 2012-09-22 [39] 2012-09-23 - 2012-09-29 2012-09-30 - 2012-10-06 [41] 2012-10-07 - 2012-10-13 2012-10-14 - 2012-10-20 [43] 2012-10-21 - 2012-10-27 2012-10-28 - 2012-11-03 [45] 2012-11-04 - 2012-11-10 2012-11-11 - 2012-11-17 [47] 2012-11-18 - 2012-11-24 2012-11-25 - 2012-12-01 [49] 2012-12-02 - 2012-12-08 2012-12-09 - 2012-12-15 [51] 2012-12-16 - 2012-12-22 2012-12-23 - 2012-12-29 52 Levels: 2012-01-01 - 2012-01-07 ... 2012-12-23 - 2012-12-29 FluTest[11, ]  Week ILI Queries 11 2012-03-11 - 2012-03-17 2.293422 0.4329349 PredTest1[11]  11 2.187378  2.293422\n Problem 3.2 - Performance on the Test Set What is the relative error betweeen the estimate (our prediction) and the observed value for the week of March 11, 2012? Note that the relative error is calculated as (Observed ILI - Estimated ILI)/Observed ILI.\n(FluTest[11, 2] - PredTest1[11]) / FluTest[11, 2]  11 0.04623827   Problem 3.3 - Performance on the Test Set What is the Root Mean Square Error (RMSE) between our estimates and the actual observations for the percentage of ILI-related physician visits, on the test-set?\nFluTestSSE = sum((PredTest1 - FluTest$ILI)^2) FluTestRMSE = sqrt(FluTestSSE/nrow(FluTest)) FluTestRMSE [1] 0.7490645  Problem 4.1 - Training a Time Series Model The observations in this dataset are consecutive weekly measurements of the dependent and independent variables. This sort of dataset is called a “time series.”\nOften, statistical models can be improved by predicting the current value of the dependent variable using the value of the dependent variable from earlier weeks. In our models, this means we will predict the ILI variable in the current week using values of the ILI variable from previous weeks.\nFirst, we need to decide the amount of time to lag the observations. Because the ILI variable is reported with a 1- or 2-week lag, a decision maker cannot rely on the previous week’s ILI value to predict the current week’s value. Instead, the decision maker will only have data available from 2 or more weeks ago.\nWe will build a variable called ILILag2 that contains the ILI value from 2 weeks before the current observation.\nTo do so, we’ll use the “zoo” package, which provides a number of helpful methods for time series models. While many functions are built into R, you need to add new packages to use some functions. New packages can be installed and loaded easily in R. Run the following two codes to install and load the zoo package.\nIn the first code, you will be prompted to select a CRAN mirror to use for your download. Select a mirror near you geographically. install.packages(“zoo”)\nAfter installing and loading the zoo package, create the ILILag2 variable in the training set.\nILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE) FluTrain$ILILag2 = coredata(ILILag2) The value of -2 passed to lag means to return 2 observations before the current one; a positive value would have returned future observations. The parameter na.pad=TRUE means to add missing values for the first two weeks of our dataset, where we can’t compute the data from 2 weeks earlier.\n?lag ?coredata ILILag2  1 2 3 4 5 6 7 NA NA 2.4183312 1.8090560 1.7120239 1.5424951 1.4378683 8 9 10 11 12 13 14 1.3242740 1.3072567 1.0369770 1.0103204 1.0524925 1.0200901 0.9244187 15 16 17 18 19 20 21 0.7906450 0.8026098 0.8361300 0.7924358 0.6835877 0.7574523 0.7885854 22 23 24 25 26 27 28 0.8121710 0.8044629 0.8777009 0.7414530 0.6610222 0.7151092 0.5622412 29 30 31 32 33 34 35 0.7868082 0.8606578 0.6899440 0.7796912 0.6281439 0.9024586 0.8064432 36 37 38 39 40 41 42 0.8748878 0.9932130 0.8761408 0.9480916 0.9269426 0.9716430 0.8971591 43 44 45 46 47 48 49 1.0224828 1.0629632 1.1469570 1.2049501 1.3051655 1.2869916 1.5946756 50 51 52 53 54 55 56 1.3971432 1.4499567 1.6174545 2.1911192 2.5664893 2.1764491 2.2017121 57 58 59 60 61 62 63 2.5301211 3.0652381 3.9806083 4.5956803 4.7519706 4.1796206 3.4535851 64 65 66 67 68 69 70 3.1585224 2.6732010 2.3516104 1.8924285 1.5249048 1.4113441 1.2506826 71 72 73 74 75 76 77 1.2070250 1.0789550 1.1452080 1.0612426 1.0567977 1.2519310 1.0141893 78 79 80 81 82 83 84 1.0419693 0.9540274 0.8482299 0.8418715 0.7308936 0.7134316 0.6706772 85 86 87 88 89 90 91 0.6892776 0.7049290 0.6159033 0.6094256 0.6802587 0.7754884 0.6834214 92 93 94 95 96 97 98 0.7810748 0.8069435 1.0763468 1.0586890 1.1152326 1.1238125 1.2548892 99 100 101 102 103 104 105 1.3366090 1.3786364 1.6082900 1.4831056 1.6537399 2.0067892 2.5685716 106 107 108 109 110 111 112 3.0527762 2.4250373 2.0019506 2.0586902 2.2127697 2.3222001 2.4927920 113 114 115 116 117 118 119 2.7948942 2.9691114 2.8395905 2.7779902 2.4728693 2.1806146 2.0167951 120 121 122 123 124 125 126 1.6410133 1.3582865 1.1427983 1.0403125 0.9643469 0.9379817 0.9474493 127 128 129 130 131 132 133 0.8919182 0.8646427 0.9703199 0.8443901 0.7748704 0.8213725 0.8727445 134 135 136 137 138 139 140 0.9226345 0.8994868 0.8430824 0.8818244 0.8171452 0.8715001 0.7386205 141 142 143 144 145 146 147 0.7979660 1.0139373 0.8809358 0.9433663 0.8915462 1.2032228 1.0578822 148 149 150 151 152 153 154 1.1305354 1.1255230 1.2080820 1.3495244 1.4689004 1.8276716 1.6656012 155 156 157 158 159 160 161 1.8596834 2.3889130 2.7897759 3.1154858 2.2694245 1.8635464 1.9998635 162 163 164 165 166 167 168 2.4406044 2.8301821 3.1234256 3.2701949 3.1775688 2.7236366 2.5020140 169 170 171 172 173 174 175 2.4271992 1.9604132 1.5913980 1.3697835 1.3631668 1.1736951 1.0635756 176 177 178 179 180 181 182 0.9697111 0.9653617 0.8567489 0.8633465 0.9353695 0.7455694 0.7404281 183 184 185 186 187 188 189 0.6728965 0.6662820 0.6627473 0.5456190 0.5862306 0.6606867 0.5340928 190 191 192 193 194 195 196 0.5855491 0.6180750 0.6874647 0.7156961 0.8293131 0.8009115 0.9184839 197 198 199 200 201 202 203 0.8142590 1.0719708 1.2178574 1.2457554 1.3598449 1.4467085 1.5328638 204 205 206 207 208 209 210 1.6665324 1.9748773 1.6730547 1.6340509 1.7459475 1.9364319 2.4890534 211 212 213 214 215 216 217 2.2540484 2.0914715 2.3593428 3.3233143 4.4338100 5.3454714 5.4225751 218 219 220 221 222 223 224 5.3030330 4.2445550 3.6280001 3.0346275 2.5359536 2.0573015 1.7415035 225 226 227 228 229 230 231 1.4065217 1.2686070 1.0771887 0.9934452 0.9112119 0.9721091 0.9932575 232 233 234 235 236 237 238 1.0913202 0.8884460 0.8876915 0.8831874 0.8267564 0.7832014 0.7806103 239 240 241 242 243 244 245 0.7690726 0.7212979 0.7525273 0.7527210 0.7927660 0.7438962 0.8141663 246 247 248 249 250 251 252 0.8384009 0.8511236 1.1097575 1.0311436 1.0228436 1.0301739 1.0124478 253 254 255 256 257 258 259 1.0835911 1.1657765 1.1912964 1.2807470 1.2705251 1.5957825 1.4584994 260 261 262 263 264 265 266 1.4992072 1.6298157 2.1556121 2.0205270 1.5456623 1.6422367 1.9652378 267 268 269 270 271 272 273 2.3436784 2.8605744 3.3421049 3.2056588 3.1004908 2.9581850 2.4638058 274 275 276 277 278 279 280 2.1927224 1.8739459 1.6481690 1.4987776 1.2923267 1.2716411 2.9815890 281 282 283 284 285 286 287 2.4370224 2.2813011 3.8157199 4.2131523 3.1783224 2.5097162 2.0663177 288 289 290 291 292 293 294 1.7180460 1.5596467 1.3085629 1.1869460 1.1379623 1.1500523 1.1126189 295 296 297 298 299 300 301 1.1614188 1.6410714 2.4716598 3.7196936 3.9497480 4.0875636 4.0189724 302 303 304 305 306 307 308 4.6036164 5.6608671 6.8152222 7.6188921 7.3883586 6.3392723 4.9434950 309 310 311 312 313 314 315 3.8099612 3.4410588 2.6677306 2.4718250 2.3449995 2.7143498 2.6766718 316 317 318 319 320 321 322 1.9828382 1.8274862 1.9260563 1.9249472 2.0887684 2.0343408 1.9764946 323 324 325 326 327 328 329 1.9936177 1.8538260 1.8673036 1.6998677 1.4974082 1.4511188 1.2071478 330 331 332 333 334 335 336 1.1741508 1.1620668 1.1721343 1.1216765 1.1498116 1.1332758 1.0817133 337 338 339 340 341 342 343 1.1995860 0.9528083 0.9160321 0.9265822 0.8696197 0.9031331 0.7737757 344 345 346 347 348 349 350 0.7427744 0.7309345 0.7868818 0.7630507 0.8410432 0.7915728 0.9127318 351 352 353 354 355 356 357 1.0339765 0.9340091 1.0818888 1.0656260 1.1350529 1.2525629 1.2456956 358 359 360 361 362 363 364 1.2677380 1.4372295 1.5334125 1.6944544 1.9915024 1.8130453 2.0142579 365 366 367 368 369 370 371 2.5565913 3.3818486 3.4317231 2.6915111 2.9106289 3.4923189 4.0036963 372 373 374 375 376 377 378 4.4353368 4.2421482 4.3971861 3.9025565 3.1507275 2.7242234 2.3333563 379 380 381 382 383 384 385 1.9250003 1.7524260 1.5770365 1.3576558 1.3122310 1.1493747 1.1145057 386 387 388 389 390 391 392 1.1098449 1.0524026 1.0353647 1.1177658 0.9829495 0.9251944 0.8355311 393 394 395 396 397 398 399 0.8323927 0.8555910 0.7069494 0.6943868 0.6879762 0.6447430 0.6753299 400 401 402 403 404 405 406 0.7282297 0.8065263 0.8604084 0.9360754 0.9666827 0.9960071 1.1084635 407 408 409 410 411 412 413 1.2030858 1.2369566 1.2525865 1.3054612 1.4528432 1.4408922 1.4622115 414 415 416 417 1.6554147 1.4657230 1.5181061 1.6639544  How many values are missing in the new ILILag2 variable?\nsum(is.na(FluTrain$ILILag2)) [1] 2  Problem 4.2 - Training a Time Series Model Use the plot() function to plot the log of ILILag2 against the log of ILI.\nWhich best describes the relationship between these two variables?\nplot(log(FluTrain$ILILag2), log(FluTrain$ILI)) There is a strong positive relationship between log(ILILag2) and log(ILI).\n Problem 4.3 - Training a Time Series Model Train a linear regression model on the FluTrain dataset to predict the log of the ILI variable using the Queries variable as well as the log of the ILILag2 variable. Call this model FluTrend2.\nFluTrend2 \u0026lt;- lm(log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Which coefficients are significant at the p=0.05 level in this regression model?\nsummary(FluTrend2)  Call: lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.52209 -0.11082 -0.01819 0.08143 0.76785 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.24064 0.01953 -12.32 \u0026lt;2e-16 *** Queries 1.25578 0.07910 15.88 \u0026lt;2e-16 *** log(ILILag2) 0.65569 0.02251 29.14 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.1703 on 412 degrees of freedom (2 observations deleted due to missingness) Multiple R-squared: 0.9063, Adjusted R-squared: 0.9059 F-statistic: 1993 on 2 and 412 DF, p-value: \u0026lt; 2.2e-16 All are significant at p\u0026lt;0.05\nWhat is the R^2 value of the FluTrend2 model? #### 0.9063\n Problem 4.4 - Training a Time Series Model On the basis of R-squared value and significance of coefficients, which statement is the most accurate?\nsummary(FluTrend1)  Call: lm(formula = log(ILI) ~ Queries, data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.76003 -0.19696 -0.01657 0.18685 1.06450 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.49934 0.03041 -16.42 \u0026lt;2e-16 *** Queries 2.96129 0.09312 31.80 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.2995 on 415 degrees of freedom Multiple R-squared: 0.709, Adjusted R-squared: 0.7083 F-statistic: 1011 on 1 and 415 DF, p-value: \u0026lt; 2.2e-16 summary(FluTrend2)  Call: lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain) Residuals: Min 1Q Median 3Q Max -0.52209 -0.11082 -0.01819 0.08143 0.76785 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -0.24064 0.01953 -12.32 \u0026lt;2e-16 *** Queries 1.25578 0.07910 15.88 \u0026lt;2e-16 *** log(ILILag2) 0.65569 0.02251 29.14 \u0026lt;2e-16 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.1703 on 412 degrees of freedom (2 observations deleted due to missingness) Multiple R-squared: 0.9063, Adjusted R-squared: 0.9059 F-statistic: 1993 on 2 and 412 DF, p-value: \u0026lt; 2.2e-16 FluTrend2 is a stronger model than FluTrend1 on the training set, due to it’s higher R^2 value.   Problem 5.1 - Evaluating the Time Series Model in the Test Set So far, we have only added the ILILag2 variable to the FluTrain dataframe. To make predictions with our FluTrend2 model, we’ll also need to add ILILag2 to the FluTest dataframe (note that adding variables before splitting into a training and testing set can prevent this duplication of effort).\nModifying the code from the previous subproblem to add an ILILag2 variable to the FluTest dataframe.\nHow many missing values are there in this new variable?\nTest_ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE) FluTest$ILILag2 = coredata(Test_ILILag2) sum(is.na(FluTest$ILILag2)) [1] 2  Problem 5.2 - Evaluating the Time Series Model in the Test Set In this problem, the training and testing sets are split sequentially – the training set contains all observations from 2004-2011 and the testing set contains all observations from 2012.\nThere is no time gap between the two datasets, meaning the first observation in FluTest was recorded one week after the last observation in FluTrain. From this, we can identify how to fill in the missing values for the ILILag2 variable in FluTest. Which value should be used to fill in the ILILag2 variable for the first observation in FluTest?\nThe ILI value of the second-to-last observation in the FluTrain dataframe. Which value should be used to fill in the ILILag2 variable for the second observation in FluTest? #### The ILI value of the last observation in the FluTrain dataframe.\n Problem 5.3 - Evaluating the Time Series Model in the Test Set Fill in the missing values for ILILag2 in FluTest. In terms of syntax, you could set the value of ILILag2 in row “x” of the FluTest dataframe to the value of ILI in row “y” of the FluTrain dataframe with “FluTest\\(ILILag2[x] = FluTrain\\)ILI[y]”.\nUse the answer to the previous questions to determine the appropriate values of “x” and “y”. It may be helpful to check the total number of rows in FluTrain using str(FluTrain) or nrow(FluTrain).\nnrow(FluTrain) [1] 417 FluTest$ILILag2[1] = FluTrain$ILI[416] FluTest$ILILag2[2] = FluTrain$ILI[417] What is the new value of the ILILag2 variable in the first row of FluTest?\nFluTrain$ILI[416] [1] 1.852736 FluTest$ILILag2[1] [1] 1.852736 What is the new value of the ILILag2 variable in the second row of FluTest?\nFluTrain$ILI[417] [1] 2.12413 FluTest$ILILag2[2] [1] 2.12413  Problem 5.4 - Evaluating the Time Series Model in the Test Set Obtain test-set predictions of the ILI variable from the FluTrend2 model, again remembering to call the exp() function on the result of the predict() function to obtain predictions for ILI instead of log(ILI).\nWhat is the test-set RMSE of the FluTrend2 model?\nPredTest2 = exp(predict(FluTrend2, newdata=FluTest)) FluTestSSE2 = sum((PredTest2 - FluTest$ILI)^2) FluTestRMSE2 = sqrt(FluTestSSE2/nrow(FluTest)) FluTestRMSE2 [1] 0.2942029  Problem 5.5 - Evaluating the Time Series Model in the Test Set Which model obtained the best test-set RMSE? #### FluTrend2 (less RMSE is better)\nConclusion In this analysis, I’ve used a simple time series model with a single lag term. ARIMA models are a more general form of the model we built, which can include multiple lag terms as well as more complicated combinations of previous values of the dependent variable.\n  ","date":1554508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554508800,"objectID":"fc1cd3da47244505d6c2a5717a1bee4f","permalink":"/project/flu_epidemics/flu_epidemics/","publishdate":"2019-04-06T00:00:00Z","relpermalink":"/project/flu_epidemics/flu_epidemics/","section":"project","summary":"The Google Flu Trends project","tags":["R","Data Analytics","Machine Learning"],"title":"Detecting Flu Epidemics via Search Engine Query Data","type":"project"},{"authors":null,"categories":null,"content":" The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in mathematics, reading, and science.\nThe test provides a quantitative way to compare the performance of students from different parts of the world.\nIn this analysis, I’ll predict the reading scores of students from the USA on the 2009 PISA exam.\nThe datasets contain information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data we are bound by the NCES data use agreement, which prohibits any attempt to determine the identity of any student in the datasets.\nEach row in the datasets represents one student taking the exam. The datasets have the following variables:\n grade: The grade in school of the student (most 15-year-olds in America are in 10th grade) male: Whether the student is male (1/0) raceeth: The race/ethnicity composite of the student preschool: Whether the student attended preschool (1/0) expectBachelors: Whether the student expects to obtain a bachelor’s degree (1/0) motherHS: Whether the student’s mother completed high school (1/0) motherBachelors: Whether the student’s mother obtained a bachelor’s degree (1/0) motherWork: Whether the student’s mother has part-time or full-time work (1/0) fatherHS: Whether the student’s father completed high school (1/0) fatherBachelors: Whether the student’s father obtained a bachelor’s degree (1/0) fatherWork: Whether the student’s father has part-time or full-time work (1/0) selfBornUS: Whether the student was born in the United States of America (1/0) motherBornUS: Whether the student’s mother was born in the United States of America (1/0) fatherBornUS: Whether the student’s father was born in the United States of America (1/0) englishAtHome: Whether the student speaks English at home (1/0) computerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0) read30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0) minutesPerWeekEnglish: The number of minutes per week the student spend in English class studentsInEnglish: The number of students in this student’s English class at school schoolHasLibrary: Whether this student’s school has a library (1/0) publicSchool: Whether this student attends a public school (1/0) urban: Whether this student’s school is in an urban area (1/0) schoolSize: The number of students in this student’s school readingScore: The student’s reading score, on a 1000-point scale  Problem 1.1 - Dataset size Load the training and testing sets using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.\npisaTrain \u0026lt;- read.csv(\u0026quot;pisa2009train.csv\u0026quot;) pisaTest \u0026lt;- read.csv(\u0026quot;pisa2009test.csv\u0026quot;) str(pisaTrain) \u0026#39;data.frame\u0026#39;: 3663 obs. of 24 variables: $ grade : int 11 11 9 10 10 10 10 10 9 10 ... $ male : int 1 1 1 0 1 1 0 0 0 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: NA 7 7 3 4 3 2 7 7 5 ... $ preschool : int NA 0 1 1 1 1 0 1 1 1 ... $ expectBachelors : int 0 0 1 1 0 1 1 1 0 1 ... $ motherHS : int NA 1 1 0 1 NA 1 1 1 1 ... $ motherBachelors : int NA 1 1 0 0 NA 0 0 NA 1 ... $ motherWork : int 1 1 1 1 1 1 1 0 1 1 ... $ fatherHS : int NA 1 1 1 1 1 NA 1 0 0 ... $ fatherBachelors : int NA 0 NA 0 0 0 NA 0 NA 0 ... $ fatherWork : int 1 1 1 1 0 1 NA 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 1 0 1 1 1 ... $ motherBornUS : int 0 1 1 1 1 1 1 1 1 1 ... $ fatherBornUS : int 0 1 1 1 0 1 NA 1 1 1 ... $ englishAtHome : int 0 1 1 1 1 1 1 1 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 1 1 1 1 1 ... $ read30MinsADay : int 0 1 0 1 1 0 0 1 0 0 ... $ minutesPerWeekEnglish: int 225 450 250 200 250 300 250 300 378 294 ... $ studentsInEnglish : int NA 25 28 23 35 20 28 30 20 24 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 0 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 1 ... $ urban : int 1 0 0 1 1 0 1 0 1 0 ... $ schoolSize : int 673 1173 1233 2640 1095 227 2080 1913 502 899 ... $ readingScore : num 476 575 555 458 614 ... summary(pisaTrain)  grade male raceeth Min. : 8.00 Min. :0.0000 White :2015 1st Qu.:10.00 1st Qu.:0.0000 Hispanic : 834 Median :10.00 Median :1.0000 Black : 444 Mean :10.09 Mean :0.5111 Asian : 143 3rd Qu.:10.00 3rd Qu.:1.0000 More than one race: 124 Max. :12.00 Max. :1.0000 (Other) : 68 NA\u0026#39;s : 35 preschool expectBachelors motherHS motherBachelors Min. :0.0000 Min. :0.0000 Min. :0.00 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:1.00 1st Qu.:0.0000 Median :1.0000 Median :1.0000 Median :1.00 Median :0.0000 Mean :0.7228 Mean :0.7859 Mean :0.88 Mean :0.3481 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.00 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00 Max. :1.0000 NA\u0026#39;s :56 NA\u0026#39;s :62 NA\u0026#39;s :97 NA\u0026#39;s :397 motherWork fatherHS fatherBachelors fatherWork Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :0.0000 Median :1.0000 Mean :0.7345 Mean :0.8593 Mean :0.3319 Mean :0.8531 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :93 NA\u0026#39;s :245 NA\u0026#39;s :569 NA\u0026#39;s :233 selfBornUS motherBornUS fatherBornUS englishAtHome Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Mean :0.9313 Mean :0.7725 Mean :0.7668 Mean :0.8717 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :69 NA\u0026#39;s :71 NA\u0026#39;s :113 NA\u0026#39;s :71 computerForSchoolwork read30MinsADay minutesPerWeekEnglish Min. :0.0000 Min. :0.0000 Min. : 0.0 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 225.0 Median :1.0000 Median :0.0000 Median : 250.0 Mean :0.8994 Mean :0.2899 Mean : 266.2 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.: 300.0 Max. :1.0000 Max. :1.0000 Max. :2400.0 NA\u0026#39;s :65 NA\u0026#39;s :34 NA\u0026#39;s :186 studentsInEnglish schoolHasLibrary publicSchool urban Min. : 1.0 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:20.0 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:0.0000 Median :25.0 Median :1.0000 Median :1.0000 Median :0.0000 Mean :24.5 Mean :0.9676 Mean :0.9339 Mean :0.3849 3rd Qu.:30.0 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :75.0 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :249 NA\u0026#39;s :143 schoolSize readingScore Min. : 100 Min. :168.6 1st Qu.: 712 1st Qu.:431.7 Median :1212 Median :499.7 Mean :1369 Mean :497.9 3rd Qu.:1900 3rd Qu.:566.2 Max. :6694 Max. :746.0 NA\u0026#39;s :162  Number of students in the training set is 3663\n Problem 1.2 - Summarizing the dataset Using tapply() on pisaTrain, what is the average reading test score of males?\ntapply(pisaTrain$readingScore, pisaTrain$male, mean)  0 1 512.9406 483.5325  Males reading score, 483.5325 and Females reading score is 512.9406\n Problem 1.3 - Locating missing values Which variables are missing data in at least one observation in the training set?\nsummary(pisaTrain)  grade male raceeth Min. : 8.00 Min. :0.0000 White :2015 1st Qu.:10.00 1st Qu.:0.0000 Hispanic : 834 Median :10.00 Median :1.0000 Black : 444 Mean :10.09 Mean :0.5111 Asian : 143 3rd Qu.:10.00 3rd Qu.:1.0000 More than one race: 124 Max. :12.00 Max. :1.0000 (Other) : 68 NA\u0026#39;s : 35 preschool expectBachelors motherHS motherBachelors Min. :0.0000 Min. :0.0000 Min. :0.00 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:1.00 1st Qu.:0.0000 Median :1.0000 Median :1.0000 Median :1.00 Median :0.0000 Mean :0.7228 Mean :0.7859 Mean :0.88 Mean :0.3481 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.00 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.00 Max. :1.0000 NA\u0026#39;s :56 NA\u0026#39;s :62 NA\u0026#39;s :97 NA\u0026#39;s :397 motherWork fatherHS fatherBachelors fatherWork Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:0.0000 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :0.0000 Median :1.0000 Mean :0.7345 Mean :0.8593 Mean :0.3319 Mean :0.8531 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :93 NA\u0026#39;s :245 NA\u0026#39;s :569 NA\u0026#39;s :233 selfBornUS motherBornUS fatherBornUS englishAtHome Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Median :1.0000 Mean :0.9313 Mean :0.7725 Mean :0.7668 Mean :0.8717 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :69 NA\u0026#39;s :71 NA\u0026#39;s :113 NA\u0026#39;s :71 computerForSchoolwork read30MinsADay minutesPerWeekEnglish Min. :0.0000 Min. :0.0000 Min. : 0.0 1st Qu.:1.0000 1st Qu.:0.0000 1st Qu.: 225.0 Median :1.0000 Median :0.0000 Median : 250.0 Mean :0.8994 Mean :0.2899 Mean : 266.2 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.: 300.0 Max. :1.0000 Max. :1.0000 Max. :2400.0 NA\u0026#39;s :65 NA\u0026#39;s :34 NA\u0026#39;s :186 studentsInEnglish schoolHasLibrary publicSchool urban Min. : 1.0 Min. :0.0000 Min. :0.0000 Min. :0.0000 1st Qu.:20.0 1st Qu.:1.0000 1st Qu.:1.0000 1st Qu.:0.0000 Median :25.0 Median :1.0000 Median :1.0000 Median :0.0000 Mean :24.5 Mean :0.9676 Mean :0.9339 Mean :0.3849 3rd Qu.:30.0 3rd Qu.:1.0000 3rd Qu.:1.0000 3rd Qu.:1.0000 Max. :75.0 Max. :1.0000 Max. :1.0000 Max. :1.0000 NA\u0026#39;s :249 NA\u0026#39;s :143 schoolSize readingScore Min. : 100 Min. :168.6 1st Qu.: 712 1st Qu.:431.7 Median :1212 Median :499.7 Mean :1369 Mean :497.9 3rd Qu.:1900 3rd Qu.:566.2 Max. :6694 Max. :746.0 NA\u0026#39;s :162  raceeth, preschool, expectBachelors, motherHS, motherBachelors, motherWork, fatherHS, fatherBachelors, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, computerForSchoolWork, read30MinsADay, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, schoolSize   Problem 1.4 - Removing missing values Linear regression discards observations with missing data, so I’ll remove all such observations from the training and testing sets. Later, we’ll learn about imputation, which deals with missing data by filling in missing values with plausible information.\nRemoving observations with missing value from pisaTrain and pisaTest:\npisaTrain = na.omit(pisaTrain) pisaTest = na.omit(pisaTest) How many observations are now in the training set?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ... 2414\nHow many observations are now in the testing set?\nstr(pisaTest) \u0026#39;data.frame\u0026#39;: 990 obs. of 24 variables: $ grade : int 10 10 10 10 11 10 10 10 10 10 ... $ male : int 0 0 0 0 0 1 0 1 1 0 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 7 1 7 7 4 7 4 7 4 ... $ preschool : int 1 1 1 1 0 1 0 1 1 1 ... $ expectBachelors : int 0 1 0 0 0 1 1 0 1 1 ... $ motherHS : int 1 1 1 1 1 1 1 1 1 1 ... $ motherBachelors : int 1 0 0 0 1 1 0 0 1 0 ... $ motherWork : int 1 0 0 1 1 1 0 1 1 1 ... $ fatherHS : int 1 1 1 1 1 1 1 1 1 1 ... $ fatherBachelors : int 0 1 0 0 1 0 0 0 1 1 ... $ fatherWork : int 0 1 0 1 1 1 1 0 1 1 ... $ selfBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ motherBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ fatherBornUS : int 1 1 1 1 1 1 1 1 1 1 ... $ englishAtHome : int 1 1 1 1 1 1 1 1 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 1 1 1 1 1 ... $ read30MinsADay : int 0 0 1 1 1 1 0 0 0 1 ... $ minutesPerWeekEnglish: int 240 240 240 270 270 350 350 360 350 360 ... $ studentsInEnglish : int 30 30 30 35 30 25 27 28 25 27 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 1 ... $ urban : int 0 0 0 0 0 0 0 0 0 0 ... $ schoolSize : int 808 808 808 808 808 899 899 899 899 899 ... $ readingScore : num 355 454 405 665 605 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 2 3 4 6 12 16 17 19 22 23 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; ... 990\n Problem 2.1 - Factor variables Factor variables are variables that take on a discrete set of values. This is an unordered factor because there isn’t any natural ordering between the levels.\nAn ordered factor has a natural ordering between the levels (an example would be the classifications “large,” “medium,” and “small”).\nWhich of the following variables is an unordered factor with at least 3 levels?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ... raceeth Which of the following variables is an ordered factor with at least 3 levels? #### grade\n  Problem 2.2 - Unordered factors in regression models To include unordered factors in a linear regression model, we define one level as the “reference level” and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.\nAs an example, consider the unordered factor variable “color”, with levels “red”, “green”, and “blue”. If “green” were the reference level, then we would add binary variables “colored” and “colorblue” to a linear regression problem. All red examples would have colored=1 and colorblue=0. All blue examples would have colored=0 and colorblue=1. All green examples would have colored=0 and colorblue=0.\nNow, consider the variable “raceeth” in our problem, which has levels “American Indian/Alaska Native”, “Asian”, “Black”, “Hispanic”,\n“More than one race”, “Native Hawaiian/Other Pacific Islander”, and “White”.\nBecause it’s the most common in our population, we will select White as the reference level.\nWhich binary variables will be included in the regression model?\nstr(pisaTrain) \u0026#39;data.frame\u0026#39;: 2414 obs. of 24 variables: $ grade : int 11 10 10 10 10 10 10 10 11 9 ... $ male : int 1 0 1 0 1 0 0 0 1 1 ... $ raceeth : Factor w/ 7 levels \u0026quot;American Indian/Alaska Native\u0026quot;,..: 7 3 4 7 5 4 7 4 7 7 ... $ preschool : int 0 1 1 1 1 1 1 1 1 1 ... $ expectBachelors : int 0 1 0 1 1 1 1 0 1 1 ... $ motherHS : int 1 0 1 1 1 1 1 0 1 1 ... $ motherBachelors : int 1 0 0 0 1 0 0 0 0 1 ... $ motherWork : int 1 1 1 0 1 1 1 0 0 1 ... $ fatherHS : int 1 1 1 1 0 1 1 0 1 1 ... $ fatherBachelors : int 0 0 0 0 0 0 1 0 1 1 ... $ fatherWork : int 1 1 0 1 1 0 1 1 1 1 ... $ selfBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ motherBornUS : int 1 1 1 1 1 0 1 0 1 1 ... $ fatherBornUS : int 1 1 0 1 1 0 1 0 1 1 ... $ englishAtHome : int 1 1 1 1 1 0 1 0 1 1 ... $ computerForSchoolwork: int 1 1 1 1 1 0 1 1 1 1 ... $ read30MinsADay : int 1 1 1 1 0 1 1 1 0 0 ... $ minutesPerWeekEnglish: int 450 200 250 300 294 232 225 270 275 225 ... $ studentsInEnglish : int 25 23 35 30 24 14 20 25 30 15 ... $ schoolHasLibrary : int 1 1 1 1 1 1 1 1 1 1 ... $ publicSchool : int 1 1 1 1 1 1 1 1 1 0 ... $ urban : int 0 1 1 0 0 0 0 1 1 1 ... $ schoolSize : int 1173 2640 1095 1913 899 1733 149 1400 1988 915 ... $ readingScore : num 575 458 614 439 466 ... - attr(*, \u0026quot;na.action\u0026quot;)= \u0026#39;omit\u0026#39; Named int 1 3 6 7 9 11 13 21 29 30 ... ..- attr(*, \u0026quot;names\u0026quot;)= chr \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;6\u0026quot; \u0026quot;7\u0026quot; ...  raceethAmerican Indian/Alaska Native raceethAsian raceethBlack raceethHispanic raceethMore than one race raceethNative Hawaiian/Other Pacific Islander   Problem 2.3 - Example unordered factors Consider again adding our unordered factor race to the regression model with reference level “White”. For a student who is Asian, which binary variables would be set to 0. All remaining variables will be set to 1. (all except raceethAsian) #### all\n Problem 3.1 - Building a model Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() – you can see this when you run str(pisaTrain) or str(pisaTest).\nHowever, by default R selects the first level alphabetically (“American Indian/Alaska Native”) as the reference level of our factor instead of the most common level (“White”).\nLet’s Set the reference level of the factor.\npisaTrain$raceeth = relevel(pisaTrain$raceeth, \u0026quot;White\u0026quot;) pisaTest$raceeth = relevel(pisaTest$raceeth, \u0026quot;White\u0026quot;) Now, building a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables. It would be time-consuming to type all the variables, but R provides the shorthand notation “readingScore ~ .” to mean “predict readingScore using all the other variables in the dataframe.” The period is used to replace listing out all of the independent variables.\nAs an example, if our dependent variable is called “Y”, our independent variables are called “X1”, “X2”, and “X3”, and our training dataset is called “Train”, instead of the regular notation: LinReg = lm(Y ~ X1 + X2 + X3, data = Train)\nYou would use the following code to build our model: LinReg = lm(Y ~ ., data = Train)\nlmScore \u0026lt;- lm(readingScore ~ ., data = pisaTrain) What is the Multiple R-squared value of lmScore on the training set?\nsummary(lmScore)  Call: lm(formula = readingScore ~ ., data = pisaTrain) Residuals: Min 1Q Median 3Q Max -247.44 -48.86 1.86 49.77 217.18 Coefficients: Estimate Std. Error (Intercept) 143.766333 33.841226 grade 29.542707 2.937399 male -14.521653 3.155926 raceethAmerican Indian/Alaska Native -67.277327 16.786935 raceethAsian -4.110325 9.220071 raceethBlack -67.012347 5.460883 raceethHispanic -38.975486 5.177743 raceethMore than one race -16.922522 8.496268 raceethNative Hawaiian/Other Pacific Islander -5.101601 17.005696 preschool -4.463670 3.486055 expectBachelors 55.267080 4.293893 motherHS 6.058774 6.091423 motherBachelors 12.638068 3.861457 motherWork -2.809101 3.521827 fatherHS 4.018214 5.579269 fatherBachelors 16.929755 3.995253 fatherWork 5.842798 4.395978 selfBornUS -3.806278 7.323718 motherBornUS -8.798153 6.587621 fatherBornUS 4.306994 6.263875 englishAtHome 8.035685 6.859492 computerForSchoolwork 22.500232 5.702562 read30MinsADay 34.871924 3.408447 minutesPerWeekEnglish 0.012788 0.010712 studentsInEnglish -0.286631 0.227819 schoolHasLibrary 12.215085 9.264884 publicSchool -16.857475 6.725614 urban -0.110132 3.962724 schoolSize 0.006540 0.002197 t value Pr(\u0026gt;|t|) (Intercept) 4.248 2.24e-05 *** grade 10.057 \u0026lt; 2e-16 *** male -4.601 4.42e-06 *** raceethAmerican Indian/Alaska Native -4.008 6.32e-05 *** raceethAsian -0.446 0.65578 raceethBlack -12.271 \u0026lt; 2e-16 *** raceethHispanic -7.528 7.29e-14 *** raceethMore than one race -1.992 0.04651 * raceethNative Hawaiian/Other Pacific Islander -0.300 0.76421 preschool -1.280 0.20052 expectBachelors 12.871 \u0026lt; 2e-16 *** motherHS 0.995 0.32001 motherBachelors 3.273 0.00108 ** motherWork -0.798 0.42517 fatherHS 0.720 0.47147 fatherBachelors 4.237 2.35e-05 *** fatherWork 1.329 0.18393 selfBornUS -0.520 0.60331 motherBornUS -1.336 0.18182 fatherBornUS 0.688 0.49178 englishAtHome 1.171 0.24153 computerForSchoolwork 3.946 8.19e-05 *** read30MinsADay 10.231 \u0026lt; 2e-16 *** minutesPerWeekEnglish 1.194 0.23264 studentsInEnglish -1.258 0.20846 schoolHasLibrary 1.318 0.18749 publicSchool -2.506 0.01226 * urban -0.028 0.97783 schoolSize 2.977 0.00294 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 73.81 on 2385 degrees of freedom Multiple R-squared: 0.3251, Adjusted R-squared: 0.3172 F-statistic: 41.04 on 28 and 2385 DF, p-value: \u0026lt; 2.2e-16 0.3251\nNote, that this R-squared is lower than the ones prevously observed. This does not necessarily imply that the model is of poor quality. More often than not, it simply means that the prediction problem at hand (predicting a student’s test score based on demographic and school-related variables) is more difficult than other prediction problems (like predicting a team’s number of wins from their runs scored and allowed, or predicting the quality of wine from weather conditions).\n Problem 3.2 - Computing the root-mean squared error of the model What is the training-set root mean squared error (RMSE) of lmScore?\nlmScoreSSE \u0026lt;- sum(lmScore$residuals^2) lmScoreSSE [1] 12993365 sqrt(lmScoreSSE/nrow(pisaTrain)) [1] 73.36555  Problem 3.3 - Comparing predictions for similar students Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9.\nWhat is the predicted reading score of student A minus the predicted reading score of student B?\npisaPred \u0026lt;- pisaTest[1,] pisaPred \u0026lt;- rbind(pisaPred, pisaTest[1,]) pisaPred[1,1] \u0026lt;- 11 ## grade 11 for student A pisaPred[2,1] \u0026lt;- 9 ## grade 9 for student B pisaPred  grade male raceeth preschool expectBachelors motherHS motherBachelors 1 11 0 White 1 0 1 1 2 9 0 White 1 0 1 1 motherWork fatherHS fatherBachelors fatherWork selfBornUS motherBornUS 1 1 1 0 0 1 1 2 1 1 0 0 1 1 fatherBornUS englishAtHome computerForSchoolwork read30MinsADay 1 1 1 1 0 2 1 1 1 0 minutesPerWeekEnglish studentsInEnglish schoolHasLibrary publicSchool 1 240 30 1 1 2 240 30 1 1 urban schoolSize readingScore 1 0 808 355.24 2 0 808 355.24 predictedScores \u0026lt;- predict(lmScore, pisaPred) predictedScores  1 2 501.5294 442.4440  predictedScores[1] - predictedScores[2]  1 59.08541  59.08541 ~ 59.09\n Problem 3.4 - Interpreting model coefficients What is the meaning of the coefficient associated with variable raceethAsian?\nsummary(lmScore)  Call: lm(formula = readingScore ~ ., data = pisaTrain) Residuals: Min 1Q Median 3Q Max -247.44 -48.86 1.86 49.77 217.18 Coefficients: Estimate Std. Error (Intercept) 143.766333 33.841226 grade 29.542707 2.937399 male -14.521653 3.155926 raceethAmerican Indian/Alaska Native -67.277327 16.786935 raceethAsian -4.110325 9.220071 raceethBlack -67.012347 5.460883 raceethHispanic -38.975486 5.177743 raceethMore than one race -16.922522 8.496268 raceethNative Hawaiian/Other Pacific Islander -5.101601 17.005696 preschool -4.463670 3.486055 expectBachelors 55.267080 4.293893 motherHS 6.058774 6.091423 motherBachelors 12.638068 3.861457 motherWork -2.809101 3.521827 fatherHS 4.018214 5.579269 fatherBachelors 16.929755 3.995253 fatherWork 5.842798 4.395978 selfBornUS -3.806278 7.323718 motherBornUS -8.798153 6.587621 fatherBornUS 4.306994 6.263875 englishAtHome 8.035685 6.859492 computerForSchoolwork 22.500232 5.702562 read30MinsADay 34.871924 3.408447 minutesPerWeekEnglish 0.012788 0.010712 studentsInEnglish -0.286631 0.227819 schoolHasLibrary 12.215085 9.264884 publicSchool -16.857475 6.725614 urban -0.110132 3.962724 schoolSize 0.006540 0.002197 t value Pr(\u0026gt;|t|) (Intercept) 4.248 2.24e-05 *** grade 10.057 \u0026lt; 2e-16 *** male -4.601 4.42e-06 *** raceethAmerican Indian/Alaska Native -4.008 6.32e-05 *** raceethAsian -0.446 0.65578 raceethBlack -12.271 \u0026lt; 2e-16 *** raceethHispanic -7.528 7.29e-14 *** raceethMore than one race -1.992 0.04651 * raceethNative Hawaiian/Other Pacific Islander -0.300 0.76421 preschool -1.280 0.20052 expectBachelors 12.871 \u0026lt; 2e-16 *** motherHS 0.995 0.32001 motherBachelors 3.273 0.00108 ** motherWork -0.798 0.42517 fatherHS 0.720 0.47147 fatherBachelors 4.237 2.35e-05 *** fatherWork 1.329 0.18393 selfBornUS -0.520 0.60331 motherBornUS -1.336 0.18182 fatherBornUS 0.688 0.49178 englishAtHome 1.171 0.24153 computerForSchoolwork 3.946 8.19e-05 *** read30MinsADay 10.231 \u0026lt; 2e-16 *** minutesPerWeekEnglish 1.194 0.23264 studentsInEnglish -1.258 0.20846 schoolHasLibrary 1.318 0.18749 publicSchool -2.506 0.01226 * urban -0.028 0.97783 schoolSize 2.977 0.00294 ** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 73.81 on 2385 degrees of freedom Multiple R-squared: 0.3251, Adjusted R-squared: 0.3172 F-statistic: 41.04 on 28 and 2385 DF, p-value: \u0026lt; 2.2e-16 Predicted difference in the reading score between an Asian student and a white student who is otherwise identical.   Problem 3.5 - Identifying variables lacking statistical significance Based on the significance codes, which variables are candidates for removal from the model? (We’ll assume that the factor variable raceeth should only be removed if none of its levels are significant.)\n preschool, motherHS, motherWork, fatherHS, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, urban   Problem 4.1 - Predicting on unseen data Using the “predict” function and supplying the “newdata” argument, use the lmScore model to predict the reading scores of students in pisaTest. Call this vector of predictions “predTest”. Do not change the variables in the model (for example, do not remove variables that we found were not significant in the previous part of this problem). Use the summary function to describe the test-set predictions.\nWhat is the range between the max and min predicted reading score on the test-set?\npredTest \u0026lt;- predict(lmScore, newdata = pisaTest) summary(predTest)  Min. 1st Qu. Median Mean 3rd Qu. Max. 353.2 482.0 524.0 516.7 555.7 637.7  637.7 - 353.2\n Problem 4.2 - Test set SSE and RMSE What is the sum of squared errors (SSE) of lmScore on the testing set?\ntest_set_SSE = sum((predTest - pisaTest$readingScore)^2) test_set_SSE [1] 5762082 What is the root mean squared error (RMSE) of lmScore on the testing set?\ntest_set_RMSE = sqrt(test_set_SSE/nrow(pisaTest)) test_set_RMSE [1] 76.29079  Problem 4.3 - Baseline prediction and test-set SSE What is the predicted test score used in the baseline model?\nmean(pisaTrain$readingScore) [1] 517.9629 What is the sum of squared errors of the baseline model on the testing set? HINT: We call the sum of squared errors for the baseline model the total sum of squares (SST).\ntest_set_SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2) test_set_SST [1] 7802354  Problem 4.4 - Test-set R-squared What is the test-set R-squared value of lmScore?\n1 - test_set_SSE/test_set_SST [1] 0.2614944  ","date":1554508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554508800,"objectID":"ee363738541f1a895e2e849df95cac70","permalink":"/project/pisa2009/pisa/","publishdate":"2019-04-06T00:00:00Z","relpermalink":"/project/pisa2009/pisa/","section":"project","summary":"Predict reading scores of students in the 2009 PISA exam","tags":["R","Data Analytics","Machine Learning"],"title":"Reading Test Scores","type":"project"},{"authors":null,"categories":null,"content":" There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.\nIn this analysis, I’ll attempt to study the relationship between average global temperature and several other factors.\nThe file climate_change.csv contains climate data from May 1983 to December 2008. The available variables include:\n Year: the observation year. Month: the observation month. Temp: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia. CO2, N2O, CH4, CFC.11, CFC.12: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.  CO2, N2O and CH4 are expressed in ppmv (parts per million by volume – i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere) CFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume).  Aerosols: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun’s energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA. TSI: the total solar irradiance (TSI) in W/m2 (the rate at which the sun’s energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website. MEI: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.  We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv.\nclimate \u0026lt;- read.csv(\u0026quot;climate_change.csv\u0026quot;) str(climate) \u0026#39;data.frame\u0026#39;: 308 obs. of 11 variables: $ Year : int 1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ... $ Month : int 5 6 7 8 9 10 11 12 1 2 ... $ MEI : num 2.556 2.167 1.741 1.13 0.428 ... $ CO2 : num 346 346 344 342 340 ... $ CH4 : num 1639 1634 1633 1631 1648 ... $ N2O : num 304 304 304 304 304 ... $ CFC.11 : num 191 192 193 194 194 ... $ CFC.12 : num 350 352 354 356 357 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ... $ Temp : num 0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ... summary(climate)  Year Month MEI CO2 Min. :1983 Min. : 1.000 Min. :-1.6350 Min. :340.2 1st Qu.:1989 1st Qu.: 4.000 1st Qu.:-0.3987 1st Qu.:353.0 Median :1996 Median : 7.000 Median : 0.2375 Median :361.7 Mean :1996 Mean : 6.552 Mean : 0.2756 Mean :363.2 3rd Qu.:2002 3rd Qu.:10.000 3rd Qu.: 0.8305 3rd Qu.:373.5 Max. :2008 Max. :12.000 Max. : 3.0010 Max. :388.5 CH4 N2O CFC.11 CFC.12 Min. :1630 Min. :303.7 Min. :191.3 Min. :350.1 1st Qu.:1722 1st Qu.:308.1 1st Qu.:246.3 1st Qu.:472.4 Median :1764 Median :311.5 Median :258.3 Median :528.4 Mean :1750 Mean :312.4 Mean :252.0 Mean :497.5 3rd Qu.:1787 3rd Qu.:317.0 3rd Qu.:267.0 3rd Qu.:540.5 Max. :1814 Max. :322.2 Max. :271.5 Max. :543.8 TSI Aerosols Temp Min. :1365 Min. :0.00160 Min. :-0.2820 1st Qu.:1366 1st Qu.:0.00280 1st Qu.: 0.1217 Median :1366 Median :0.00575 Median : 0.2480 Mean :1366 Mean :0.01666 Mean : 0.2568 3rd Qu.:1366 3rd Qu.:0.01260 3rd Qu.: 0.4073 Max. :1367 Max. :0.14940 Max. : 0.7390  ML Workflow Then, split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.\nclimate_train \u0026lt;- subset(climate, Year \u0026lt;= 2006) climate_test \u0026lt;- subset(climate, Year \u0026gt; 2006) str(climate_train) \u0026#39;data.frame\u0026#39;: 284 obs. of 11 variables: $ Year : int 1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ... $ Month : int 5 6 7 8 9 10 11 12 1 2 ... $ MEI : num 2.556 2.167 1.741 1.13 0.428 ... $ CO2 : num 346 346 344 342 340 ... $ CH4 : num 1639 1634 1633 1631 1648 ... $ N2O : num 304 304 304 304 304 ... $ CFC.11 : num 191 192 193 194 194 ... $ CFC.12 : num 350 352 354 356 357 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ... $ Temp : num 0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ... summary(climate_train)  Year Month MEI CO2 Min. :1983 Min. : 1.000 Min. :-1.5860 Min. :340.2 1st Qu.:1989 1st Qu.: 4.000 1st Qu.:-0.3230 1st Qu.:352.3 Median :1995 Median : 7.000 Median : 0.3085 Median :359.9 Mean :1995 Mean : 6.556 Mean : 0.3419 Mean :361.4 3rd Qu.:2001 3rd Qu.:10.000 3rd Qu.: 0.8980 3rd Qu.:370.6 Max. :2006 Max. :12.000 Max. : 3.0010 Max. :385.0 CH4 N2O CFC.11 CFC.12 Min. :1630 Min. :303.7 Min. :191.3 Min. :350.1 1st Qu.:1716 1st Qu.:307.7 1st Qu.:249.6 1st Qu.:462.5 Median :1759 Median :310.8 Median :260.4 Median :522.1 Mean :1746 Mean :311.7 Mean :252.5 Mean :494.2 3rd Qu.:1782 3rd Qu.:316.1 3rd Qu.:267.4 3rd Qu.:541.0 Max. :1808 Max. :320.5 Max. :271.5 Max. :543.8 TSI Aerosols Temp Min. :1365 Min. :0.00160 Min. :-0.2820 1st Qu.:1366 1st Qu.:0.00270 1st Qu.: 0.1180 Median :1366 Median :0.00620 Median : 0.2325 Mean :1366 Mean :0.01772 Mean : 0.2478 3rd Qu.:1366 3rd Qu.:0.01400 3rd Qu.: 0.4065 Max. :1367 Max. :0.14940 Max. : 0.7390  str(climate_test) \u0026#39;data.frame\u0026#39;: 24 obs. of 11 variables: $ Year : int 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... $ Month : int 1 2 3 4 5 6 7 8 9 10 ... $ MEI : num 0.974 0.51 0.074 -0.049 0.183 ... $ CO2 : num 383 384 385 386 387 ... $ CH4 : num 1800 1803 1803 1802 1796 ... $ N2O : num 321 321 321 321 320 ... $ CFC.11 : num 248 248 248 248 247 ... $ CFC.12 : num 539 539 539 539 538 ... $ TSI : num 1366 1366 1366 1366 1366 ... $ Aerosols: num 0.0054 0.0051 0.0045 0.0045 0.0041 0.004 0.004 0.0041 0.0042 0.0041 ... $ Temp : num 0.601 0.498 0.435 0.466 0.372 0.382 0.394 0.358 0.402 0.362 ... summary(climate_test)  Year Month MEI CO2 Min. :2007 Min. : 1.00 Min. :-1.6350 Min. :380.9 1st Qu.:2007 1st Qu.: 3.75 1st Qu.:-1.0437 1st Qu.:383.1 Median :2008 Median : 6.50 Median :-0.5305 Median :384.5 Mean :2008 Mean : 6.50 Mean :-0.5098 Mean :384.7 3rd Qu.:2008 3rd Qu.: 9.25 3rd Qu.:-0.0360 3rd Qu.:386.1 Max. :2008 Max. :12.00 Max. : 0.9740 Max. :388.5 CH4 N2O CFC.11 CFC.12 Min. :1772 Min. :320.3 Min. :244.1 Min. :534.9 1st Qu.:1792 1st Qu.:320.6 1st Qu.:244.6 1st Qu.:535.1 Median :1798 Median :321.3 Median :246.2 Median :537.0 Mean :1797 Mean :321.1 Mean :245.9 Mean :536.7 3rd Qu.:1804 3rd Qu.:321.4 3rd Qu.:246.6 3rd Qu.:537.4 Max. :1814 Max. :322.2 Max. :248.4 Max. :539.2 TSI Aerosols Temp Min. :1366 Min. :0.003100 Min. :0.074 1st Qu.:1366 1st Qu.:0.003600 1st Qu.:0.307 Median :1366 Median :0.004100 Median :0.380 Mean :1366 Mean :0.004071 Mean :0.363 3rd Qu.:1366 3rd Qu.:0.004500 3rd Qu.:0.414 Max. :1366 Max. :0.005400 Max. :0.601  Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.\nfit.climate \u0026lt;- lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) summary(fit.climate)  Call: lm(formula = Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.25888 -0.05913 -0.00082 0.05649 0.32433 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.246e+02 1.989e+01 -6.265 1.43e-09 *** MEI 6.421e-02 6.470e-03 9.923 \u0026lt; 2e-16 *** CO2 6.457e-03 2.285e-03 2.826 0.00505 ** CH4 1.240e-04 5.158e-04 0.240 0.81015 N2O -1.653e-02 8.565e-03 -1.930 0.05467 . CFC.11 -6.631e-03 1.626e-03 -4.078 5.96e-05 *** CFC.12 3.808e-03 1.014e-03 3.757 0.00021 *** TSI 9.314e-02 1.475e-02 6.313 1.10e-09 *** Aerosols -1.538e+00 2.133e-01 -7.210 5.41e-12 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09171 on 275 degrees of freedom Multiple R-squared: 0.7509, Adjusted R-squared: 0.7436 F-statistic: 103.6 on 8 and 275 DF, p-value: \u0026lt; 2.2e-16 The model R2 (the “Multiple R-squared” value) is 0.7509\n Creating Our First Model Which variables are significant in the model? We will consider a variable signficant only if the p-value is below 0.05. #### MEI, CO2, CFC.11, CFC.12, TSI, Aerosols\n Understanding the Model Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are negative, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.\nWhich of the following is the simplest correct explanation for this contradiction? #### All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the dataset.\nCompute the correlations between all the variables in the training set.\ncor(climate_train)  Year Month MEI CO2 CH4 Year 1.00000000 -0.0279419602 -0.0369876842 0.98274939 0.91565945 Month -0.02794196 1.0000000000 0.0008846905 -0.10673246 0.01856866 MEI -0.03698768 0.0008846905 1.0000000000 -0.04114717 -0.03341930 CO2 0.98274939 -0.1067324607 -0.0411471651 1.00000000 0.87727963 CH4 0.91565945 0.0185686624 -0.0334193014 0.87727963 1.00000000 N2O 0.99384523 0.0136315303 -0.0508197755 0.97671982 0.89983864 CFC.11 0.56910643 -0.0131112236 0.0690004387 0.51405975 0.77990402 CFC.12 0.89701166 0.0006751102 0.0082855443 0.85268963 0.96361625 TSI 0.17030201 -0.0346061935 -0.1544919227 0.17742893 0.24552844 Aerosols -0.34524670 0.0148895406 0.3402377871 -0.35615480 -0.26780919 Temp 0.78679714 -0.0998567411 0.1724707512 0.78852921 0.70325502 N2O CFC.11 CFC.12 TSI Aerosols Year 0.99384523 0.56910643 0.8970116635 0.17030201 -0.34524670 Month 0.01363153 -0.01311122 0.0006751102 -0.03460619 0.01488954 MEI -0.05081978 0.06900044 0.0082855443 -0.15449192 0.34023779 CO2 0.97671982 0.51405975 0.8526896272 0.17742893 -0.35615480 CH4 0.89983864 0.77990402 0.9636162478 0.24552844 -0.26780919 N2O 1.00000000 0.52247732 0.8679307757 0.19975668 -0.33705457 CFC.11 0.52247732 1.00000000 0.8689851828 0.27204596 -0.04392120 CFC.12 0.86793078 0.86898518 1.0000000000 0.25530281 -0.22513124 TSI 0.19975668 0.27204596 0.2553028138 1.00000000 0.05211651 Aerosols -0.33705457 -0.04392120 -0.2251312440 0.05211651 1.00000000 Temp 0.77863893 0.40771029 0.6875575483 0.24338269 -0.38491375 Temp Year 0.78679714 Month -0.09985674 MEI 0.17247075 CO2 0.78852921 CH4 0.70325502 N2O 0.77863893 CFC.11 0.40771029 CFC.12 0.68755755 TSI 0.24338269 Aerosols -0.38491375 Temp 1.00000000 The following independent variables is N2O, highly correlated with (absolute correlation greater than 0.7)? #### CO2, CH4, CFC.12\nThe following independent variables is CFC.11, highly correlated with? #### CH4, CFC.12\n Simplifying the Model Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Note, using the training set to build the model.\nfit.climate.2 \u0026lt;- lm(Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train) summary(fit.climate.2)  Call: lm(formula = Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.27916 -0.05975 -0.00595 0.05672 0.34195 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.162e+02 2.022e+01 -5.747 2.37e-08 *** MEI 6.419e-02 6.652e-03 9.649 \u0026lt; 2e-16 *** N2O 2.532e-02 1.311e-03 19.307 \u0026lt; 2e-16 *** TSI 7.949e-02 1.487e-02 5.344 1.89e-07 *** Aerosols -1.702e+00 2.180e-01 -7.806 1.19e-13 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09547 on 279 degrees of freedom Multiple R-squared: 0.7261, Adjusted R-squared: 0.7222 F-statistic: 184.9 on 4 and 279 DF, p-value: \u0026lt; 2.2e-16 The coefficient of N2O in this reduced model is 2.532e-02\n(How does this compare to the coefficient in the previous model with all of the variables?) The model R2 is 0.7261\n Automatically Building the Model We have many variables in this analysis, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2.\nThis trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.\nThe step function has one argument - the name of the initial model. It returns a simplified model. Using the step function in R to derive a new model, with the full model as the initial model (HINT: If your initial full model was called “climateLM”, you could create a new model with the step function by typing step(climateLM). Be sure to save your new model to a variable name so that you can look at the summary. For more information about the step function, type? step in your R console.)\nfit.climate.step \u0026lt;- step(fit.climate) Start: AIC=-1348.16 Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols Df Sum of Sq RSS AIC - CH4 1 0.00049 2.3135 -1350.1 \u0026lt;none\u0026gt; 2.3130 -1348.2 - N2O 1 0.03132 2.3443 -1346.3 - CO2 1 0.06719 2.3802 -1342.0 - CFC.12 1 0.11874 2.4318 -1335.9 - CFC.11 1 0.13986 2.4529 -1333.5 - TSI 1 0.33516 2.6482 -1311.7 - Aerosols 1 0.43727 2.7503 -1301.0 - MEI 1 0.82823 3.1412 -1263.2 Step: AIC=-1350.1 Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols Df Sum of Sq RSS AIC \u0026lt;none\u0026gt; 2.3135 -1350.1 - N2O 1 0.03133 2.3448 -1348.3 - CO2 1 0.06672 2.3802 -1344.0 - CFC.12 1 0.13023 2.4437 -1336.5 - CFC.11 1 0.13938 2.4529 -1335.5 - TSI 1 0.33500 2.6485 -1313.7 - Aerosols 1 0.43987 2.7534 -1302.7 - MEI 1 0.83118 3.1447 -1264.9 summary(fit.climate.step)  Call: lm(formula = Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = climate_train) Residuals: Min 1Q Median 3Q Max -0.25770 -0.05994 -0.00104 0.05588 0.32203 Coefficients: Estimate Std. Error t value Pr(\u0026gt;|t|) (Intercept) -1.245e+02 1.985e+01 -6.273 1.37e-09 *** MEI 6.407e-02 6.434e-03 9.958 \u0026lt; 2e-16 *** CO2 6.402e-03 2.269e-03 2.821 0.005129 ** N2O -1.602e-02 8.287e-03 -1.933 0.054234 . CFC.11 -6.609e-03 1.621e-03 -4.078 5.95e-05 *** CFC.12 3.868e-03 9.812e-04 3.942 0.000103 *** TSI 9.312e-02 1.473e-02 6.322 1.04e-09 *** Aerosols -1.540e+00 2.126e-01 -7.244 4.36e-12 *** --- Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Residual standard error: 0.09155 on 276 degrees of freedom Multiple R-squared: 0.7508, Adjusted R-squared: 0.7445 F-statistic: 118.8 on 7 and 276 DF, p-value: \u0026lt; 2.2e-16 R2 value of the model produced by the step function is 0.7508\nWhich of the following variable(s) were eliminated from the full model by the step function? #### It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).\n Testing on Unseen Data We have developed an understanding of how well we can fit a linear regression to the training data, but does the model quality hold when applied to unseen data?\nUsing the model produced from the step function, calculate temperature predictions for the testing dataset, using the predict function.\nTempPredictions \u0026lt;- predict(fit.climate.step, newdata = climate_test) climate.SSE = sum((TempPredictions - climate_test$Temp)^2) climate.SST = sum((climate_test$Temp - mean(climate_train$Temp))^2) 1 - climate.SSE/climate.SST [1] 0.6286051 Testing set R2 is 0.6286\n ","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"8a0aae4ba257ba1a8eaa06545f983374","permalink":"/project/climate_change/climate_change/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/project/climate_change/climate_change/","section":"project","summary":"Study the relationship between average global temperature \u0026 several other factors","tags":["R","Data Analytics"],"title":"Climate Change","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]