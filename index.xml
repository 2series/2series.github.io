<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rihad Variawa on Rihad Variawa</title>
    <link>/</link>
    <description>Recent content in Rihad Variawa on Rihad Variawa</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Earnings Prediction From Census Data</title>
      <link>/project/earnings/earnings/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/earnings/earnings/</guid>
      <description>


&lt;p&gt;The United States government periodically collects demographic information by conducting a census.&lt;/p&gt;
&lt;p&gt;In this analysis, we are going to use census information about an individual to predict how much a person earns – in particular, whether the person earns more than $50,000 per year. This data comes from the UCI Machine Learning Repository.&lt;/p&gt;
&lt;p&gt;The file census.csv contains 1994 census data for 31,978 individuals in the United States.&lt;/p&gt;
&lt;p&gt;The dataset includes the following 13 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;age = the age of the individual in years&lt;/li&gt;
&lt;li&gt;workclass = the classification of the individual’s working status (does the person work for the federal government, work for the local government, work without pay, and so on)
education = the level of education of the individual (e.g., 5th-6th grade, high school graduate, PhD, so on)&lt;/li&gt;
&lt;li&gt;maritalstatus = the marital status of the individual&lt;/li&gt;
&lt;li&gt;occupation = the type of work the individual does (e.g., administrative/clerical work, farming/fishing, sales and so on)&lt;/li&gt;
&lt;li&gt;relationship = relationship of individual to his/her household&lt;/li&gt;
&lt;li&gt;race = the individual’s race&lt;/li&gt;
&lt;li&gt;sex = the individual’s sex&lt;/li&gt;
&lt;li&gt;capitalgain = the capital gains of the individual in 1994 (from selling an asset such as a stock or bond for more than the original purchase price)&lt;/li&gt;
&lt;li&gt;capitalloss = the capital losses of the individual in 1994 (from selling an asset such as a stock or bond for less than the original purchase price)&lt;/li&gt;
&lt;li&gt;hoursperweek = the number of hours the individual works per week&lt;/li&gt;
&lt;li&gt;nativecountry = the native country of the individual&lt;/li&gt;
&lt;li&gt;over50k = whether or not the individual earned more than $50,000 in 1994&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;Let’s begin by building a logistic regression model to predict whether an individual’s earnings are above $50,000 (the variable “over50k”) using all of the other variables as independent variables.&lt;/p&gt;
&lt;p&gt;First, read the dataset census.csv into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census &amp;lt;- read.csv(&amp;quot;census.csv&amp;quot;)
str(census)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    31978 obs. of  13 variables:
##  $ age          : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ workclass    : Factor w/ 9 levels &amp;quot; ?&amp;quot;,&amp;quot; Federal-gov&amp;quot;,..: 8 7 5 5 5 5 5 7 5 5 ...
##  $ education    : Factor w/ 16 levels &amp;quot; 10th&amp;quot;,&amp;quot; 11th&amp;quot;,..: 10 10 12 2 10 13 7 12 13 10 ...
##  $ maritalstatus: Factor w/ 7 levels &amp;quot; Divorced&amp;quot;,&amp;quot; Married-AF-spouse&amp;quot;,..: 5 3 1 3 3 3 4 3 5 3 ...
##  $ occupation   : Factor w/ 15 levels &amp;quot; ?&amp;quot;,&amp;quot; Adm-clerical&amp;quot;,..: 2 5 7 7 11 5 9 5 11 5 ...
##  $ relationship : Factor w/ 6 levels &amp;quot; Husband&amp;quot;,&amp;quot; Not-in-family&amp;quot;,..: 2 1 2 1 6 6 2 1 2 1 ...
##  $ race         : Factor w/ 5 levels &amp;quot; Amer-Indian-Eskimo&amp;quot;,..: 5 5 5 3 3 5 3 5 5 5 ...
##  $ sex          : Factor w/ 2 levels &amp;quot; Female&amp;quot;,&amp;quot; Male&amp;quot;: 2 2 2 2 1 1 1 2 1 2 ...
##  $ capitalgain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ capitalloss  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ hoursperweek : int  40 13 40 40 40 40 16 45 50 40 ...
##  $ nativecountry: Factor w/ 41 levels &amp;quot; Cambodia&amp;quot;,&amp;quot; Canada&amp;quot;,..: 39 39 39 39 5 39 23 39 39 39 ...
##  $ over50k      : Factor w/ 2 levels &amp;quot; &amp;lt;=50K&amp;quot;,&amp;quot; &amp;gt;50K&amp;quot;: 1 1 1 1 1 1 1 2 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(census)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       age                    workclass             education    
##  Min.   :17.00    Private         :22286    HS-grad     :10368  
##  1st Qu.:28.00    Self-emp-not-inc: 2499    Some-college: 7187  
##  Median :37.00    Local-gov       : 2067    Bachelors   : 5210  
##  Mean   :38.58    ?               : 1809    Masters     : 1674  
##  3rd Qu.:48.00    State-gov       : 1279    Assoc-voc   : 1366  
##  Max.   :90.00    Self-emp-inc    : 1074    11th        : 1167  
##                  (Other)          :  964   (Other)      : 5006  
##                 maritalstatus              occupation  
##   Divorced             : 4394    Prof-specialty :4038  
##   Married-AF-spouse    :   23    Craft-repair   :4030  
##   Married-civ-spouse   :14692    Exec-managerial:3992  
##   Married-spouse-absent:  397    Adm-clerical   :3721  
##   Never-married        :10488    Sales          :3584  
##   Separated            : 1005    Other-service  :3212  
##   Widowed              :  979   (Other)         :9401  
##           relationship                    race            sex       
##   Husband       :12947    Amer-Indian-Eskimo:  311    Female:10608  
##   Not-in-family : 8156    Asian-Pac-Islander:  956    Male  :21370  
##   Other-relative:  952    Black             : 3028                  
##   Own-child     : 5005    Other             :  253                  
##   Unmarried     : 3384    White             :27430                  
##   Wife          : 1534                                              
##                                                                     
##   capitalgain     capitalloss       hoursperweek          nativecountry  
##  Min.   :    0   Min.   :   0.00   Min.   : 1.00    United-States:29170  
##  1st Qu.:    0   1st Qu.:   0.00   1st Qu.:40.00    Mexico       :  643  
##  Median :    0   Median :   0.00   Median :40.00    Philippines  :  198  
##  Mean   : 1064   Mean   :  86.74   Mean   :40.42    Germany      :  137  
##  3rd Qu.:    0   3rd Qu.:   0.00   3rd Qu.:45.00    Canada       :  121  
##  Max.   :99999   Max.   :4356.00   Max.   :99.00    Puerto-Rico  :  114  
##                                                    (Other)       : 1595  
##    over50k     
##   &amp;lt;=50K:24283  
##   &amp;gt;50K : 7695  
##                
##                
##                
##                
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, split the data randomly into a training set and a testing set, setting the seed to 2000 before creating the split. Split the data so that the training set contains 60% of the observations, while the testing set contains 40% of the observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
set.seed(2000)
censusSplit = sample.split(census$over50k, SplitRatio = 0.6)
censusTrain = subset(census, censusSplit == TRUE)
censusTest = subset(census, censusSplit == FALSE)
nrow(censusTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19187&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12791&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, build a logistic regression model to predict the dependent variable “over50k”, using all of the other variables in the dataset as independent variables. Use the training set to build the model.&lt;/p&gt;
&lt;p&gt;Which variables are significant, or have factors that are significant? (Use 0.1 as your significance threshold, so variables with a period or dot in the stars column should be counted too. You might see a warning message here - you can ignore it and proceed. This message is a warning that we might be overfitting our model to the training set.) Select all that apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CensusLog &amp;lt;- glm(over50k ~ ., data = censusTrain, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(CensusLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = over50k ~ ., family = binomial, data = censusTrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.1065  -0.5037  -0.1804  -0.0008   3.3383  
## 
## Coefficients: (1 not defined because of singularities)
##                                            Estimate Std. Error z value
## (Intercept)                              -8.658e+00  1.379e+00  -6.279
## age                                       2.548e-02  2.139e-03  11.916
## workclass Federal-gov                     1.105e+00  2.014e-01   5.489
## workclass Local-gov                       3.675e-01  1.821e-01   2.018
## workclass Never-worked                   -1.283e+01  8.453e+02  -0.015
## workclass Private                         6.012e-01  1.626e-01   3.698
## workclass Self-emp-inc                    7.575e-01  1.950e-01   3.884
## workclass Self-emp-not-inc                1.855e-01  1.774e-01   1.046
## workclass State-gov                       4.012e-01  1.961e-01   2.046
## workclass Without-pay                    -1.395e+01  6.597e+02  -0.021
## education 11th                            2.225e-01  2.867e-01   0.776
## education 12th                            6.380e-01  3.597e-01   1.774
## education 1st-4th                        -7.075e-01  7.760e-01  -0.912
## education 5th-6th                        -3.170e-01  4.880e-01  -0.650
## education 7th-8th                        -3.498e-01  3.126e-01  -1.119
## education 9th                            -1.258e-01  3.539e-01  -0.355
## education Assoc-acdm                      1.602e+00  2.427e-01   6.601
## education Assoc-voc                       1.541e+00  2.368e-01   6.506
## education Bachelors                       2.177e+00  2.218e-01   9.817
## education Doctorate                       2.761e+00  2.893e-01   9.544
## education HS-grad                         1.006e+00  2.169e-01   4.638
## education Masters                         2.421e+00  2.353e-01  10.289
## education Preschool                      -2.237e+01  6.864e+02  -0.033
## education Prof-school                     2.938e+00  2.753e-01  10.672
## education Some-college                    1.365e+00  2.195e-01   6.219
## maritalstatus Married-AF-spouse           2.540e+00  7.145e-01   3.555
## maritalstatus Married-civ-spouse          2.458e+00  3.573e-01   6.880
## maritalstatus Married-spouse-absent      -9.486e-02  3.204e-01  -0.296
## maritalstatus Never-married              -4.515e-01  1.139e-01  -3.962
## maritalstatus Separated                   3.609e-02  1.984e-01   0.182
## maritalstatus Widowed                     1.858e-01  1.962e-01   0.947
## occupation Adm-clerical                   9.470e-02  1.288e-01   0.735
## occupation Armed-Forces                  -1.008e+00  1.487e+00  -0.677
## occupation Craft-repair                   2.174e-01  1.109e-01   1.960
## occupation Exec-managerial                9.400e-01  1.138e-01   8.257
## occupation Farming-fishing               -1.068e+00  1.908e-01  -5.599
## occupation Handlers-cleaners             -6.237e-01  1.946e-01  -3.204
## occupation Machine-op-inspct             -1.862e-01  1.376e-01  -1.353
## occupation Other-service                 -8.183e-01  1.641e-01  -4.987
## occupation Priv-house-serv               -1.297e+01  2.267e+02  -0.057
## occupation Prof-specialty                 6.331e-01  1.222e-01   5.180
## occupation Protective-serv                6.267e-01  1.710e-01   3.664
## occupation Sales                          3.276e-01  1.175e-01   2.789
## occupation Tech-support                   6.173e-01  1.533e-01   4.028
## occupation Transport-moving                      NA         NA      NA
## relationship Not-in-family                7.881e-01  3.530e-01   2.233
## relationship Other-relative              -2.194e-01  3.137e-01  -0.699
## relationship Own-child                   -7.489e-01  3.507e-01  -2.136
## relationship Unmarried                    7.041e-01  3.720e-01   1.893
## relationship Wife                         1.324e+00  1.331e-01   9.942
## race Asian-Pac-Islander                   4.830e-01  3.548e-01   1.361
## race Black                                3.644e-01  2.882e-01   1.265
## race Other                                2.204e-01  4.513e-01   0.488
## race White                                4.108e-01  2.737e-01   1.501
## sex Male                                  7.729e-01  1.024e-01   7.545
## capitalgain                               3.280e-04  1.372e-05  23.904
## capitalloss                               6.445e-04  4.854e-05  13.277
## hoursperweek                              2.897e-02  2.101e-03  13.791
## nativecountry Canada                      2.593e-01  1.308e+00   0.198
## nativecountry China                      -9.695e-01  1.327e+00  -0.730
## nativecountry Columbia                   -1.954e+00  1.526e+00  -1.280
## nativecountry Cuba                        5.735e-02  1.323e+00   0.043
## nativecountry Dominican-Republic         -1.435e+01  3.092e+02  -0.046
## nativecountry Ecuador                    -3.550e-02  1.477e+00  -0.024
## nativecountry El-Salvador                -6.095e-01  1.395e+00  -0.437
## nativecountry England                    -6.707e-02  1.327e+00  -0.051
## nativecountry France                      5.301e-01  1.419e+00   0.374
## nativecountry Germany                     5.474e-02  1.306e+00   0.042
## nativecountry Greece                     -2.646e+00  1.714e+00  -1.544
## nativecountry Guatemala                  -1.293e+01  3.345e+02  -0.039
## nativecountry Haiti                      -9.221e-01  1.615e+00  -0.571
## nativecountry Holand-Netherlands         -1.282e+01  2.400e+03  -0.005
## nativecountry Honduras                   -9.584e-01  3.412e+00  -0.281
## nativecountry Hong                       -2.362e-01  1.492e+00  -0.158
## nativecountry Hungary                     1.412e-01  1.555e+00   0.091
## nativecountry India                      -8.218e-01  1.314e+00  -0.625
## nativecountry Iran                       -3.299e-02  1.366e+00  -0.024
## nativecountry Ireland                     1.579e-01  1.473e+00   0.107
## nativecountry Italy                       6.100e-01  1.333e+00   0.458
## nativecountry Jamaica                    -2.279e-01  1.387e+00  -0.164
## nativecountry Japan                       5.072e-01  1.375e+00   0.369
## nativecountry Laos                       -6.831e-01  1.661e+00  -0.411
## nativecountry Mexico                     -9.182e-01  1.303e+00  -0.705
## nativecountry Nicaragua                  -1.987e-01  1.507e+00  -0.132
## nativecountry Outlying-US(Guam-USVI-etc) -1.373e+01  8.502e+02  -0.016
## nativecountry Peru                       -9.660e-01  1.678e+00  -0.576
## nativecountry Philippines                 4.393e-02  1.281e+00   0.034
## nativecountry Poland                      2.410e-01  1.383e+00   0.174
## nativecountry Portugal                    7.276e-01  1.477e+00   0.493
## nativecountry Puerto-Rico                -5.769e-01  1.357e+00  -0.425
## nativecountry Scotland                   -1.188e+00  1.719e+00  -0.691
## nativecountry South                      -8.183e-01  1.341e+00  -0.610
## nativecountry Taiwan                     -2.590e-01  1.350e+00  -0.192
## nativecountry Thailand                   -1.693e+00  1.737e+00  -0.975
## nativecountry Trinadad&amp;amp;Tobago            -1.346e+00  1.721e+00  -0.782
## nativecountry United-States              -8.594e-02  1.269e+00  -0.068
## nativecountry Vietnam                    -1.008e+00  1.523e+00  -0.662
## nativecountry Yugoslavia                  1.402e+00  1.648e+00   0.851
##                                          Pr(&amp;gt;|z|)    
## (Intercept)                              3.41e-10 ***
## age                                       &amp;lt; 2e-16 ***
## workclass Federal-gov                    4.03e-08 ***
## workclass Local-gov                      0.043641 *  
## workclass Never-worked                   0.987885    
## workclass Private                        0.000218 ***
## workclass Self-emp-inc                   0.000103 ***
## workclass Self-emp-not-inc               0.295646    
## workclass State-gov                      0.040728 *  
## workclass Without-pay                    0.983134    
## education 11th                           0.437738    
## education 12th                           0.076064 .  
## education 1st-4th                        0.361897    
## education 5th-6th                        0.516008    
## education 7th-8th                        0.263152    
## education 9th                            0.722228    
## education Assoc-acdm                     4.10e-11 ***
## education Assoc-voc                      7.74e-11 ***
## education Bachelors                       &amp;lt; 2e-16 ***
## education Doctorate                       &amp;lt; 2e-16 ***
## education HS-grad                        3.52e-06 ***
## education Masters                         &amp;lt; 2e-16 ***
## education Preschool                      0.973996    
## education Prof-school                     &amp;lt; 2e-16 ***
## education Some-college                   5.00e-10 ***
## maritalstatus Married-AF-spouse          0.000378 ***
## maritalstatus Married-civ-spouse         6.00e-12 ***
## maritalstatus Married-spouse-absent      0.767155    
## maritalstatus Never-married              7.42e-05 ***
## maritalstatus Separated                  0.855672    
## maritalstatus Widowed                    0.343449    
## occupation Adm-clerical                  0.462064    
## occupation Armed-Forces                  0.498170    
## occupation Craft-repair                  0.049972 *  
## occupation Exec-managerial                &amp;lt; 2e-16 ***
## occupation Farming-fishing               2.15e-08 ***
## occupation Handlers-cleaners             0.001353 ** 
## occupation Machine-op-inspct             0.176061    
## occupation Other-service                 6.14e-07 ***
## occupation Priv-house-serv               0.954385    
## occupation Prof-specialty                2.22e-07 ***
## occupation Protective-serv               0.000248 ***
## occupation Sales                         0.005282 ** 
## occupation Tech-support                  5.63e-05 ***
## occupation Transport-moving                    NA    
## relationship Not-in-family               0.025562 *  
## relationship Other-relative              0.484263    
## relationship Own-child                   0.032716 *  
## relationship Unmarried                   0.058392 .  
## relationship Wife                         &amp;lt; 2e-16 ***
## race Asian-Pac-Islander                  0.173504    
## race Black                               0.206001    
## race Other                               0.625263    
## race White                               0.133356    
## sex Male                                 4.52e-14 ***
## capitalgain                               &amp;lt; 2e-16 ***
## capitalloss                               &amp;lt; 2e-16 ***
## hoursperweek                              &amp;lt; 2e-16 ***
## nativecountry Canada                     0.842879    
## nativecountry China                      0.465157    
## nativecountry Columbia                   0.200470    
## nativecountry Cuba                       0.965432    
## nativecountry Dominican-Republic         0.962972    
## nativecountry Ecuador                    0.980829    
## nativecountry El-Salvador                0.662181    
## nativecountry England                    0.959686    
## nativecountry France                     0.708642    
## nativecountry Germany                    0.966572    
## nativecountry Greece                     0.122527    
## nativecountry Guatemala                  0.969180    
## nativecountry Haiti                      0.568105    
## nativecountry Holand-Netherlands         0.995736    
## nativecountry Honduras                   0.778775    
## nativecountry Hong                       0.874155    
## nativecountry Hungary                    0.927653    
## nativecountry India                      0.531661    
## nativecountry Iran                       0.980736    
## nativecountry Ireland                    0.914628    
## nativecountry Italy                      0.647194    
## nativecountry Jamaica                    0.869467    
## nativecountry Japan                      0.712179    
## nativecountry Laos                       0.680866    
## nativecountry Mexico                     0.481103    
## nativecountry Nicaragua                  0.895132    
## nativecountry Outlying-US(Guam-USVI-etc) 0.987115    
## nativecountry Peru                       0.564797    
## nativecountry Philippines                0.972640    
## nativecountry Poland                     0.861624    
## nativecountry Portugal                   0.622327    
## nativecountry Puerto-Rico                0.670837    
## nativecountry Scotland                   0.489616    
## nativecountry South                      0.541809    
## nativecountry Taiwan                     0.847878    
## nativecountry Thailand                   0.329678    
## nativecountry Trinadad&amp;amp;Tobago            0.434105    
## nativecountry United-States              0.946020    
## nativecountry Vietnam                    0.507799    
## nativecountry Yugoslavia                 0.394874    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 21175  on 19186  degrees of freedom
## Residual deviance: 12104  on 19090  degrees of freedom
## AIC: 12298
## 
## Number of Fisher Scoring iterations: 15&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;age-workclass-education-maritalstatus-occupation-relationship-sex&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;age, workclass, education, maritalstatus, occupation, relationship, sex,&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;capitalgain-capitalloss-housperweek&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;capitalgain, capitalloss, housperweek&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You might see a warning message when you make predictions on the test set - you can safely ignore it.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictLog &amp;lt;- predict(CensusLog, newdata = censusTest, type = &amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in predict.lm(object, newdata, se.fit, scale = 1, type =
## ifelse(type == : prediction from a rank-deficient fit may be misleading&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k, predictLog &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         
##          FALSE TRUE
##    &amp;lt;=50K  9051  662
##    &amp;gt;50K   1190 1888&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9051 + 1888) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8552107&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---a-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - A Logistic Regression Model&lt;/h3&gt;
&lt;p&gt;What is the baseline accuracy for the testing set? table(censusTest$over50k)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  &amp;lt;=50K   &amp;gt;50K 
##   9713   3078&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;9713 / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7593621&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;problem-1.4---a-logistic-regression-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Problem 1.4 - A Logistic Regression Model&lt;/h4&gt;
&lt;p&gt;What is the area-under-the-curve (AUC) for this model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpredLog = prediction(predictLog, censusTest$over50k)
as.numeric(performance(ROCRpredLog, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9061598&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - A CART Model&lt;/h3&gt;
&lt;p&gt;We have just seen how the logistic regression model for this data achieves a high accuracy. Moreover, the significances of the variables give us a way to gauge which variables are relevant for this prediction task. However, it is not immediately clear which variables are more important than the others, especially due to the large number of factor variables in this problem.&lt;/p&gt;
&lt;p&gt;Let us now build a classification tree to predict “over50k”. Using the training set to build the model, and all of the other variables as independent variables. Using the default parameters, so don’t set a value for minbucket or cp. Remember to specify method=“class” as an argument to rpart, since this is a classification problem. After you are done building the model, plot the resulting tree.&lt;/p&gt;
&lt;p&gt;How many splits does the tree have in total?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
censusTree &amp;lt;- rpart(over50k ~ ., data = censusTrain, method=&amp;quot;class&amp;quot;)
prp(censusTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;4&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - A CART Model&lt;/h3&gt;
&lt;p&gt;Which variable does the tree split on at the first level (the very first split of the tree)?&lt;/p&gt;
&lt;div id=&#34;relationship&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;relationship&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - A CART Model&lt;/h3&gt;
&lt;p&gt;Which variables does the tree split on at the second level (immediately after the first split of the tree)? Select all that apply.&lt;/p&gt;
&lt;div id=&#34;capitalgain-education&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;capitalgain, education&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - A CART Model&lt;/h3&gt;
&lt;p&gt;What is the accuracy of the model on the testing set? Use a threshold of 0.5. (You can either add the argument type=“class”, or generate probabilities and use a threshold of 0.5 like in logistic regression.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictTree &amp;lt;- 
    as.vector(predict(censusTree, newdata = censusTest, type = &amp;quot;class&amp;quot;))
head(predictTree)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; &amp;gt;50K&amp;quot;  &amp;quot; &amp;gt;50K&amp;quot;  &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;lt;=50K&amp;quot; &amp;quot; &amp;gt;50K&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(censusTest$over50k, predictTree)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         predictTree
##           &amp;lt;=50K  &amp;gt;50K
##    &amp;lt;=50K   9243   470
##    &amp;gt;50K    1482  1596&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9243 + 1596) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8473927&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This highlights a very regular phenomenon when comparing CART and logistic regression. CART often performs a little worse than logistic regression in out-of-sample accuracy. However, as is the case here, the CART model is often much simpler to describe and understand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - A CART Model&lt;/h3&gt;
&lt;p&gt;Let us now consider the ROC curve and AUC for the CART model on the test-set. We will need to get predicted probabilities for the observations in the test-set to build the ROC curve and compute the AUC. Remember that you can do this by removing the type=“class” argument when making predictions, and taking the second column of the resulting object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictTreeProb &amp;lt;- predict(censusTree, newdata = censusTest)
head(predictTreeProb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        &amp;lt;=50K       &amp;gt;50K
## 2  0.2794982 0.72050176
## 5  0.2794982 0.72050176
## 7  0.9490143 0.05098572
## 8  0.6972807 0.30271934
## 11 0.6972807 0.30271934
## 12 0.2794982 0.72050176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predictTreeProb[, 2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          2          5          7          8         11         12 
## 0.72050176 0.72050176 0.05098572 0.30271934 0.30271934 0.72050176&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the ROC curve for the CART model you have estimated. Observe that compared to the logistic regression ROC curve, the CART ROC curve is less smooth than the logistic regression ROC curve.&lt;/p&gt;
&lt;p&gt;Which of the following explanations for this behavior is most correct? (HINT: Think about what the ROC curve is plotting and what changing the threshold does.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpredTree = prediction(predictTreeProb[, 2], censusTest$over50k)
ROCRperfTree = performance(ROCRpredTree, &amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)
plot(ROCRperfTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-probabilities-from-the-cart-model-take-only-a-handful-of-values-five-one-for-each-end-bucketleaf-of-the-tree-the-changes-in-the-roc-curve-correspond-to-setting-the-threshold-to-one-of-those-values.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The probabilities from the CART model take only a handful of values (five, one for each end bucket/leaf of the tree); the changes in the ROC curve correspond to setting the threshold to one of those values.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.6---a-cart-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.6 - A CART Model&lt;/h3&gt;
&lt;p&gt;What is the AUC of the CART model on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(performance(ROCRpredTree, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8470256&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;Before building a random forest model, we’ll down-sample our training set. While some modern personal computers can build a random forest model on the entire training set, others might run out of memory when trying to train the model since random forests is much more computationally intensive than CART or Logistic Regression.&lt;/p&gt;
&lt;p&gt;For this reason, before continuing I’ll define a new training set to be used when building our random forest model, that contains 2000 randomly selected obervations from the original training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
trainSmall &amp;lt;- censusTrain[sample(nrow(censusTrain), 2000), ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us now build a random forest model to predict “over50k”, using the dataset “trainSmall” as the data used to build the model. Set the seed to 1 again right before building the model, and use all of the other variables in the dataset as independent variables. (If you get an error that random forest “cannot handle categorical predictors with more than 32 categories”, re-build the model without the nativecountry variable as one of the independent variables.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
CensusForest &amp;lt;- randomForest(over50k ~ ., data = trainSmall)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, make predictions using this model on the entire test-set.&lt;/p&gt;
&lt;p&gt;What is the accuracy of the model on the test-set, using a threshold of 0.5? (Remember that you don’t need a “type” argument when making predictions with a random forest model if you want to use a threshold of 0.5. Also, note that your accuracy might be different from the one reported here, since random forest models can still differ depending on your operating system, even when the random seed is set.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictForest &amp;lt;- predict(CensusForest, newdata = censusTest)
table(censusTest$over50k, predictForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         predictForest
##           &amp;lt;=50K  &amp;gt;50K
##    &amp;lt;=50K   8843   870
##    &amp;gt;50K    1029  2049&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9586 + 1093) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8348839&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;As we discussed, random forest models work by building a large collection of trees. As a result, we lose some of the interpretability that comes with CART in terms of seeing how predictions are made and which variables are important. However, we can still compute metrics that give us insight into which variables are important. One metric that we can look at is the number of times, aggregated over all of the trees in the random forest model, that a certain variable is selected for a split. To view this metric, run the following lines of code (replace “MODEL” with the name of your random forest model):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vu &amp;lt;- varUsed(CensusForest, count=TRUE)
vusorted &amp;lt;- sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(CensusForest$forest$xlevels[vusorted$ix]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This code produces a chart that for each variable measures the number of times that variable was selected for splitting (the value on the x-axis).&lt;/p&gt;
&lt;p&gt;Which of the following variables is the most important in terms of the number of splits?&lt;/p&gt;
&lt;div id=&#34;age&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;age&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---a-random-forest-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - A Random Forest Model&lt;/h3&gt;
&lt;p&gt;A different metric we can look at is related to “impurity”, which measures how homogenous each bucket or leaf of the tree is. In each tree in the forest, whenever we select a variable and perform a split, the impurity is decreased. Therefore, one way to measure the importance of a variable is to average the reduction in impurity, taken over all the times that variable is selected for splitting in all of the trees in the forest.&lt;/p&gt;
&lt;p&gt;To compute this metric, run the following code (replace “MODEL” with the name of your random forest model):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;varImpPlot(CensusForest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which one of the following variables is the most important in terms of mean reduction in impurity?&lt;/p&gt;
&lt;div id=&#34;occupation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;occupation&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;We now conclude our analysis of this dataset by looking at how CART behaves with different choices of its parameters. Let us select the cp parameter for our CART model using k-fold cross validation, with k = 10 folds. Do this by using the train function. Set the seed beforehand to 2. Test cp values from 0.002 to 0.1 in 0.002 increments, by using the following code:&lt;/p&gt;
&lt;p&gt;Also, remember we using the entire training set “train” when building this model. The train function might take some time to run.&lt;/p&gt;
&lt;p&gt;Which value of cp does the train function recommend?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;ggplot2&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:randomForest&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)
# number of folds
tr.control = trainControl(method = &amp;quot;cv&amp;quot;, number = 10)

# cp values
cartGrid &amp;lt;- expand.grid( .cp = seq(0.002,0.1,0.002))
cartGrid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      .cp
## 1  0.002
## 2  0.004
## 3  0.006
## 4  0.008
## 5  0.010
## 6  0.012
## 7  0.014
## 8  0.016
## 9  0.018
## 10 0.020
## 11 0.022
## 12 0.024
## 13 0.026
## 14 0.028
## 15 0.030
## 16 0.032
## 17 0.034
## 18 0.036
## 19 0.038
## 20 0.040
## 21 0.042
## 22 0.044
## 23 0.046
## 24 0.048
## 25 0.050
## 26 0.052
## 27 0.054
## 28 0.056
## 29 0.058
## 30 0.060
## 31 0.062
## 32 0.064
## 33 0.066
## 34 0.068
## 35 0.070
## 36 0.072
## 37 0.074
## 38 0.076
## 39 0.078
## 40 0.080
## 41 0.082
## 42 0.084
## 43 0.086
## 44 0.088
## 45 0.090
## 46 0.092
## 47 0.094
## 48 0.096
## 49 0.098
## 50 0.100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cross-validation
set.seed(2)
tr = train(over50k ~ ., data = censusTrain, method = &amp;quot;rpart&amp;quot;, 
           trControl = tr.control, tuneGrid = cartGrid)
tr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CART 
## 
## 19187 samples
##    12 predictor
##     2 classes: &amp;#39; &amp;lt;=50K&amp;#39;, &amp;#39; &amp;gt;50K&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 17268, 17268, 17269, 17269, 17269, 17268, ... 
## Resampling results across tuning parameters:
## 
##   cp     Accuracy   Kappa     
##   0.002  0.8510972  0.55404931
##   0.004  0.8482829  0.55537475
##   0.006  0.8452078  0.53914084
##   0.008  0.8442176  0.53817486
##   0.010  0.8433317  0.53305978
##   0.012  0.8433317  0.53305978
##   0.014  0.8433317  0.53305978
##   0.016  0.8413510  0.52349296
##   0.018  0.8400480  0.51528594
##   0.020  0.8381193  0.50351272
##   0.022  0.8381193  0.50351272
##   0.024  0.8381193  0.50351272
##   0.026  0.8381193  0.50351272
##   0.028  0.8381193  0.50351272
##   0.030  0.8381193  0.50351272
##   0.032  0.8381193  0.50351272
##   0.034  0.8352011  0.48749911
##   0.036  0.8326470  0.47340390
##   0.038  0.8267570  0.44688035
##   0.040  0.8248289  0.43893150
##   0.042  0.8248289  0.43893150
##   0.044  0.8248289  0.43893150
##   0.046  0.8248289  0.43893150
##   0.048  0.8248289  0.43893150
##   0.050  0.8231084  0.42467058
##   0.052  0.8174798  0.37478096
##   0.054  0.8138837  0.33679015
##   0.056  0.8118514  0.30751485
##   0.058  0.8118514  0.30751485
##   0.060  0.8118514  0.30751485
##   0.062  0.8118514  0.30751485
##   0.064  0.8118514  0.30751485
##   0.066  0.8099233  0.29697206
##   0.068  0.7971025  0.22226318
##   0.070  0.7958512  0.21465656
##   0.072  0.7958512  0.21465656
##   0.074  0.7958512  0.21465656
##   0.076  0.7689601  0.05701508
##   0.078  0.7593684  0.00000000
##   0.080  0.7593684  0.00000000
##   0.082  0.7593684  0.00000000
##   0.084  0.7593684  0.00000000
##   0.086  0.7593684  0.00000000
##   0.088  0.7593684  0.00000000
##   0.090  0.7593684  0.00000000
##   0.092  0.7593684  0.00000000
##   0.094  0.7593684  0.00000000
##   0.096  0.7593684  0.00000000
##   0.098  0.7593684  0.00000000
##   0.100  0.7593684  0.00000000
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.002.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;Fit a CART model to the training data using this value of cp. What is the prediction accuracy on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;censusTree2 &amp;lt;- 
    rpart(over50k ~ ., data = censusTrain, method=&amp;quot;class&amp;quot;, cp = 0.002)
predictTree2 &amp;lt;- 
    as.vector(predict(censusTree2, newdata = censusTest, type = &amp;quot;class&amp;quot;))
table(censusTest$over50k, predictTree2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         predictTree2
##           &amp;lt;=50K  &amp;gt;50K
##    &amp;lt;=50K   9178   535
##    &amp;gt;50K    1240  1838&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(9178 + 1838) / nrow(censusTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8612306&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---selecting-cp-by-cross-validation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Selecting cp by Cross-Validation&lt;/h3&gt;
&lt;p&gt;Compared to the original accuracy using the default value of cp, this new CART model is an improvement, and so we should clearly favor this new model over the old one – or should we?&lt;/p&gt;
&lt;p&gt;Plot the CART tree for this model. How many splits are there?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(censusTree2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/earnings/earnings_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;18&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This highlights one important tradeoff in building predictive models. By tuning cp, we improved our accuracy by over 1%, but our tree became significantly more complicated. In some applications, such an improvement in accuracy would be worth the loss in interpretability. In others, we may prefer a less accurate model that is simpler to understand and describe over a more accurate – but more complicated – model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Why People Vote?</title>
      <link>/project/understanding_votes/votes/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/understanding_votes/votes/</guid>
      <description>


&lt;p&gt;In August 2006 three researchers (Alan Gerber and Donald Green of Yale University, and Christopher Larimer of the University of Northern Iowa) carried out a large scale field experiment in Michigan, USA to test the hypothesis that one of the reasons people vote is &lt;strong&gt;social, or extrinsic, pressure.&lt;/strong&gt; To quote the first paragraph of their 2008 research paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“Among the most striking features of a democratic political system is the participation of millions of voters in elections. Why do large numbers of people vote, despite the fact that …”the casting of a single vote is of no significance where there is a multitude of electors“? One hypothesis is adherence to social norms. Voting is widely regarded as a citizen duty, and citizens worry that others will think less of them if they fail to participate in elections. Voters’ sense of civic duty has long been a leading explanation of voters turnout…”&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;in-this-analysis-ill-use-both-logistic-regression-and-classification-trees-to-analyze-the-data-they-collected.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In this analysis, I’ll use both logistic regression and classification trees to analyze the data they collected.&lt;/h2&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data&lt;/h3&gt;
&lt;p&gt;The researchers grouped about 344,000 voters into different groups randomly - about 191,000 voters were a “control” group, and the rest were categorized into one of four “treatment” groups. These five groups correspond to five binary variables in the dataset.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Civic Duty” (variable civicduty) group members were sent a letter that simply said “DO YOUR CIVIC DUTY - VOTE!”&lt;/li&gt;
&lt;li&gt;“Hawthorne Effect” (variable hawthorne) group members were sent a letter that had the “Civic Duty” message plus the additional message “YOU ARE BEING STUDIED” and they were informed that their voting behavior would be examined by means of public records.&lt;/li&gt;
&lt;li&gt;“Self” (variable self) group members received the “Civic Duty” message as well as the recent voting record of everyone in that household and a message stating that another message would be sent after the election with updated records.&lt;/li&gt;
&lt;li&gt;“Neighbors” (variable neighbors) group members were given the same message as that for the “Self” group, except the message not only had the household voting records but, also that of neighbors - maximizing social pressure.&lt;/li&gt;
&lt;li&gt;“Control” (variable control) group members were not sent anything, and represented the typical voting situation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additional variables include sex (0 for male, 1 for female), yob (year of birth), and the dependent variable voting (1 if they voted, 0 otherwise).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;We will first get familiar with the data.&lt;/p&gt;
&lt;p&gt;What proportion of people in this dataset voted in this election?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gerber &amp;lt;- read.csv(&amp;quot;gerber.csv&amp;quot;)
str(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    344084 obs. of  8 variables:
##  $ sex      : int  0 1 1 1 0 1 0 0 1 0 ...
##  $ yob      : int  1941 1947 1982 1950 1951 1959 1956 1981 1968 1967 ...
##  $ voting   : int  0 0 1 1 1 1 1 0 0 0 ...
##  $ hawthorne: int  0 0 1 1 1 0 0 0 0 0 ...
##  $ civicduty: int  1 1 0 0 0 0 0 0 0 0 ...
##  $ neighbors: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ self     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ control  : int  0 0 0 0 0 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      0      1 
## 235388 108696&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;108696 / (235388 + 108696)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3158996&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Which of the four “treatment groups” had the largest percentage of people who actually voted (voting = 1)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# howthorne
table(gerber$voting, gerber$hawthorne)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##          0      1
##   0 209500  25888
##   1  96380  12316&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12316 / (25888 + 12316)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3223746&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# civicduty
table(gerber$voting, gerber$civicduty)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##          0      1
##   0 209191  26197
##   1  96675  12021&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12021 / (26197 + 12021)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3145377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# neighbors
table(gerber$voting, gerber$neighbors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##          0      1
##   0 211625  23763
##   1  94258  14438&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;14438 / (23763 + 14438)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3779482&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# self
table(gerber$voting, gerber$self)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##          0      1
##   0 210361  25027
##   1  95505  13191&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;13191 / (25027 + 13191)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3451515&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;neighbors&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Neighbors&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Using all the data to build the model (NOT spliting the data into a training set and testing set).&lt;/p&gt;
&lt;p&gt;Which of the following coefficients are significant in the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VotingLog &amp;lt;- glm(voting ~ civicduty + hawthorne + self + neighbors, 
                 data = gerber, family = binomial)
summary(VotingLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = voting ~ civicduty + hawthorne + self + neighbors, 
##     family = binomial, data = gerber)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9744  -0.8691  -0.8389   1.4586   1.5590  
## 
## Coefficients:
##              Estimate Std. Error  z value Pr(&amp;gt;|z|)    
## (Intercept) -0.863358   0.005006 -172.459  &amp;lt; 2e-16 ***
## civicduty    0.084368   0.012100    6.972 3.12e-12 ***
## hawthorne    0.120477   0.012037   10.009  &amp;lt; 2e-16 ***
## self         0.222937   0.011867   18.786  &amp;lt; 2e-16 ***
## neighbors    0.365092   0.011679   31.260  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 429238  on 344083  degrees of freedom
## Residual deviance: 428090  on 344079  degrees of freedom
## AIC: 428100
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;all-coefficients-are-significant&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;All coefficients are significant&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Using a threshold of 0.3, what is the accuracy of the logistic regression model? (When making predictions, you don’t need to use the new data argument since we didn’t split our data.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictVoting &amp;lt;- predict(VotingLog, type = &amp;quot;response&amp;quot;)
table(gerber$voting, predictVoting &amp;gt; 0.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##      FALSE   TRUE
##   0 134513 100875
##   1  56730  51966&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(134513 + 51966) / nrow(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5419578&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.5---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.5 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Using a threshold of 0.5, what is the accuracy of the logistic regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting, predictVoting &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##      FALSE
##   0 235388
##   1 108696&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(235388) / nrow(gerber)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6841004&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(gerber$voting)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##      0      1 
## 235388 108696&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;235388 / (235388 + 108696)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6841004&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;equal-to-accuracy-of-threshold-of-0.5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.6841004 =&amp;gt; equal to accuracy of threshold of 0.5&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.6---exploration-and-logistic-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.6 - Exploration and Logistic Regression&lt;/h3&gt;
&lt;p&gt;Compare our previous two answers to the percentage of people who did not vote (the baseline accuracy) and computing the AUC of the model. What is happening here?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred = prediction(predictVoting, gerber$voting)
as.numeric(performance(ROCRpred, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5308461&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;even-though-all-of-the-variables-are-significant-this-is-a-weak-predictive-model.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Even though all of the variables are significant, this is a weak predictive model.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Trees&lt;/h3&gt;
&lt;p&gt;We will now try out trees! Building a CART tree for voting using all data and the same four treatment variables we used before. Don’t set the option method=“class” - we are actually going to create a &lt;strong&gt;regression tree&lt;/strong&gt; here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We are interested in building a tree to explore the fraction of people who vote, or the probability of voting.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different).&lt;/p&gt;
&lt;p&gt;However, with regression trees, CART will split even if both groups have probability less than 50%. Leave all the parameters at their default values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rpart)
library(rpart.plot)
CARTmodel &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors, data = gerber)
# plot the tree. What happens, and if relevant, why?
prp(CARTmodel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;no-variables-are-used-the-tree-is-only-a-root-node---none-of-the-variables-make-a-big-enough-effect-to-be-split-on.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No variables are used (the tree is only a root node) - none of the variables make a big enough effect to be split on.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Trees&lt;/h3&gt;
&lt;p&gt;Now build the tree:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTmodel2 &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors, 
          data=gerber, cp=0.0)
prp(CARTmodel2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What do we observe about the order of the splits?
#### Neighbor is the first split, civic duty is the last.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Trees&lt;/h3&gt;
&lt;p&gt;Using only the CART tree plot, we note that the fraction (a number between 0 and 1) of “Civic Duty” people voted amounted to:&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.31&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Trees&lt;/h3&gt;
&lt;p&gt;Building a new tree that includes the “sex” variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTmodel3 &amp;lt;- 
    rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, 
          data=gerber, cp=0.0)
prp(CARTmodel3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the control group, which gender is more likely to vote?
#### Men (0)&lt;/p&gt;
&lt;p&gt;In the “Civic Duty” group, which gender is more likely to vote?
#### Men (0)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;We know trees can handle “nonlinear” relationships, e.g. “in the ‘Civic Duty’ group and female”, but as we will see in the next few questions, it is possible to do the same for logistic regression.&lt;/p&gt;
&lt;p&gt;Firstly, let’s explore what trees can tell us. Let’s just focus on the “Control” treatment group. Creating a regression tree using just the “control” variable, then creating another tree with the “control” and “sex” variables, both with cp=0.0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CARTcontrol &amp;lt;- rpart(voting ~ control, data = gerber, cp = 0.0)
CARTcontrolAndSex &amp;lt;- rpart(voting ~ control + sex, data = gerber, cp = 0.0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the “control” only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group?
Using the absolute value function to get an answer, i.e. abs(Control Prediction - Non-Control Prediction). we add the argument “digits = 6” to the prp code to get a more accurate estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(CARTcontrol, digits = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.296638 - 0.34)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.043362&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0.043362&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Now, using the second tree (with control and sex), determine who is affected more by NOT being in the control group (being in any of the four treatment groups):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prp(CARTcontrolAndSex, digits = 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/understanding_votes/votes_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;they-are-affected-about-the-same-change-in-probability-within-0.001-of-each-other.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;They are affected about the same (change in probability within 0.001 of each other).&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Going back to logistic regression now, we build a model using “sex” and “control”. Interpreting the coefficient for “sex”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VotingControlAndSexLog &amp;lt;- 
    glm(voting ~ control + sex, data = gerber, family = binomial)
summary(VotingControlAndSexLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = voting ~ control + sex, family = binomial, data = gerber)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9220  -0.9012  -0.8290   1.4564   1.5717  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -0.635538   0.006511 -97.616  &amp;lt; 2e-16 ***
## control     -0.200142   0.007364 -27.179  &amp;lt; 2e-16 ***
## sex         -0.055791   0.007343  -7.597 3.02e-14 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 429238  on 344083  degrees of freedom
## Residual deviance: 428443  on 344081  degrees of freedom
## AIC: 428449
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;coefficient-is-negative-reflecting-that-women-are-less-likely-to-vote&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Coefficient is negative, reflecting that women are less likely to vote&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;The regression tree calculated the percentage voting exactly for every one of the four possibilities (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control).&lt;/p&gt;
&lt;p&gt;Logistic regression has attempted to do the same, although it wasn’t able to do as well because it can’t consider exactly the joint possibility of being a women and in the control group.&lt;/p&gt;
&lt;p&gt;We can quantify this precisely. By creating the following dataframe (this contains all of the possible values of sex and control), and evaluating our logistic regression using the predict function (where “LogModelSex” is the name of our logistic regression model that uses both control and sex):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Possibilities &amp;lt;- data.frame(sex=c(0,0,1,1), control=c(0,1,0,1))
predict(VotingControlAndSexLog, newdata=Possibilities, type=&amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1         2         3         4 
## 0.3462559 0.3024455 0.3337375 0.2908065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The four values in the results correspond to the four possibilities in the order they are stated above ( (Man, Not Control), (Man, Control), (Woman, Not Control), (Woman, Control) ).&lt;/p&gt;
&lt;p&gt;What is the absolute difference between the tree and the logistic regression for the (Woman, Control) case?&lt;/p&gt;
&lt;p&gt;The answer contains five numbers after the decimal point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.290456 - 0.2908065)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0003505&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(0.0003505, digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.00035&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;So the difference is not too big for this dataset, but it’s there. We’re going to add a new term to our logistic regression now, that is the combination of the “sex” and “control” variables - so if this new variable is 1, that means the person is a woman AND in the control group. We can do that with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LogModel2 &amp;lt;- glm(voting ~ sex + control + sex:control, 
                 data = gerber, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do you interpret the coefficient for the new variable in isolation? That is, how does it relate to the dependent variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LogModel2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = voting ~ sex + control + sex:control, family = &amp;quot;binomial&amp;quot;, 
##     data = gerber)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9213  -0.9019  -0.8284   1.4573   1.5724  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -0.637471   0.007603 -83.843  &amp;lt; 2e-16 ***
## sex         -0.051888   0.010801  -4.804 1.55e-06 ***
## control     -0.196553   0.010356 -18.980  &amp;lt; 2e-16 ***
## sex:control -0.007259   0.014729  -0.493    0.622    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 429238  on 344083  degrees of freedom
## Residual deviance: 428442  on 344080  degrees of freedom
## AIC: 428450
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;if-a-person-is-a-woman-and-in-the-control-group-the-chance-that-she-voted-goes-down.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;If a person is a woman and in the control group, the chance that she voted goes down.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.6---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.6 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;Run the same code as before to calculate the average for each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(LogModel2, newdata=Possibilities, type=&amp;quot;response&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1         2         3         4 
## 0.3458183 0.3027947 0.3341757 0.2904558&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now what is the difference between the logistic regression model and the CART model for the (Woman, Control) case?&lt;/p&gt;
&lt;p&gt;Again, our answer has five numbers after the decimal point.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abs(0.290456 - 0.2904558)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(2e-07, digits = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.7---interaction-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.7 - Interaction Terms&lt;/h3&gt;
&lt;p&gt;This example has shown that trees can capture nonlinear relationships that logistic regression cannot, but that we can get around this sometimes by using variables that are the combination of two variables.&lt;/p&gt;
&lt;p&gt;Should we always include all possible interaction terms of the independent variables when building a logistic regression model?&lt;/p&gt;
&lt;div id=&#34;no-because-of-overfitting&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No (because of overfitting)&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict Loan Repayments</title>
      <link>/project/loan_repayment/loan_repayments/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/loan_repayment/loan_repayments/</guid>
      <description>


&lt;p&gt;In the lending industry, investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, if the borrower is unable to repay the loan, then the lender loses money. Therefore, lenders face the problem of predicting the risk of a borrower being unable to repay a loan.&lt;/p&gt;
&lt;p&gt;To address this problem, we will use publicly available data from LendingClub.com, a website that connects borrowers and investors over the Internet. This dataset represents 9,578 3-year loans that were funded through the LendingClub.com platform between May 2007 and February 2010. The binary dependent variable not.fully.paid indicates that the loan was not paid back in full (the borrower either defaulted or the loan was “charged off,” meaning the borrower was deemed unlikely to ever pay it back).&lt;/p&gt;
&lt;p&gt;To predict this dependent variable, we will use the following independent variables available to the investor when deciding whether to fund a loan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.
purpose: The purpose of the loan (takes values “credit_card”, “debt_consolidation”, “educational”, “major_purchase”, “small_business”, and “all_other”).&lt;/li&gt;
&lt;li&gt;int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.
installment: The monthly installments ($) owed by the borrower if the loan is funded.&lt;/li&gt;
&lt;li&gt;log.annual.inc: The natural log of the self-reported annual income of the borrower.&lt;/li&gt;
&lt;li&gt;dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).&lt;/li&gt;
&lt;li&gt;fico: The FICO credit score of the borrower.
days.with.cr.line: The number of days the borrower has had a credit line.
revol.bal: The borrower’s revolving balance (amount unpaid at the end of the credit card billing cycle).&lt;/li&gt;
&lt;li&gt;revol.util: The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available).&lt;/li&gt;
&lt;li&gt;inq.last.6mths: The borrower’s number of inquiries by creditors in the last 6 months.&lt;/li&gt;
&lt;li&gt;delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.&lt;/li&gt;
&lt;li&gt;pub.rec: The borrower’s number of derogatory public records (bankruptcy filings, tax liens, or judgments).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Load the dataset loans.csv into a data frame called loans, and explore it using the str() and summary() functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loans &amp;lt;- read.csv(&amp;quot;loans.csv&amp;quot;)
str(loans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    9578 obs. of  14 variables:
##  $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 3 2 3 3 2 2 3 1 5 3 ...
##  $ int.rate         : num  0.119 0.107 0.136 0.101 0.143 ...
##  $ installment      : num  829 228 367 162 103 ...
##  $ log.annual.inc   : num  11.4 11.1 10.4 11.4 11.3 ...
##  $ dti              : num  19.5 14.3 11.6 8.1 15 ...
##  $ fico             : int  737 707 682 712 667 727 667 722 682 707 ...
##  $ days.with.cr.line: num  5640 2760 4710 2700 4066 ...
##  $ revol.bal        : int  28854 33623 3511 33667 4740 50807 3839 24220 69909 5630 ...
##  $ revol.util       : num  52.1 76.7 25.6 73.2 39.5 51 76.8 68.6 51.1 23 ...
##  $ inq.last.6mths   : int  0 0 1 1 0 0 0 0 1 1 ...
##  $ delinq.2yrs      : int  0 0 0 0 1 0 0 0 0 0 ...
##  $ pub.rec          : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ not.fully.paid   : int  0 0 0 0 0 0 1 1 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(loans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  credit.policy                 purpose        int.rate     
##  Min.   :0.000   all_other         :2331   Min.   :0.0600  
##  1st Qu.:1.000   credit_card       :1262   1st Qu.:0.1039  
##  Median :1.000   debt_consolidation:3957   Median :0.1221  
##  Mean   :0.805   educational       : 343   Mean   :0.1226  
##  3rd Qu.:1.000   home_improvement  : 629   3rd Qu.:0.1407  
##  Max.   :1.000   major_purchase    : 437   Max.   :0.2164  
##                  small_business    : 619                   
##   installment     log.annual.inc        dti              fico      
##  Min.   : 15.67   Min.   : 7.548   Min.   : 0.000   Min.   :612.0  
##  1st Qu.:163.77   1st Qu.:10.558   1st Qu.: 7.213   1st Qu.:682.0  
##  Median :268.95   Median :10.928   Median :12.665   Median :707.0  
##  Mean   :319.09   Mean   :10.932   Mean   :12.607   Mean   :710.8  
##  3rd Qu.:432.76   3rd Qu.:11.290   3rd Qu.:17.950   3rd Qu.:737.0  
##  Max.   :940.14   Max.   :14.528   Max.   :29.960   Max.   :827.0  
##                   NA&amp;#39;s   :4                                        
##  days.with.cr.line   revol.bal         revol.util     inq.last.6mths  
##  Min.   :  179     Min.   :      0   Min.   :  0.00   Min.   : 0.000  
##  1st Qu.: 2820     1st Qu.:   3187   1st Qu.: 22.70   1st Qu.: 0.000  
##  Median : 4140     Median :   8596   Median : 46.40   Median : 1.000  
##  Mean   : 4562     Mean   :  16914   Mean   : 46.87   Mean   : 1.572  
##  3rd Qu.: 5730     3rd Qu.:  18250   3rd Qu.: 71.00   3rd Qu.: 2.000  
##  Max.   :17640     Max.   :1207359   Max.   :119.00   Max.   :33.000  
##  NA&amp;#39;s   :29                          NA&amp;#39;s   :62       NA&amp;#39;s   :29      
##   delinq.2yrs         pub.rec       not.fully.paid  
##  Min.   : 0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.: 0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
##  Median : 0.0000   Median :0.0000   Median :0.0000  
##  Mean   : 0.1638   Mean   :0.0621   Mean   :0.1601  
##  3rd Qu.: 0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  
##  Max.   :13.0000   Max.   :5.0000   Max.   :1.0000  
##  NA&amp;#39;s   :29        NA&amp;#39;s   :29&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of the loans in the dataset were not paid in full? Please input a number between 0 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(loans$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    0    1 
## 8045 1533&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1533 / (8045 + 1533)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1600543&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Which of the following variables has at least one missing observation? Select all that apply.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;log.annual.inc&lt;/li&gt;
&lt;li&gt;days.with.cr.line&lt;/li&gt;
&lt;li&gt;revol.util&lt;/li&gt;
&lt;li&gt;inq.last.6mths&lt;/li&gt;
&lt;li&gt;delinq.2yrs&lt;/li&gt;
&lt;li&gt;pub.rec&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;Which of the following is the best reason to fill in the missing values for these variables instead of removing observations with missing data? (Hint: you can use the subset() function to build a dataframe with the observations missing at least one value. To test if a variable, for example pub.rec, is missing a value, use is.na(pub.rec).)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loansNA &amp;lt;- subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line)
                  | is.na(revol.util) | is.na(inq.last.6mths)
                  | is.na(delinq.2yrs) | is.na(pub.rec))
str(loansNA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    62 obs. of  14 variables:
##  $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 1 4 3 3 6 2 1 4 1 1 ...
##  $ int.rate         : num  0.113 0.11 0.113 0.123 0.106 ...
##  $ installment      : num  98.7 52.4 263.2 23.4 182.4 ...
##  $ log.annual.inc   : num  10.53 10.53 10.71 9.85 11.26 ...
##  $ dti              : num  7.72 15.84 8.75 12.38 4.26 ...
##  $ fico             : int  677 682 682 662 697 667 687 687 722 752 ...
##  $ days.with.cr.line: num  1680 1830 2490 1200 4141 ...
##  $ revol.bal        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ revol.util       : num  NA NA NA NA NA NA NA NA NA NA ...
##  $ inq.last.6mths   : int  1 0 1 1 0 0 1 0 1 0 ...
##  $ delinq.2yrs      : int  0 0 1 0 0 0 0 0 0 0 ...
##  $ pub.rec          : int  0 0 0 0 1 0 0 0 0 0 ...
##  $ not.fully.paid   : int  1 0 1 0 0 1 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(loansNA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  credit.policy                  purpose      int.rate     
##  Min.   :0.0000   all_other         :41   Min.   :0.0712  
##  1st Qu.:0.0000   credit_card       : 3   1st Qu.:0.0933  
##  Median :0.0000   debt_consolidation: 8   Median :0.1122  
##  Mean   :0.3871   educational       : 3   Mean   :0.1187  
##  3rd Qu.:1.0000   home_improvement  : 1   3rd Qu.:0.1456  
##  Max.   :1.0000   major_purchase    : 5   Max.   :0.1913  
##                   small_business    : 1                   
##   installment     log.annual.inc        dti              fico      
##  Min.   : 23.35   Min.   : 8.294   Min.   : 0.000   Min.   :642.0  
##  1st Qu.: 78.44   1st Qu.:10.096   1st Qu.: 5.147   1st Qu.:682.0  
##  Median :145.91   Median :10.639   Median :10.000   Median :707.0  
##  Mean   :159.19   Mean   :10.558   Mean   : 9.184   Mean   :711.5  
##  3rd Qu.:192.73   3rd Qu.:11.248   3rd Qu.:11.540   3rd Qu.:740.8  
##  Max.   :859.57   Max.   :13.004   Max.   :22.720   Max.   :802.0  
##                   NA&amp;#39;s   :4                                        
##  days.with.cr.line   revol.bal        revol.util  inq.last.6mths 
##  Min.   : 179      Min.   :     0   Min.   : NA   Min.   :0.000  
##  1st Qu.:1830      1st Qu.:     0   1st Qu.: NA   1st Qu.:0.000  
##  Median :2580      Median :     0   Median : NA   Median :1.000  
##  Mean   :3158      Mean   :  5476   Mean   :NaN   Mean   :1.182  
##  3rd Qu.:4621      3rd Qu.:     0   3rd Qu.: NA   3rd Qu.:2.000  
##  Max.   :7890      Max.   :290291   Max.   : NA   Max.   :6.000  
##  NA&amp;#39;s   :29                         NA&amp;#39;s   :62    NA&amp;#39;s   :29     
##   delinq.2yrs        pub.rec       not.fully.paid  
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  
##  Median :0.0000   Median :0.0000   Median :0.0000  
##  Mean   :0.2121   Mean   :0.0303   Mean   :0.1935  
##  3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.0000  
##  Max.   :4.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :29       NA&amp;#39;s   :29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(loansNA$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  0  1 
## 50 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;12 / (50 + 12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1935484&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;we-want-to-be-able-to-predict-risk-for-all-borrowers-instead-of-just-the-ones-with-all-data-reported.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;We want to be able to predict risk for all borrowers, instead of just the ones with all data reported.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---preparing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Preparing the Dataset&lt;/h3&gt;
&lt;p&gt;For the rest of this problem, we’ll be using a revised version of the dataset that has the missing values filled in with multiple imputation (which was discussed in the Recitation of this Unit). To ensure everybody has the same dataframe going forward, you can either run the commands below in your R console (if you haven’t already, run the command install.packages(“mice”) first), or you can download and load into R the dataset we created after running the imputation: loans_imputed.csv.&lt;/p&gt;
&lt;p&gt;IMPORTANT NOTE: On certain operating systems, the imputation results are not the same even if you set the random seed. If you decide to do the imputation yourself, please still read the provided imputed dataset (loans_imputed.csv) into R and compare your results, using the summary function. If the results are different, please make sure to use the data in loans_imputed.csv for the rest of the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;library(mice)&lt;/li&gt;
&lt;li&gt;set.seed(144)&lt;/li&gt;
&lt;li&gt;vars.for.imputation = setdiff(names(loans), “not.fully.paid”)&lt;/li&gt;
&lt;li&gt;imputed = complete(mice(loans[vars.for.imputation]))&lt;/li&gt;
&lt;li&gt;loans[vars.for.imputation] = imputed&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loans &amp;lt;- read.csv(&amp;quot;loans_imputed.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that to do this imputation, we set vars.for.imputation to all variables in the data frame except for not.fully.paid, to impute the values using all of the other independent variables. What best describes the process we just used to handle missing values?&lt;/p&gt;
&lt;div id=&#34;we-predicted-missing-variable-values-using-the-available-independent-variables-for-each-observation.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;We predicted missing variable values using the available independent variables for each observation.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Now that we have prepared the dataset, we need to split it into a training and testing set. To ensure everybody obtains the same split, set the random seed to 144 (even though you already did so earlier in the problem) and use the sample.split function to select the 70% of observations for the training set (the dependent variable for sample.split is not.fully.paid). Name the dataframes train and test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(144)
library(caTools)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train = subset(loans, split == TRUE)
test = subset(loans, split == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, use logistic regression trained on the training set to predict the dependent variable not.fully.paid using all the independent variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LoansLog &amp;lt;- glm(not.fully.paid ~ ., data = train, family = binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which independent variables are significant in our model? (Significant variables have at least one star, or a Pr(&amp;gt;|z|) value less than 0.05.) Select all that apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(LoansLog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = not.fully.paid ~ ., family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2049  -0.6205  -0.4951  -0.3606   2.6397  
## 
## Coefficients:
##                             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)                9.187e+00  1.554e+00   5.910 3.42e-09 ***
## credit.policy             -3.368e-01  1.011e-01  -3.332 0.000861 ***
## purposecredit_card        -6.141e-01  1.344e-01  -4.568 4.93e-06 ***
## purposedebt_consolidation -3.212e-01  9.183e-02  -3.498 0.000469 ***
## purposeeducational         1.347e-01  1.753e-01   0.768 0.442201    
## purposehome_improvement    1.727e-01  1.480e-01   1.167 0.243135    
## purposemajor_purchase     -4.830e-01  2.009e-01  -2.404 0.016203 *  
## purposesmall_business      4.120e-01  1.419e-01   2.905 0.003678 ** 
## int.rate                   6.110e-01  2.085e+00   0.293 0.769446    
## installment                1.275e-03  2.092e-04   6.093 1.11e-09 ***
## log.annual.inc            -4.337e-01  7.148e-02  -6.067 1.30e-09 ***
## dti                        4.638e-03  5.502e-03   0.843 0.399288    
## fico                      -9.317e-03  1.710e-03  -5.448 5.08e-08 ***
## days.with.cr.line          2.371e-06  1.588e-05   0.149 0.881343    
## revol.bal                  3.085e-06  1.168e-06   2.641 0.008273 ** 
## revol.util                 1.839e-03  1.535e-03   1.199 0.230722    
## inq.last.6mths             8.437e-02  1.600e-02   5.275 1.33e-07 ***
## delinq.2yrs               -8.320e-02  6.561e-02  -1.268 0.204762    
## pub.rec                    3.300e-01  1.139e-01   2.898 0.003756 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 5896.6  on 6704  degrees of freedom
## Residual deviance: 5485.2  on 6686  degrees of freedom
## AIC: 5523.2
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;credit.policy&lt;/li&gt;
&lt;li&gt;purposecredit_card&lt;/li&gt;
&lt;li&gt;purposedebt_consolidation&lt;/li&gt;
&lt;li&gt;purposemajor_purchase&lt;/li&gt;
&lt;li&gt;purposesmall_business&lt;/li&gt;
&lt;li&gt;installment&lt;/li&gt;
&lt;li&gt;log.annual.inc&lt;/li&gt;
&lt;li&gt;fico&lt;/li&gt;
&lt;li&gt;revol.bal&lt;/li&gt;
&lt;li&gt;inq.last.6mths&lt;/li&gt;
&lt;li&gt;pub.rec&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Consider two loan applications, which are identical other than the fact that the borrower in Application A has FICO credit score 700 while the borrower in Application B has FICO credit score 710. Let Logit(A) be the log odds of loan A not being paid back in full, according to our logistic regression model, and define Logit(B) similarly for loan B. What is the value of Logit(A) - Logit(B)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicationA &amp;lt;- train[1, ]
applicationB &amp;lt;- applicationA
applicationA$fico = 700
applicationB$fico = 710
applicationA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   credit.policy            purpose int.rate installment log.annual.inc
## 1             1 debt_consolidation   0.1189       829.1       11.35041
##     dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
## 1 19.48  700          5639.958     28854       52.1              0
##   delinq.2yrs pub.rec not.fully.paid
## 1           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   credit.policy            purpose int.rate installment log.annual.inc
## 1             1 debt_consolidation   0.1189       829.1       11.35041
##     dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
## 1 19.48  710          5639.958     28854       52.1              0
##   delinq.2yrs pub.rec not.fully.paid
## 1           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;applications &amp;lt;- rbind(applicationA, applicationB)
applications&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   credit.policy            purpose int.rate installment log.annual.inc
## 1             1 debt_consolidation   0.1189       829.1       11.35041
## 2             1 debt_consolidation   0.1189       829.1       11.35041
##     dti fico days.with.cr.line revol.bal revol.util inq.last.6mths
## 1 19.48  700          5639.958     28854       52.1              0
## 2 19.48  710          5639.958     28854       52.1              0
##   delinq.2yrs pub.rec not.fully.paid
## 1           0       0              0
## 2           0       0              0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredApplications &amp;lt;- predict(LoansLog, type = &amp;quot;response&amp;quot;, newdata = applications)
PredApplications&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1         2 
## 0.1828795 0.1693660&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredApplications[1] - PredApplications[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          1 
## 0.01351347&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationA &amp;lt;- 
    1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 700))) # A = 0.9349356&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationB &amp;lt;- 
    1 / (1 + exp(-1 * (9.187e+00 + -9.317e-03 * 710))) # B = 0.929033&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalcApplicationA - CalcApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.005902546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let O(A) be the odds of loan A not being paid back in full, according to our logistic regression model, and define O(B) similarly for loan B. What is the value of O(A)/O(B)? (HINT: Use the mathematical rule that exp(A + B + C) = exp(A)&lt;em&gt;exp(B)&lt;/em&gt;exp(C). Also, remember that exp() is the exponential function in R.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationA &amp;lt;- PredApplications[1] / (1 - PredApplications[1])
OddsApplicationA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1 
## 0.2238097&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationB &amp;lt;- PredApplications[2] / (1 - PredApplications[2])
OddsApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         2 
## 0.2038997&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OddsApplicationA / OddsApplicationB&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 1.097646&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;ok&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1.097646 =&amp;gt; OK!&lt;/h4&gt;
&lt;p&gt;After computing the logs, try log(odds) for previous problem&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1 
## -1.496959&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         2 
## -1.590127&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(OddsApplicationA) - log(OddsApplicationB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1 
## 0.0931679&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ok-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.0931679 =&amp;gt; OK!&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Predict the probability of the test set loans not being paid back in full (remember type=“response” for the predict function). Store these predicted probabilities in a variable named predicted.risk and add it to your test set (we will use this variable in later parts of the problem). Compute the confusion matrix using a threshold of 0.5.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted.risk &amp;lt;- predict(LoansLog, type = &amp;quot;response&amp;quot;, newdata = test)
str(predicted.risk)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Named num [1:2873] 0.0771 0.1728 0.1087 0.1016 0.0681 ...
##  - attr(*, &amp;quot;names&amp;quot;)= chr [1:2873] &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;10&amp;quot; &amp;quot;12&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2873 obs. of  14 variables:
##  $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 2 3 3 3 1 3 1 2 7 2 ...
##  $ int.rate         : num  0.107 0.136 0.122 0.132 0.08 ...
##  $ installment      : num  228.2 366.9 84.1 253.6 188 ...
##  $ log.annual.inc   : num  11.1 10.4 10.2 11.8 11.2 ...
##  $ dti              : num  14.29 11.63 10 9.16 16.08 ...
##  $ fico             : int  707 682 707 662 772 662 772 797 712 682 ...
##  $ days.with.cr.line: num  2760 4710 2730 4298 4889 ...
##  $ revol.bal        : int  33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ...
##  $ revol.util       : num  76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ...
##  $ inq.last.6mths   : int  0 1 1 2 1 0 0 0 0 3 ...
##  $ delinq.2yrs      : int  0 0 0 1 0 1 0 0 0 0 ...
##  $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ not.fully.paid   : int  0 0 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$predicted.risk &amp;lt;- predicted.risk
summary(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  credit.policy                  purpose        int.rate     
##  Min.   :0.0000   all_other         : 688   Min.   :0.0600  
##  1st Qu.:1.0000   credit_card       : 411   1st Qu.:0.1028  
##  Median :1.0000   debt_consolidation:1206   Median :0.1221  
##  Mean   :0.8047   educational       :  93   Mean   :0.1225  
##  3rd Qu.:1.0000   home_improvement  : 186   3rd Qu.:0.1393  
##  Max.   :1.0000   major_purchase    : 105   Max.   :0.2121  
##                   small_business    : 184                   
##   installment     log.annual.inc        dti             fico      
##  Min.   : 15.67   Min.   : 8.102   Min.   : 0.00   Min.   :612.0  
##  1st Qu.:163.57   1st Qu.:10.560   1st Qu.: 7.16   1st Qu.:682.0  
##  Median :267.74   Median :10.933   Median :12.85   Median :707.0  
##  Mean   :316.99   Mean   :10.928   Mean   :12.75   Mean   :710.8  
##  3rd Qu.:421.89   3rd Qu.:11.290   3rd Qu.:18.30   3rd Qu.:737.0  
##  Max.   :926.83   Max.   :13.459   Max.   :29.96   Max.   :822.0  
##                                                                   
##  days.with.cr.line   revol.bal         revol.util     inq.last.6mths  
##  Min.   :  179     Min.   :      0   Min.   :  0.00   Min.   : 0.000  
##  1st Qu.: 2795     1st Qu.:   3362   1st Qu.: 23.40   1st Qu.: 0.000  
##  Median : 4140     Median :   8712   Median : 46.90   Median : 1.000  
##  Mean   : 4494     Mean   :  17198   Mean   : 47.02   Mean   : 1.576  
##  3rd Qu.: 5670     3rd Qu.:  18728   3rd Qu.: 70.40   3rd Qu.: 2.000  
##  Max.   :17640     Max.   :1207359   Max.   :108.80   Max.   :24.000  
##                                                                       
##   delinq.2yrs         pub.rec        not.fully.paid   predicted.risk   
##  Min.   : 0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.02114  
##  1st Qu.: 0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.09461  
##  Median : 0.0000   Median :0.00000   Median :0.0000   Median :0.13697  
##  Mean   : 0.1605   Mean   :0.05743   Mean   :0.1601   Mean   :0.15785  
##  3rd Qu.: 0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.19658  
##  Max.   :13.0000   Max.   :3.00000   Max.   :1.0000   Max.   :0.95373  
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid, test$predicted.risk &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     FALSE TRUE
##   0  2400   13
##   1   457    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the logistic regression model? Input the accuracy as a number between 0 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(2400 + 3) / nrow(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8364079&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the accuracy of the baseline model? Input the accuracy as a number between 0 and 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    0    1 
## 2413  460&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2413 / (2413 + 460)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8398886&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---prediction-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Prediction Models&lt;/h3&gt;
&lt;p&gt;Use the ROCR package to compute the test set AUC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred = prediction(test$predicted.risk, test$not.fully.paid)
as.numeric(performance(ROCRpred, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6720995&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model has poor accuracy at the threshold 0.5. But despite the poor accuracy, we will see later how an investor can still leverage this logistic regression model to make profitable investments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;In the previous problem, we built a logistic regression model that has an AUC significantly higher than the AUC of 0.5 that would be obtained by randomly ordering observations. However, LendingClub.com assigns the interest rate to a loan based on their estimate of that loan’s risk. This variable, int.rate, is an independent variable in our dataset. In this part, we will investigate using the loan’s interest rate as a “smart baseline” to order the loans according to risk.&lt;/p&gt;
&lt;p&gt;Using the training set, build a bivariate logistic regression model (aka a logistic regression model with a single independent variable) that predicts the dependent variable not.fully.paid using only the variable int.rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;LoansLog2 &amp;lt;- glm(not.fully.paid ~ int.rate, data = train, family = binomial)
summary(LoansLog2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = not.fully.paid ~ int.rate, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0547  -0.6271  -0.5442  -0.4361   2.2914  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  -3.6726     0.1688  -21.76   &amp;lt;2e-16 ***
## int.rate     15.9214     1.2702   12.54   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 5896.6  on 6704  degrees of freedom
## Residual deviance: 5734.8  on 6703  degrees of freedom
## AIC: 5738.8
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable int.rate is highly significant in the bivariate model, but it is not significant at the 0.05 level in the model trained with all the independent variables. What is the most likely explanation for this difference?&lt;/p&gt;
&lt;div id=&#34;int.rate-is-correlated-with-other-risk-related-variables-and-therefore-does-not-incrementally-improve-the-model-when-those-other-variables-are-included.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;Make test set predictions for the bivariate model. What is the highest predicted probability of a loan not being paid in full on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted.risk2 &amp;lt;- predict(LoansLog2, type = &amp;quot;response&amp;quot;, newdata = test)
max(predicted.risk2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.426624&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0.426624&lt;/h4&gt;
&lt;p&gt;With a logistic regression cutoff of 0.5, how many loans would be predicted as not being paid in full on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(test$not.fully.paid, predicted.risk2 &amp;gt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     FALSE
##   0  2413
##   1   460&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2873 obs. of  15 variables:
##  $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 2 3 3 3 1 3 1 2 7 2 ...
##  $ int.rate         : num  0.107 0.136 0.122 0.132 0.08 ...
##  $ installment      : num  228.2 366.9 84.1 253.6 188 ...
##  $ log.annual.inc   : num  11.1 10.4 10.2 11.8 11.2 ...
##  $ dti              : num  14.29 11.63 10 9.16 16.08 ...
##  $ fico             : int  707 682 707 662 772 662 772 797 712 682 ...
##  $ days.with.cr.line: num  2760 4710 2730 4298 4889 ...
##  $ revol.bal        : int  33623 3511 5630 5122 29797 4175 3660 6844 3534 43039 ...
##  $ revol.util       : num  76.7 25.6 23 18.2 23.2 51.5 6.8 14.4 54.4 93.4 ...
##  $ inq.last.6mths   : int  0 1 1 2 1 0 0 0 0 3 ...
##  $ delinq.2yrs      : int  0 0 0 1 0 1 0 0 0 0 ...
##  $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ not.fully.paid   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ predicted.risk   : num  0.0771 0.1728 0.1087 0.1016 0.0681 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;section-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;0&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---a-smart-baseline&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - A “Smart Baseline”&lt;/h3&gt;
&lt;p&gt;What is the test set AUC of the bivariate model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ROCRpred2 = prediction(predicted.risk2, test$not.fully.paid)
as.numeric(performance(ROCRpred2, &amp;quot;auc&amp;quot;)@y.values)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6239081&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;While thus far we have predicted if a loan will be paid back or not, an investor needs to identify loans that are expected to be profitable. If the loan is paid back in full, then the investor makes interest on the loan. However, if the loan is not paid back, the investor loses the money invested. Therefore, the investor should seek loans that best balance this risk and reward.&lt;/p&gt;
&lt;p&gt;To compute interest revenue, consider a $c investment in a loan that has an annual interest rate r over a period of t years. Using continuous compounding of interest, this investment pays back c * exp(rt) dollars by the end of the t years, where exp(rt) is e raised to the r*t power. How much does a $10 investment with an annual interest rate of 6% pay back after 3 years, using continuous compounding of interest? Hint: remember to convert the percentage to a proportion before doing the math. Enter the number of dollars, without the $ sign.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;10 * exp(0.06 * 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11.97217&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;While the investment has value c * exp(rt) dollars after collecting interest, the investor had to pay $c for the investment. What is the profit to the investor if the investment is paid back in full?&lt;/p&gt;
&lt;div id=&#34;c-exprt---c&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;c * exp(rt) - c&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---computing-the-profitability-of-an-investment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Computing the Profitability of an Investment&lt;/h3&gt;
&lt;p&gt;Now, consider the case where the investor made a $c investment, but it was not paid back in full. Assume, conservatively, that no money was received from the borrower (often a lender will receive some but not all of the value of the loan, making this a pessimistic assumption of how much is received). What is the profit to the investor in this scenario?&lt;/p&gt;
&lt;div id=&#34;c&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;-c&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---a-simple-investment-strategy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - A Simple Investment Strategy&lt;/h3&gt;
&lt;p&gt;In the previous subproblem, we concluded that an investor who invested c dollars in a loan with interest rate r for t years makes c * (exp(rt) - 1) dollars of profit if the loan is paid back in full and -c dollars of profit if the loan is not paid back in full (pessimistically).&lt;/p&gt;
&lt;p&gt;In order to evaluate the quality of an investment strategy, we need to compute this profit for each loan in the test set. For this variable, we will assume a $1 investment (aka c=1). To create the variable, we first assign to the profit for a fully paid loan, exp(rt)-1, to every observation, and we then replace this value with -1 in the cases where the loan was not paid in full. All the loans in our dataset are 3-year loans, meaning t=3 in our calculations. Enter the following commands in your R console to create this new variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$profit = exp(test$int.rate*3) - 1
test$profit[test$not.fully.paid == 1] = -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the maximum profit of a $10 investment in any loan in the testing set (do not include the $ sign in your answer)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(test$profit) * 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.894769&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6.1---an-investment-strategy-based-on-risk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6.1 - An Investment Strategy Based on Risk&lt;/h3&gt;
&lt;p&gt;A simple investment strategy of equally investing in all the loans would yield profit $20.94 for a $100 investment. But this simple investment strategy does not leverage the prediction model we built earlier in this problem. As stated earlier, investors seek loans that balance reward with risk, in that they simultaneously have high interest rates and a low risk of not being paid back.&lt;/p&gt;
&lt;p&gt;To meet this objective, we will analyze an investment strategy in which the investor only purchases loans with a high interest rate (a rate of at least 15%), but amongst these loans selects the ones with the lowest predicted risk of not being paid back in full. We will model an investor who invests $1 in each of the most promising 100 loans.&lt;/p&gt;
&lt;p&gt;First, use the subset() function to build a data frame called highInterest consisting of the test set loans with an interest rate of at least 15%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;highInterest &amp;lt;- subset(test, int.rate &amp;gt;= 0.15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the average profit of a $1 investment in one of these high-interest loans (do not include the $ sign in your answer)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(highInterest$profit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2251015&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of the high-interest loans were not paid back in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(highInterest$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   0   1 
## 327 110&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;110 / (327 + 110)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2517162&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-6.2---an-investment-strategy-based-on-risk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 6.2 - An Investment Strategy Based on Risk&lt;/h3&gt;
&lt;p&gt;Next, we will determine the 100th smallest predicted probability of not paying in full by sorting the predicted risks in increasing order and selecting the 100th element of this sorted list. Find the highest predicted risk that we will include by typing the following command into your R console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutoff = sort(highInterest$predicted.risk, decreasing=FALSE)[100]
cutoff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1763305&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the subset() function to build a data frame called selectedLoans consisting of the high-interest loans with predicted risk not exceeding the cutoff we just computed. Check to make sure you have selected 100 loans for investment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;selectedLoans &amp;lt;- subset(highInterest, highInterest$predicted.risk &amp;lt;= cutoff)
summary(selectedLoans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  credit.policy                purpose      int.rate       installment    
##  Min.   :0.00   all_other         : 8   Min.   :0.1501   Min.   : 48.79  
##  1st Qu.:1.00   credit_card       :17   1st Qu.:0.1533   1st Qu.:176.16  
##  Median :1.00   debt_consolidation:60   Median :0.1570   Median :309.37  
##  Mean   :0.93   educational       : 1   Mean   :0.1610   Mean   :358.30  
##  3rd Qu.:1.00   home_improvement  : 1   3rd Qu.:0.1645   3rd Qu.:473.10  
##  Max.   :1.00   major_purchase    : 7   Max.   :0.2052   Max.   :907.60  
##                 small_business    : 6                                    
##  log.annual.inc        dti             fico       days.with.cr.line
##  Min.   : 9.575   Min.   : 0.00   Min.   :642.0   Min.   : 1140    
##  1st Qu.:10.776   1st Qu.: 6.05   1st Qu.:662.0   1st Qu.: 2162    
##  Median :11.127   Median :12.35   Median :672.0   Median : 3630    
##  Mean   :11.203   Mean   :12.19   Mean   :680.5   Mean   : 3911    
##  3rd Qu.:11.670   3rd Qu.:18.23   3rd Qu.:692.0   3rd Qu.: 5010    
##  Max.   :13.305   Max.   :28.15   Max.   :782.0   Max.   :13170    
##                                                                    
##    revol.bal        revol.util    inq.last.6mths   delinq.2yrs  
##  Min.   :     0   Min.   : 0.00   Min.   : 0.00   Min.   :0.00  
##  1st Qu.:  3768   1st Qu.:45.92   1st Qu.: 0.00   1st Qu.:0.00  
##  Median :  9691   Median :71.65   Median : 0.00   Median :0.00  
##  Mean   : 19923   Mean   :65.79   Mean   : 0.89   Mean   :0.33  
##  3rd Qu.: 24534   3rd Qu.:93.80   3rd Qu.: 1.00   3rd Qu.:0.00  
##  Max.   :168496   Max.   :99.70   Max.   :10.00   Max.   :4.00  
##                                                                 
##     pub.rec     not.fully.paid predicted.risk        profit       
##  Min.   :0.00   Min.   :0.00   Min.   :0.06871   Min.   :-1.0000  
##  1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.13596   1st Qu.: 0.5823  
##  Median :0.00   Median :0.00   Median :0.15327   Median : 0.5992  
##  Mean   :0.03   Mean   :0.19   Mean   :0.14794   Mean   : 0.3128  
##  3rd Qu.:0.00   3rd Qu.:0.00   3rd Qu.:0.16514   3rd Qu.: 0.6317  
##  Max.   :1.00   Max.   :1.00   Max.   :0.17633   Max.   : 0.8508  
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(selectedLoans)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    100 obs. of  16 variables:
##  $ credit.policy    : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ purpose          : Factor w/ 7 levels &amp;quot;all_other&amp;quot;,&amp;quot;credit_card&amp;quot;,..: 7 2 3 1 3 5 2 3 2 3 ...
##  $ int.rate         : num  0.15 0.153 0.158 0.159 0.156 ...
##  $ installment      : num  225 444 420 246 245 ...
##  $ log.annual.inc   : num  12.3 11 11.5 11.5 10.8 ...
##  $ dti              : num  6.45 19.52 18.55 24.19 2.72 ...
##  $ fico             : int  677 667 667 667 672 702 667 672 662 682 ...
##  $ days.with.cr.line: num  6240 2701 4560 5376 3010 ...
##  $ revol.bal        : int  56411 33074 34841 590 3273 4980 15977 16473 22783 87502 ...
##  $ revol.util       : num  75.3 68.8 89.6 84.3 69.6 55.3 83.6 94.1 93.7 96.4 ...
##  $ inq.last.6mths   : int  0 2 0 0 1 1 0 2 3 0 ...
##  $ delinq.2yrs      : int  0 0 0 0 0 0 0 2 1 1 ...
##  $ pub.rec          : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ not.fully.paid   : int  1 0 0 0 1 0 0 0 0 0 ...
##  $ predicted.risk   : num  0.164 0.169 0.158 0.162 0.147 ...
##  $ profit           : num  -1 0.584 0.604 0.61 -1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the profit of the investor, who invested $1 in each of these 100 loans (do not include the $ sign in your answer)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(selectedLoans$profit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.27825&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many of 100 selected loans were not paid back in full?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(selectedLoans$not.fully.paid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  0  1 
## 81 19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;19&lt;/p&gt;
&lt;p&gt;We have now seen how analytics can be used to select a subset of the high-interest loans that were paid back at only a slightly lower rate than average, resulting in a significant increase in the profit from our investor’s $100 investment. Although the logistic regression models developed in this problem did not have large AUC values, we see that they still provided the edge needed to improve the profitability of an investment portfolio.&lt;/p&gt;
&lt;p&gt;We conclude with a note of warning. Throughout this analysis we assume that the loans we invest in will perform in the same way as the loans we used to train our model, even though our training set covers a relatively short period of time. If there is an economic shock like a large financial downturn, default rates might be significantly higher than those observed in the training set and we might end up losing money instead of profiting. Investors must pay careful attention to such risk when making investment decisions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict Popular Songs</title>
      <link>/project/music/music/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/music/music/</guid>
      <description>


&lt;p&gt;Popularity of music records&lt;/p&gt;
&lt;p&gt;The music industry has a well-developed market with a global annual revenue around $15 billion. The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales.&lt;/p&gt;
&lt;p&gt;Artists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale. A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.&lt;/p&gt;
&lt;p&gt;Unfortunately, the success of an artist’s release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads, while another single may turn out quite unpopular, and therefore unprofitable.&lt;/p&gt;
&lt;p&gt;Knowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success.&lt;/p&gt;
&lt;p&gt;How can we use analytics to predict the popularity of a song? In this project, we challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.&lt;/p&gt;
&lt;p&gt;Taking an analytics approach, we aim to use information about a song’s properties to predict its popularity. The dataset songs.csv consists of all songs which made it to the Top 10 of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn’t make the Top 10. This data comes from three sources: Wikipedia, Billboard.com, and EchoNest.&lt;/p&gt;
&lt;p&gt;The variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes: time signature, loudness, key, pitch, tempo, and timbre.&lt;/p&gt;
&lt;p&gt;Here’s a detailed description of the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;year = the year the song was released&lt;/li&gt;
&lt;li&gt;songtitle = the title of the song&lt;/li&gt;
&lt;li&gt;artistname = the name of the artist of the song&lt;/li&gt;
&lt;li&gt;songID and artistID = identifying variables for the song and artist&lt;/li&gt;
&lt;li&gt;timesignature and timesignature_confidence = a variable estimating the time signature of the song, and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;loudness = a continuous variable indicating the average amplitude of the audio in decibels&lt;/li&gt;
&lt;li&gt;tempo and tempo_confidence = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;key and key_confidence = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate&lt;/li&gt;
&lt;li&gt;energy = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness&lt;/li&gt;
&lt;li&gt;pitch = a continuous variable that indicates the pitch of the song
timbre_0_min, timbre_0_max, timbre_1_min, timbre_1_max, . . . , timbre_11_min, and timbre_11_max = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables)&lt;/li&gt;
&lt;li&gt;Top10 = a binary variable indicating whether or not the song made it to the Top 10 of the Billboard Hot 100 Chart (1 if it was in the top 10, and 0 if it was not)&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Use the read.csv function to load the dataset “songs.csv” into R. How many observations (songs) are from the year 2010?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs &amp;lt;- read.csv(&amp;quot;songs.csv&amp;quot;)
str(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    7574 obs. of  39 variables:
##  $ year                    : int  2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ...
##  $ songtitle               : Factor w/ 7141 levels &amp;quot;̈́ l&amp;#39;or_e des bois&amp;quot;,..: 6204 5522 241 3115 48 608 255 4419 2886 6756 ...
##  $ artistname              : Factor w/ 1032 levels &amp;quot;50 Cent&amp;quot;,&amp;quot;98 Degrees&amp;quot;,..: 3 3 3 3 3 3 3 3 3 12 ...
##  $ songID                  : Factor w/ 7549 levels &amp;quot;SOAACNI1315CD4AC42&amp;quot;,..: 595 5439 5252 1716 3431 1020 1831 3964 6904 2473 ...
##  $ artistID                : Factor w/ 1047 levels &amp;quot;AR00B1I1187FB433EB&amp;quot;,..: 671 671 671 671 671 671 671 671 671 507 ...
##  $ timesignature           : int  3 4 4 4 4 4 4 4 4 4 ...
##  $ timesignature_confidence: num  0.853 1 1 1 0.788 1 0.968 0.861 0.622 0.938 ...
##  $ loudness                : num  -4.26 -4.05 -3.57 -3.81 -4.71 ...
##  $ tempo                   : num  91.5 140 160.5 97.5 140.1 ...
##  $ tempo_confidence        : num  0.953 0.921 0.489 0.794 0.286 0.347 0.273 0.83 0.018 0.929 ...
##  $ key                     : int  11 10 2 1 6 4 10 5 9 11 ...
##  $ key_confidence          : num  0.453 0.469 0.209 0.632 0.483 0.627 0.715 0.423 0.751 0.602 ...
##  $ energy                  : num  0.967 0.985 0.99 0.939 0.988 ...
##  $ pitch                   : num  0.024 0.025 0.026 0.013 0.063 0.038 0.026 0.033 0.027 0.004 ...
##  $ timbre_0_min            : num  0.002 0 0.003 0 0 ...
##  $ timbre_0_max            : num  57.3 57.4 57.4 57.8 56.9 ...
##  $ timbre_1_min            : num  -6.5 -37.4 -17.2 -32.1 -223.9 ...
##  $ timbre_1_max            : num  171 171 171 221 171 ...
##  $ timbre_2_min            : num  -81.7 -149.6 -72.9 -138.6 -147.2 ...
##  $ timbre_2_max            : num  95.1 180.3 157.9 173.4 166 ...
##  $ timbre_3_min            : num  -285 -380.1 -204 -73.5 -128.1 ...
##  $ timbre_3_max            : num  259 384 251 373 389 ...
##  $ timbre_4_min            : num  -40.4 -48.7 -66 -55.6 -43.9 ...
##  $ timbre_4_max            : num  73.6 100.4 152.1 119.2 99.3 ...
##  $ timbre_5_min            : num  -104.7 -87.3 -98.7 -77.5 -96.1 ...
##  $ timbre_5_max            : num  183.1 42.8 141.4 141.2 38.3 ...
##  $ timbre_6_min            : num  -88.8 -86.9 -88.9 -70.8 -110.8 ...
##  $ timbre_6_max            : num  73.5 75.5 66.5 64.5 72.4 ...
##  $ timbre_7_min            : num  -71.1 -65.8 -67.4 -63.7 -55.9 ...
##  $ timbre_7_max            : num  82.5 106.9 80.6 96.7 110.3 ...
##  $ timbre_8_min            : num  -52 -61.3 -59.8 -78.7 -56.5 ...
##  $ timbre_8_max            : num  39.1 35.4 46 41.1 37.6 ...
##  $ timbre_9_min            : num  -35.4 -81.9 -46.3 -49.2 -48.6 ...
##  $ timbre_9_max            : num  71.6 74.6 59.9 95.4 67.6 ...
##  $ timbre_10_min           : num  -126.4 -103.8 -108.3 -102.7 -52.8 ...
##  $ timbre_10_max           : num  18.7 121.9 33.3 46.4 22.9 ...
##  $ timbre_11_min           : num  -44.8 -38.9 -43.7 -59.4 -50.4 ...
##  $ timbre_11_max           : num  26 22.5 25.7 37.1 32.8 ...
##  $ Top10                   : int  0 0 0 0 0 0 0 0 0 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       year          songtitle              artistname  
##  Min.   :1990   Intro    :  15   Various artists: 162  
##  1st Qu.:1997   Forever  :   8   Anal Cunt      :  49  
##  Median :2002   Home     :   7   Various Artists:  44  
##  Mean   :2001   Goodbye  :   6   Tori Amos      :  41  
##  3rd Qu.:2006   Again    :   5   Eels           :  37  
##  Max.   :2010   Beautiful:   5   Napalm Death   :  37  
##                 (Other)  :7528   (Other)        :7204  
##                 songID                   artistID    timesignature  
##  SOALSZJ1370F1A7C75:   2   ARAGWS81187FB3F768: 222   Min.   :0.000  
##  SOANPAC13936E0B640:   2   ARL14X91187FB4CF14:  49   1st Qu.:4.000  
##  SOBDGMX12B0B80808E:   2   AR4KS8C1187FB4CF3D:  41   Median :4.000  
##  SOBUDCZ12A58A80013:   2   AR0JZZ01187B9B2C99:  37   Mean   :3.894  
##  SODFRLK13134387FB5:   2   ARZGTK71187B9AC7F5:  37   3rd Qu.:4.000  
##  SOEJPOK12A6D4FAFE4:   2   AR95XYH1187FB53951:  31   Max.   :7.000  
##  (Other)           :7562   (Other)           :7157                  
##  timesignature_confidence    loudness           tempo       
##  Min.   :0.0000           Min.   :-42.451   Min.   :  0.00  
##  1st Qu.:0.8193           1st Qu.:-10.847   1st Qu.: 88.86  
##  Median :0.9790           Median : -7.649   Median :103.27  
##  Mean   :0.8533           Mean   : -8.817   Mean   :107.35  
##  3rd Qu.:1.0000           3rd Qu.: -5.640   3rd Qu.:124.80  
##  Max.   :1.0000           Max.   :  1.305   Max.   :244.31  
##                                                             
##  tempo_confidence      key         key_confidence       energy       
##  Min.   :0.0000   Min.   : 0.000   Min.   :0.0000   Min.   :0.00002  
##  1st Qu.:0.3720   1st Qu.: 2.000   1st Qu.:0.2040   1st Qu.:0.50014  
##  Median :0.7015   Median : 6.000   Median :0.4515   Median :0.71816  
##  Mean   :0.6229   Mean   : 5.385   Mean   :0.4338   Mean   :0.67547  
##  3rd Qu.:0.8920   3rd Qu.: 9.000   3rd Qu.:0.6460   3rd Qu.:0.88740  
##  Max.   :1.0000   Max.   :11.000   Max.   :1.0000   Max.   :0.99849  
##                                                                      
##      pitch          timbre_0_min     timbre_0_max    timbre_1_min    
##  Min.   :0.00000   Min.   : 0.000   Min.   :12.58   Min.   :-333.72  
##  1st Qu.:0.00300   1st Qu.: 0.000   1st Qu.:53.12   1st Qu.:-160.12  
##  Median :0.00700   Median : 0.027   Median :55.53   Median :-107.75  
##  Mean   :0.01082   Mean   : 4.123   Mean   :54.46   Mean   :-110.79  
##  3rd Qu.:0.01400   3rd Qu.: 2.772   3rd Qu.:57.08   3rd Qu.: -59.71  
##  Max.   :0.54100   Max.   :48.353   Max.   :64.01   Max.   : 123.73  
##                                                                      
##   timbre_1_max     timbre_2_min      timbre_2_max      timbre_3_min    
##  Min.   :-74.37   Min.   :-324.86   Min.   : -0.832   Min.   :-495.36  
##  1st Qu.:171.13   1st Qu.:-167.64   1st Qu.:100.519   1st Qu.:-226.87  
##  Median :194.40   Median :-136.60   Median :129.908   Median :-170.61  
##  Mean   :212.34   Mean   :-136.89   Mean   :136.673   Mean   :-186.11  
##  3rd Qu.:239.24   3rd Qu.:-106.51   3rd Qu.:166.121   3rd Qu.:-131.56  
##  Max.   :549.97   Max.   :  34.57   Max.   :397.095   Max.   : -21.55  
##                                                                        
##   timbre_3_max     timbre_4_min      timbre_4_max      timbre_5_min    
##  Min.   : 12.85   Min.   :-207.07   Min.   : -0.651   Min.   :-262.48  
##  1st Qu.:127.14   1st Qu.: -77.69   1st Qu.: 83.966   1st Qu.:-113.58  
##  Median :189.50   Median : -63.83   Median :107.422   Median : -95.47  
##  Mean   :211.81   Mean   : -65.28   Mean   :108.227   Mean   :-104.00  
##  3rd Qu.:290.72   3rd Qu.: -51.34   3rd Qu.:130.286   3rd Qu.: -81.02  
##  Max.   :499.62   Max.   :  51.43   Max.   :257.801   Max.   : -42.17  
##                                                                        
##   timbre_5_max     timbre_6_min       timbre_6_max     timbre_7_min     
##  Min.   :-22.41   Min.   :-152.170   Min.   : 12.70   Min.   :-214.791  
##  1st Qu.: 84.64   1st Qu.: -94.792   1st Qu.: 59.04   1st Qu.:-101.171  
##  Median :119.90   Median : -80.418   Median : 70.47   Median : -81.797  
##  Mean   :127.04   Mean   : -80.944   Mean   : 72.17   Mean   : -84.313  
##  3rd Qu.:162.34   3rd Qu.: -66.521   3rd Qu.: 83.19   3rd Qu.: -64.301  
##  Max.   :350.94   Max.   :   4.503   Max.   :208.39   Max.   :   5.153  
##                                                                         
##   timbre_7_max     timbre_8_min       timbre_8_max     timbre_9_min    
##  Min.   : 15.70   Min.   :-158.756   Min.   :-25.95   Min.   :-149.51  
##  1st Qu.: 76.50   1st Qu.: -73.051   1st Qu.: 40.58   1st Qu.: -70.28  
##  Median : 94.63   Median : -62.661   Median : 49.22   Median : -58.65  
##  Mean   : 95.65   Mean   : -63.704   Mean   : 50.06   Mean   : -59.52  
##  3rd Qu.:112.71   3rd Qu.: -52.983   3rd Qu.: 58.46   3rd Qu.: -47.70  
##  Max.   :214.82   Max.   :  -2.382   Max.   :144.99   Max.   :   1.14  
##                                                                        
##   timbre_9_max     timbre_10_min     timbre_10_max     timbre_11_min     
##  Min.   :  8.415   Min.   :-208.82   Min.   : -6.359   Min.   :-145.599  
##  1st Qu.: 53.037   1st Qu.:-105.13   1st Qu.: 39.196   1st Qu.: -58.058  
##  Median : 65.935   Median : -83.07   Median : 50.895   Median : -50.892  
##  Mean   : 68.028   Mean   : -87.34   Mean   : 55.521   Mean   : -50.868  
##  3rd Qu.: 81.267   3rd Qu.: -64.52   3rd Qu.: 66.593   3rd Qu.: -43.292  
##  Max.   :161.518   Max.   : -10.64   Max.   :192.417   Max.   :  -6.497  
##                                                                          
##  timbre_11_max        Top10       
##  Min.   :  7.20   Min.   :0.0000  
##  1st Qu.: 38.98   1st Qu.:0.0000  
##  Median : 46.44   Median :0.0000  
##  Mean   : 47.49   Mean   :0.1477  
##  3rd Qu.: 55.03   3rd Qu.:0.0000  
##  Max.   :110.27   Max.   :1.0000  
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(songs$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 
##  328  196  186  324  198  258  178  329  380  357  363  282  518  434  479 
## 2005 2006 2007 2008 2009 2010 
##  392  479  622  415  483  373&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;373&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;How many songs does the dataset include for which the artist name is “Michael Jackson”?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(songs, artistname == &amp;quot;Michael Jackson&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 18&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Which of these songs by Michael Jackson made it to the Top 10? Select all that apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(songs, 
       artistname == &amp;quot;Michael Jackson&amp;quot; &amp;amp; Top10 == 1,
       select = c(artistname, songtitle))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           artistname         songtitle
## 4329 Michael Jackson You Rock My World
## 6207 Michael Jackson You Are Not Alone
## 6210 Michael Jackson    Black or White
## 6218 Michael Jackson Remember the Time
## 6915 Michael Jackson     In The Closet&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You Rock My World, You Are Not Alone&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). What are the values of this variable that occur in our dataset?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs$timesignature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   4.000   4.000   3.894   4.000   7.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(songs$timesignature)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##    0    1    3    4    5    7 
##   10  143  503 6787  112   19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which timesignature value is the most frequent among songs in our dataset?
## 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.5---understanding-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.5 - Understanding the Data&lt;/h3&gt;
&lt;p&gt;Out of all of the songs in our dataset, the song with the highest tempo is one of the following songs. Which one is it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(songs$tempo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00   88.86  103.27  107.35  124.80  244.31&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which.max(songs$tempo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6206&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs$tempo[6206]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 244.307&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(subset(songs, tempo == 244.307))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;songs$songtitle[6206]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Wanna Be Startin&amp;#39; Somethin&amp;#39;
## 7141 Levels: ̈́ l&amp;#39;or_e des bois _\x84_ _\x84\x8d ... Zumbi&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wanna Be Startin’ Somethin’&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set “SongsTrain” consisting of all the observations up to and including 2009 song releases, and a testing set “SongsTest”, consisting of the 2010 song releases.&lt;/p&gt;
&lt;p&gt;How many observations (songs) are in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsTrain &amp;lt;- subset(songs, year &amp;lt;= 2009)
SongsTest &amp;lt;- subset(songs, year == 2010)
nrow(songs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7574&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(SongsTrain) + nrow(SongsTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7574&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Creating our Prediction Model&lt;/h3&gt;
&lt;p&gt;In this problem, our outcome variable is “Top10” - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart. Since the outcome variable is binary, we will build a &lt;strong&gt;logistic regression&lt;/strong&gt; model. We’ll start by using all song attributes as our independent variables, which we’ll call Model 1. We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model. So we won’t use the variables “year”, “songtitle”, “artistname”, “songID” or “artistID”. We have seen in the lecture that, to build the logistic regression model, we would normally explicitly input the formula including all the independent variables in R. However, in this case, this is a tedious amount of work since we have a large number of independent variables. There is a nice trick to avoid doing so. Let’s suppose that, except for the outcome variable Top10, all other variables in the training set are inputs to Model 1. Then, we can use the formula SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial) to build our model. Notice that the “.” is used in place of enumerating all the independent variables. (Also, keep in mind that you can choose to put quotes around binomial, or leave out the quotes. R can understand this argument either way.) However, in our case, we want to exclude some of the variables in our dataset from being used as independent variables (“year”, “songtitle”, “artistname”, “songID”, and “artistID”). To do this, we can use the following trick. First define a vector of variable names called nonvars - these are the variables that we won’t use in our model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nonvars = c(&amp;quot;year&amp;quot;, &amp;quot;songtitle&amp;quot;, &amp;quot;artistname&amp;quot;, &amp;quot;songID&amp;quot;, &amp;quot;artistID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove these variables from your training and testing sets, type the following commands in your R console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. You should use SongsTrain to build the model.&lt;/p&gt;
&lt;p&gt;Looking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog1 &amp;lt;- glm(Top10 ~ ., data = SongsTrain, family=binomial)
summary(SongsLog1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Top10 ~ ., family = binomial, data = SongsTrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9220  -0.5399  -0.3459  -0.1845   3.0770  
## 
## Coefficients:
##                            Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)               1.470e+01  1.806e+00   8.138 4.03e-16 ***
## timesignature             1.264e-01  8.674e-02   1.457 0.145050    
## timesignature_confidence  7.450e-01  1.953e-01   3.815 0.000136 ***
## loudness                  2.999e-01  2.917e-02  10.282  &amp;lt; 2e-16 ***
## tempo                     3.634e-04  1.691e-03   0.215 0.829889    
## tempo_confidence          4.732e-01  1.422e-01   3.329 0.000873 ***
## key                       1.588e-02  1.039e-02   1.529 0.126349    
## key_confidence            3.087e-01  1.412e-01   2.187 0.028760 *  
## energy                   -1.502e+00  3.099e-01  -4.847 1.25e-06 ***
## pitch                    -4.491e+01  6.835e+00  -6.570 5.02e-11 ***
## timbre_0_min              2.316e-02  4.256e-03   5.441 5.29e-08 ***
## timbre_0_max             -3.310e-01  2.569e-02 -12.882  &amp;lt; 2e-16 ***
## timbre_1_min              5.881e-03  7.798e-04   7.542 4.64e-14 ***
## timbre_1_max             -2.449e-04  7.152e-04  -0.342 0.732087    
## timbre_2_min             -2.127e-03  1.126e-03  -1.889 0.058843 .  
## timbre_2_max              6.586e-04  9.066e-04   0.726 0.467571    
## timbre_3_min              6.920e-04  5.985e-04   1.156 0.247583    
## timbre_3_max             -2.967e-03  5.815e-04  -5.103 3.34e-07 ***
## timbre_4_min              1.040e-02  1.985e-03   5.237 1.63e-07 ***
## timbre_4_max              6.110e-03  1.550e-03   3.942 8.10e-05 ***
## timbre_5_min             -5.598e-03  1.277e-03  -4.385 1.16e-05 ***
## timbre_5_max              7.736e-05  7.935e-04   0.097 0.922337    
## timbre_6_min             -1.686e-02  2.264e-03  -7.445 9.66e-14 ***
## timbre_6_max              3.668e-03  2.190e-03   1.675 0.093875 .  
## timbre_7_min             -4.549e-03  1.781e-03  -2.554 0.010661 *  
## timbre_7_max             -3.774e-03  1.832e-03  -2.060 0.039408 *  
## timbre_8_min              3.911e-03  2.851e-03   1.372 0.170123    
## timbre_8_max              4.011e-03  3.003e-03   1.336 0.181620    
## timbre_9_min              1.367e-03  2.998e-03   0.456 0.648356    
## timbre_9_max              1.603e-03  2.434e-03   0.659 0.510188    
## timbre_10_min             4.126e-03  1.839e-03   2.244 0.024852 *  
## timbre_10_max             5.825e-03  1.769e-03   3.292 0.000995 ***
## timbre_11_min            -2.625e-02  3.693e-03  -7.108 1.18e-12 ***
## timbre_11_max             1.967e-02  3.385e-03   5.811 6.21e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6017.5  on 7200  degrees of freedom
## Residual deviance: 4759.2  on 7167  degrees of freedom
## AIC: 4827.2
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AIC: 4827.2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;Let’s now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). What does the model suggest?&lt;/p&gt;
&lt;div id=&#34;the-higher-our-confidence-about-time-signature-key-and-tempo-the-more-likely-the-song-is-to-be-in-the-top-10&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.4---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.4 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex. What does Model 1 suggest in terms of complexity?&lt;/p&gt;
&lt;div id=&#34;mainstream-listeners-tend-to-prefer-less-complex-songs&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Mainstream listeners tend to prefer less complex songs&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.5---creating-our-prediction-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.5 - Creating Our Prediction Model&lt;/h3&gt;
&lt;p&gt;Songs with heavier instrumentation tend to be louder (have higher values in the variable “loudness”) and more energetic (have higher values in the variable “energy”). By inspecting the coefficient of the variable “loudness”, what does Model 1 suggest?&lt;/p&gt;
&lt;div id=&#34;mainstream-listeners-prefer-songs-with-heavy-instrumentation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Mainstream listeners prefer songs with heavy instrumentation&lt;/h4&gt;
&lt;p&gt;By inspecting the coefficient of the variable “energy”, do we draw the same conclusions as above?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;no&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;No&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;What is the correlation between the variables “loudness” and “energy” in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(SongsTrain$loudness, SongsTrain$energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7399067&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given that these two variables are highly correlated, Model 1 suffers from multicollinearity. To avoid this issue, we will omit one of these two variables and rerun the logistic regression. In the rest of this problem, we’ll build two variations of our original model: Model 2, in which we keep “energy” and omit “loudness”, and Model 3, in which we keep “loudness” and omit “energy”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;Create Model 2, which is Model 1 without the independent variable “loudness”. This can be done with the following command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We just subtracted the variable loudness. We couldn’t do this with the variables “songtitle” and “artistname”, because they are not numeric variables, and we might get different values in the test-set that the training set has never seen. But this approach (subtracting the variable from the model formula) will always work when you want to remove numeric variables. Look at the summary of SongsLog2, and inspect the coefficient of the variable “energy”. What do you observe?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(SongsLog2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Top10 ~ . - loudness, family = binomial, data = SongsTrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0983  -0.5607  -0.3602  -0.1902   3.3107  
## 
## Coefficients:
##                            Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)              -2.241e+00  7.465e-01  -3.002 0.002686 ** 
## timesignature             1.625e-01  8.734e-02   1.860 0.062873 .  
## timesignature_confidence  6.885e-01  1.924e-01   3.578 0.000346 ***
## tempo                     5.521e-04  1.665e-03   0.332 0.740226    
## tempo_confidence          5.497e-01  1.407e-01   3.906 9.40e-05 ***
## key                       1.740e-02  1.026e-02   1.697 0.089740 .  
## key_confidence            2.954e-01  1.394e-01   2.118 0.034163 *  
## energy                    1.813e-01  2.608e-01   0.695 0.486991    
## pitch                    -5.150e+01  6.857e+00  -7.511 5.87e-14 ***
## timbre_0_min              2.479e-02  4.240e-03   5.847 5.01e-09 ***
## timbre_0_max             -1.007e-01  1.178e-02  -8.551  &amp;lt; 2e-16 ***
## timbre_1_min              7.143e-03  7.710e-04   9.265  &amp;lt; 2e-16 ***
## timbre_1_max             -7.830e-04  7.064e-04  -1.108 0.267650    
## timbre_2_min             -1.579e-03  1.109e-03  -1.424 0.154531    
## timbre_2_max              3.889e-04  8.964e-04   0.434 0.664427    
## timbre_3_min              6.500e-04  5.949e-04   1.093 0.274524    
## timbre_3_max             -2.462e-03  5.674e-04  -4.339 1.43e-05 ***
## timbre_4_min              9.115e-03  1.952e-03   4.670 3.02e-06 ***
## timbre_4_max              6.306e-03  1.532e-03   4.115 3.87e-05 ***
## timbre_5_min             -5.641e-03  1.255e-03  -4.495 6.95e-06 ***
## timbre_5_max              6.937e-04  7.807e-04   0.889 0.374256    
## timbre_6_min             -1.612e-02  2.235e-03  -7.214 5.45e-13 ***
## timbre_6_max              3.814e-03  2.157e-03   1.768 0.076982 .  
## timbre_7_min             -5.102e-03  1.755e-03  -2.907 0.003644 ** 
## timbre_7_max             -3.158e-03  1.811e-03  -1.744 0.081090 .  
## timbre_8_min              4.488e-03  2.810e-03   1.597 0.110254    
## timbre_8_max              6.423e-03  2.950e-03   2.177 0.029497 *  
## timbre_9_min             -4.282e-04  2.955e-03  -0.145 0.884792    
## timbre_9_max              3.525e-03  2.377e-03   1.483 0.138017    
## timbre_10_min             2.993e-03  1.804e-03   1.660 0.097004 .  
## timbre_10_max             7.367e-03  1.731e-03   4.255 2.09e-05 ***
## timbre_11_min            -2.837e-02  3.630e-03  -7.815 5.48e-15 ***
## timbre_11_max             1.829e-02  3.341e-03   5.476 4.34e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6017.5  on 7200  degrees of freedom
## Residual deviance: 4871.8  on 7168  degrees of freedom
## AIC: 4937.8
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Model 2 suggests that songs with high energy levels tend to be more popular. This contradicts our observation in Model 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---beware-of-multicollinearity-issues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Beware of Multicollinearity Issues!&lt;/h3&gt;
&lt;p&gt;Now, create Model 3, which should be exactly like Model 1, but without the variable “energy”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at the summary of Model 3 and inspect the coefficient of the variable “loudness”. Remembering that higher loudness and energy both occur in songs with heavier instrumentation, do we make the same observation about the popularity of heavy instrumentation as we did with Model 2?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(SongsLog3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Top10 ~ . - energy, family = binomial, data = SongsTrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9182  -0.5417  -0.3481  -0.1874   3.4171  
## 
## Coefficients:
##                            Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)               1.196e+01  1.714e+00   6.977 3.01e-12 ***
## timesignature             1.151e-01  8.726e-02   1.319 0.187183    
## timesignature_confidence  7.143e-01  1.946e-01   3.670 0.000242 ***
## loudness                  2.306e-01  2.528e-02   9.120  &amp;lt; 2e-16 ***
## tempo                    -6.460e-04  1.665e-03  -0.388 0.698107    
## tempo_confidence          3.841e-01  1.398e-01   2.747 0.006019 ** 
## key                       1.649e-02  1.035e-02   1.593 0.111056    
## key_confidence            3.394e-01  1.409e-01   2.409 0.015984 *  
## pitch                    -5.328e+01  6.733e+00  -7.914 2.49e-15 ***
## timbre_0_min              2.205e-02  4.239e-03   5.200 1.99e-07 ***
## timbre_0_max             -3.105e-01  2.537e-02 -12.240  &amp;lt; 2e-16 ***
## timbre_1_min              5.416e-03  7.643e-04   7.086 1.38e-12 ***
## timbre_1_max             -5.115e-04  7.110e-04  -0.719 0.471928    
## timbre_2_min             -2.254e-03  1.120e-03  -2.012 0.044190 *  
## timbre_2_max              4.119e-04  9.020e-04   0.457 0.647915    
## timbre_3_min              3.179e-04  5.869e-04   0.542 0.588083    
## timbre_3_max             -2.964e-03  5.758e-04  -5.147 2.64e-07 ***
## timbre_4_min              1.105e-02  1.978e-03   5.585 2.34e-08 ***
## timbre_4_max              6.467e-03  1.541e-03   4.196 2.72e-05 ***
## timbre_5_min             -5.135e-03  1.269e-03  -4.046 5.21e-05 ***
## timbre_5_max              2.979e-04  7.855e-04   0.379 0.704526    
## timbre_6_min             -1.784e-02  2.246e-03  -7.945 1.94e-15 ***
## timbre_6_max              3.447e-03  2.182e-03   1.580 0.114203    
## timbre_7_min             -5.128e-03  1.768e-03  -2.900 0.003733 ** 
## timbre_7_max             -3.394e-03  1.820e-03  -1.865 0.062208 .  
## timbre_8_min              3.686e-03  2.833e-03   1.301 0.193229    
## timbre_8_max              4.658e-03  2.988e-03   1.559 0.119022    
## timbre_9_min             -9.318e-05  2.957e-03  -0.032 0.974859    
## timbre_9_max              1.342e-03  2.424e-03   0.554 0.579900    
## timbre_10_min             4.050e-03  1.827e-03   2.217 0.026637 *  
## timbre_10_max             5.793e-03  1.759e-03   3.294 0.000988 ***
## timbre_11_min            -2.638e-02  3.683e-03  -7.162 7.96e-13 ***
## timbre_11_max             1.984e-02  3.365e-03   5.896 3.74e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6017.5  on 7200  degrees of freedom
## Residual deviance: 4782.7  on 7168  degrees of freedom
## AIC: 4848.7
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the remainder of this problem, we’ll just use Model 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;Make predictions on the test set using Model 3. What is the accuracy of Model 3 on the test set, using a threshold of 0.45? (Compute the accuracy as a number between 0 and 1.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predSongsTest = predict(SongsLog3, type=&amp;quot;response&amp;quot;, newdata = SongsTest)
table(SongsTest$Top10, predSongsTest &amp;gt; 0.45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     FALSE TRUE
##   0   309    5
##   1    40   19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(309 + 19) / nrow(SongsTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8793566&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;Let’s check if there’s any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs. What would the accuracy of the baseline model be on the test set? (Give your answer as a number between 0 and 1.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(SongsTest$Top10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   0   1 
## 314  59&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;314/(314 + 59)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8418231&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Validating Our Model&lt;/h3&gt;
&lt;p&gt;It seems that Model 3 gives us a small improvement over the baseline model. Still, does it create an edge? Let’s view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. The company’s objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular. A competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer. How many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(SongsTest$Top10, predSongsTest &amp;gt; 0.45)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     FALSE TRUE
##   0   309    5
##   1    40   19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;19&lt;/p&gt;
&lt;p&gt;How many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45?&lt;/p&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---validating-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Validating Our Model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the sensitivity of Model 3 on the test set, using a threshold of 0.45?
19 / (40 + 19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3220339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the specificity of Model 3 on the test set, using a threshold of 0.45?
309 / (309 + 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9840764&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Model 3 favors specificity over sensitivity.&lt;/li&gt;
&lt;li&gt;Model 3 provides conservative predictions, and predicts that a song will make it to the Top 10 very rarely. So while it detects less than half of the Top 10 songs, we can be very confident in the songs that it does predict to be Top 10 hits.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Flu Epidemics via Search Engine Query Data</title>
      <link>/project/flu_epidemics/flu_epidemics/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/flu_epidemics/flu_epidemics/</guid>
      <description>


&lt;p&gt;Flu epidemics constitute a major public health concern causing respiratory illnesses, hospitalizations, and deaths. According to the National Vital Statistics Reports published in October 2012, influenza ranked as the eighth leading cause of death in 2011 in the United States. Each year, 250,000 to 500,000 deaths are attributed to influenza related diseases throughout the world.&lt;/p&gt;
&lt;p&gt;The U.S. Centers for Disease Control and Prevention (CDC) and the European Influenza Surveillance Scheme (EISS) detect influenza activity through virologic and clinical data, including Influenza-like Illness (ILI) physician visits. Reporting national and regional data, however, are published with a 1-2 week lag.&lt;/p&gt;
&lt;p&gt;The Google Flu Trends project was initiated to see if faster reporting can be made possible by considering flu-related online search queries – data that is available almost immediately.&lt;/p&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain &amp;lt;- read.csv(&amp;quot;FluTrain.csv&amp;quot;)
summary(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       Week          ILI            Queries       
##  2004-01-04 - 2004-01-10:  1   Min.   :0.5341   Min.   :0.04117  
##  2004-01-11 - 2004-01-17:  1   1st Qu.:0.9025   1st Qu.:0.15671  
##  2004-01-18 - 2004-01-24:  1   Median :1.2526   Median :0.28154  
##  2004-01-25 - 2004-01-31:  1   Mean   :1.6769   Mean   :0.28603  
##  2004-02-01 - 2004-02-07:  1   3rd Qu.:2.0587   3rd Qu.:0.37849  
##  2004-02-08 - 2004-02-14:  1   Max.   :7.6189   Max.   :1.00000  
##  (Other)                :411&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    417 obs. of  3 variables:
##  $ Week   : Factor w/ 417 levels &amp;quot;2004-01-04 - 2004-01-10&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ ILI    : num  2.42 1.81 1.71 1.54 1.44 ...
##  $ Queries: num  0.238 0.22 0.226 0.238 0.224 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would like to estimate influenza-like illness (ILI) activity using Google web search logs. Fortunately, one can easily access this data online:
- ILI Data - The CDC publishes on its website the official regional and state-level percentage of patient visits to healthcare providers for ILI purposes on a weekly basis.
- Google Search Queries - Google Trends allows public retrieval of weekly counts for every query searched by users around the world. For each location, the counts are normalized by dividing the count for each query in a particular week by the total number of online search queries submitted in that location during the week. Then, the values are adjusted to be between 0 and 1.&lt;/p&gt;
&lt;p&gt;The csv file FluTrain.csv aggregates this data from January 1, 2004 until December 31, 2011 as follows:
- “Week” - The range of dates represented by this observation, in year/month/day format.
- “ILI” - This column lists the percentage of ILI-related physician visits for the corresponding week.
- “Queries” - This column lists the fraction of queries that are ILI-related for the corresponding week, adjusted to be between 0 and 1 (higher values correspond to more ILI-related search queries).&lt;/p&gt;
&lt;p&gt;Before applying analytics tools on the training set, we first need to understand the data at hand. Looking at the time period 2004-2011, which week corresponds to the highest percentage of ILI-related physician visits?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.1---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - EDA&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select the day of the month corresponding to the start of this week
FluTrain[which.max(FluTrain$ILI),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        Week      ILI Queries
## 303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# which week corresponds to the highest percentage of ILI-related query fraction?
FluTrain[which.max(FluTrain$Queries),]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        Week      ILI Queries
## 303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset(FluTrain, Queries == 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        Week      ILI Queries
## 303 2009-10-18 - 2009-10-24 7.618892       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;October 18, 2009&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - EDA&lt;/h3&gt;
&lt;p&gt;Let us now understand the data at a high level. Plot the histogram of the dependent variable, ILI. What best describes the distribution of values of ILI?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(FluTrain$ILI)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the ILI values are small, with a relatively small number of much larger values (in statistics, this sort of data is called “skew right”).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---eda&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - EDA&lt;/h3&gt;
&lt;p&gt;When handling a skewed dependent variable, it is often useful to predict the logarithm of the dependent variable instead of the dependent variable itself – this prevents the small number of unusually large or small observations from having an undue influence on the sum of squared errors of predictive models.&lt;/p&gt;
&lt;p&gt;In this problem, we will predict the natural log of the ILI variable, which can be computed using the log() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the natural logarithm of ILI versus Queries
plot(log(FluTrain$ILI), FluTrain$Queries)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(FluTrain$Queries, log(FluTrain$ILI))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What does the plot suggest?
&lt;strong&gt;There is a positive, linear relationship between log(ILI) and Queries.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;Based on the plot we just made, it seems that a linear regression model could be a good modeling choice. Based on our understanding of the data from the previous subproblem, which model best describes our estimation problem?&lt;/p&gt;
&lt;p&gt;log(ILI) = intercept + coefficient x Queries, where the coefficient is positive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;Let’s call the regression model from the previous problem (Problem 2.1). FluTrend1 and run it. Hint: to take the logarithm of a variable Var in a regression equation, you simply use log(Var) when specifying the formula to the lm() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrend1 &amp;lt;- lm(log(ILI) ~ Queries, data = FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the training set R-squared value for FluTrend1 model (the “Multiple R-squared”)?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = log(ILI) ~ Queries, data = FluTrain)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76003 -0.19696 -0.01657  0.18685  1.06450 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.49934    0.03041  -16.42   &amp;lt;2e-16 ***
## Queries      2.96129    0.09312   31.80   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2995 on 415 degrees of freedom
## Multiple R-squared:  0.709,  Adjusted R-squared:  0.7083 
## F-statistic:  1011 on 1 and 415 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0.709&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Linear Regression Model&lt;/h3&gt;
&lt;p&gt;For a single variable linear regression model, there is a direct relationship between the R-squared and the correlation between the independent and the dependent variables. What is the relationship we infer from our problem? (Don’t forget that you can use the cor function to compute the correlation between two variables.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corILIQueries &amp;lt;- cor(log(FluTrain$ILI), FluTrain$Queries)
cor(FluTrain$ILI, FluTrain$Queries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8142115&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corILIQueries^2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7090201&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(1/corILIQueries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1719357&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(-0.5 * corILIQueries)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6563792&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ANSWER = R-squared = Correlation^2&lt;br /&gt;
Note that the “exp” function stands for the exponential function. The exponential can be computed in R using the function exp().&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;The csv file FluTest.csv provides the 2012 weekly data of the ILI-related search queries and the observed weekly percentage of ILI-related physician visits.&lt;/p&gt;
&lt;p&gt;Load this data into a dataframe called FluTest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest &amp;lt;- read.csv(&amp;quot;FluTest.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Normally, we would obtain test-set predictions from the model FluTrend1 using the code PredTest1 = predict(FluTrend1, newdata=FluTest) However, the dependent variable in our model is log(ILI), so PredTest1 would contain predictions of the log(ILI) value.&lt;/p&gt;
&lt;p&gt;We are instead interested in obtaining predictions of the ILI value. We can convert from predictions of log(ILI) to predictions of ILI via exponentiation, or the exp() function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the new code, which predicts the ILI value
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is our estimate for the percentage of ILI-related physician visits for the week of March 11, 2012? (HINT: You can either just output FluTest$Week to find which element corresponds to March 11, 2012, or you can use the “which” function in R. To learn more about the which function, type ?which in your R console.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$Week&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 2012-01-01 - 2012-01-07 2012-01-08 - 2012-01-14
##  [3] 2012-01-15 - 2012-01-21 2012-01-22 - 2012-01-28
##  [5] 2012-01-29 - 2012-02-04 2012-02-05 - 2012-02-11
##  [7] 2012-02-12 - 2012-02-18 2012-02-19 - 2012-02-25
##  [9] 2012-02-26 - 2012-03-03 2012-03-04 - 2012-03-10
## [11] 2012-03-11 - 2012-03-17 2012-03-18 - 2012-03-24
## [13] 2012-03-25 - 2012-03-31 2012-04-01 - 2012-04-07
## [15] 2012-04-08 - 2012-04-14 2012-04-15 - 2012-04-21
## [17] 2012-04-22 - 2012-04-28 2012-04-29 - 2012-05-05
## [19] 2012-05-06 - 2012-05-12 2012-05-13 - 2012-05-19
## [21] 2012-05-20 - 2012-05-26 2012-05-27 - 2012-06-02
## [23] 2012-06-03 - 2012-06-09 2012-06-10 - 2012-06-16
## [25] 2012-06-17 - 2012-06-23 2012-06-24 - 2012-06-30
## [27] 2012-07-01 - 2012-07-07 2012-07-08 - 2012-07-14
## [29] 2012-07-15 - 2012-07-21 2012-07-22 - 2012-07-28
## [31] 2012-07-29 - 2012-08-04 2012-08-05 - 2012-08-11
## [33] 2012-08-12 - 2012-08-18 2012-08-19 - 2012-08-25
## [35] 2012-08-26 - 2012-09-01 2012-09-02 - 2012-09-08
## [37] 2012-09-09 - 2012-09-15 2012-09-16 - 2012-09-22
## [39] 2012-09-23 - 2012-09-29 2012-09-30 - 2012-10-06
## [41] 2012-10-07 - 2012-10-13 2012-10-14 - 2012-10-20
## [43] 2012-10-21 - 2012-10-27 2012-10-28 - 2012-11-03
## [45] 2012-11-04 - 2012-11-10 2012-11-11 - 2012-11-17
## [47] 2012-11-18 - 2012-11-24 2012-11-25 - 2012-12-01
## [49] 2012-12-02 - 2012-12-08 2012-12-09 - 2012-12-15
## [51] 2012-12-16 - 2012-12-22 2012-12-23 - 2012-12-29
## 52 Levels: 2012-01-01 - 2012-01-07 ... 2012-12-23 - 2012-12-29&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest[11, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       Week      ILI   Queries
## 11 2012-03-11 - 2012-03-17 2.293422 0.4329349&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PredTest1[11]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       11 
## 2.187378&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.293422&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;What is the relative error betweeen the estimate (our prediction) and the observed value for the week of March 11, 2012? Note that the relative error is calculated as (Observed ILI - Estimated ILI)/Observed ILI
(FluTest[11, 2] - PredTest1[11]) / FluTest[11, 2]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(FluTest[11, 2] - PredTest1[11]) / FluTest[11, 2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         11 
## 0.04623827&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---performance-on-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Performance on the Test Set&lt;/h3&gt;
&lt;p&gt;What is the Root Mean Square Error (RMSE) between our estimates and the actual observations for the percentage of ILI-related physician visits, on the test-set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTestSSE = sum((PredTest1 - FluTest$ILI)^2)
FluTestRMSE = sqrt(FluTestSSE/nrow(FluTest))
FluTestRMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7490645&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;The observations in this dataset are consecutive weekly measurements of the dependent and independent variables. This sort of dataset is called a &lt;strong&gt;“time series.”&lt;/strong&gt; Often, statistical models can be improved by predicting the current value of the dependent variable using the value of the dependent variable from earlier weeks. In our models, this means we will predict the ILI variable in the current week using values of the ILI variable from previous weeks.&lt;/p&gt;
&lt;p&gt;First, we need to decide the amount of time to lag the observations. Because the ILI variable is reported with a 1- or 2-week lag, a decision maker cannot rely on the previous week’s ILI value to predict the current week’s value. Instead, the decision maker will only have data available from 2 or more weeks ago.&lt;/p&gt;
&lt;p&gt;We will build a variable called ILILag2 that contains the ILI value from 2 weeks before the current observation.&lt;/p&gt;
&lt;p&gt;To do so, we will use the “zoo” package, which provides a number of helpful methods for time series models. While many functions are built into R, you need to add new packages to use some functions. New packages can be installed and loaded easily in R. Run the following two commands to install and load the zoo package. In the first command, you will be prompted to select a CRAN mirror to use for your download. Select a mirror near you geographically. install.packages(“zoo”)&lt;/p&gt;
&lt;p&gt;After installing and loading the zoo package, run the following commands to create the ILILag2 variable in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In these commands, the value of -2 passed to lag means to return 2 observations before the current one; a positive value would have returned future observations. The parameter na.pad=TRUE means to add missing values for the first two weeks of our dataset, where we can’t compute the data from 2 weeks earlier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?lag
?coredata
ILILag2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         1         2         3         4         5         6         7 
##        NA        NA 2.4183312 1.8090560 1.7120239 1.5424951 1.4378683 
##         8         9        10        11        12        13        14 
## 1.3242740 1.3072567 1.0369770 1.0103204 1.0524925 1.0200901 0.9244187 
##        15        16        17        18        19        20        21 
## 0.7906450 0.8026098 0.8361300 0.7924358 0.6835877 0.7574523 0.7885854 
##        22        23        24        25        26        27        28 
## 0.8121710 0.8044629 0.8777009 0.7414530 0.6610222 0.7151092 0.5622412 
##        29        30        31        32        33        34        35 
## 0.7868082 0.8606578 0.6899440 0.7796912 0.6281439 0.9024586 0.8064432 
##        36        37        38        39        40        41        42 
## 0.8748878 0.9932130 0.8761408 0.9480916 0.9269426 0.9716430 0.8971591 
##        43        44        45        46        47        48        49 
## 1.0224828 1.0629632 1.1469570 1.2049501 1.3051655 1.2869916 1.5946756 
##        50        51        52        53        54        55        56 
## 1.3971432 1.4499567 1.6174545 2.1911192 2.5664893 2.1764491 2.2017121 
##        57        58        59        60        61        62        63 
## 2.5301211 3.0652381 3.9806083 4.5956803 4.7519706 4.1796206 3.4535851 
##        64        65        66        67        68        69        70 
## 3.1585224 2.6732010 2.3516104 1.8924285 1.5249048 1.4113441 1.2506826 
##        71        72        73        74        75        76        77 
## 1.2070250 1.0789550 1.1452080 1.0612426 1.0567977 1.2519310 1.0141893 
##        78        79        80        81        82        83        84 
## 1.0419693 0.9540274 0.8482299 0.8418715 0.7308936 0.7134316 0.6706772 
##        85        86        87        88        89        90        91 
## 0.6892776 0.7049290 0.6159033 0.6094256 0.6802587 0.7754884 0.6834214 
##        92        93        94        95        96        97        98 
## 0.7810748 0.8069435 1.0763468 1.0586890 1.1152326 1.1238125 1.2548892 
##        99       100       101       102       103       104       105 
## 1.3366090 1.3786364 1.6082900 1.4831056 1.6537399 2.0067892 2.5685716 
##       106       107       108       109       110       111       112 
## 3.0527762 2.4250373 2.0019506 2.0586902 2.2127697 2.3222001 2.4927920 
##       113       114       115       116       117       118       119 
## 2.7948942 2.9691114 2.8395905 2.7779902 2.4728693 2.1806146 2.0167951 
##       120       121       122       123       124       125       126 
## 1.6410133 1.3582865 1.1427983 1.0403125 0.9643469 0.9379817 0.9474493 
##       127       128       129       130       131       132       133 
## 0.8919182 0.8646427 0.9703199 0.8443901 0.7748704 0.8213725 0.8727445 
##       134       135       136       137       138       139       140 
## 0.9226345 0.8994868 0.8430824 0.8818244 0.8171452 0.8715001 0.7386205 
##       141       142       143       144       145       146       147 
## 0.7979660 1.0139373 0.8809358 0.9433663 0.8915462 1.2032228 1.0578822 
##       148       149       150       151       152       153       154 
## 1.1305354 1.1255230 1.2080820 1.3495244 1.4689004 1.8276716 1.6656012 
##       155       156       157       158       159       160       161 
## 1.8596834 2.3889130 2.7897759 3.1154858 2.2694245 1.8635464 1.9998635 
##       162       163       164       165       166       167       168 
## 2.4406044 2.8301821 3.1234256 3.2701949 3.1775688 2.7236366 2.5020140 
##       169       170       171       172       173       174       175 
## 2.4271992 1.9604132 1.5913980 1.3697835 1.3631668 1.1736951 1.0635756 
##       176       177       178       179       180       181       182 
## 0.9697111 0.9653617 0.8567489 0.8633465 0.9353695 0.7455694 0.7404281 
##       183       184       185       186       187       188       189 
## 0.6728965 0.6662820 0.6627473 0.5456190 0.5862306 0.6606867 0.5340928 
##       190       191       192       193       194       195       196 
## 0.5855491 0.6180750 0.6874647 0.7156961 0.8293131 0.8009115 0.9184839 
##       197       198       199       200       201       202       203 
## 0.8142590 1.0719708 1.2178574 1.2457554 1.3598449 1.4467085 1.5328638 
##       204       205       206       207       208       209       210 
## 1.6665324 1.9748773 1.6730547 1.6340509 1.7459475 1.9364319 2.4890534 
##       211       212       213       214       215       216       217 
## 2.2540484 2.0914715 2.3593428 3.3233143 4.4338100 5.3454714 5.4225751 
##       218       219       220       221       222       223       224 
## 5.3030330 4.2445550 3.6280001 3.0346275 2.5359536 2.0573015 1.7415035 
##       225       226       227       228       229       230       231 
## 1.4065217 1.2686070 1.0771887 0.9934452 0.9112119 0.9721091 0.9932575 
##       232       233       234       235       236       237       238 
## 1.0913202 0.8884460 0.8876915 0.8831874 0.8267564 0.7832014 0.7806103 
##       239       240       241       242       243       244       245 
## 0.7690726 0.7212979 0.7525273 0.7527210 0.7927660 0.7438962 0.8141663 
##       246       247       248       249       250       251       252 
## 0.8384009 0.8511236 1.1097575 1.0311436 1.0228436 1.0301739 1.0124478 
##       253       254       255       256       257       258       259 
## 1.0835911 1.1657765 1.1912964 1.2807470 1.2705251 1.5957825 1.4584994 
##       260       261       262       263       264       265       266 
## 1.4992072 1.6298157 2.1556121 2.0205270 1.5456623 1.6422367 1.9652378 
##       267       268       269       270       271       272       273 
## 2.3436784 2.8605744 3.3421049 3.2056588 3.1004908 2.9581850 2.4638058 
##       274       275       276       277       278       279       280 
## 2.1927224 1.8739459 1.6481690 1.4987776 1.2923267 1.2716411 2.9815890 
##       281       282       283       284       285       286       287 
## 2.4370224 2.2813011 3.8157199 4.2131523 3.1783224 2.5097162 2.0663177 
##       288       289       290       291       292       293       294 
## 1.7180460 1.5596467 1.3085629 1.1869460 1.1379623 1.1500523 1.1126189 
##       295       296       297       298       299       300       301 
## 1.1614188 1.6410714 2.4716598 3.7196936 3.9497480 4.0875636 4.0189724 
##       302       303       304       305       306       307       308 
## 4.6036164 5.6608671 6.8152222 7.6188921 7.3883586 6.3392723 4.9434950 
##       309       310       311       312       313       314       315 
## 3.8099612 3.4410588 2.6677306 2.4718250 2.3449995 2.7143498 2.6766718 
##       316       317       318       319       320       321       322 
## 1.9828382 1.8274862 1.9260563 1.9249472 2.0887684 2.0343408 1.9764946 
##       323       324       325       326       327       328       329 
## 1.9936177 1.8538260 1.8673036 1.6998677 1.4974082 1.4511188 1.2071478 
##       330       331       332       333       334       335       336 
## 1.1741508 1.1620668 1.1721343 1.1216765 1.1498116 1.1332758 1.0817133 
##       337       338       339       340       341       342       343 
## 1.1995860 0.9528083 0.9160321 0.9265822 0.8696197 0.9031331 0.7737757 
##       344       345       346       347       348       349       350 
## 0.7427744 0.7309345 0.7868818 0.7630507 0.8410432 0.7915728 0.9127318 
##       351       352       353       354       355       356       357 
## 1.0339765 0.9340091 1.0818888 1.0656260 1.1350529 1.2525629 1.2456956 
##       358       359       360       361       362       363       364 
## 1.2677380 1.4372295 1.5334125 1.6944544 1.9915024 1.8130453 2.0142579 
##       365       366       367       368       369       370       371 
## 2.5565913 3.3818486 3.4317231 2.6915111 2.9106289 3.4923189 4.0036963 
##       372       373       374       375       376       377       378 
## 4.4353368 4.2421482 4.3971861 3.9025565 3.1507275 2.7242234 2.3333563 
##       379       380       381       382       383       384       385 
## 1.9250003 1.7524260 1.5770365 1.3576558 1.3122310 1.1493747 1.1145057 
##       386       387       388       389       390       391       392 
## 1.1098449 1.0524026 1.0353647 1.1177658 0.9829495 0.9251944 0.8355311 
##       393       394       395       396       397       398       399 
## 0.8323927 0.8555910 0.7069494 0.6943868 0.6879762 0.6447430 0.6753299 
##       400       401       402       403       404       405       406 
## 0.7282297 0.8065263 0.8604084 0.9360754 0.9666827 0.9960071 1.1084635 
##       407       408       409       410       411       412       413 
## 1.2030858 1.2369566 1.2525865 1.3054612 1.4528432 1.4408922 1.4622115 
##       414       415       416       417 
## 1.6554147 1.4657230 1.5181061 1.6639544&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many values are missing in the new ILILag2 variable?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(FluTrain$ILILag2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;Use the plot() function to plot the log of ILILag2 against the log of ILI. Which best describes the relationship between these two variables?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(log(FluTrain$ILILag2), log(FluTrain$ILI))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/flu_epidemics/flu_epidemics_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a strong positive relationship between log(ILILag2) and log(ILI).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;Train a linear regression model on the FluTrain dataset to predict the log of the ILI variable using the Queries variable as well as the log of the ILILag2 variable. Call this model FluTrend2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrend2 &amp;lt;- lm(log(ILI) ~ Queries + log(ILILag2), data = FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which coefficients are significant at the p=0.05 level in this regression model? (Select all that apply.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.52209 -0.11082 -0.01819  0.08143  0.76785 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -0.24064    0.01953  -12.32   &amp;lt;2e-16 ***
## Queries       1.25578    0.07910   15.88   &amp;lt;2e-16 ***
## log(ILILag2)  0.65569    0.02251   29.14   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1703 on 412 degrees of freedom
##   (2 observations deleted due to missingness)
## Multiple R-squared:  0.9063, Adjusted R-squared:  0.9059 
## F-statistic:  1993 on 2 and 412 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All are significant at p&amp;lt;0.05&lt;/p&gt;
&lt;p&gt;What is the R^2 value of the FluTrend2 model?
#### 0.9063&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---training-a-time-series-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Training a Time Series Model&lt;/h3&gt;
&lt;p&gt;On the basis of R-squared value and significance of coefficients, which statement is the most accurate?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = log(ILI) ~ Queries, data = FluTrain)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76003 -0.19696 -0.01657  0.18685  1.06450 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.49934    0.03041  -16.42   &amp;lt;2e-16 ***
## Queries      2.96129    0.09312   31.80   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2995 on 415 degrees of freedom
## Multiple R-squared:  0.709,  Adjusted R-squared:  0.7083 
## F-statistic:  1011 on 1 and 415 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(FluTrend2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = log(ILI) ~ Queries + log(ILILag2), data = FluTrain)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.52209 -0.11082 -0.01819  0.08143  0.76785 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  -0.24064    0.01953  -12.32   &amp;lt;2e-16 ***
## Queries       1.25578    0.07910   15.88   &amp;lt;2e-16 ***
## log(ILILag2)  0.65569    0.02251   29.14   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1703 on 412 degrees of freedom
##   (2 observations deleted due to missingness)
## Multiple R-squared:  0.9063, Adjusted R-squared:  0.9059 
## F-statistic:  1993 on 2 and 412 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;FluTrend2 is a stronger model than FluTrend1 on the training set, due to it’s higher R^2 value.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.1---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.1 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;So far, we have only added the ILILag2 variable to the FluTrain dataframe. To make predictions with our FluTrend2 model, we will also need to add ILILag2 to the FluTest dataframe (note that adding variables before splitting into a training and testing set can prevent this duplication of effort).&lt;/p&gt;
&lt;p&gt;Modifying the code from the previous subproblem to add an ILILag2 variable to the FluTest dataframe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how many missing values are there in this new variable?
Test_ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE)
FluTest$ILILag2 = coredata(Test_ILILag2)
sum(is.na(FluTest$ILILag2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.2---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.2 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;In this problem, the training and testing sets are split sequentially – the training set contains all observations from 2004-2011 and the testing set contains all observations from 2012.&lt;/p&gt;
&lt;p&gt;There is no time gap between the two datasets, meaning the first observation in FluTest was recorded one week after the last observation in FluTrain. From this, we can identify how to fill in the missing values for the ILILag2 variable in FluTest. Which value should be used to fill in the ILILag2 variable for the first observation in FluTest?&lt;/p&gt;
&lt;p&gt;The ILI value of the second-to-last observation in the FluTrain dataframe. Which value should be used to fill in the ILILag2 variable for the second observation in FluTest? The ILI value of the last observation in the FluTrain dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.3---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.3 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Fill in the missing values for ILILag2 in FluTest. In terms of syntax, you could set the value of ILILag2 in row “x” of the FluTest dataframe to the value of ILI in row “y” of the FluTrain dataframe with “FluTest&lt;span class=&#34;math inline&#34;&gt;\(ILILag2[x] = FluTrain\)&lt;/span&gt;ILI[y]”.&lt;/p&gt;
&lt;p&gt;Use the answer to the previous questions to determine the appropriate values of “x” and “y”. It may be helpful to check the total number of rows in FluTrain using str(FluTrain) or nrow(FluTrain).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(FluTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 417&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[1] = FluTrain$ILI[416]
FluTest$ILILag2[2] = FluTrain$ILI[417]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new value of the ILILag2 variable in the first row of FluTest?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain$ILI[416]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.852736&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.852736&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the new value of the ILILag2 variable in the second row of FluTest?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTrain$ILI[417]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.12413&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FluTest$ILILag2[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.12413&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.4---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.4 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Obtain test-set predictions of the ILI variable from the FluTrend2 model, again remembering to call the exp() function on the result of the predict() function to obtain predictions for ILI instead of log(ILI).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# what is the test-set RMSE of the FluTrend2 model?
PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
FluTestSSE2 = sum((PredTest2 - FluTest$ILI)^2)
FluTestRMSE2 = sqrt(FluTestSSE2/nrow(FluTest))
FluTestRMSE2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2942029&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-5.5---evaluating-the-time-series-model-in-the-test-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 5.5 - Evaluating the Time Series Model in the Test Set&lt;/h3&gt;
&lt;p&gt;Which model obtained the best test-set RMSE?&lt;/p&gt;
&lt;p&gt;FluTrend2 (less RMSE is better)&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;In this problem, we used a simple time series model with a single lag term. ARIMA models are a more general form of the model we built, which can include multiple lag terms as well as more complicated combinations of previous values of the dependent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reading Test Scores</title>
      <link>/project/pisa2009/pisa/</link>
      <pubDate>Sat, 06 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/pisa2009/pisa/</guid>
      <description>


&lt;p&gt;The Programme for International Student Assessment (PISA) is a test given every three years to 15-year-old students from around the world to evaluate their performance in &lt;strong&gt;mathematics, reading, and science.&lt;/strong&gt; This test provides a quantitative way to compare the performance of students from different parts of the world. In this analysis, I’ll predict the reading scores of students from the USA on the 2009 PISA exam.&lt;/p&gt;
&lt;p&gt;The datasets contain information about the demographics and schools for American students taking the exam, derived from 2009 PISA Public-Use Data Files distributed by the United States National Center for Education Statistics (NCES). While the datasets are not supposed to contain identifying information about students taking the test, by using the data you are bound by the NCES data use agreement, which prohibits any attempt to determine the identity of any student in the datasets.&lt;/p&gt;
&lt;p&gt;Each row in the datasets represents one student taking the exam. The datasets have the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;grade: The grade in school of the student (most 15-year-olds in America are in 10th grade)&lt;/li&gt;
&lt;li&gt;male: Whether the student is male (1/0)&lt;/li&gt;
&lt;li&gt;raceeth: The race/ethnicity composite of the student&lt;/li&gt;
&lt;li&gt;preschool: Whether the student attended preschool (1/0)&lt;/li&gt;
&lt;li&gt;expectBachelors: Whether the student expects to obtain a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;motherHS: Whether the student’s mother completed high school (1/0)&lt;/li&gt;
&lt;li&gt;motherBachelors: Whether the student’s mother obtained a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;motherWork: Whether the student’s mother has part-time or full-time work (1/0)&lt;/li&gt;
&lt;li&gt;fatherHS: Whether the student’s father completed high school (1/0)&lt;/li&gt;
&lt;li&gt;fatherBachelors: Whether the student’s father obtained a bachelor’s degree (1/0)&lt;/li&gt;
&lt;li&gt;fatherWork: Whether the student’s father has part-time or full-time work (1/0)&lt;/li&gt;
&lt;li&gt;selfBornUS: Whether the student was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;motherBornUS: Whether the student’s mother was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;fatherBornUS: Whether the student’s father was born in the United States of America (1/0)&lt;/li&gt;
&lt;li&gt;englishAtHome: Whether the student speaks English at home (1/0)&lt;/li&gt;
&lt;li&gt;computerForSchoolwork: Whether the student has access to a computer for schoolwork (1/0)&lt;/li&gt;
&lt;li&gt;read30MinsADay: Whether the student reads for pleasure for 30 minutes/day (1/0)&lt;/li&gt;
&lt;li&gt;minutesPerWeekEnglish: The number of minutes per week the student spend in English class&lt;/li&gt;
&lt;li&gt;studentsInEnglish: The number of students in this student’s English class at school&lt;/li&gt;
&lt;li&gt;schoolHasLibrary: Whether this student’s school has a library (1/0)&lt;/li&gt;
&lt;li&gt;publicSchool: Whether this student attends a public school (1/0)&lt;/li&gt;
&lt;li&gt;urban: Whether this student’s school is in an urban area (1/0)&lt;/li&gt;
&lt;li&gt;schoolSize: The number of students in this student’s school&lt;/li&gt;
&lt;li&gt;readingScore: The student’s reading score, on a 1000-point scale&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;problem-1.1---dataset-size&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.1 - Dataset size&lt;/h3&gt;
&lt;p&gt;Load the training and testing sets using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaTrain &amp;lt;- read.csv(&amp;quot;pisa2009train.csv&amp;quot;)
pisaTest &amp;lt;- read.csv(&amp;quot;pisa2009test.csv&amp;quot;)
str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3663 obs. of  24 variables:
##  $ grade                : int  11 11 9 10 10 10 10 10 9 10 ...
##  $ male                 : int  1 1 1 0 1 1 0 0 0 1 ...
##  $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: NA 7 7 3 4 3 2 7 7 5 ...
##  $ preschool            : int  NA 0 1 1 1 1 0 1 1 1 ...
##  $ expectBachelors      : int  0 0 1 1 0 1 1 1 0 1 ...
##  $ motherHS             : int  NA 1 1 0 1 NA 1 1 1 1 ...
##  $ motherBachelors      : int  NA 1 1 0 0 NA 0 0 NA 1 ...
##  $ motherWork           : int  1 1 1 1 1 1 1 0 1 1 ...
##  $ fatherHS             : int  NA 1 1 1 1 1 NA 1 0 0 ...
##  $ fatherBachelors      : int  NA 0 NA 0 0 0 NA 0 NA 0 ...
##  $ fatherWork           : int  1 1 1 1 0 1 NA 1 1 1 ...
##  $ selfBornUS           : int  1 1 1 1 1 1 0 1 1 1 ...
##  $ motherBornUS         : int  0 1 1 1 1 1 1 1 1 1 ...
##  $ fatherBornUS         : int  0 1 1 1 0 1 NA 1 1 1 ...
##  $ englishAtHome        : int  0 1 1 1 1 1 1 1 1 1 ...
##  $ computerForSchoolwork: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ read30MinsADay       : int  0 1 0 1 1 0 0 1 0 0 ...
##  $ minutesPerWeekEnglish: int  225 450 250 200 250 300 250 300 378 294 ...
##  $ studentsInEnglish    : int  NA 25 28 23 35 20 28 30 20 24 ...
##  $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 0 1 ...
##  $ publicSchool         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ urban                : int  1 0 0 1 1 0 1 0 1 0 ...
##  $ schoolSize           : int  673 1173 1233 2640 1095 227 2080 1913 502 899 ...
##  $ readingScore         : num  476 575 555 458 614 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      grade            male                      raceeth    
##  Min.   : 8.00   Min.   :0.0000   White             :2015  
##  1st Qu.:10.00   1st Qu.:0.0000   Hispanic          : 834  
##  Median :10.00   Median :1.0000   Black             : 444  
##  Mean   :10.09   Mean   :0.5111   Asian             : 143  
##  3rd Qu.:10.00   3rd Qu.:1.0000   More than one race: 124  
##  Max.   :12.00   Max.   :1.0000   (Other)           :  68  
##                                   NA&amp;#39;s              :  35  
##    preschool      expectBachelors     motherHS    motherBachelors 
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  
##  Median :1.0000   Median :1.0000   Median :1.00   Median :0.0000  
##  Mean   :0.7228   Mean   :0.7859   Mean   :0.88   Mean   :0.3481  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  
##  NA&amp;#39;s   :56       NA&amp;#39;s   :62       NA&amp;#39;s   :97     NA&amp;#39;s   :397     
##    motherWork        fatherHS      fatherBachelors    fatherWork    
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  
##  Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  
##  Mean   :0.7345   Mean   :0.8593   Mean   :0.3319   Mean   :0.8531  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :93       NA&amp;#39;s   :245      NA&amp;#39;s   :569      NA&amp;#39;s   :233     
##    selfBornUS      motherBornUS     fatherBornUS    englishAtHome   
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  
##  Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  
##  Mean   :0.9313   Mean   :0.7725   Mean   :0.7668   Mean   :0.8717  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :69       NA&amp;#39;s   :71       NA&amp;#39;s   :113      NA&amp;#39;s   :71      
##  computerForSchoolwork read30MinsADay   minutesPerWeekEnglish
##  Min.   :0.0000        Min.   :0.0000   Min.   :   0.0       
##  1st Qu.:1.0000        1st Qu.:0.0000   1st Qu.: 225.0       
##  Median :1.0000        Median :0.0000   Median : 250.0       
##  Mean   :0.8994        Mean   :0.2899   Mean   : 266.2       
##  3rd Qu.:1.0000        3rd Qu.:1.0000   3rd Qu.: 300.0       
##  Max.   :1.0000        Max.   :1.0000   Max.   :2400.0       
##  NA&amp;#39;s   :65            NA&amp;#39;s   :34       NA&amp;#39;s   :186          
##  studentsInEnglish schoolHasLibrary  publicSchool        urban       
##  Min.   : 1.0      Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:20.0      1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  
##  Median :25.0      Median :1.0000   Median :1.0000   Median :0.0000  
##  Mean   :24.5      Mean   :0.9676   Mean   :0.9339   Mean   :0.3849  
##  3rd Qu.:30.0      3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :75.0      Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :249       NA&amp;#39;s   :143                                       
##    schoolSize    readingScore  
##  Min.   : 100   Min.   :168.6  
##  1st Qu.: 712   1st Qu.:431.7  
##  Median :1212   Median :499.7  
##  Mean   :1369   Mean   :497.9  
##  3rd Qu.:1900   3rd Qu.:566.2  
##  Max.   :6694   Max.   :746.0  
##  NA&amp;#39;s   :162&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Number of students in the training set is 3663&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.2---summarizing-the-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.2 - Summarizing the dataset&lt;/h3&gt;
&lt;p&gt;Using tapply() on pisaTrain, what is the average reading test score of males?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tapply(pisaTrain$readingScore, pisaTrain$male, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        0        1 
## 512.9406 483.5325&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Males reading score, 483.5325 and Females reading score is 512.9406&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.3---locating-missing-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.3 - Locating missing values&lt;/h3&gt;
&lt;p&gt;Which variables are missing data in at least one observation in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      grade            male                      raceeth    
##  Min.   : 8.00   Min.   :0.0000   White             :2015  
##  1st Qu.:10.00   1st Qu.:0.0000   Hispanic          : 834  
##  Median :10.00   Median :1.0000   Black             : 444  
##  Mean   :10.09   Mean   :0.5111   Asian             : 143  
##  3rd Qu.:10.00   3rd Qu.:1.0000   More than one race: 124  
##  Max.   :12.00   Max.   :1.0000   (Other)           :  68  
##                                   NA&amp;#39;s              :  35  
##    preschool      expectBachelors     motherHS    motherBachelors 
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.00   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:1.00   1st Qu.:0.0000  
##  Median :1.0000   Median :1.0000   Median :1.00   Median :0.0000  
##  Mean   :0.7228   Mean   :0.7859   Mean   :0.88   Mean   :0.3481  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.00   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.00   Max.   :1.0000  
##  NA&amp;#39;s   :56       NA&amp;#39;s   :62       NA&amp;#39;s   :97     NA&amp;#39;s   :397     
##    motherWork        fatherHS      fatherBachelors    fatherWork    
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000   1st Qu.:1.0000  
##  Median :1.0000   Median :1.0000   Median :0.0000   Median :1.0000  
##  Mean   :0.7345   Mean   :0.8593   Mean   :0.3319   Mean   :0.8531  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :93       NA&amp;#39;s   :245      NA&amp;#39;s   :569      NA&amp;#39;s   :233     
##    selfBornUS      motherBornUS     fatherBornUS    englishAtHome   
##  Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000  
##  Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  
##  Mean   :0.9313   Mean   :0.7725   Mean   :0.7668   Mean   :0.8717  
##  3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :69       NA&amp;#39;s   :71       NA&amp;#39;s   :113      NA&amp;#39;s   :71      
##  computerForSchoolwork read30MinsADay   minutesPerWeekEnglish
##  Min.   :0.0000        Min.   :0.0000   Min.   :   0.0       
##  1st Qu.:1.0000        1st Qu.:0.0000   1st Qu.: 225.0       
##  Median :1.0000        Median :0.0000   Median : 250.0       
##  Mean   :0.8994        Mean   :0.2899   Mean   : 266.2       
##  3rd Qu.:1.0000        3rd Qu.:1.0000   3rd Qu.: 300.0       
##  Max.   :1.0000        Max.   :1.0000   Max.   :2400.0       
##  NA&amp;#39;s   :65            NA&amp;#39;s   :34       NA&amp;#39;s   :186          
##  studentsInEnglish schoolHasLibrary  publicSchool        urban       
##  Min.   : 1.0      Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:20.0      1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  
##  Median :25.0      Median :1.0000   Median :1.0000   Median :0.0000  
##  Mean   :24.5      Mean   :0.9676   Mean   :0.9339   Mean   :0.3849  
##  3rd Qu.:30.0      3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  
##  Max.   :75.0      Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  
##  NA&amp;#39;s   :249       NA&amp;#39;s   :143                                       
##    schoolSize    readingScore  
##  Min.   : 100   Min.   :168.6  
##  1st Qu.: 712   1st Qu.:431.7  
##  Median :1212   Median :499.7  
##  Mean   :1369   Mean   :497.9  
##  3rd Qu.:1900   3rd Qu.:566.2  
##  Max.   :6694   Max.   :746.0  
##  NA&amp;#39;s   :162&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;raceeth, preschool, expectBachelors, motherHS, motherBachelors, motherWork, fatherHS, fatherBachelors, fatherWork, selfBornUS, motherBornUS, fatherBornUS, englishAtHome, computerForSchoolWork, read30MinsADay, minutesPerWeekEnglish, studentsInEnglish, schoolHasLibrary, schoolSize&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1.4---removing-missing-values&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 1.4 - Removing missing values&lt;/h3&gt;
&lt;p&gt;Linear regression discards observations with missing data, so we’ll remove all such observations from the training and testing sets. Later, we will learn about imputation, which deals with missing data by filling in missing values with plausible information.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to remove observations with any missing value from pisaTrain and pisaTest:
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many observations are now in the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2414 obs. of  24 variables:
##  $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
##  $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
##  $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
##  $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
##  $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
##  $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
##  $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
##  $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
##  $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
##  $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
##  $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
##  $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
##  $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
##  $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
##  $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
##  $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
##  $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
##  $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
##  $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
##  $ readingScore         : num  575 458 614 439 466 ...
##  - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
##   ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2414&lt;/p&gt;
&lt;p&gt;How many observations are now in the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    990 obs. of  24 variables:
##  $ grade                : int  10 10 10 10 11 10 10 10 10 10 ...
##  $ male                 : int  0 0 0 0 0 1 0 1 1 0 ...
##  $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 7 1 7 7 4 7 4 7 4 ...
##  $ preschool            : int  1 1 1 1 0 1 0 1 1 1 ...
##  $ expectBachelors      : int  0 1 0 0 0 1 1 0 1 1 ...
##  $ motherHS             : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ motherBachelors      : int  1 0 0 0 1 1 0 0 1 0 ...
##  $ motherWork           : int  1 0 0 1 1 1 0 1 1 1 ...
##  $ fatherHS             : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ fatherBachelors      : int  0 1 0 0 1 0 0 0 1 1 ...
##  $ fatherWork           : int  0 1 0 1 1 1 1 0 1 1 ...
##  $ selfBornUS           : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ motherBornUS         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ fatherBornUS         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ englishAtHome        : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ computerForSchoolwork: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ read30MinsADay       : int  0 0 1 1 1 1 0 0 0 1 ...
##  $ minutesPerWeekEnglish: int  240 240 240 270 270 350 350 360 350 360 ...
##  $ studentsInEnglish    : int  30 30 30 35 30 25 27 28 25 27 ...
##  $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ publicSchool         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ urban                : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ schoolSize           : int  808 808 808 808 808 899 899 899 899 899 ...
##  $ readingScore         : num  355 454 405 665 605 ...
##  - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  2 3 4 6 12 16 17 19 22 23 ...
##   ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;6&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;990&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.1---factor-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.1 - Factor variables&lt;/h3&gt;
&lt;p&gt;Factor variables are variables that take on a discrete set of values. This is an unordered factor because there isn’t any natural ordering between the levels.&lt;/p&gt;
&lt;p&gt;An ordered factor has a natural ordering between the levels (an example would be the classifications “large,” “medium,” and “small”).&lt;/p&gt;
&lt;p&gt;Which of the following variables is an unordered factor with at least 3 levels?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2414 obs. of  24 variables:
##  $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
##  $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
##  $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
##  $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
##  $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
##  $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
##  $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
##  $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
##  $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
##  $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
##  $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
##  $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
##  $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
##  $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
##  $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
##  $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
##  $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
##  $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
##  $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
##  $ readingScore         : num  575 458 614 439 466 ...
##  - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
##   ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;raceeth&lt;/p&gt;
&lt;p&gt;Which of the following variables is an ordered factor with at least 3 levels?&lt;/p&gt;
&lt;p&gt;grade&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.2---unordered-factors-in-regression-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.2 - Unordered factors in regression models&lt;/h3&gt;
&lt;p&gt;To include unordered factors in a linear regression model, we define one level as the “reference level” and add a binary variable for each of the remaining levels. In this way, a factor with n levels is replaced by n-1 binary variables. The reference level is typically selected to be the most frequently occurring level in the dataset.&lt;/p&gt;
&lt;p&gt;As an example, consider the unordered factor variable “color”, with levels “red”, “green”, and “blue”. If “green” were the reference level, then we would add binary variables “colored” and “colorblue” to a linear regression problem. All red examples would have colored=1 and colorblue=0. All blue examples would have colored=0 and colorblue=1. All green examples would have colored=0 and colorblue=0.&lt;/p&gt;
&lt;p&gt;Now, consider the variable “raceeth” in our problem, which has levels &lt;strong&gt;“American Indian/Alaska Native”, “Asian”, “Black”, “Hispanic”,&lt;/strong&gt; “More than one race”, &lt;strong&gt;“Native Hawaiian/Other Pacific Islander”, and “White”.&lt;/strong&gt; Because it’s the most common in our population, we will select White as the reference level.&lt;/p&gt;
&lt;p&gt;Which binary variables will be included in the regression model?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    2414 obs. of  24 variables:
##  $ grade                : int  11 10 10 10 10 10 10 10 11 9 ...
##  $ male                 : int  1 0 1 0 1 0 0 0 1 1 ...
##  $ raceeth              : Factor w/ 7 levels &amp;quot;American Indian/Alaska Native&amp;quot;,..: 7 3 4 7 5 4 7 4 7 7 ...
##  $ preschool            : int  0 1 1 1 1 1 1 1 1 1 ...
##  $ expectBachelors      : int  0 1 0 1 1 1 1 0 1 1 ...
##  $ motherHS             : int  1 0 1 1 1 1 1 0 1 1 ...
##  $ motherBachelors      : int  1 0 0 0 1 0 0 0 0 1 ...
##  $ motherWork           : int  1 1 1 0 1 1 1 0 0 1 ...
##  $ fatherHS             : int  1 1 1 1 0 1 1 0 1 1 ...
##  $ fatherBachelors      : int  0 0 0 0 0 0 1 0 1 1 ...
##  $ fatherWork           : int  1 1 0 1 1 0 1 1 1 1 ...
##  $ selfBornUS           : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ motherBornUS         : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ fatherBornUS         : int  1 1 0 1 1 0 1 0 1 1 ...
##  $ englishAtHome        : int  1 1 1 1 1 0 1 0 1 1 ...
##  $ computerForSchoolwork: int  1 1 1 1 1 0 1 1 1 1 ...
##  $ read30MinsADay       : int  1 1 1 1 0 1 1 1 0 0 ...
##  $ minutesPerWeekEnglish: int  450 200 250 300 294 232 225 270 275 225 ...
##  $ studentsInEnglish    : int  25 23 35 30 24 14 20 25 30 15 ...
##  $ schoolHasLibrary     : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ publicSchool         : int  1 1 1 1 1 1 1 1 1 0 ...
##  $ urban                : int  0 1 1 0 0 0 0 1 1 1 ...
##  $ schoolSize           : int  1173 2640 1095 1913 899 1733 149 1400 1988 915 ...
##  $ readingScore         : num  575 458 614 439 466 ...
##  - attr(*, &amp;quot;na.action&amp;quot;)= &amp;#39;omit&amp;#39; Named int  1 3 6 7 9 11 13 21 29 30 ...
##   ..- attr(*, &amp;quot;names&amp;quot;)= chr  &amp;quot;1&amp;quot; &amp;quot;3&amp;quot; &amp;quot;6&amp;quot; &amp;quot;7&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;raceethAmerican Indian/Alaska Native&lt;/li&gt;
&lt;li&gt;raceethAsian&lt;/li&gt;
&lt;li&gt;raceethBlack&lt;/li&gt;
&lt;li&gt;raceethHispanic&lt;/li&gt;
&lt;li&gt;raceethMore than one race&lt;/li&gt;
&lt;li&gt;raceethNative Hawaiian/Other Pacific Islander&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2.3---example-unordered-factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 2.3 - Example unordered factors&lt;/h3&gt;
&lt;p&gt;Consider again adding our unordered factor race to the regression model with reference level “White”.&lt;/p&gt;
&lt;p&gt;For a student who is Asian, which binary variables would be set to 0? All remaining variables will be set to 1. (Select all that apply.) (all except raceethAsian)&lt;/p&gt;
&lt;p&gt;For a student who is white, which binary variables would be set to 0? All remaining variables will be set to 1. (Select all that apply.)
(all)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.1---building-a-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.1 - Building a model&lt;/h3&gt;
&lt;p&gt;Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() – you can see this when you run str(pisaTrain) or str(pisaTest). However, by default R selects the first level alphabetically (“American Indian/Alaska Native”) as the reference level of our factor instead of the most common level (“White”).&lt;/p&gt;
&lt;p&gt;Set the reference level of the factor by typing the following two lines in your R console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaTrain$raceeth = relevel(pisaTrain$raceeth, &amp;quot;White&amp;quot;)
pisaTest$raceeth = relevel(pisaTest$raceeth, &amp;quot;White&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, building a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables. It would be time-consuming to type all the variables, but R provides the shorthand notation “readingScore ~ .” to mean “predict readingScore using all the other variables in the dataframe.” The period is used to replace listing out all of the independent variables.&lt;/p&gt;
&lt;p&gt;As an example, if your dependent variable is called “Y”, your independent variables are called “X1”, “X2”, and “X3”, and your training dataset is called “Train”, instead of the regular notation: LinReg = lm(Y ~ X1 + X2 + X3, data = Train)&lt;/p&gt;
&lt;p&gt;You would use the following command to build your model:
LinReg = lm(Y ~ ., data = Train)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmScore &amp;lt;- lm(readingScore ~ ., data = pisaTrain)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the Multiple R-squared value of lmScore on the training set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lmScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = readingScore ~ ., data = pisaTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -247.44  -48.86    1.86   49.77  217.18 
## 
## Coefficients:
##                                                 Estimate Std. Error
## (Intercept)                                   143.766333  33.841226
## grade                                          29.542707   2.937399
## male                                          -14.521653   3.155926
## raceethAmerican Indian/Alaska Native          -67.277327  16.786935
## raceethAsian                                   -4.110325   9.220071
## raceethBlack                                  -67.012347   5.460883
## raceethHispanic                               -38.975486   5.177743
## raceethMore than one race                     -16.922522   8.496268
## raceethNative Hawaiian/Other Pacific Islander  -5.101601  17.005696
## preschool                                      -4.463670   3.486055
## expectBachelors                                55.267080   4.293893
## motherHS                                        6.058774   6.091423
## motherBachelors                                12.638068   3.861457
## motherWork                                     -2.809101   3.521827
## fatherHS                                        4.018214   5.579269
## fatherBachelors                                16.929755   3.995253
## fatherWork                                      5.842798   4.395978
## selfBornUS                                     -3.806278   7.323718
## motherBornUS                                   -8.798153   6.587621
## fatherBornUS                                    4.306994   6.263875
## englishAtHome                                   8.035685   6.859492
## computerForSchoolwork                          22.500232   5.702562
## read30MinsADay                                 34.871924   3.408447
## minutesPerWeekEnglish                           0.012788   0.010712
## studentsInEnglish                              -0.286631   0.227819
## schoolHasLibrary                               12.215085   9.264884
## publicSchool                                  -16.857475   6.725614
## urban                                          -0.110132   3.962724
## schoolSize                                      0.006540   0.002197
##                                               t value Pr(&amp;gt;|t|)    
## (Intercept)                                     4.248 2.24e-05 ***
## grade                                          10.057  &amp;lt; 2e-16 ***
## male                                           -4.601 4.42e-06 ***
## raceethAmerican Indian/Alaska Native           -4.008 6.32e-05 ***
## raceethAsian                                   -0.446  0.65578    
## raceethBlack                                  -12.271  &amp;lt; 2e-16 ***
## raceethHispanic                                -7.528 7.29e-14 ***
## raceethMore than one race                      -1.992  0.04651 *  
## raceethNative Hawaiian/Other Pacific Islander  -0.300  0.76421    
## preschool                                      -1.280  0.20052    
## expectBachelors                                12.871  &amp;lt; 2e-16 ***
## motherHS                                        0.995  0.32001    
## motherBachelors                                 3.273  0.00108 ** 
## motherWork                                     -0.798  0.42517    
## fatherHS                                        0.720  0.47147    
## fatherBachelors                                 4.237 2.35e-05 ***
## fatherWork                                      1.329  0.18393    
## selfBornUS                                     -0.520  0.60331    
## motherBornUS                                   -1.336  0.18182    
## fatherBornUS                                    0.688  0.49178    
## englishAtHome                                   1.171  0.24153    
## computerForSchoolwork                           3.946 8.19e-05 ***
## read30MinsADay                                 10.231  &amp;lt; 2e-16 ***
## minutesPerWeekEnglish                           1.194  0.23264    
## studentsInEnglish                              -1.258  0.20846    
## schoolHasLibrary                                1.318  0.18749    
## publicSchool                                   -2.506  0.01226 *  
## urban                                          -0.028  0.97783    
## schoolSize                                      2.977  0.00294 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 73.81 on 2385 degrees of freedom
## Multiple R-squared:  0.3251, Adjusted R-squared:  0.3172 
## F-statistic: 41.04 on 28 and 2385 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0.3251&lt;/p&gt;
&lt;p&gt;Note that this R-squared is lower than the ones for the models we saw in the lectures and recitation. This does not necessarily imply that the model is of poor quality. More often than not, it simply means that the prediction problem at hand (predicting a student’s test score based on demographic and school-related variables) is more difficult than other prediction problems (like predicting a team’s number of wins from their runs scored and allowed, or predicting the quality of wine from weather conditions).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.2---computing-the-root-mean-squared-error-of-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.2 - Computing the root-mean squared error of the model&lt;/h3&gt;
&lt;p&gt;What is the training-set root-mean squared error (RMSE) of lmScore?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmScoreSSE &amp;lt;- sum(lmScore$residuals^2)
lmScoreSSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12993365&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(lmScoreSSE/nrow(pisaTrain))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 73.36555&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.3---comparing-predictions-for-similar-students&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.3 - Comparing predictions for similar students&lt;/h3&gt;
&lt;p&gt;Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9.&lt;/p&gt;
&lt;p&gt;What is the predicted reading score of student A minus the predicted reading score of student B?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisaPred &amp;lt;- pisaTest[1,]
pisaPred &amp;lt;- rbind(pisaPred, pisaTest[1,])
pisaPred[1,1] &amp;lt;- 11 ## grade 11 for student A
pisaPred[2,1] &amp;lt;- 9  ## grade 9 for student B
pisaPred&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   grade male raceeth preschool expectBachelors motherHS motherBachelors
## 1    11    0   White         1               0        1               1
## 2     9    0   White         1               0        1               1
##   motherWork fatherHS fatherBachelors fatherWork selfBornUS motherBornUS
## 1          1        1               0          0          1            1
## 2          1        1               0          0          1            1
##   fatherBornUS englishAtHome computerForSchoolwork read30MinsADay
## 1            1             1                     1              0
## 2            1             1                     1              0
##   minutesPerWeekEnglish studentsInEnglish schoolHasLibrary publicSchool
## 1                   240                30                1            1
## 2                   240                30                1            1
##   urban schoolSize readingScore
## 1     0        808       355.24
## 2     0        808       355.24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictedScores &amp;lt;- predict(lmScore, pisaPred)
predictedScores&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2 
## 501.5294 442.4440&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictedScores[1] - predictedScores[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 59.08541&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;59.08541 ~ 59.09&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.4---interpreting-model-coefficients&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.4 - Interpreting model coefficients&lt;/h3&gt;
&lt;p&gt;What is the meaning of the coefficient associated with variable raceethAsian?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lmScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = readingScore ~ ., data = pisaTrain)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -247.44  -48.86    1.86   49.77  217.18 
## 
## Coefficients:
##                                                 Estimate Std. Error
## (Intercept)                                   143.766333  33.841226
## grade                                          29.542707   2.937399
## male                                          -14.521653   3.155926
## raceethAmerican Indian/Alaska Native          -67.277327  16.786935
## raceethAsian                                   -4.110325   9.220071
## raceethBlack                                  -67.012347   5.460883
## raceethHispanic                               -38.975486   5.177743
## raceethMore than one race                     -16.922522   8.496268
## raceethNative Hawaiian/Other Pacific Islander  -5.101601  17.005696
## preschool                                      -4.463670   3.486055
## expectBachelors                                55.267080   4.293893
## motherHS                                        6.058774   6.091423
## motherBachelors                                12.638068   3.861457
## motherWork                                     -2.809101   3.521827
## fatherHS                                        4.018214   5.579269
## fatherBachelors                                16.929755   3.995253
## fatherWork                                      5.842798   4.395978
## selfBornUS                                     -3.806278   7.323718
## motherBornUS                                   -8.798153   6.587621
## fatherBornUS                                    4.306994   6.263875
## englishAtHome                                   8.035685   6.859492
## computerForSchoolwork                          22.500232   5.702562
## read30MinsADay                                 34.871924   3.408447
## minutesPerWeekEnglish                           0.012788   0.010712
## studentsInEnglish                              -0.286631   0.227819
## schoolHasLibrary                               12.215085   9.264884
## publicSchool                                  -16.857475   6.725614
## urban                                          -0.110132   3.962724
## schoolSize                                      0.006540   0.002197
##                                               t value Pr(&amp;gt;|t|)    
## (Intercept)                                     4.248 2.24e-05 ***
## grade                                          10.057  &amp;lt; 2e-16 ***
## male                                           -4.601 4.42e-06 ***
## raceethAmerican Indian/Alaska Native           -4.008 6.32e-05 ***
## raceethAsian                                   -0.446  0.65578    
## raceethBlack                                  -12.271  &amp;lt; 2e-16 ***
## raceethHispanic                                -7.528 7.29e-14 ***
## raceethMore than one race                      -1.992  0.04651 *  
## raceethNative Hawaiian/Other Pacific Islander  -0.300  0.76421    
## preschool                                      -1.280  0.20052    
## expectBachelors                                12.871  &amp;lt; 2e-16 ***
## motherHS                                        0.995  0.32001    
## motherBachelors                                 3.273  0.00108 ** 
## motherWork                                     -0.798  0.42517    
## fatherHS                                        0.720  0.47147    
## fatherBachelors                                 4.237 2.35e-05 ***
## fatherWork                                      1.329  0.18393    
## selfBornUS                                     -0.520  0.60331    
## motherBornUS                                   -1.336  0.18182    
## fatherBornUS                                    0.688  0.49178    
## englishAtHome                                   1.171  0.24153    
## computerForSchoolwork                           3.946 8.19e-05 ***
## read30MinsADay                                 10.231  &amp;lt; 2e-16 ***
## minutesPerWeekEnglish                           1.194  0.23264    
## studentsInEnglish                              -1.258  0.20846    
## schoolHasLibrary                                1.318  0.18749    
## publicSchool                                   -2.506  0.01226 *  
## urban                                          -0.028  0.97783    
## schoolSize                                      2.977  0.00294 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 73.81 on 2385 degrees of freedom
## Multiple R-squared:  0.3251, Adjusted R-squared:  0.3172 
## F-statistic: 41.04 on 28 and 2385 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Predicted difference in the reading score between an Asian student and a white student who is otherwise identical.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-3.5---identifying-variables-lacking-statistical-significance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 3.5 - Identifying variables lacking statistical significance&lt;/h3&gt;
&lt;p&gt;Based on the significance codes, which variables are candidates for removal from the model?&lt;/p&gt;
&lt;p&gt;Select all that apply. (We’ll assume that the factor variable raceeth should only be removed if none of its levels are significant.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;preschool, motherHS, motherWork, fatherHS, fatherWork, selfBornUS,&lt;/li&gt;
&lt;li&gt;motherBornUS, fatherBornUS, englishAtHome, minutesPerWeekEnglish,&lt;/li&gt;
&lt;li&gt;studentsInEnglish, schoolHasLibrary, urban&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.1---predicting-on-unseen-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.1 - Predicting on unseen data&lt;/h3&gt;
&lt;p&gt;Using the “predict” function and supplying the “newdata” argument, use the lmScore model to predict the reading scores of students in pisaTest. Call this vector of predictions “predTest”. Do not change the variables in the model (for example, do not remove variables that we found were not significant in the previous part of this problem). Use the summary function to describe the test-set predictions.&lt;/p&gt;
&lt;p&gt;What is the range between the maximum and minimum predicted reading score on the test set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predTest &amp;lt;- predict(lmScore, newdata = pisaTest)
summary(predTest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   353.2   482.0   524.0   516.7   555.7   637.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;637.7 - 353.2&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.2---test-set-sse-and-rmse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.2 - Test set SSE and RMSE&lt;/h3&gt;
&lt;p&gt;What is the sum of squared errors (SSE) of lmScore on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_SSE = sum((predTest - pisaTest$readingScore)^2)
test_set_SSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5762082&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the root-mean squared error (RMSE) of lmScore on the testing set?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_RMSE = sqrt(test_set_SSE/nrow(pisaTest))
test_set_RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 76.29079&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.3---baseline-prediction-and-test-set-sse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.3 - Baseline prediction and test-set SSE&lt;/h3&gt;
&lt;p&gt;What is the predicted test score used in the baseline model? Remember to compute this value using the training set and not the test-set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(pisaTrain$readingScore)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 517.9629&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the sum of squared errors of the baseline model on the testing set? HINT: We call the sum of squared errors for the baseline model the total sum of squares (SST).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_set_SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
test_set_SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7802354&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-4.4---test-set-r-squared&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem 4.4 - Test-set R-squared&lt;/h3&gt;
&lt;p&gt;What is the test-set R-squared value of lmScore?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - test_set_SSE/test_set_SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2614944&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>climate_change</title>
      <link>/project/climate_change/climate_change/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/climate_change/climate_change/</guid>
      <description>


&lt;p&gt;There have been many studies documenting that the average global temperature has been increasing over the last century. The consequences of a continued rise in global temperature will be dire. Rising sea levels and an increased frequency of extreme weather events will affect billions of people.&lt;/p&gt;
&lt;p&gt;In this analysis, we will attempt to study the relationship between average global temperature and several other factors.&lt;/p&gt;
&lt;p&gt;The file climate_change.csv contains climate data from May 1983 to December 2008. The available variables include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Year: the observation year.&lt;/li&gt;
&lt;li&gt;Month: the observation month.&lt;/li&gt;
&lt;li&gt;Temp: the difference in degrees Celsius between the average global temperature in that period and a reference value. This data comes from the Climatic Research Unit at the University of East Anglia.&lt;/li&gt;
&lt;li&gt;CO2, N2O, CH4, CFC.11, CFC.12: atmospheric concentrations of carbon dioxide (CO2), nitrous oxide (N2O), methane (CH4), trichlorofluoromethane (CCl3F; commonly referred to as CFC-11) and dichlorodifluoromethane (CCl2F2; commonly referred to as CFC-12), respectively. This data comes from the ESRL/NOAA Global Monitoring Division.
&lt;ul&gt;
&lt;li&gt;CO2, N2O and CH4 are expressed in ppmv (parts per million by volume – i.e., 397 ppmv of CO2 means that CO2 constitutes 397 millionths of the total volume of the atmosphere)&lt;/li&gt;
&lt;li&gt;CFC.11 and CFC.12 are expressed in ppbv (parts per billion by volume).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Aerosols: the mean stratospheric aerosol optical depth at 550 nm. This variable is linked to volcanoes, as volcanic eruptions result in new particles being added to the atmosphere, which affect how much of the sun’s energy is reflected back into space. This data is from the Godard Institute for Space Studies at NASA.&lt;/li&gt;
&lt;li&gt;TSI: the total solar irradiance (TSI) in W/m2 (the rate at which the sun’s energy is deposited per unit area). Due to sunspots and other solar phenomena, the amount of energy that is given off by the sun varies substantially with time. This data is from the SOLARIS-HEPPA project website.&lt;/li&gt;
&lt;li&gt;MEI: multivariate El Nino Southern Oscillation index (MEI), a measure of the strength of the El Nino/La Nina-Southern Oscillation (a weather effect in the Pacific Ocean that affects global temperatures). This data comes from the ESRL/NOAA Physical Sciences Division.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the data&lt;/h3&gt;
&lt;p&gt;We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate &amp;lt;- read.csv(&amp;quot;climate_change.csv&amp;quot;)
str(climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    308 obs. of  11 variables:
##  $ Year    : int  1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ...
##  $ Month   : int  5 6 7 8 9 10 11 12 1 2 ...
##  $ MEI     : num  2.556 2.167 1.741 1.13 0.428 ...
##  $ CO2     : num  346 346 344 342 340 ...
##  $ CH4     : num  1639 1634 1633 1631 1648 ...
##  $ N2O     : num  304 304 304 304 304 ...
##  $ CFC.11  : num  191 192 193 194 194 ...
##  $ CFC.12  : num  350 352 354 356 357 ...
##  $ TSI     : num  1366 1366 1366 1366 1366 ...
##  $ Aerosols: num  0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ...
##  $ Temp    : num  0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Year          Month             MEI               CO2       
##  Min.   :1983   Min.   : 1.000   Min.   :-1.6350   Min.   :340.2  
##  1st Qu.:1989   1st Qu.: 4.000   1st Qu.:-0.3987   1st Qu.:353.0  
##  Median :1996   Median : 7.000   Median : 0.2375   Median :361.7  
##  Mean   :1996   Mean   : 6.552   Mean   : 0.2756   Mean   :363.2  
##  3rd Qu.:2002   3rd Qu.:10.000   3rd Qu.: 0.8305   3rd Qu.:373.5  
##  Max.   :2008   Max.   :12.000   Max.   : 3.0010   Max.   :388.5  
##       CH4            N2O            CFC.11          CFC.12     
##  Min.   :1630   Min.   :303.7   Min.   :191.3   Min.   :350.1  
##  1st Qu.:1722   1st Qu.:308.1   1st Qu.:246.3   1st Qu.:472.4  
##  Median :1764   Median :311.5   Median :258.3   Median :528.4  
##  Mean   :1750   Mean   :312.4   Mean   :252.0   Mean   :497.5  
##  3rd Qu.:1787   3rd Qu.:317.0   3rd Qu.:267.0   3rd Qu.:540.5  
##  Max.   :1814   Max.   :322.2   Max.   :271.5   Max.   :543.8  
##       TSI          Aerosols            Temp        
##  Min.   :1365   Min.   :0.00160   Min.   :-0.2820  
##  1st Qu.:1366   1st Qu.:0.00280   1st Qu.: 0.1217  
##  Median :1366   Median :0.00575   Median : 0.2480  
##  Mean   :1366   Mean   :0.01666   Mean   : 0.2568  
##  3rd Qu.:1366   3rd Qu.:0.01260   3rd Qu.: 0.4073  
##  Max.   :1367   Max.   :0.14940   Max.   : 0.7390&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ml-flowflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ML Flowflow&lt;/h3&gt;
&lt;p&gt;Then, split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate_train &amp;lt;- subset(climate, Year &amp;lt;= 2006)
climate_test &amp;lt;- subset(climate, Year &amp;gt; 2006)
str(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    284 obs. of  11 variables:
##  $ Year    : int  1983 1983 1983 1983 1983 1983 1983 1983 1984 1984 ...
##  $ Month   : int  5 6 7 8 9 10 11 12 1 2 ...
##  $ MEI     : num  2.556 2.167 1.741 1.13 0.428 ...
##  $ CO2     : num  346 346 344 342 340 ...
##  $ CH4     : num  1639 1634 1633 1631 1648 ...
##  $ N2O     : num  304 304 304 304 304 ...
##  $ CFC.11  : num  191 192 193 194 194 ...
##  $ CFC.12  : num  350 352 354 356 357 ...
##  $ TSI     : num  1366 1366 1366 1366 1366 ...
##  $ Aerosols: num  0.0863 0.0794 0.0731 0.0673 0.0619 0.0569 0.0524 0.0486 0.0451 0.0416 ...
##  $ Temp    : num  0.109 0.118 0.137 0.176 0.149 0.093 0.232 0.078 0.089 0.013 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Year          Month             MEI               CO2       
##  Min.   :1983   Min.   : 1.000   Min.   :-1.5860   Min.   :340.2  
##  1st Qu.:1989   1st Qu.: 4.000   1st Qu.:-0.3230   1st Qu.:352.3  
##  Median :1995   Median : 7.000   Median : 0.3085   Median :359.9  
##  Mean   :1995   Mean   : 6.556   Mean   : 0.3419   Mean   :361.4  
##  3rd Qu.:2001   3rd Qu.:10.000   3rd Qu.: 0.8980   3rd Qu.:370.6  
##  Max.   :2006   Max.   :12.000   Max.   : 3.0010   Max.   :385.0  
##       CH4            N2O            CFC.11          CFC.12     
##  Min.   :1630   Min.   :303.7   Min.   :191.3   Min.   :350.1  
##  1st Qu.:1716   1st Qu.:307.7   1st Qu.:249.6   1st Qu.:462.5  
##  Median :1759   Median :310.8   Median :260.4   Median :522.1  
##  Mean   :1746   Mean   :311.7   Mean   :252.5   Mean   :494.2  
##  3rd Qu.:1782   3rd Qu.:316.1   3rd Qu.:267.4   3rd Qu.:541.0  
##  Max.   :1808   Max.   :320.5   Max.   :271.5   Max.   :543.8  
##       TSI          Aerosols            Temp        
##  Min.   :1365   Min.   :0.00160   Min.   :-0.2820  
##  1st Qu.:1366   1st Qu.:0.00270   1st Qu.: 0.1180  
##  Median :1366   Median :0.00620   Median : 0.2325  
##  Mean   :1366   Mean   :0.01772   Mean   : 0.2478  
##  3rd Qu.:1366   3rd Qu.:0.01400   3rd Qu.: 0.4065  
##  Max.   :1367   Max.   :0.14940   Max.   : 0.7390&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    24 obs. of  11 variables:
##  $ Year    : int  2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...
##  $ Month   : int  1 2 3 4 5 6 7 8 9 10 ...
##  $ MEI     : num  0.974 0.51 0.074 -0.049 0.183 ...
##  $ CO2     : num  383 384 385 386 387 ...
##  $ CH4     : num  1800 1803 1803 1802 1796 ...
##  $ N2O     : num  321 321 321 321 320 ...
##  $ CFC.11  : num  248 248 248 248 247 ...
##  $ CFC.12  : num  539 539 539 539 538 ...
##  $ TSI     : num  1366 1366 1366 1366 1366 ...
##  $ Aerosols: num  0.0054 0.0051 0.0045 0.0045 0.0041 0.004 0.004 0.0041 0.0042 0.0041 ...
##  $ Temp    : num  0.601 0.498 0.435 0.466 0.372 0.382 0.394 0.358 0.402 0.362 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       Year          Month            MEI               CO2       
##  Min.   :2007   Min.   : 1.00   Min.   :-1.6350   Min.   :380.9  
##  1st Qu.:2007   1st Qu.: 3.75   1st Qu.:-1.0437   1st Qu.:383.1  
##  Median :2008   Median : 6.50   Median :-0.5305   Median :384.5  
##  Mean   :2008   Mean   : 6.50   Mean   :-0.5098   Mean   :384.7  
##  3rd Qu.:2008   3rd Qu.: 9.25   3rd Qu.:-0.0360   3rd Qu.:386.1  
##  Max.   :2008   Max.   :12.00   Max.   : 0.9740   Max.   :388.5  
##       CH4            N2O            CFC.11          CFC.12     
##  Min.   :1772   Min.   :320.3   Min.   :244.1   Min.   :534.9  
##  1st Qu.:1792   1st Qu.:320.6   1st Qu.:244.6   1st Qu.:535.1  
##  Median :1798   Median :321.3   Median :246.2   Median :537.0  
##  Mean   :1797   Mean   :321.1   Mean   :245.9   Mean   :536.7  
##  3rd Qu.:1804   3rd Qu.:321.4   3rd Qu.:246.6   3rd Qu.:537.4  
##  Max.   :1814   Max.   :322.2   Max.   :248.4   Max.   :539.2  
##       TSI          Aerosols             Temp      
##  Min.   :1366   Min.   :0.003100   Min.   :0.074  
##  1st Qu.:1366   1st Qu.:0.003600   1st Qu.:0.307  
##  Median :1366   Median :0.004100   Median :0.380  
##  Mean   :1366   Mean   :0.004071   Mean   :0.363  
##  3rd Qu.:1366   3rd Qu.:0.004500   3rd Qu.:0.414  
##  Max.   :1366   Max.   :0.005400   Max.   :0.601&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate &amp;lt;- 
  lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, 
     data = climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + 
##     TSI + Aerosols, data = climate_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.25888 -0.05913 -0.00082  0.05649  0.32433 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -1.246e+02  1.989e+01  -6.265 1.43e-09 ***
## MEI          6.421e-02  6.470e-03   9.923  &amp;lt; 2e-16 ***
## CO2          6.457e-03  2.285e-03   2.826  0.00505 ** 
## CH4          1.240e-04  5.158e-04   0.240  0.81015    
## N2O         -1.653e-02  8.565e-03  -1.930  0.05467 .  
## CFC.11      -6.631e-03  1.626e-03  -4.078 5.96e-05 ***
## CFC.12       3.808e-03  1.014e-03   3.757  0.00021 ***
## TSI          9.314e-02  1.475e-02   6.313 1.10e-09 ***
## Aerosols    -1.538e+00  2.133e-01  -7.210 5.41e-12 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.09171 on 275 degrees of freedom
## Multiple R-squared:  0.7509, Adjusted R-squared:  0.7436 
## F-statistic: 103.6 on 8 and 275 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model R2 (the “Multiple R-squared” value) is &lt;strong&gt;0.7509&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-our-first-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating Our First Model&lt;/h3&gt;
&lt;p&gt;Which variables are significant in the model? We will consider a variable signficant only if the p-value is below 0.05. (Select all that apply.) MEI, CO2, CFC.11, CFC.12, TSI, Aerosols&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;understanding-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Understanding the Model&lt;/h3&gt;
&lt;p&gt;Current scientific opinion is that nitrous oxide and CFC-11 are greenhouse gases: gases that are able to trap heat from the sun and contribute to the heating of the Earth. However, the regression coefficients of both the N2O and CFC-11 variables are negative, indicating that increasing atmospheric concentrations of either of these two compounds is associated with lower global temperatures.&lt;/p&gt;
&lt;p&gt;Which of the following is the simplest correct explanation for this contradiction?
All of the gas concentration variables reflect human development - N2O and CFC.11 are correlated with other variables in the dataset.&lt;/p&gt;
&lt;p&gt;Compute the correlations between all the variables in the training set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Year         Month           MEI         CO2         CH4
## Year      1.00000000 -0.0279419602 -0.0369876842  0.98274939  0.91565945
## Month    -0.02794196  1.0000000000  0.0008846905 -0.10673246  0.01856866
## MEI      -0.03698768  0.0008846905  1.0000000000 -0.04114717 -0.03341930
## CO2       0.98274939 -0.1067324607 -0.0411471651  1.00000000  0.87727963
## CH4       0.91565945  0.0185686624 -0.0334193014  0.87727963  1.00000000
## N2O       0.99384523  0.0136315303 -0.0508197755  0.97671982  0.89983864
## CFC.11    0.56910643 -0.0131112236  0.0690004387  0.51405975  0.77990402
## CFC.12    0.89701166  0.0006751102  0.0082855443  0.85268963  0.96361625
## TSI       0.17030201 -0.0346061935 -0.1544919227  0.17742893  0.24552844
## Aerosols -0.34524670  0.0148895406  0.3402377871 -0.35615480 -0.26780919
## Temp      0.78679714 -0.0998567411  0.1724707512  0.78852921  0.70325502
##                  N2O      CFC.11        CFC.12         TSI    Aerosols
## Year      0.99384523  0.56910643  0.8970116635  0.17030201 -0.34524670
## Month     0.01363153 -0.01311122  0.0006751102 -0.03460619  0.01488954
## MEI      -0.05081978  0.06900044  0.0082855443 -0.15449192  0.34023779
## CO2       0.97671982  0.51405975  0.8526896272  0.17742893 -0.35615480
## CH4       0.89983864  0.77990402  0.9636162478  0.24552844 -0.26780919
## N2O       1.00000000  0.52247732  0.8679307757  0.19975668 -0.33705457
## CFC.11    0.52247732  1.00000000  0.8689851828  0.27204596 -0.04392120
## CFC.12    0.86793078  0.86898518  1.0000000000  0.25530281 -0.22513124
## TSI       0.19975668  0.27204596  0.2553028138  1.00000000  0.05211651
## Aerosols -0.33705457 -0.04392120 -0.2251312440  0.05211651  1.00000000
## Temp      0.77863893  0.40771029  0.6875575483  0.24338269 -0.38491375
##                 Temp
## Year      0.78679714
## Month    -0.09985674
## MEI       0.17247075
## CO2       0.78852921
## CH4       0.70325502
## N2O       0.77863893
## CFC.11    0.40771029
## CFC.12    0.68755755
## TSI       0.24338269
## Aerosols -0.38491375
## Temp      1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following independent variables is N2O highly correlated with (absolute correlation greater than 0.7)? &lt;strong&gt;CO2, CH4, CFC.12&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following independent variables is CFC.11 highly correlated with? &lt;strong&gt;CH4, CFC.12&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simplifying-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simplifying the Model&lt;/h3&gt;
&lt;p&gt;Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Note, using the training set to build the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate.2 &amp;lt;- 
  lm(Temp ~ MEI + N2O + TSI + Aerosols, 
     data = climate_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Temp ~ MEI + N2O + TSI + Aerosols, data = climate_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.27916 -0.05975 -0.00595  0.05672  0.34195 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -1.162e+02  2.022e+01  -5.747 2.37e-08 ***
## MEI          6.419e-02  6.652e-03   9.649  &amp;lt; 2e-16 ***
## N2O          2.532e-02  1.311e-03  19.307  &amp;lt; 2e-16 ***
## TSI          7.949e-02  1.487e-02   5.344 1.89e-07 ***
## Aerosols    -1.702e+00  2.180e-01  -7.806 1.19e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.09547 on 279 degrees of freedom
## Multiple R-squared:  0.7261, Adjusted R-squared:  0.7222 
## F-statistic: 184.9 on 4 and 279 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of N2O in this reduced model is &lt;strong&gt;2.532e-02&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(How does this compare to the coefficient in the previous model with all of the variables?) The model R2 is &lt;strong&gt;0.7261&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automatically-building-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Automatically Building the Model&lt;/h3&gt;
&lt;p&gt;We have many variables in this analysis, and as we have seen above, dropping some from the model does not decrease model quality. R provides a function, step, that will automate the procedure of trying different combinations of variables to find a good compromise of model simplicity and R2. This trade-off is formalized by the Akaike information criterion (AIC) - it can be informally thought of as the quality of the model with a penalty for the number of variables in the model.&lt;/p&gt;
&lt;p&gt;The step function has one argument - the name of the initial model. It returns a simplified model. Using the step function in R to derive a new model, with the full model as the initial model (HINT: If your initial full model was called “climateLM”, you could create a new model with the step function by typing step(climateLM). Be sure to save your new model to a variable name so that you can look at the summary. For more information about the step function, type? step in your R console.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit.climate.step &amp;lt;- step(fit.climate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start:  AIC=-1348.16
## Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols
## 
##            Df Sum of Sq    RSS     AIC
## - CH4       1   0.00049 2.3135 -1350.1
## &amp;lt;none&amp;gt;                  2.3130 -1348.2
## - N2O       1   0.03132 2.3443 -1346.3
## - CO2       1   0.06719 2.3802 -1342.0
## - CFC.12    1   0.11874 2.4318 -1335.9
## - CFC.11    1   0.13986 2.4529 -1333.5
## - TSI       1   0.33516 2.6482 -1311.7
## - Aerosols  1   0.43727 2.7503 -1301.0
## - MEI       1   0.82823 3.1412 -1263.2
## 
## Step:  AIC=-1350.1
## Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols
## 
##            Df Sum of Sq    RSS     AIC
## &amp;lt;none&amp;gt;                  2.3135 -1350.1
## - N2O       1   0.03133 2.3448 -1348.3
## - CO2       1   0.06672 2.3802 -1344.0
## - CFC.12    1   0.13023 2.4437 -1336.5
## - CFC.11    1   0.13938 2.4529 -1335.5
## - TSI       1   0.33500 2.6485 -1313.7
## - Aerosols  1   0.43987 2.7534 -1302.7
## - MEI       1   0.83118 3.1447 -1264.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.climate.step)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + 
##     Aerosols, data = climate_train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.25770 -0.05994 -0.00104  0.05588  0.32203 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -1.245e+02  1.985e+01  -6.273 1.37e-09 ***
## MEI          6.407e-02  6.434e-03   9.958  &amp;lt; 2e-16 ***
## CO2          6.402e-03  2.269e-03   2.821 0.005129 ** 
## N2O         -1.602e-02  8.287e-03  -1.933 0.054234 .  
## CFC.11      -6.609e-03  1.621e-03  -4.078 5.95e-05 ***
## CFC.12       3.868e-03  9.812e-04   3.942 0.000103 ***
## TSI          9.312e-02  1.473e-02   6.322 1.04e-09 ***
## Aerosols    -1.540e+00  2.126e-01  -7.244 4.36e-12 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.09155 on 276 degrees of freedom
## Multiple R-squared:  0.7508, Adjusted R-squared:  0.7445 
## F-statistic: 118.8 on 7 and 276 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R2 value of the model produced by the step function is &lt;strong&gt;0.7508&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Which of the following variable(s) were eliminated from the full model by the step function? Select all that apply.&lt;/p&gt;
&lt;p&gt;It is interesting to note that the step function does not address the collinearity of the variables, except that adding highly correlated variables will not improve the R2 significantly. The consequence of this is that the step function will not necessarily produce a very interpretable model - just a model that has balanced quality and simplicity for a particular weighting of quality and simplicity (AIC).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-on-unseen-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing on Unseen Data&lt;/h3&gt;
&lt;p&gt;We have developed an understanding of how well we can fit a linear regression to the training data, but does the model quality hold when applied to unseen data?&lt;/p&gt;
&lt;p&gt;Using the model produced from the step function, calculate temperature predictions for the testing dataset, using the predict function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TempPredictions &amp;lt;- predict(fit.climate.step, newdata = climate_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;climate.SSE = sum((TempPredictions - climate_test$Temp)^2)
climate.SST = sum((climate_test$Temp - mean(climate_train$Temp))^2)
1 - climate.SSE/climate.SST&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6286051&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Testing set R2 is &lt;strong&gt;0.6286&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sales Analysis</title>
      <link>/post/sales_analysis/sales-analysis/</link>
      <pubDate>Thu, 28 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/sales_analysis/sales-analysis/</guid>
      <description>


&lt;div id=&#34;sample-sales-data-order-info-sales-customer-shipping-etc.-used-for-segmentation-customer-analytics-clustering-and-more.-inspired-for-retail-analytics.-this-was-originally-used-for-pentaho-di-kettle-but-i-found-the-set-could-be-useful-for-sales-simulation-training.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample Sales Data, Order Info, Sales, Customer, Shipping, etc., Used for Segmentation, Customer Analytics, Clustering and More. Inspired for retail analytics. This was originally used for Pentaho DI Kettle, But I found the set could be useful for Sales Simulation training.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;originally-written-by-maria-carina-roldan-pentaho-community-member-bi-consultant-assert-solutions-argentina.-this-work-is-licensed-under-the-creative-commons-attribution-noncommercial-share-alike-3.0-unported-license.-modified-by-gus-segura-june-2014.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Originally Written by María Carina Roldán, Pentaho Community Member, BI consultant (Assert Solutions), Argentina. This work is licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License. Modified by Gus Segura June 2014.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;taken-from-the-link&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Taken from the &lt;a href=&#34;https://www.kaggle.com/kyanyoga/sample-sales-data/version/1#_=_&#34;&gt;link&lt;/a&gt;&lt;/h3&gt;
&lt;div id=&#34;reading-in-a-csv-file&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reading in a csv file&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sales_data &amp;lt;- read.csv(&amp;#39;sales_data_sample.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;sales_data is a ‘data.frame’. It is the main way that R deals with tables of data.
Click on the arrow next to sales_data in the Environment pane to see the data types of each column
Click on sales_data in the Environment pane to see the table. You can also type View(sales_data) to do this.&lt;/p&gt;
&lt;p&gt;Run summary to see a summary of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   ORDERNUMBER    QUANTITYORDERED   PRICEEACH      ORDERLINENUMBER 
##  Min.   :10100   Min.   : 6.00   Min.   : 26.88   Min.   : 1.000  
##  1st Qu.:10180   1st Qu.:27.00   1st Qu.: 68.86   1st Qu.: 3.000  
##  Median :10262   Median :35.00   Median : 95.70   Median : 6.000  
##  Mean   :10259   Mean   :35.09   Mean   : 83.66   Mean   : 6.466  
##  3rd Qu.:10334   3rd Qu.:43.00   3rd Qu.:100.00   3rd Qu.: 9.000  
##  Max.   :10425   Max.   :97.00   Max.   :100.00   Max.   :18.000  
##                                                                   
##      SALES                   ORDERDATE           STATUS    
##  Min.   :  482.1   11/14/2003 0:00:  38   Cancelled :  60  
##  1st Qu.: 2203.4   11/24/2004 0:00:  35   Disputed  :  14  
##  Median : 3184.8   11/12/2003 0:00:  34   In Process:  41  
##  Mean   : 3553.9   11/17/2004 0:00:  32   On Hold   :  44  
##  3rd Qu.: 4508.0   11/4/2004 0:00 :  29   Resolved  :  47  
##  Max.   :14082.8   10/16/2004 0:00:  28   Shipped   :2617  
##                    (Other)        :2627                    
##      QTR_ID         MONTH_ID         YEAR_ID               PRODUCTLINE 
##  Min.   :1.000   Min.   : 1.000   Min.   :2003   Classic Cars    :967  
##  1st Qu.:2.000   1st Qu.: 4.000   1st Qu.:2003   Motorcycles     :331  
##  Median :3.000   Median : 8.000   Median :2004   Planes          :306  
##  Mean   :2.718   Mean   : 7.092   Mean   :2004   Ships           :234  
##  3rd Qu.:4.000   3rd Qu.:11.000   3rd Qu.:2004   Trains          : 77  
##  Max.   :4.000   Max.   :12.000   Max.   :2005   Trucks and Buses:301  
##                                                  Vintage Cars    :607  
##       MSRP         PRODUCTCODE                         CUSTOMERNAME 
##  Min.   : 33.0   S18_3232:  52   Euro Shopping Channel       : 259  
##  1st Qu.: 68.0   S10_1949:  28   Mini Gifts Distributors Ltd.: 180  
##  Median : 99.0   S10_4962:  28   Australian Collectors, Co.  :  55  
##  Mean   :100.7   S12_1666:  28   La Rochelle Gifts           :  53  
##  3rd Qu.:124.0   S18_1097:  28   AV Stores, Co.              :  51  
##  Max.   :214.0   S18_2432:  28   Land of Toys Inc.           :  49  
##                  (Other) :2631   (Other)                     :2176  
##             PHONE                            ADDRESSLINE1 
##  (91) 555 94 44: 259   C/ Moralzarzal, 86          : 259  
##  4155551450    : 180   5677 Strong St.             : 180  
##  03 9520 4555  :  55   636 St Kilda Road           :  55  
##  40.67.8555    :  53   67, rue des Cinquante Otages:  53  
##  (171) 555-1555:  51   Fauntleroy Circus           :  51  
##  6175558555    :  51   897 Long Airport Avenue     :  49  
##  (Other)       :2174   (Other)                     :2176  
##     ADDRESSLINE2             CITY           STATE        POSTALCODE  
##           :2521   Madrid       : 304           :1486   28034  : 259  
##  Level 3  :  55   San Rafael   : 180   CA      : 416   97562  : 205  
##  Suite 400:  48   NYC          : 152   MA      : 190   10022  : 152  
##  Level 15 :  46   Singapore    :  79   NY      : 178   94217  :  89  
##  Level 6  :  46   Paris        :  70   NSW     :  92          :  76  
##  2nd Floor:  36   San Francisco:  62   Victoria:  78   50553  :  61  
##  (Other)  :  71   (Other)      :1976   (Other) : 383   (Other):1981  
##       COUNTRY     TERRITORY    CONTACTLASTNAME CONTACTFIRSTNAME
##  USA      :1004   APAC : 221   Freyre : 259    Diego  : 259    
##  Spain    : 342   EMEA :1407   Nelson : 204    Valarie: 257    
##  France   : 314   Japan: 121   Young  : 115    Julie  : 117    
##  Australia: 185   NA&amp;#39;s :1074   Frick  :  91    Michael:  84    
##  UK       : 144                Brown  :  88    Sue    :  84    
##  Italy    : 113                Yu     :  80    Juri   :  60    
##  (Other)  : 721                (Other):1986    (Other):1962    
##    DEALSIZE   
##  Large : 157  
##  Medium:1384  
##  Small :1282  
##               
##               
##               
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or can run summary on individual columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   26.88   68.86   95.70   83.66  100.00  100.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$PRODUCTLINE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Classic Cars      Motorcycles           Planes            Ships 
##              967              331              306              234 
##           Trains Trucks and Buses     Vintage Cars 
##               77              301              607&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14082.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3553.889&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;min(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 482.13&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(sales_data$SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1841.865&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will now go through select(), arrange(), filter(), mutate(), group_by(), summarise()&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;select() function
select specific columns
first argument is always the dataset, and each argument after is the fields you want&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(sales_data, QUANTITYORDERED, PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      QUANTITYORDERED PRICEEACH
## 1                 30     95.70
## 2                 34     81.35
## 3                 41     94.74
## 4                 45     83.26
## 5                 49    100.00
## 6                 36     96.66
## 7                 29     86.13
## 8                 48    100.00
## 9                 22     98.57
## 10                41    100.00
## 11                37    100.00
## 12                23    100.00
## 13                28    100.00
## 14                34    100.00
## 15                45     92.83
## 16                36    100.00
## 17                23    100.00
## 18                41    100.00
## 19                46     94.74
## 20                42    100.00
## 21                41    100.00
## 22                20     72.55
## 23                21     34.91
## 24                42     76.36
## 25                24    100.00
## 26                66    100.00
## 27                26    100.00
## 28                29    100.00
## 29                38    100.00
## 30                37    100.00
## 31                45    100.00
## 32                21    100.00
## 33                34    100.00
## 34                23    100.00
## 35                42    100.00
## 36                47    100.00
## 37                35    100.00
## 38                29    100.00
## 39                34    100.00
## 40                32    100.00
## 41                21    100.00
## 42                34    100.00
## 43                37    100.00
## 44                47    100.00
## 45                48    100.00
## 46                40    100.00
## 47                26    100.00
## 48                30    100.00
## 49                32    100.00
## 50                41    100.00
## 51                36    100.00
## 52                24    100.00
## 53                23    100.00
## 54                50    100.00
## 55                39     99.91
## 56                29     96.34
## 57                27    100.00
## 58                37    100.00
## 59                37    100.00
## 60                27    100.00
## 61                42    100.00
## 62                38     96.34
## 63                24    100.00
## 64                23    100.00
## 65                47    100.00
## 66                22    100.00
## 67                44    100.00
## 68                40    100.00
## 69                22    100.00
## 70                47    100.00
## 71                39     96.34
## 72                34    100.00
## 73                45    100.00
## 74                20    100.00
## 75                40     68.92
## 76                26     51.15
## 77                39    100.00
## 78                50     44.51
## 79                45    100.00
## 80                45    100.00
## 81                27    100.00
## 82                46    100.00
## 83                31    100.00
## 84                33    100.00
## 85                22    100.00
## 86                20    100.00
## 87                41    100.00
## 88                45    100.00
## 89                49    100.00
## 90                34    100.00
## 91                49    100.00
## 92                39    100.00
## 93                43    100.00
## 94                41    100.00
## 95                36    100.00
## 96                27    100.00
## 97                29    100.00
## 98                20    100.00
## 99                37    100.00
## 100               26    100.00
## 101               39     76.67
## 102               22    100.00
## 103               22    100.00
## 104               21     86.77
## 105               66    100.00
## 106               56    100.00
## 107               50    100.00
## 108               46    100.00
## 109               33    100.00
## 110               49    100.00
## 111               32    100.00
## 112               44    100.00
## 113               24    100.00
## 114               26    100.00
## 115               45    100.00
## 116               39    100.00
## 117               49    100.00
## 118               20    100.00
## 119               27    100.00
## 120               30    100.00
## 121               25    100.00
## 122               24    100.00
## 123               22    100.00
## 124               33    100.00
## 125               47     64.93
## 126               25     48.05
## 127               26     75.47
## 128               48     54.68
## 129               39    100.00
## 130               34    100.00
## 131               32    100.00
## 132               64    100.00
## 133               19    100.00
## 134               42    100.00
## 135               31    100.00
## 136               22    100.00
## 137               26    100.00
## 138               20    100.00
## 139               21    100.00
## 140               33    100.00
## 141               28    100.00
## 142               26    100.00
## 143               31    100.00
## 144               48    100.00
## 145               50    100.00
## 146               28    100.00
## 147               26    100.00
## 148               32    100.00
## 149               44    100.00
## 150               30    100.00
## 151               38    100.00
## 152               40    100.00
## 153               46     61.99
## 154               26    100.00
## 155               27    100.00
## 156               43    100.00
## 157               35     65.63
## 158               37    100.00
## 159               37     46.90
## 160               27    100.00
## 161               38    100.00
## 162               33    100.00
## 163               42    100.00
## 164               42    100.00
## 165               48    100.00
## 166               41    100.00
## 167               30    100.00
## 168               27    100.00
## 169               21    100.00
## 170               20    100.00
## 171               41    100.00
## 172               27    100.00
## 173               28    100.00
## 174               24    100.00
## 175               44    100.00
## 176               50    100.00
## 177               21    100.00
## 178               33    100.00
## 179               33    100.00
## 180               31    100.00
## 181               41     71.47
## 182               45     79.65
## 183               33     85.39
## 184               45     76.00
## 185               26     99.04
## 186               12    100.00
## 187               41    100.00
## 188               33    100.00
## 189               46    100.00
## 190               33    100.00
## 191               20    100.00
## 192               44    100.00
## 193               33    100.00
## 194               21    100.00
## 195               47    100.00
## 196               46    100.00
## 197               32    100.00
## 198               42    100.00
## 199               44    100.00
## 200               35    100.00
## 201               41    100.00
## 202               46    100.00
## 203               31    100.00
## 204               38    100.00
## 205               42     64.00
## 206               33     57.22
## 207               48     52.36
## 208               42    100.00
## 209               32    100.00
## 210               34    100.00
## 211               33     69.12
## 212               36    100.00
## 213               27    100.00
## 214               21    100.00
## 215               21    100.00
## 216               38    100.00
## 217               30    100.00
## 218               49    100.00
## 219               43    100.00
## 220               41    100.00
## 221               38    100.00
## 222               28    100.00
## 223               43    100.00
## 224               25    100.00
## 225               38    100.00
## 226               41    100.00
## 227               28    100.00
## 228               25    100.00
## 229               41    100.00
## 230               39    100.00
## 231               21    100.00
## 232               27    100.00
## 233               33     99.21
## 234               29    100.00
## 235               49    100.00
## 236               49    100.00
## 237               20    100.00
## 238               39     63.20
## 239               40    100.00
## 240               49    100.00
## 241               21    100.00
## 242               50    100.00
## 243               20    100.00
## 244               49    100.00
## 245               38    100.00
## 246               35    100.00
## 247               40    100.00
## 248               28    100.00
## 249               25    100.00
## 250               36    100.00
## 251               43    100.00
## 252               32    100.00
## 253               46    100.00
## 254               48    100.00
## 255               43    100.00
## 256               49    100.00
## 257               24    100.00
## 258               26    100.00
## 259               30    100.00
## 260               24    100.00
## 261               55    100.00
## 262               22    100.00
## 263               49     78.92
## 264               44    100.00
## 265               66    100.00
## 266               21    100.00
## 267               34    100.00
## 268               43    100.00
## 269               46    100.00
## 270               33    100.00
## 271               42    100.00
## 272               34    100.00
## 273               47    100.00
## 274               33    100.00
## 275               24    100.00
## 276               26    100.00
## 277               30    100.00
## 278               43    100.00
## 279               25    100.00
## 280               27    100.00
## 281               27    100.00
## 282               24    100.00
## 283               34    100.00
## 284               46    100.00
## 285               27     54.33
## 286               33    100.00
## 287               47    100.00
## 288               49     55.34
## 289               40    100.00
## 290               37    100.00
## 291               47    100.00
## 292               45    100.00
## 293               37     99.82
## 294               48    100.00
## 295               31    100.00
## 296               46    100.00
## 297               47    100.00
## 298               28    100.00
## 299               40    100.00
## 300               20    100.00
## 301               39    100.00
## 302               25     99.82
## 303               29    100.00
## 304               22    100.00
## 305               22    100.00
## 306               47    100.00
## 307               45    100.00
## 308               29    100.00
## 309               24    100.00
## 310               35    100.00
## 311               46     83.63
## 312               44     95.93
## 313               34     96.73
## 314               35    100.00
## 315               25     72.38
## 316               10    100.00
## 317               29    100.00
## 318               39    100.00
## 319               42    100.00
## 320               46    100.00
## 321               49    100.00
## 322               27    100.00
## 323               50    100.00
## 324               43    100.00
## 325               38    100.00
## 326               20    100.00
## 327               27    100.00
## 328               49    100.00
## 329               27    100.00
## 330               39    100.00
## 331               24    100.00
## 332               45    100.00
## 333               20    100.00
## 334               36    100.00
## 335               24    100.00
## 336               49     63.38
## 337               26    100.00
## 338               49     62.09
## 339               34    100.00
## 340               34     95.35
## 341               33    100.00
## 342               22    100.00
## 343               39     89.38
## 344               32     63.84
## 345               24     75.01
## 346               21     63.84
## 347               24     73.42
## 348               36     63.84
## 349               20     81.40
## 350               30     64.64
## 351               44     82.99
## 352               28     92.57
## 353               37     77.41
## 354               20     74.21
## 355               25     90.17
## 356               35     76.61
## 357               38     83.79
## 358               41     69.43
## 359               22     76.61
## 360               49     81.40
## 361               38     73.42
## 362               33    100.00
## 363               36     93.56
## 364               34     81.62
## 365               24     67.83
## 366               36     70.26
## 367               34     90.17
## 368               41    100.00
## 369               46    100.00
## 370               24    100.00
## 371               21    100.00
## 372               24    100.00
## 373               48    100.00
## 374               26    100.00
## 375               37    100.00
## 376               49    100.00
## 377               34     99.54
## 378               48    100.00
## 379               36    100.00
## 380               46    100.00
## 381               46    100.00
## 382               31     97.17
## 383               41    100.00
## 384               21    100.00
## 385               38    100.00
## 386               45    100.00
## 387               26     58.38
## 388               38    100.00
## 389               48    100.00
## 390               42     64.16
## 391               49     35.71
## 392               32     66.58
## 393               54    100.00
## 394               33    100.00
## 395               36    100.00
## 396               20    100.00
## 397               29     97.89
## 398               33     97.89
## 399               50    100.00
## 400               41    100.00
## 401               36    100.00
## 402               27    100.00
## 403               47    100.00
## 404               33    100.00
## 405               21    100.00
## 406               21     93.28
## 407               41    100.00
## 408               40    100.00
## 409               28    100.00
## 410               23    100.00
## 411               23    100.00
## 412               25    100.00
## 413               24    100.00
## 414               39     64.74
## 415               55     75.20
## 416               46     88.45
## 417               50    100.00
## 418               47    100.00
## 419               97     93.28
## 420               32    100.00
## 421               35    100.00
## 422               49    100.00
## 423               38    100.00
## 424               32    100.00
## 425               34    100.00
## 426               36     99.17
## 427               48     93.34
## 428               21     96.84
## 429               21     93.34
## 430               34    100.00
## 431               46    100.00
## 432               32    100.00
## 433               29    100.00
## 434               41    100.00
## 435               43     96.84
## 436               24    100.00
## 437               41    100.00
## 438               46     98.00
## 439               32    100.00
## 440               22    100.00
## 441               29     40.25
## 442               42     49.60
## 443               39     98.00
## 444               27    100.00
## 445               48     98.00
## 446               29     85.10
## 447               27    100.00
## 448               54    100.00
## 449               26    100.00
## 450               34    100.00
## 451               25    100.00
## 452               23    100.00
## 453               28    100.00
## 454               35    100.00
## 455               44    100.00
## 456               22    100.00
## 457               42    100.00
## 458               29    100.00
## 459               32    100.00
## 460               41    100.00
## 461               26    100.00
## 462               21    100.00
## 463               34    100.00
## 464               41    100.00
## 465               37    100.00
## 466               37    100.00
## 467               41    100.00
## 468               46    100.00
## 469               40    100.00
## 470               43     97.60
## 471               30     87.06
## 472               35    100.00
## 473               36     93.77
## 474               61    100.00
## 475               38    100.00
## 476               39    100.00
## 477               33     99.66
## 478               32    100.00
## 479               31    100.00
## 480               50    100.00
## 481               48     91.44
## 482               43    100.00
## 483               25     87.33
## 484               28    100.00
## 485               36    100.00
## 486               27     89.38
## 487               25    100.00
## 488               40    100.00
## 489               34     95.55
## 490               50    100.00
## 491               38    100.00
## 492               37     95.55
## 493               43     89.38
## 494               43     86.30
## 495               46     95.13
## 496               42     36.11
## 497               50     50.18
## 498               44    100.00
## 499               27     93.16
## 500               35    100.00
## 501               51     95.55
## 502               41     50.14
## 503               48     49.06
## 504               42     54.99
## 505               49     43.13
## 506               30     58.22
## 507               45     51.21
## 508               48     44.21
## 509               32     54.45
## 510               46     53.37
## 511               48     63.61
## 512               33     43.13
## 513               31     48.52
## 514               20     58.22
## 515               29     51.75
## 516               27     57.68
## 517               24     56.07
## 518               37     48.52
## 519               25     44.21
## 520               41     57.68
## 521               27     89.89
## 522               21     58.95
## 523               22     72.41
## 524               32     98.63
## 525               25     52.83
## 526               42    100.00
## 527               25     51.75
## 528               37    100.00
## 529               26    100.00
## 530               44     99.55
## 531               47    100.00
## 532               43    100.00
## 533               42    100.00
## 534               42    100.00
## 535               29    100.00
## 536               40    100.00
## 537               38    100.00
## 538               38    100.00
## 539               21    100.00
## 540               24    100.00
## 541               36    100.00
## 542               23    100.00
## 543               20    100.00
## 544               32    100.00
## 545               29    100.00
## 546               44    100.00
## 547               44    100.00
## 548               36    100.00
## 549               49     56.30
## 550               34     42.64
## 551               59    100.00
## 552               37    100.00
## 553               36    100.00
## 554               43    100.00
## 555               21    100.00
## 556               32    100.00
## 557               38    100.00
## 558               43    100.00
## 559               42    100.00
## 560               32    100.00
## 561               42    100.00
## 562               31    100.00
## 563               49    100.00
## 564               45    100.00
## 565               49    100.00
## 566               41    100.00
## 567               45    100.00
## 568               36    100.00
## 569               39    100.00
## 570               27    100.00
## 571               25    100.00
## 572               41    100.00
## 573               39     99.52
## 574               28     57.55
## 575               25     54.57
## 576               33    100.00
## 577               34    100.00
## 578               24    100.00
## 579               30    100.00
## 580               42    100.00
## 581               21    100.00
## 582               34    100.00
## 583               29    100.00
## 584               24    100.00
## 585               44    100.00
## 586               21    100.00
## 587               33    100.00
## 588               30    100.00
## 589               26    100.00
## 590               41    100.00
## 591               26    100.00
## 592               32    100.00
## 593               43    100.00
## 594               48    100.00
## 595               44     74.04
## 596               45    100.00
## 597               37    100.00
## 598               39    100.00
## 599               76    100.00
## 600               37    100.00
## 601               38     82.39
## 602               43     72.38
## 603               48     79.31
## 604               26     82.39
## 605               38     88.55
## 606               20     63.14
## 607               22     73.92
## 608               45     90.86
## 609               45     85.47
## 610               20     66.99
## 611               47     64.68
## 612               46     73.92
## 613               23     83.93
## 614               33     74.69
## 615               29     90.86
## 616               44     82.39
## 617               41     92.40
## 618               20     91.63
## 619               37     78.54
## 620               29    100.00
## 621               55     65.45
## 622               22    100.00
## 623               31     67.76
## 624               49     79.22
## 625               61     73.92
## 626               39     83.93
## 627               38    100.00
## 628               31    100.00
## 629               36    100.00
## 630               25    100.00
## 631               48    100.00
## 632               35    100.00
## 633               21    100.00
## 634               47    100.00
## 635               38    100.00
## 636               41    100.00
## 637               24    100.00
## 638               37    100.00
## 639               33    100.00
## 640               49    100.00
## 641               29    100.00
## 642               24    100.00
## 643               47    100.00
## 644               24    100.00
## 645               25    100.00
## 646               30     32.47
## 647               22    100.00
## 648               27     64.69
## 649               34    100.00
## 650               36    100.00
## 651               34     43.05
## 652               48    100.00
## 653               34    100.00
## 654               24    100.00
## 655               46    100.00
## 656               45    100.00
## 657               39    100.00
## 658               43    100.00
## 659               29    100.00
## 660               20    100.00
## 661               46    100.00
## 662               27    100.00
## 663               44    100.00
## 664               43    100.00
## 665               49    100.00
## 666               40    100.00
## 667               30    100.00
## 668               50    100.00
## 669               23    100.00
## 670               26    100.00
## 671               27    100.00
## 672               42    100.00
## 673               47    100.00
## 674               49    100.00
## 675               38    100.00
## 676               20    100.00
## 677               25    100.00
## 678               25     88.00
## 679               41    100.00
## 680               28    100.00
## 681               50     67.80
## 682               32     50.25
## 683               42     53.88
## 684               24     62.36
## 685               27     69.62
## 686               26     57.51
## 687               38     61.15
## 688               42     59.33
## 689               23     71.44
## 690               21     62.96
## 691               28     50.85
## 692               33     72.65
## 693               25     62.96
## 694               28     61.75
## 695               46     49.04
## 696               30     61.15
## 697               38     84.25
## 698               40     56.91
## 699               45    100.00
## 700               27     49.30
## 701               42     72.65
## 702               36     63.57
## 703               29    100.00
## 704               39    100.00
## 705               45    100.00
## 706               47    100.00
## 707               49    100.00
## 708               46    100.00
## 709               48    100.00
## 710               46    100.00
## 711               35    100.00
## 712               43    100.00
## 713               26    100.00
## 714               22     98.18
## 715               34     99.41
## 716               50    100.00
## 717               48    100.00
## 718               41    100.00
## 719               36    100.00
## 720               29    100.00
## 721               33     37.48
## 722               46    100.00
## 723               38    100.00
## 724               20     36.42
## 725               22    100.00
## 726               27    100.00
## 727               56     98.18
## 728               38     99.41
## 729               25    100.00
## 730               33    100.00
## 731               42    100.00
## 732               33    100.00
## 733               38    100.00
## 734               31    100.00
## 735               20    100.00
## 736               44    100.00
## 737               26    100.00
## 738               27    100.00
## 739               46    100.00
## 740               47    100.00
## 741               37    100.00
## 742               31    100.00
## 743               24    100.00
## 744               31    100.00
## 745               50    100.00
## 746               35     64.69
## 747               30    100.00
## 748               29    100.00
## 749               27    100.00
## 750               40    100.00
## 751               31     98.99
## 752                6    100.00
## 753               45    100.00
## 754               22     54.09
## 755               45     68.67
## 756               43     65.02
## 757               46     61.99
## 758               39     69.28
## 759               31     71.10
## 760               41     69.28
## 761               44     60.16
## 762               45     70.49
## 763               37     69.89
## 764               35     61.38
## 765               28     59.55
## 766               30     61.99
## 767               30     49.22
## 768               25     69.28
## 769               29     57.73
## 770               26     57.73
## 771               41     53.48
## 772               34     52.87
## 773               35     61.21
## 774               34     61.38
## 775               50    100.00
## 776               41     61.99
## 777               22     96.86
## 778               35     48.62
## 779               44     38.50
## 780               47     61.99
## 781               19     49.22
## 782               34     90.39
## 783               29     71.81
## 784               49     69.27
## 785               30     85.32
## 786               21     70.96
## 787               50     76.88
## 788               47    100.00
## 789               24     76.03
## 790               27     98.84
## 791               33     86.17
## 792               35     90.39
## 793               31     71.81
## 794               25     82.79
## 795               27     82.79
## 796               31    100.00
## 797               45    100.00
## 798               27    100.00
## 799               27    100.00
## 800               42     69.27
## 801               21     74.77
## 802               34     76.88
## 803               42     76.03
## 804               15     98.84
## 805               29     70.87
## 806               46     58.15
## 807               30     61.78
## 808               30     49.67
## 809               42     51.48
## 810               46     61.18
## 811               25     64.20
## 812               32     65.42
## 813               30     64.81
## 814               40     49.67
## 815               28     60.57
## 816               23     55.72
## 817               29     61.18
## 818               34     58.75
## 819               37     63.60
## 820               20     49.06
## 821               32     48.46
## 822               34     52.09
## 823               42     52.70
## 824               38    100.00
## 825               30     62.16
## 826               23     49.67
## 827               22     53.30
## 828               39    100.00
## 829               55     55.72
## 830               36     61.18
## 831               26    100.00
## 832               31    100.00
## 833               34    100.00
## 834               41    100.00
## 835               23    100.00
## 836               48    100.00
## 837               22    100.00
## 838               21    100.00
## 839               22    100.00
## 840               40    100.00
## 841               50    100.00
## 842               29    100.00
## 843               43    100.00
## 844               24    100.00
## 845               22    100.00
## 846               43    100.00
## 847               20    100.00
## 848               25    100.00
## 849               36    100.00
## 850               24     52.67
## 851               21    100.00
## 852               30    100.00
## 853               32     94.79
## 854               21     47.18
## 855               26     78.11
## 856               35    100.00
## 857               26    100.00
## 858               46    100.00
## 859               37    100.00
## 860               27    100.00
## 861               23    100.00
## 862               39    100.00
## 863               27    100.00
## 864               38    100.00
## 865               27    100.00
## 866               40    100.00
## 867               24    100.00
## 868               44    100.00
## 869               37    100.00
## 870               20    100.00
## 871               39    100.00
## 872               44    100.00
## 873               22    100.00
## 874               43    100.00
## 875               27    100.00
## 876               26     64.90
## 877               25     52.32
## 878               49    100.00
## 879               29    100.00
## 880               41    100.00
## 881               55    100.00
## 882               27     83.07
## 883               23    100.00
## 884               31     90.17
## 885               46    100.00
## 886               47     91.18
## 887               31    100.00
## 888               46    100.00
## 889               37     89.15
## 890               28     93.21
## 891               37     90.17
## 892               49    100.00
## 893               24    100.00
## 894               30    100.00
## 895               50     88.14
## 896               31     96.24
## 897               46    100.00
## 898               47    100.00
## 899               46    100.00
## 900               37    100.00
## 901               33    100.00
## 902               31     90.17
## 903               48    100.00
## 904               41     87.13
## 905               42    100.00
## 906               41    100.00
## 907               32     45.25
## 908               10     88.14
## 909               35     57.46
## 910               28     64.33
## 911               46     73.70
## 912               20     71.20
## 913               30     49.97
## 914               48     69.96
## 915               28     53.72
## 916               39     68.08
## 917               24     51.84
## 918               28     67.46
## 919               31     58.71
## 920               45     63.71
## 921               24     58.09
## 922               49     53.72
## 923               32     63.08
## 924               43     68.71
## 925               37     50.59
## 926               24     64.96
## 927               35     53.72
## 928               41     29.87
## 929               26    100.00
## 930               34     64.96
## 931               49     70.58
## 932               28     44.21
## 933               40     68.08
## 934               37     59.96
## 935               31     53.72
## 936               41     83.44
## 937               21     89.46
## 938               40     96.34
## 939               46     74.84
## 940               44     79.14
## 941               46     73.12
## 942               41     81.72
## 943               32     89.46
## 944               46     87.74
## 945               28    100.00
## 946               49     94.62
## 947               21     73.98
## 948               32     84.30
## 949               34     98.06
## 950               21     98.06
## 951               21     96.34
## 952               31     83.44
## 953               21     94.62
## 954               25     45.86
## 955               28     82.58
## 956               43     64.97
## 957               22     86.74
## 958               37     93.01
## 959               28     72.26
## 960               30     74.84
## 961               44     73.98
## 962               25    100.00
## 963               43    100.00
## 964               30     97.39
## 965               20     90.06
## 966               26    100.00
## 967               40    100.00
## 968               31     89.01
## 969               22    100.00
## 970               23    100.00
## 971               30    100.00
## 972               49    100.00
## 973               31    100.00
## 974               29    100.00
## 975               37     84.82
## 976               38    100.00
## 977               29    100.00
## 978               23    100.00
## 979               26     85.87
## 980               38    100.00
## 981               48     47.04
## 982               40     39.80
## 983               45    100.00
## 984               44    100.00
## 985               21     94.22
## 986               35    100.00
## 987               29     86.92
## 988               21     84.82
## 989               22    100.00
## 990               26    100.00
## 991               41    100.00
## 992               47    100.00
## 993               31    100.00
## 994               43    100.00
## 995               23    100.00
## 996               28    100.00
## 997               49    100.00
## 998               24    100.00
## 999               33    100.00
## 1000              22    100.00
## 1001              32    100.00
## 1002              40    100.00
## 1003              43    100.00
## 1004              24    100.00
## 1005              32    100.00
## 1006              20    100.00
## 1007              24     69.12
## 1008              48    100.00
## 1009              44    100.00
## 1010              28    100.00
## 1011              24     61.52
## 1012              33    100.00
## 1013              41    100.00
## 1014              23    100.00
## 1015              46    100.00
## 1016              48    100.00
## 1017              25    100.00
## 1018              22    100.00
## 1019              41    100.00
## 1020              34    100.00
## 1021              32    100.00
## 1022              21    100.00
## 1023              20    100.00
## 1024              47    100.00
## 1025              39    100.00
## 1026              29    100.00
## 1027              45    100.00
## 1028              28    100.00
## 1029              26    100.00
## 1030              50    100.00
## 1031              48    100.00
## 1032              25    100.00
## 1033              40    100.00
## 1034              43    100.00
## 1035              22    100.00
## 1036              47    100.00
## 1037              36    100.00
## 1038              40    100.00
## 1039              27    100.00
## 1040              29    100.00
## 1041              20    100.00
## 1042              42    100.00
## 1043              25    100.00
## 1044              36    100.00
## 1045              21    100.00
## 1046              23    100.00
## 1047              37    100.00
## 1048              48    100.00
## 1049              25    100.00
## 1050              33    100.00
## 1051              27    100.00
## 1052              27    100.00
## 1053              20    100.00
## 1054              30    100.00
## 1055              48    100.00
## 1056              32     93.49
## 1057              34    100.00
## 1058              27     56.85
## 1059              39    100.00
## 1060              47    100.00
## 1061              22    100.00
## 1062              55    100.00
## 1063              60    100.00
## 1064              35    100.00
## 1065              28    100.00
## 1066              38    100.00
## 1067              21     95.80
## 1068              41    100.00
## 1069              22     97.81
## 1070              29     88.74
## 1071              50    100.00
## 1072              29    100.00
## 1073              49     80.67
## 1074              35    100.00
## 1075              48    100.00
## 1076              23     80.67
## 1077              48     95.80
## 1078              42    100.00
## 1079              47    100.00
## 1080              36    100.00
## 1081              22    100.00
## 1082              40     91.76
## 1083              23    100.00
## 1084              32    100.00
## 1085              21    100.00
## 1086              41     93.04
## 1087              25     84.71
## 1088              26    100.00
## 1089              24     89.75
## 1090              48    100.00
## 1091              26     68.35
## 1092              21     73.17
## 1093              45     78.00
## 1094              36     86.04
## 1095              21     81.21
## 1096              32     70.76
## 1097              30     82.82
## 1098              36     94.88
## 1099              33     86.04
## 1100              35     78.00
## 1101              37     95.69
## 1102              41     73.17
## 1103              20     76.39
## 1104              45     86.84
## 1105              38     69.96
## 1106              43     70.76
## 1107              49     78.80
## 1108              27     80.41
## 1109              46     73.98
## 1110              38     59.10
## 1111              25     66.74
## 1112              46     60.30
## 1113              22    100.00
## 1114              40    100.00
## 1115              46    100.00
## 1116              39    100.00
## 1117              38     82.34
## 1118              30    100.00
## 1119              42     94.25
## 1120              43    100.00
## 1121              29     95.24
## 1122              33     86.31
## 1123              32     79.37
## 1124              28     87.30
## 1125              41    100.00
## 1126              33    100.00
## 1127              36     84.33
## 1128              26     89.29
## 1129              34    100.00
## 1130              26     96.23
## 1131              38    100.00
## 1132              33    100.00
## 1133              33     91.27
## 1134              46    100.00
## 1135              26    100.00
## 1136              25    100.00
## 1137              45     73.08
## 1138              50    100.00
## 1139              36    100.00
## 1140              21     89.29
## 1141              29    100.00
## 1142              21    100.00
## 1143              42    100.00
## 1144              37    100.00
## 1145              25    100.00
## 1146              36    100.00
## 1147              22    100.00
## 1148              23    100.00
## 1149              32    100.00
## 1150              28    100.00
## 1151              27    100.00
## 1152              49    100.00
## 1153              41    100.00
## 1154              49    100.00
## 1155              30    100.00
## 1156              40    100.00
## 1157              23    100.00
## 1158              49    100.00
## 1159              25    100.00
## 1160              37    100.00
## 1161              55    100.00
## 1162              23    100.00
## 1163              24    100.00
## 1164              43     96.49
## 1165              50    100.00
## 1166              47    100.00
## 1167              34    100.00
## 1168              31    100.00
## 1169              28    100.00
## 1170              36    100.00
## 1171              48    100.00
## 1172              39    100.00
## 1173              45    100.00
## 1174              35    100.00
## 1175              45    100.00
## 1176              46    100.00
## 1177              37    100.00
## 1178              31    100.00
## 1179              33    100.00
## 1180              31    100.00
## 1181              27    100.00
## 1182              39    100.00
## 1183              32    100.00
## 1184              28    100.00
## 1185              26     67.91
## 1186              44     84.88
## 1187              46    100.00
## 1188              32     70.83
## 1189              65    100.00
## 1190              43    100.00
## 1191              43     67.77
## 1192              35     49.74
## 1193              45     50.36
## 1194              47     67.14
## 1195              21     64.66
## 1196              38     68.39
## 1197              21     50.36
## 1198              43     72.74
## 1199              46     54.09
## 1200              38     58.44
## 1201              26     52.22
## 1202              31     52.84
## 1203              48     54.71
## 1204              33     50.36
## 1205              38     57.20
## 1206              39     55.95
## 1207              42     67.14
## 1208              44     59.06
## 1209              29     69.63
## 1210              26     55.95
## 1211              31     53.47
## 1212              32     89.12
## 1213              28    100.00
## 1214              36    100.00
## 1215              36     52.22
## 1216              41    100.00
## 1217              27     99.52
## 1218              33    100.00
## 1219              34    100.00
## 1220              29    100.00
## 1221              34    100.00
## 1222              48    100.00
## 1223              46    100.00
## 1224              22    100.00
## 1225              20    100.00
## 1226              45     85.75
## 1227              46    100.00
## 1228              34    100.00
## 1229              50     85.75
## 1230              46    100.00
## 1231              22     84.70
## 1232              48     86.81
## 1233              47     86.81
## 1234              34    100.00
## 1235              45    100.00
## 1236              20    100.00
## 1237              50     60.49
## 1238              22     57.55
## 1239              45    100.00
## 1240              58    100.00
## 1241              51    100.00
## 1242              38    100.00
## 1243              22    100.00
## 1244              25    100.00
## 1245              24    100.00
## 1246              35    100.00
## 1247              28    100.00
## 1248              36    100.00
## 1249              39    100.00
## 1250              27    100.00
## 1251              40    100.00
## 1252              50    100.00
## 1253              42    100.00
## 1254              48    100.00
## 1255              25    100.00
## 1256              31    100.00
## 1257              44    100.00
## 1258              23    100.00
## 1259              29    100.00
## 1260              49    100.00
## 1261              36    100.00
## 1262              34    100.00
## 1263              25    100.00
## 1264              48    100.00
## 1265              38    100.00
## 1266              37    100.00
## 1267              49    100.00
## 1268              22     86.51
## 1269              28     89.27
## 1270              36     85.59
## 1271              34    100.00
## 1272              39    100.00
## 1273              21     75.46
## 1274              36    100.00
## 1275              24     97.55
## 1276              29     85.59
## 1277              38     94.79
## 1278              34    100.00
## 1279              42     90.19
## 1280              35    100.00
## 1281              35     80.99
## 1282              38     89.27
## 1283              41     81.91
## 1284              50    100.00
## 1285              21    100.00
## 1286              43     62.72
## 1287              32    100.00
## 1288               6     90.19
## 1289              66     92.95
## 1290              41     82.50
## 1291              23     97.42
## 1292              43     92.16
## 1293              24     70.22
## 1294              22     83.38
## 1295              26     73.73
## 1296              35     74.60
## 1297              47     77.24
## 1298              50    100.00
## 1299              45     87.77
## 1300              39     89.53
## 1301              23     89.53
## 1302              42     75.48
## 1303              20     89.53
## 1304              33     71.09
## 1305              34    100.00
## 1306              49    100.00
## 1307              39     90.40
## 1308              36    100.00
## 1309              50     86.01
## 1310              29    100.00
## 1311              30    100.00
## 1312              41     86.89
## 1313              28     58.58
## 1314              45    100.00
## 1315              16     75.48
## 1316              36    100.00
## 1317              41    100.00
## 1318              50    100.00
## 1319              40    100.00
## 1320              49    100.00
## 1321              45    100.00
## 1322              47    100.00
## 1323              21    100.00
## 1324              32    100.00
## 1325              47    100.00
## 1326              38    100.00
## 1327              41    100.00
## 1328              21    100.00
## 1329              41    100.00
## 1330              38    100.00
## 1331              25     99.29
## 1332              48    100.00
## 1333              22     99.29
## 1334              28    100.00
## 1335              47    100.00
## 1336              49    100.00
## 1337              45    100.00
## 1338              28    100.00
## 1339              29     57.53
## 1340              39    100.00
## 1341              46    100.00
## 1342              38    100.00
## 1343              41     47.29
## 1344              50     49.81
## 1345              43     53.83
## 1346              29     43.27
## 1347              30     42.76
## 1348              25     53.83
## 1349              49     44.78
## 1350              40     49.30
## 1351              41     44.78
## 1352              21     53.33
## 1353              46     45.28
## 1354              39     40.25
## 1355              45     59.87
## 1356              21     59.87
## 1357              44     58.36
## 1358              44     59.87
## 1359              29     51.82
## 1360              34     49.30
## 1361              39     56.85
## 1362              38    100.00
## 1363              24     79.86
## 1364              29    100.00
## 1365              30    100.00
## 1366              20    100.00
## 1367              39    100.00
## 1368              35     59.87
## 1369              26     59.87
## 1370              44    100.00
## 1371              28    100.00
## 1372              31    100.00
## 1373              29    100.00
## 1374              32    100.00
## 1375              33    100.00
## 1376              44    100.00
## 1377              32    100.00
## 1378              41    100.00
## 1379              35    100.00
## 1380              44    100.00
## 1381              26    100.00
## 1382              20    100.00
## 1383              48    100.00
## 1384              34    100.00
## 1385              49    100.00
## 1386              40    100.00
## 1387              45    100.00
## 1388              50    100.00
## 1389              38    100.00
## 1390              25    100.00
## 1391              28     58.18
## 1392              49     67.14
## 1393              49    100.00
## 1394              42     61.29
## 1395              23     57.73
## 1396              29     81.25
## 1397              25     80.54
## 1398              39     71.98
## 1399              44     69.84
## 1400              25     76.26
## 1401              45     76.26
## 1402              25     83.39
## 1403              37     57.73
## 1404              30     66.99
## 1405              36     75.55
## 1406              26     60.58
## 1407              23     73.41
## 1408              23     72.70
## 1409              25     66.99
## 1410              21    100.00
## 1411              26     63.43
## 1412              44     85.25
## 1413              24    100.00
## 1414              66     66.99
## 1415              36     57.73
## 1416              36     85.25
## 1417              22     77.90
## 1418              25     60.26
## 1419              37     72.76
## 1420              32     75.69
## 1421              47     74.22
## 1422              37     69.82
## 1423              20     62.47
## 1424              41     82.31
## 1425              21     60.26
## 1426              22     76.43
## 1427              40     80.10
## 1428              32     74.96
## 1429              36     66.14
## 1430              27     72.02
## 1431              26     87.45
## 1432              30     70.55
## 1433              23     56.84
## 1434              29     59.53
## 1435              21     60.37
## 1436              34    100.00
## 1437              26     76.43
## 1438              60     64.67
## 1439              35     55.49
## 1440              47     69.36
## 1441              20     60.69
## 1442              20     54.33
## 1443              25     65.31
## 1444              25     69.36
## 1445              27     68.78
## 1446              31     60.11
## 1447              44     66.47
## 1448              49     46.82
## 1449              26     56.07
## 1450              36     54.33
## 1451              44     52.60
## 1452              28     46.82
## 1453              45     64.74
## 1454              29     46.82
## 1455              40     53.75
## 1456              45     61.85
## 1457              44     53.18
## 1458              25     69.16
## 1459              45    100.00
## 1460              48     47.40
## 1461              44     60.76
## 1462              25     97.27
## 1463              22     91.76
## 1464              31     50.29
## 1465              21     52.60
## 1466              55     46.82
## 1467              25    100.00
## 1468              35     98.05
## 1469              35     93.54
## 1470              43     95.80
## 1471              44    100.00
## 1472              50    100.00
## 1473              48    100.00
## 1474              25    100.00
## 1475              39    100.00
## 1476              25     90.16
## 1477              32     91.29
## 1478              20    100.00
## 1479              26    100.00
## 1480              42    100.00
## 1481              21    100.00
## 1482              34    100.00
## 1483              47    100.00
## 1484              21    100.00
## 1485              48    100.00
## 1486              30     87.78
## 1487              27     84.39
## 1488              50     96.92
## 1489              38    100.00
## 1490              45    100.00
## 1491              46    100.00
## 1492              35    100.00
## 1493              29     59.37
## 1494              50     59.87
## 1495              26     49.81
## 1496              47     56.85
## 1497              23     53.33
## 1498              34     42.76
## 1499              34     53.83
## 1500              47     53.83
## 1501              45     49.81
## 1502              45     53.33
## 1503              36     43.27
## 1504              21     40.25
## 1505              28     48.30
## 1506              35     45.28
## 1507              50     52.32
## 1508              22     51.32
## 1509              45     49.30
## 1510              48     42.26
## 1511              20     87.96
## 1512              27     36.21
## 1513              38     38.50
## 1514              32    100.00
## 1515              64     40.25
## 1516              37     60.37
## 1517              28     88.63
## 1518              39    100.00
## 1519              41     94.10
## 1520              40     87.54
## 1521              49    100.00
## 1522              27     98.48
## 1523              34    100.00
## 1524              23     96.29
## 1525              31     88.63
## 1526              34     97.38
## 1527              25     95.20
## 1528              22    100.00
## 1529              32    100.00
## 1530              31    100.00
## 1531              25    100.00
## 1532              47     87.54
## 1533              21     50.65
## 1534              28     71.73
## 1535              46     94.10
## 1536              33     41.71
## 1537              43    100.00
## 1538              38     96.29
## 1539              47     88.63
## 1540              45     31.20
## 1541              20     35.51
## 1542              45     37.84
## 1543              36     33.19
## 1544              37     27.22
## 1545              31     31.53
## 1546              39     36.84
## 1547              26     29.21
## 1548              32     37.17
## 1549              20     34.19
## 1550              42     29.21
## 1551              33     29.54
## 1552              20     28.88
## 1553              29     38.17
## 1554              23     30.20
## 1555              39     29.54
## 1556              20    100.00
## 1557              45     81.91
## 1558              20     35.18
## 1559              48    100.00
## 1560              23     36.29
## 1561              32     70.56
## 1562              33    100.00
## 1563              61     29.54
## 1564              45     26.88
## 1565              38     83.03
## 1566              34     83.79
## 1567              43     83.03
## 1568              47     83.03
## 1569              22     67.03
## 1570              29     75.41
## 1571              28     68.55
## 1572              40     91.40
## 1573              25     73.88
## 1574              30     61.70
## 1575              38     69.31
## 1576              36     87.60
## 1577              32     87.60
## 1578              37     62.46
## 1579              30     79.98
## 1580              39     70.08
## 1581              32     65.51
## 1582              47     63.22
## 1583              26     86.83
## 1584              37     94.43
## 1585              55     79.98
## 1586              21    100.00
## 1587              23    100.00
## 1588              49     81.40
## 1589              59     87.60
## 1590              32     87.60
## 1591              43    100.00
## 1592              41    100.00
## 1593              45    100.00
## 1594              33    100.00
## 1595              40    100.00
## 1596              33    100.00
## 1597              50    100.00
## 1598              30    100.00
## 1599              41    100.00
## 1600              35    100.00
## 1601              49    100.00
## 1602              46    100.00
## 1603              48    100.00
## 1604              36    100.00
## 1605              22    100.00
## 1606              42    100.00
## 1607              21    100.00
## 1608              29    100.00
## 1609              35    100.00
## 1610              41    100.00
## 1611              29     71.97
## 1612              34     50.33
## 1613              37    100.00
## 1614              28     80.54
## 1615              49    100.00
## 1616              23    100.00
## 1617              46     53.76
## 1618              39     44.35
## 1619              22     45.25
## 1620              49     49.28
## 1621              43     36.29
## 1622              27     41.22
## 1623              31     36.74
## 1624              20     50.62
## 1625              24     38.08
## 1626              49     47.94
## 1627              24     48.38
## 1628              39     45.25
## 1629              37     45.70
## 1630              45     47.49
## 1631              45     48.38
## 1632              44     39.42
## 1633              23     37.63
## 1634              30    100.00
## 1635              26     85.52
## 1636              43     53.76
## 1637              26     31.86
## 1638              28     30.59
## 1639              27     68.35
## 1640              24    100.00
## 1641              40     45.70
## 1642              36    100.00
## 1643              21    100.00
## 1644              27    100.00
## 1645              47    100.00
## 1646              42    100.00
## 1647              32    100.00
## 1648              28    100.00
## 1649              24    100.00
## 1650              49    100.00
## 1651              46    100.00
## 1652              28    100.00
## 1653              48    100.00
## 1654              29    100.00
## 1655              47    100.00
## 1656              43    100.00
## 1657              25    100.00
## 1658              48    100.00
## 1659              24    100.00
## 1660              42    100.00
## 1661              31    100.00
## 1662              42    100.00
## 1663              37    100.00
## 1664              41    100.00
## 1665              20    100.00
## 1666              20    100.00
## 1667              70    100.00
## 1668              49    100.00
## 1669              35     58.87
## 1670              32     76.88
## 1671              29     61.64
## 1672              27     60.95
## 1673              27     80.34
## 1674              38     74.11
## 1675              35     72.03
## 1676              42     76.19
## 1677              21     63.72
## 1678              37     80.34
## 1679              26     79.65
## 1680              47     65.80
## 1681              37     65.10
## 1682              46     75.49
## 1683              38     59.56
## 1684              33     66.49
## 1685              24     56.10
## 1686              31     81.73
## 1687              42     81.03
## 1688              32    100.00
## 1689              41     70.65
## 1690              43     61.23
## 1691              20    100.00
## 1692              35     65.13
## 1693              27     79.65
## 1694              43     78.15
## 1695              32     72.70
## 1696              21     73.60
## 1697              20    100.00
## 1698              22     74.51
## 1699              36     73.60
## 1700              46     83.60
## 1701              47     96.32
## 1702              45     88.14
## 1703              47     88.14
## 1704              47     94.50
## 1705              38     87.24
## 1706              49     79.97
## 1707              35     80.87
## 1708              49    100.00
## 1709              28     93.60
## 1710              30     72.70
## 1711              39     86.72
## 1712              25    100.00
## 1713              40    100.00
## 1714              36     37.50
## 1715              76     94.50
## 1716              39    100.00
## 1717              44     39.60
## 1718              24     30.06
## 1719              39     38.19
## 1720              21     42.43
## 1721              30     40.31
## 1722              27     31.82
## 1723              37     31.12
## 1724              42     31.82
## 1725              32     28.29
## 1726              42     29.70
## 1727              21     40.31
## 1728              33     32.88
## 1729              49     36.07
## 1730              31     33.24
## 1731              38     41.72
## 1732              20     40.66
## 1733              39     30.06
## 1734              48     31.47
## 1735              39     37.13
## 1736              30    100.00
## 1737              33     37.13
## 1738              36     37.13
## 1739              36     82.94
## 1740              45    100.00
## 1741              40    100.00
## 1742              46     38.90
## 1743              30     36.07
## 1744              31     33.24
## 1745              49     74.68
## 1746              41     59.60
## 1747              35     67.14
## 1748              27     60.97
## 1749              23     72.62
## 1750              21     69.88
## 1751              34     80.84
## 1752              22     69.20
## 1753              48     67.82
## 1754              43     82.21
## 1755              32     81.53
## 1756              20     67.82
## 1757              24     67.14
## 1758              40     65.08
## 1759              30     73.99
## 1760              21     71.25
## 1761              25     75.36
## 1762              34     63.71
## 1763              48     58.92
## 1764              55    100.00
## 1765              25     74.68
## 1766              38     70.44
## 1767              39     55.96
## 1768              28     57.55
## 1769              24     61.66
## 1770              21     67.82
## 1771              46    100.00
## 1772              25     93.95
## 1773              34    100.00
## 1774              25    100.00
## 1775              23    100.00
## 1776              20    100.00
## 1777              23    100.00
## 1778              42    100.00
## 1779              27    100.00
## 1780              33    100.00
## 1781              28     98.65
## 1782              43    100.00
## 1783              48    100.00
## 1784              48    100.00
## 1785              45    100.00
## 1786              43    100.00
## 1787              44     42.26
## 1788              24     87.24
## 1789              31    100.00
## 1790              44     36.29
## 1791              59     98.65
## 1792              55     96.30
## 1793              29     32.10
## 1794              39     30.96
## 1795              20     35.87
## 1796              25     42.67
## 1797              42     37.00
## 1798              36     35.49
## 1799              37     42.67
## 1800              30     30.59
## 1801              21     37.00
## 1802              34     43.42
## 1803              42     36.63
## 1804              20     44.56
## 1805              40     42.67
## 1806              34     40.40
## 1807              31     38.89
## 1808              36     39.65
## 1809              48     34.36
## 1810              33     41.91
## 1811              37     33.23
## 1812              27     42.24
## 1813              39     40.40
## 1814              36     38.52
## 1815              36    100.00
## 1816              41    100.00
## 1817              37    100.00
## 1818              47     44.56
## 1819              15     42.67
## 1820              44     72.58
## 1821              35     87.62
## 1822              41     94.71
## 1823              49     98.25
## 1824              31     91.17
## 1825              20     79.66
## 1826              45     72.58
## 1827              33     74.35
## 1828              47     83.20
## 1829              20     89.40
## 1830              47     70.81
## 1831              40     94.71
## 1832              30    100.00
## 1833              22     91.17
## 1834              27    100.00
## 1835              34     92.94
## 1836              46     84.97
## 1837              31     84.08
## 1838              24     86.74
## 1839              41     85.85
## 1840              55    100.00
## 1841              30    100.00
## 1842              33     57.32
## 1843              43     97.87
## 1844              27     83.20
## 1845              60    100.00
## 1846              27     73.62
## 1847              49     83.04
## 1848              31     73.62
## 1849              20     77.05
## 1850              24     81.33
## 1851              33     94.17
## 1852              32     72.77
## 1853              40     79.62
## 1854              27     79.62
## 1855              40     79.62
## 1856              26     81.33
## 1857              44     96.74
## 1858              33     71.06
## 1859              34     68.49
## 1860              48     74.48
## 1861              25     83.04
## 1862              39     84.75
## 1863              45     34.19
## 1864              24    100.00
## 1865              46     79.62
## 1866              44     79.06
## 1867              13     81.33
## 1868              35     96.74
## 1869              30     63.07
## 1870              34     50.21
## 1871              27     66.13
## 1872              30     68.58
## 1873              50     69.80
## 1874              34     50.21
## 1875              23     65.52
## 1876              48     60.01
## 1877              34     64.90
## 1878              48     48.98
## 1879              24     50.21
## 1880              47     62.45
## 1881              24     52.66
## 1882              47     62.45
## 1883              20     61.23
## 1884              20     67.97
## 1885              31     58.78
## 1886              38     56.94
## 1887              26     61.23
## 1888              25    100.00
## 1889              48     62.45
## 1890              44     62.45
## 1891              21    100.00
## 1892              46     41.54
## 1893              46     52.84
## 1894              55     52.66
## 1895              31     52.60
## 1896              20     72.98
## 1897              29     59.18
## 1898              33     77.59
## 1899              34     55.89
## 1900              32     63.12
## 1901              27     73.64
## 1902              21     69.04
## 1903              27     71.67
## 1904              36     77.59
## 1905              43     70.35
## 1906              25     69.70
## 1907              46     70.35
## 1908              24     72.33
## 1909              39     71.67
## 1910              31     53.92
## 1911              22     71.67
## 1912              47     76.93
## 1913              20     72.98
## 1914              29     99.69
## 1915              38     68.38
## 1916              34    100.00
## 1917              46     66.00
## 1918              35     63.76
## 1919              34     71.67
## 1920              38     57.20
## 1921              18     69.70
## 1922              37    100.00
## 1923              43    100.00
## 1924              27    100.00
## 1925              30    100.00
## 1926              22     98.51
## 1927              49    100.00
## 1928              46    100.00
## 1929              48     91.02
## 1930              46     87.81
## 1931              48     92.09
## 1932              27     86.73
## 1933              43    100.00
## 1934              48    100.00
## 1935              41    100.00
## 1936              22     96.37
## 1937              46     92.09
## 1938              21     89.95
## 1939              31     37.18
## 1940              26     95.88
## 1941              20     99.58
## 1942              34    100.00
## 1943              43     86.73
## 1944              26    100.00
## 1945              50     79.67
## 1946              35     90.57
## 1947              50     77.99
## 1948              23     80.51
## 1949              37     67.93
## 1950              29     83.86
## 1951              21     72.12
## 1952              36     85.54
## 1953              22     86.38
## 1954              22     89.73
## 1955              46     80.51
## 1956              23     76.31
## 1957              49     87.21
## 1958              48     83.02
## 1959              33     72.96
## 1960              22     77.15
## 1961              22     91.41
## 1962              25     92.25
## 1963              20     92.25
## 1964              42     59.36
## 1965              25     60.34
## 1966              23    100.00
## 1967              37     85.54
## 1968              37     90.57
## 1969              42     72.96
## 1970              51     76.31
## 1971              40    100.00
## 1972              43    100.00
## 1973              47    100.00
## 1974              23    100.00
## 1975              35    100.00
## 1976              34    100.00
## 1977              25    100.00
## 1978              45    100.00
## 1979              47    100.00
## 1980              49    100.00
## 1981              40    100.00
## 1982              29    100.00
## 1983              39    100.00
## 1984              24    100.00
## 1985              25    100.00
## 1986              36    100.00
## 1987              50    100.00
## 1988              45    100.00
## 1989              26    100.00
## 1990              21    100.00
## 1991              42    100.00
## 1992              32    100.00
## 1993              31     94.58
## 1994              33     53.27
## 1995              45    100.00
## 1996              76    100.00
## 1997              70    100.00
## 1998              50     64.83
## 1999              28     70.29
## 2000              50     81.89
## 2001              28     66.19
## 2002              44     77.11
## 2003              27     73.02
## 2004              30     72.33
## 2005              43     66.19
## 2006              29     69.60
## 2007              48     56.64
## 2008              33     60.05
## 2009              40     75.06
## 2010              48     61.42
## 2011              41     81.89
## 2012              21     55.96
## 2013              32     71.65
## 2014              43     76.43
## 2015              30     77.79
## 2016              35     76.43
## 2017              45     96.92
## 2018              34     59.37
## 2019              26    100.00
## 2020              39     73.00
## 2021              41     73.32
## 2022              41     68.24
## 2023              64     60.05
## 2024              18     75.06
## 2025              49     34.47
## 2026              48     34.47
## 2027              46     33.23
## 2028              26     38.98
## 2029              37     38.98
## 2030              35     33.23
## 2031              23     42.26
## 2032              22     41.03
## 2033              39     33.23
## 2034              44     34.88
## 2035              27     43.90
## 2036              46     36.93
## 2037              33     41.85
## 2038              33     40.62
## 2039              24     40.21
## 2040              31     35.29
## 2041              41     77.24
## 2042              22     97.44
## 2043              46     37.34
## 2044              43     95.03
## 2045              15     36.93
## 2046              15     43.49
## 2047              26    100.00
## 2048              44    100.00
## 2049              20     96.99
## 2050              40     94.62
## 2051              23    100.00
## 2052              24     99.36
## 2053              29    100.00
## 2054              49    100.00
## 2055              34    100.00
## 2056              28    100.00
## 2057              37    100.00
## 2058              45    100.00
## 2059              46    100.00
## 2060              22    100.00
## 2061              39    100.00
## 2062              27    100.00
## 2063              36    100.00
## 2064              38    100.00
## 2065              44    100.00
## 2066              31    100.00
## 2067              23    100.00
## 2068              22    100.00
## 2069              28     50.32
## 2070              21     93.91
## 2071              37    100.00
## 2072              31    100.00
## 2073              25    100.00
## 2074              26     86.68
## 2075              34    100.00
## 2076              29    100.00
## 2077              20     90.57
## 2078              42     91.55
## 2079              22    100.00
## 2080              47    100.00
## 2081              20    100.00
## 2082              33     97.39
## 2083              39     90.57
## 2084              33    100.00
## 2085              40     86.68
## 2086              46     78.89
## 2087              48     97.39
## 2088              21     78.89
## 2089              45    100.00
## 2090              33    100.00
## 2091              44    100.00
## 2092              33    100.00
## 2093              39    100.00
## 2094              39     50.31
## 2095              41    100.00
## 2096              40     86.92
## 2097              33    100.00
## 2098              28     78.89
## 2099              26     63.76
## 2100              29     85.49
## 2101              46     77.52
## 2102              33     84.77
## 2103              48     78.25
## 2104              40     71.00
## 2105              23     74.62
## 2106              40     81.14
## 2107              37     74.62
## 2108              24     75.35
## 2109              27     62.31
## 2110              21     71.00
## 2111              23     72.45
## 2112              44     83.32
## 2113              35     83.32
## 2114              43     60.86
## 2115              40     84.77
## 2116              35     89.90
## 2117              25     62.46
## 2118              43    100.00
## 2119              50     63.34
## 2120              45     78.25
## 2121              52     81.14
## 2122              48     74.62
## 2123              31     68.71
## 2124              29     71.14
## 2125              23     87.31
## 2126              31     64.67
## 2127              23     67.10
## 2128              24     94.58
## 2129              28     71.14
## 2130              44     66.29
## 2131              22     92.16
## 2132              46     70.33
## 2133              22     93.77
## 2134              38     87.31
## 2135              47     83.27
## 2136              48     75.18
## 2137              40     88.12
## 2138              32     80.84
## 2139              49     97.01
## 2140              43     85.69
## 2141              41    100.00
## 2142              30    100.00
## 2143              28     95.39
## 2144              43    100.00
## 2145              41    100.00
## 2146              30     82.42
## 2147              31    100.00
## 2148              32    100.00
## 2149              43     96.31
## 2150              26    100.00
## 2151              27    100.00
## 2152              24    100.00
## 2153              22    100.00
## 2154              46    100.00
## 2155              37     97.27
## 2156              49     80.90
## 2157              21    100.00
## 2158              25    100.00
## 2159              37    100.00
## 2160              45     86.68
## 2161              32     85.72
## 2162              29     82.83
## 2163              26     83.79
## 2164              28    100.00
## 2165              27     87.64
## 2166              20     98.18
## 2167              44    100.00
## 2168              42    100.00
## 2169              41    100.00
## 2170              26    100.00
## 2171              26    100.00
## 2172              26    100.00
## 2173              41     86.68
## 2174              20     92.90
## 2175              22    100.00
## 2176              23    100.00
## 2177              33     93.90
## 2178              28    100.00
## 2179              44     98.89
## 2180              46     79.91
## 2181              21    100.00
## 2182              41    100.00
## 2183              31    100.00
## 2184              31     79.91
## 2185              23     81.91
## 2186              37     98.89
## 2187              26    100.00
## 2188              24     79.91
## 2189              47    100.00
## 2190              45     63.91
## 2191              55    100.00
## 2192              46     81.17
## 2193              50    100.00
## 2194              37    100.00
## 2195              44     94.90
## 2196              49    100.00
## 2197              45    100.00
## 2198              27     43.45
## 2199              31     44.66
## 2200              33     40.23
## 2201              31     35.80
## 2202              35     35.40
## 2203              26     39.83
## 2204              34     45.46
## 2205              46     32.99
## 2206              41     42.24
## 2207              43     39.43
## 2208              26     40.23
## 2209              36     48.28
## 2210              20     32.59
## 2211              27     36.61
## 2212              37     41.03
## 2213              24     42.24
## 2214              36     43.05
## 2215              29     38.22
## 2216              28    100.00
## 2217              29    100.00
## 2218              38     39.83
## 2219              48     48.28
## 2220              40     82.46
## 2221              41     44.56
## 2222              30     40.23
## 2223              35     47.62
## 2224              28     55.73
## 2225              45     51.95
## 2226              24     45.99
## 2227              41     63.85
## 2228              48     45.99
## 2229              50     63.31
## 2230              33     62.77
## 2231              32     43.29
## 2232              27     60.06
## 2233              35     55.19
## 2234              23     54.11
## 2235              35     48.70
## 2236              40     43.83
## 2237              35     47.62
## 2238              31     55.19
## 2239              50     46.53
## 2240              40     57.90
## 2241              38     45.45
## 2242              38    100.00
## 2243              40     60.60
## 2244              33     46.53
## 2245              36    100.00
## 2246              20     66.47
## 2247              32     53.18
## 2248              36     62.77
## 2249              19     48.70
## 2250              11     43.83
## 2251              49     65.87
## 2252              27     63.38
## 2253              29     70.84
## 2254              42     74.57
## 2255              33     50.95
## 2256              44     53.44
## 2257              22     64.00
## 2258              48     50.95
## 2259              33     54.68
## 2260              45     56.55
## 2261              20     52.82
## 2262              46     60.90
## 2263              40     49.71
## 2264              45     64.63
## 2265              36     59.65
## 2266              31     67.73
## 2267              46     50.33
## 2268              35     66.49
## 2269              28    100.00
## 2270              31     84.71
## 2271              27    100.00
## 2272              22    100.00
## 2273              30     99.55
## 2274              44     36.07
## 2275              30     60.28
## 2276              24     49.71
## 2277              45     75.63
## 2278              23     68.52
## 2279              26     62.70
## 2280              28     60.76
## 2281              49     58.18
## 2282              49     54.94
## 2283              29     74.98
## 2284              49     64.64
## 2285              39     54.94
## 2286              36     58.82
## 2287              39     62.05
## 2288              30     73.04
## 2289              44     69.16
## 2290              20     61.41
## 2291              21     63.35
## 2292              36     77.57
## 2293              32     71.75
## 2294              36     73.04
## 2295              34     56.24
## 2296              48    100.00
## 2297              33     73.69
## 2298              31    100.00
## 2299              36    100.00
## 2300              25    100.00
## 2301              48    100.00
## 2302              27     69.16
## 2303              44     61.41
## 2304              33     72.92
## 2305              29     72.23
## 2306              49     57.10
## 2307              20     81.86
## 2308              31     73.61
## 2309              39     59.16
## 2310              20     66.04
## 2311              34     77.73
## 2312              50     61.22
## 2313              40     79.11
## 2314              28     63.97
## 2315              50     81.86
## 2316              28     79.80
## 2317              46     66.04
## 2318              24     59.16
## 2319              24     81.17
## 2320              39     59.16
## 2321              40     44.51
## 2322              49     72.33
## 2323              44     82.26
## 2324              35    100.00
## 2325              22     67.41
## 2326              62     77.73
## 2327              26     61.22
## 2328              31    100.00
## 2329              25     86.74
## 2330              30     89.80
## 2331              27    100.00
## 2332              23    100.00
## 2333              34    100.00
## 2334              22    100.00
## 2335              42     85.72
## 2336              37    100.00
## 2337              30    100.00
## 2338              27    100.00
## 2339              25    100.00
## 2340              34     97.97
## 2341              38    100.00
## 2342              26    100.00
## 2343              38    100.00
## 2344              50     84.70
## 2345              22    100.00
## 2346              32    100.00
## 2347              31     71.02
## 2348              40    100.00
## 2349              22    100.00
## 2350              41    100.00
## 2351              45     48.98
## 2352              45    100.00
## 2353              39     40.15
## 2354              49     50.62
## 2355              27     50.19
## 2356              34     36.66
## 2357              20     41.02
## 2358              48     51.93
## 2359              29     38.40
## 2360              43     41.02
## 2361              41     46.26
## 2362              41     35.35
## 2363              36     51.93
## 2364              49     37.97
## 2365              38     45.39
## 2366              33     51.93
## 2367              26     48.44
## 2368              47     43.64
## 2369              34     47.57
## 2370              34     51.93
## 2371              40     50.62
## 2372              40     82.21
## 2373              33     82.59
## 2374              49     65.80
## 2375              27    100.00
## 2376              49     36.66
## 2377              56     35.35
## 2378              37     51.93
## 2379              33    100.00
## 2380              27    100.00
## 2381              46    100.00
## 2382              44    100.00
## 2383              26    100.00
## 2384              48     94.92
## 2385              23    100.00
## 2386              45    100.00
## 2387              49    100.00
## 2388              28     94.92
## 2389              37    100.00
## 2390              34    100.00
## 2391              22    100.00
## 2392              29    100.00
## 2393              34     98.39
## 2394              38    100.00
## 2395              41    100.00
## 2396              42    100.00
## 2397              28    100.00
## 2398              38    100.00
## 2399              23    100.00
## 2400              31     71.40
## 2401              46    100.00
## 2402              48     56.55
## 2403              29    100.00
## 2404              46    100.00
## 2405              26    100.00
## 2406              18    100.00
## 2407              32     53.31
## 2408              21     49.21
## 2409              46     69.12
## 2410              42     49.79
## 2411              31     57.41
## 2412              38     66.78
## 2413              38     64.44
## 2414              20     48.62
## 2415              46     62.09
## 2416              30     65.61
## 2417              30     68.54
## 2418              43     52.14
## 2419              49     63.85
## 2420              43     56.82
## 2421              37     66.78
## 2422              35     55.07
## 2423              34     60.34
## 2424              38     61.51
## 2425              44    100.00
## 2426              21    100.00
## 2427              44    100.00
## 2428              25     64.93
## 2429              24     58.58
## 2430              38     60.06
## 2431              45    100.00
## 2432              51     63.85
## 2433              34     82.99
## 2434              44     74.85
## 2435              44     96.00
## 2436              38     81.36
## 2437              31     71.60
## 2438              48     80.55
## 2439              21     93.56
## 2440              40     66.72
## 2441              40     80.55
## 2442              50     77.29
## 2443              20     68.34
## 2444              48     72.41
## 2445              47     89.50
## 2446              21     70.78
## 2447              39     78.92
## 2448              44     80.55
## 2449              28     88.68
## 2450              45     77.29
## 2451              20    100.00
## 2452              38    100.00
## 2453              26    100.00
## 2454              44    100.00
## 2455              49    100.00
## 2456              22    100.00
## 2457              31     68.34
## 2458              41     70.67
## 2459              25     76.67
## 2460              31     60.00
## 2461              41     64.00
## 2462              43     64.67
## 2463              43     75.34
## 2464              24     76.00
## 2465              21     54.00
## 2466              23     64.67
## 2467              38     74.67
## 2468              31     62.67
## 2469              36     70.67
## 2470              36     71.34
## 2471              34     62.00
## 2472              21     65.34
## 2473              45     78.67
## 2474              26     75.34
## 2475              50     54.00
## 2476              41     62.00
## 2477              39     60.00
## 2478              22    100.00
## 2479              46     76.67
## 2480              44    100.00
## 2481              25     77.34
## 2482              39     66.67
## 2483              37     71.34
## 2484              31    100.00
## 2485              47     82.21
## 2486              24     77.64
## 2487              36    100.00
## 2488              48    100.00
## 2489              28     98.65
## 2490              48     83.12
## 2491              21     78.55
## 2492              25    100.00
## 2493              25    100.00
## 2494              31     91.34
## 2495              40     84.03
## 2496              32     89.51
## 2497              24     83.12
## 2498              42    100.00
## 2499              21    100.00
## 2500              34     82.21
## 2501              27    100.00
## 2502              30     88.60
## 2503              39    100.00
## 2504              20     60.54
## 2505              37     81.87
## 2506              46    100.00
## 2507              47     87.69
## 2508              11    100.00
## 2509              23     91.34
## 2510              29     70.15
## 2511              38     79.68
## 2512              32     97.00
## 2513              43     84.01
## 2514              31     87.48
## 2515              29    100.00
## 2516              31     88.34
## 2517              30     94.40
## 2518              50     94.40
## 2519              40     80.55
## 2520              23     97.00
## 2521              26     88.34
## 2522              40    100.00
## 2523              21    100.00
## 2524              43     86.61
## 2525              29     71.89
## 2526              38     91.81
## 2527              23     76.22
## 2528              20    100.00
## 2529              36     70.30
## 2530              28    100.00
## 2531              44    100.00
## 2532              49    100.00
## 2533              32     80.55
## 2534              34    100.00
## 2535              30    100.00
## 2536              29     94.14
## 2537              22     85.99
## 2538              26    100.00
## 2539              32     91.43
## 2540              29    100.00
## 2541              34     96.86
## 2542              24     99.57
## 2543              24     90.52
## 2544              33     88.71
## 2545              26    100.00
## 2546              40     95.95
## 2547              44     94.14
## 2548              24     90.52
## 2549              20     94.14
## 2550              34    100.00
## 2551              34     97.76
## 2552              45     93.24
## 2553              41    100.00
## 2554              55     71.25
## 2555              23    100.00
## 2556              24     45.39
## 2557              32     84.41
## 2558              29     85.76
## 2559              36    100.00
## 2560              46     87.80
## 2561              32     95.95
## 2562              34    100.00
## 2563              24    100.00
## 2564              40    100.00
## 2565              26     82.77
## 2566              20    100.00
## 2567              31    100.00
## 2568              22     87.75
## 2569              42    100.00
## 2570              26     99.72
## 2571              37     87.75
## 2572              38     88.75
## 2573              35    100.00
## 2574              33     90.75
## 2575              39    100.00
## 2576              45    100.00
## 2577              24    100.00
## 2578              35     88.75
## 2579              23    100.00
## 2580              37    100.00
## 2581              55     87.75
## 2582              49    100.00
## 2583              26    100.00
## 2584              33    100.00
## 2585              37     83.84
## 2586              22     86.76
## 2587              85     88.75
## 2588              22    100.00
## 2589              31     65.77
## 2590              38     65.77
## 2591              45     85.29
## 2592              31     85.29
## 2593              36     64.33
## 2594              46     70.11
## 2595              32     76.62
## 2596              39     57.82
## 2597              50     78.79
## 2598              46     74.45
## 2599              36     80.95
## 2600              29     82.40
## 2601              32     75.89
## 2602              44     68.67
## 2603              42     62.16
## 2604              47     65.77
## 2605              44     58.55
## 2606              43     75.17
## 2607              48     74.45
## 2608              21     96.31
## 2609              50     74.35
## 2610              29     75.35
## 2611              41     70.33
## 2612              37    100.00
## 2613              22     66.50
## 2614              31     75.89
## 2615              42    100.00
## 2616              42    100.00
## 2617              45    100.00
## 2618              36    100.00
## 2619              20    100.00
## 2620              39     81.93
## 2621              42     85.98
## 2622              23     86.99
## 2623              26     89.01
## 2624              33    100.00
## 2625              31     88.00
## 2626              50    100.00
## 2627              44    100.00
## 2628              45     80.92
## 2629              46     88.00
## 2630              27     85.98
## 2631              28    100.00
## 2632              40    100.00
## 2633              30     99.13
## 2634              34    100.00
## 2635              46    100.00
## 2636              32     82.83
## 2637              27    100.00
## 2638              34    100.00
## 2639              34     54.84
## 2640              34    100.00
## 2641              46     80.92
## 2642              32    100.00
## 2643              24    100.00
## 2644              27     99.67
## 2645              20    100.00
## 2646              36    100.00
## 2647              29    100.00
## 2648              25    100.00
## 2649              29    100.00
## 2650              25     96.11
## 2651              44    100.00
## 2652              47    100.00
## 2653              48    100.00
## 2654              45    100.00
## 2655              35    100.00
## 2656              31    100.00
## 2657              50    100.00
## 2658              33    100.00
## 2659              29    100.00
## 2660              48     68.80
## 2661              44     72.42
## 2662              25     66.73
## 2663              50    100.00
## 2664              23    100.00
## 2665              21     96.11
## 2666              41    100.00
## 2667              44     74.40
## 2668              43     76.00
## 2669              28     96.00
## 2670              43     86.40
## 2671              48     96.00
## 2672              38     82.40
## 2673              31     86.40
## 2674              26     67.20
## 2675              32     92.00
## 2676              44     67.20
## 2677              27     76.00
## 2678              43     73.60
## 2679              25     69.60
## 2680              22     80.80
## 2681              21     87.20
## 2682              48     75.20
## 2683              33     64.00
## 2684              34    100.00
## 2685              43     81.95
## 2686              44    100.00
## 2687              44    100.00
## 2688              32     94.34
## 2689              29     65.60
## 2690              77     92.00
## 2691              39     67.20
## 2692              39     81.14
## 2693              36    100.00
## 2694              42     91.15
## 2695              21    100.00
## 2696              50     88.15
## 2697              24    100.00
## 2698              44     92.16
## 2699              37    100.00
## 2700              27     92.16
## 2701              37    100.00
## 2702              38    100.00
## 2703              48     96.16
## 2704              30    100.00
## 2705              25     88.15
## 2706              40     86.15
## 2707              22     88.15
## 2708              34    100.00
## 2709              32     90.15
## 2710              31     86.15
## 2711              43     80.00
## 2712              31     89.38
## 2713              31     77.34
## 2714              34     96.34
## 2715              45     92.08
## 2716              48    100.00
## 2717              28    100.00
## 2718              22    100.00
## 2719              45     83.42
## 2720              30     85.41
## 2721              38     85.41
## 2722              20    100.00
## 2723              28    100.00
## 2724              24    100.00
## 2725              22     79.45
## 2726              35     93.35
## 2727              33     85.41
## 2728              31     95.34
## 2729              35     82.43
## 2730              35     90.37
## 2731              50     81.43
## 2732              26    100.00
## 2733              38     89.38
## 2734              45    100.00
## 2735              30    100.00
## 2736              37     91.37
## 2737              37     86.61
## 2738              36     71.89
## 2739              25    100.00
## 2740              37    100.00
## 2741              30     95.48
## 2742              36    100.00
## 2743              27     90.37
## 2744              48     61.44
## 2745              26     59.22
## 2746              26     85.13
## 2747              34     85.87
## 2748              44     85.87
## 2749              39     82.91
## 2750              45     76.25
## 2751              40     63.67
## 2752              42     70.33
## 2753              43     74.03
## 2754              34     72.55
## 2755              38     62.19
## 2756              35     71.07
## 2757              31     72.55
## 2758              32     64.41
## 2759              47     86.62
## 2760              39     68.11
## 2761              44     62.19
## 2762              39     85.87
## 2763              50     57.86
## 2764              22     75.51
## 2765              35    100.00
## 2766              45     55.62
## 2767              44     86.40
## 2768              36     87.36
## 2769              28     72.55
## 2770              43     62.19
## 2771              48     52.64
## 2772              28     48.17
## 2773              21     41.71
## 2774              37     50.65
## 2775              34     49.16
## 2776              40     41.71
## 2777              45     51.15
## 2778              28     52.14
## 2779              29     41.71
## 2780              48     44.69
## 2781              31     45.69
## 2782              32     57.61
## 2783              21     57.11
## 2784              32     58.60
## 2785              43     57.61
## 2786              21     45.19
## 2787              34     53.63
## 2788              34     43.70
## 2789              44     86.13
## 2790              27     76.31
## 2791              49     52.64
## 2792              23     95.20
## 2793              25     64.97
## 2794              50     87.15
## 2795              34     40.22
## 2796              20     56.12
## 2797              42     57.61
## 2798              25     56.78
## 2799              50     43.68
## 2800              32     64.97
## 2801              39     44.23
## 2802              50     60.06
## 2803              38     48.59
## 2804              40     50.23
## 2805              28     64.43
## 2806              42     50.23
## 2807              42     63.88
## 2808              36     63.34
## 2809              24     49.69
## 2810              23     65.52
## 2811              29     50.78
## 2812              37     45.86
## 2813              33     51.32
## 2814              32     60.06
## 2815              35     59.51
## 2816              40     55.69
## 2817              37     86.74
## 2818              42     97.16
## 2819              20    100.00
## 2820              29    100.00
## 2821              43    100.00
## 2822              34     62.24
## 2823              47     65.52&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;don’t forget to assign it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select_data &amp;lt;- select(sales_data, QUANTITYORDERED, PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;arrange() function
order your data&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arranged_asc_data &amp;lt;- arrange(select_data, PRICEEACH)
arranged_desc_data &amp;lt;- arrange(select_data, -PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;filter() function
filter out data
first argument is the dataset, second argument is the filter conditions&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39;)
filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;if-i-want-to-filter-and-select-data-i-have-to-run-the-command-twice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;If I want to ‘filter’ and ‘select’ data, I have to run the command twice&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filtered_data &amp;lt;- filter(sales_data, STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;)

## notice the first argument is filtered_data
select_data &amp;lt;- select(filtered_data, QUANTITYORDERED, PRICEEACH)

## notice the first argument is select_data
arrange_data &amp;lt;- arrange(select_data, PRICEEACH) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s kind of tedious to have to run the command twice, so we will use a concept called piping (%&amp;gt;%)
Piping is sending the output of one function into the input of another. The output will be the first argument of the next function
The same command above can be written like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;piped_data &amp;lt;- sales_data %&amp;gt;%
  filter(STATE == &amp;#39;NY&amp;#39; &amp;amp; PRODUCTLINE == &amp;#39;Classic Cars&amp;#39;) %&amp;gt;%
  select(QUANTITYORDERED, PRICEEACH) %&amp;gt;%
  arrange(PRICEEACH)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;mutate() function
create your own columns using mutate
Same as above but using piping&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutated_data &amp;lt;- mutate(sales_data, discounted = 0.95 * SALES)

## same as above but using piping
mutated_data &amp;lt;- sales_data %&amp;gt;%
  mutate(discounted = 0.95 * SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;YOUR TURN!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sales_data &amp;lt;- read.csv(&amp;#39;sales_data_sample.csv&amp;#39;) # Reread the data in case you made any changes to it&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Q1: what is the most common deal size (column name: DEALSIZE)?
summary(sales_data$DEALSIZE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Large Medium  Small 
##    157   1384   1282&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Q2: what is the average quantity ordered? (HINT: Can also use mean function)
mean(sales_data$QUANTITYORDERED)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35.09281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sales_data$QUANTITYORDERED)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    6.00   27.00   35.00   35.09   43.00   97.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Q3: create a new dataset called q3_data with 
##       - a new column called MSRP_REV which is equal to the MSRP * QUANTITYORDERED
##       - filtered to only have &amp;#39;Large&amp;#39; sized deals 
##       - with only the selected columns ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV
##       - ordered in descending order by SALES
q3_data &amp;lt;- sales_data %&amp;gt;% 
  mutate(MSRP_REV = MSRP * QUANTITYORDERED) %&amp;gt;% 
  filter(DEALSIZE == &amp;#39;Large&amp;#39;) %&amp;gt;% 
  select(ORDERNUMBER, QUANTITYORDERED, PRICEEACH, MSRP, SALES, MSRP_REV) %&amp;gt;% 
  arrange(-SALES)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 5. group_by() and summarise() function
##    group_by() and summarise() will help us solve questions such as, what are the total sales by country?
grouped_data &amp;lt;- group_by(sales_data, COUNTRY)
summarised_data &amp;lt;- summarise(grouped_data, total_sales = sum(SALES))

## same as above, But using piping
summarised_data &amp;lt;- sales_data %&amp;gt;%
  group_by(COUNTRY) %&amp;gt;%
  summarise(total_sales = sum(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Instead of sum(), can also do max(), min(), mean(), n() for count, and others

## Q4: what is the average SALE by PRODUCTLINE?
summarised_data &amp;lt;- sales_data %&amp;gt;% 
  group_by(PRODUCTLINE) %&amp;gt;% 
  summarise(average_sales = mean(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a simple dot plot
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point() +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## change color
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## add labels and remove legend
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  labs(title = &amp;#39;Sales and Quantity Ordered&amp;#39;,
       y = &amp;#39;Unit Price ($)&amp;#39;,
       x = &amp;#39;Quantity Ordered (Units)&amp;#39;) + 
  theme(legend.position=&amp;quot;none&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## add regression line
ggplot(sales_data, aes(x = QUANTITYORDERED, y = SALES)) +
  geom_point(aes(color = &amp;#39;red&amp;#39;)) +
  labs(title = &amp;#39;Sales and Quantity Ordered&amp;#39;,
       y = &amp;#39;Unit Price ($)&amp;#39;,
       x = &amp;#39;Quantity Ordered (Units)&amp;#39;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)+
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## bar charts
## will first create a grouped by and summarised dataset
status_data &amp;lt;- sales_data %&amp;gt;%
  group_by(STATUS) %&amp;gt;%
  summarise(total_count = n()) %&amp;gt;%
  select(STATUS, total_count)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## now we will create a bar chart
ggplot(status_data, aes(x = STATUS, y = total_count)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;blue&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## your turn!

## Q5: create a bar chart of the total sales by country with the following properties:
##     - x axis label: Product Line
##     - y axis label: Total Sales ($)
##     - title: Sales by Product Line
##     - Outline of bars: red
##     - Fill of bars: pink

## Will first create a grouped by and summarised dataset
status_data &amp;lt;- sales_data %&amp;gt;%
  group_by(PRODUCTLINE) %&amp;gt;%
  summarise(total_sales = sum(SALES))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## bar plot
ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;pink&amp;#39;) +
  labs(title = &amp;#39;Sales by Product Line&amp;#39;) +
  theme_classic() +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Q6: using the ggplot2 cheat sheet, try constructing your own plot of choice!
## bar plot
ggplot(status_data, aes(x = PRODUCTLINE, y = total_sales)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;, color = &amp;#39;red&amp;#39;, fill = &amp;#39;yellow&amp;#39;) +
  labs(title = &amp;#39;Sales by Product Line&amp;#39;) +
  theme_classic() +
  coord_polar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/sales_analysis/2019-03-28-sales-analysis_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NYC_Fights13</title>
      <link>/post/nycflights13/nyc-fights13/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/nycflights13/nyc-fights13/</guid>
      <description>


&lt;div id=&#34;about-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the dataset&lt;/h2&gt;
&lt;p&gt;In this analysis, we explore the dplyr functions used to ask interesting questions of a more complex dataset We’ll use a dataset of flights that departed from New York city airports (including Newark, John F. Kennedy, and Laguardia airports) in 2013. This dataset is also featured online in the Introduction to dplyr vignette, and is drawn from the Bureau of Transportation Statistics database.&lt;/p&gt;
&lt;p&gt;This dataset will use over 300,000 observations to ask the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Which airline has the highest number of delayed departures?&lt;/li&gt;
&lt;li&gt;On average, to which airport do flights arrive most early?&lt;/li&gt;
&lt;li&gt;In which month do flights tend to have the longest delays?&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;get-to-know-the-dataframe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;get to know the dataframe&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## check the number of rows/columns
dim(flights)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 336776     19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## inspect the column names
colnames(flights) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;year&amp;quot;           &amp;quot;month&amp;quot;          &amp;quot;day&amp;quot;            &amp;quot;dep_time&amp;quot;      
##  [5] &amp;quot;sched_dep_time&amp;quot; &amp;quot;dep_delay&amp;quot;      &amp;quot;arr_time&amp;quot;       &amp;quot;sched_arr_time&amp;quot;
##  [9] &amp;quot;arr_delay&amp;quot;      &amp;quot;carrier&amp;quot;        &amp;quot;flight&amp;quot;         &amp;quot;tailnum&amp;quot;       
## [13] &amp;quot;origin&amp;quot;         &amp;quot;dest&amp;quot;           &amp;quot;air_time&amp;quot;       &amp;quot;distance&amp;quot;      
## [17] &amp;quot;hour&amp;quot;           &amp;quot;minute&amp;quot;         &amp;quot;time_hour&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-the-airline-carrier-that-has-the-highest-number-of-delayed-flights&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Identify the airline (&lt;code&gt;carrier&lt;/code&gt;) that has the highest number of delayed flights&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## start with the flights
has_most_delays &amp;lt;- flights %&amp;gt;%
  ## group by airline (carrier)
  group_by(carrier) %&amp;gt;%                    
  ## find only the delays
  filter(dep_delay &amp;gt; 0) %&amp;gt;%               
  ## count the observations
  summarize(num_delay = n()) %&amp;gt;%           
  ## find most delayed
  filter(num_delay == max(num_delay)) %&amp;gt;%  
  ## select the airline
  select(carrier)                          &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;get-name-of-the-most-delayed-carrier&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get name of the most delayed carrier&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## start with the previous answer
most_delayed_name &amp;lt;- has_most_delays %&amp;gt;%   
  ## join on airline ID
  left_join(airlines, by = &amp;quot;carrier&amp;quot;) %&amp;gt;%  
  ## select the airline name
  select(name)                             &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## access the value from the tibble
print(most_delayed_name$name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;United Air Lines Inc.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-the-average-arrival-delay-arr_delay-for-each-destination-dest&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculate the average arrival delay (&lt;code&gt;arr_delay&lt;/code&gt;) for each destination (&lt;code&gt;dest&lt;/code&gt;)&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_early &amp;lt;- flights %&amp;gt;%
  ## group by destination
  group_by(dest) %&amp;gt;%                     
  ## compute mean delay
  summarize(delay = mean(arr_delay))     &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-the-average-delay-by-destination-airport-omitting-na-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;# Compute the average delay by destination airport, omitting NA results&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_early &amp;lt;- flights %&amp;gt;%
  ## group by destination
  group_by(dest) %&amp;gt;%                                 
  ## compute mean delay
  summarize(delay = mean(arr_delay, na.rm = TRUE))   &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-the-destination-where-flights-on-average-arrive-most-early&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Identify the destination where flights, on average, arrive most early&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_early &amp;lt;- flights %&amp;gt;%
  ## group by destination
  group_by(dest) %&amp;gt;%                                      
  ## compute mean delay, ignore NA
  summarize(delay = mean(arr_delay, na.rm = TRUE)) %&amp;gt;%    
  ## filter for the *least* delayed
  filter(delay == min(delay, na.rm = TRUE)) %&amp;gt;%           
  ## select the destination (and delay to store it)
  select(dest, delay) %&amp;gt;%                                 
  ## join on `airports`dataframe
  left_join(airports, by = c(&amp;quot;dest&amp;quot; = &amp;quot;faa&amp;quot;)) %&amp;gt;%         
  ## select output variables of interest
  select(dest, name, delay)                               &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(most_early)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   dest  name       delay
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 LEX   Blue Grass   -22&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;identify-the-month-in-which-flights-tend-to-have-the-longest-delays&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Identify the month in which flights tend to have the longest delays&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flights %&amp;gt;%
  ## group by selected feature
  group_by(month) %&amp;gt;%                                      
  ## summarize value of interest
  summarize(delay = mean(arr_delay, na.rm = TRUE)) %&amp;gt;%     
  ## filter for the record of interest
  filter(delay == max(delay)) %&amp;gt;%                          
  ## select the column that answers the question
  select(month) %&amp;gt;%                                        
  ## print the tibble out directly
  print()                                                  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   month
##   &amp;lt;int&amp;gt;
## 1     7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compute-delay-by-month-adding-month-names-for-visual-display-note-month.name-is-a-variable-build-into-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compute delay by month, adding month names for visual display, note ‘month.name’ is a variable build into R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delay_by_month &amp;lt;- flights %&amp;gt;%
  group_by(month) %&amp;gt;%
  summarize(delay = mean(arr_delay, na.rm = TRUE)) %&amp;gt;%
  select(delay) %&amp;gt;%
  mutate(month = month.name)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-plot-using-the-ggplot2-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;# Create a plot using the ggplot2 package&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = delay_by_month) +
  geom_point(
    mapping = aes(x = delay, y = month), 
    color = &amp;quot;blue&amp;quot;,
    alpha = .4, 
    size = 3
    ) +
  geom_vline(xintercept = 0, size = .25) +
  xlim(c(-20, 20)) +
  scale_y_discrete(limits = rev(month.name)) +
  theme_classic() +
  labs(title = &amp;quot;Average Delay by Month&amp;quot;, y = &amp;quot;&amp;quot;, x = &amp;quot;Delay (minutes)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/nycflights13/2019-03-27-nyc-fights13_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bank ATM Cash Machine Forecast w/ Time Series</title>
      <link>/post/atm/bank/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/atm/bank/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. The variable ‘Cash’ is provided in hundreds of dollars.&lt;/p&gt;
&lt;p&gt;This is a time series spanning daily transactions from May 1, 2009 to April 30, 2010 from four ATMs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;forecast how much cash is taken out of 4 different ATM machines for May 2010&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ATM624Data.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
atm &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview first 5 rows
head(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         DATE  ATM Cash
## 1 2009-05-01 ATM1   96
## 2 2009-05-01 ATM2  107
## 3 2009-05-02 ATM1   82
## 4 2009-05-02 ATM2   89
## 5 2009-05-03 ATM1   85
## 6 2009-05-03 ATM2   90&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    1474 obs. of  3 variables:
##  $ DATE: Date, format: &amp;quot;2009-05-01&amp;quot; &amp;quot;2009-05-01&amp;quot; ...
##  $ ATM : Factor w/ 4 levels &amp;quot;ATM1&amp;quot;,&amp;quot;ATM2&amp;quot;,..: 1 2 1 2 1 2 1 2 1 2 ...
##  $ Cash: num  96 107 82 89 85 90 90 55 99 79 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(atm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       DATE              ATM           Cash        
##  Min.   :2009-05-01   ATM1:365   Min.   :    0.0  
##  1st Qu.:2009-08-01   ATM2:365   1st Qu.:    0.5  
##  Median :2009-11-01   ATM3:365   Median :   73.0  
##  Mean   :2009-10-31   ATM4:365   Mean   :  155.6  
##  3rd Qu.:2010-02-01   NA&amp;#39;s: 14   3rd Qu.:  114.0  
##  Max.   :2010-05-14              Max.   :10919.8  
##                                  NA&amp;#39;s   :19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Skewed distribution since the mean is higher than the third quartile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview periods between dates in the time series
xts::periodicity(unique(atm$DATE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Daily periodicity from 2009-05-01 to 2010-05-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans daily transactions from May 1, 2009 to May 14, 2010.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations that have no missing values
atm[!complete.cases(atm), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           DATE  ATM Cash
## 87  2009-06-13 ATM1   NA
## 93  2009-06-16 ATM1   NA
## 98  2009-06-18 ATM2   NA
## 105 2009-06-22 ATM1   NA
## 110 2009-06-24 ATM2   NA
## 731 2010-05-01 &amp;lt;NA&amp;gt;   NA
## 732 2010-05-02 &amp;lt;NA&amp;gt;   NA
## 733 2010-05-03 &amp;lt;NA&amp;gt;   NA
## 734 2010-05-04 &amp;lt;NA&amp;gt;   NA
## 735 2010-05-05 &amp;lt;NA&amp;gt;   NA
## 736 2010-05-06 &amp;lt;NA&amp;gt;   NA
## 737 2010-05-07 &amp;lt;NA&amp;gt;   NA
## 738 2010-05-08 &amp;lt;NA&amp;gt;   NA
## 739 2010-05-09 &amp;lt;NA&amp;gt;   NA
## 740 2010-05-10 &amp;lt;NA&amp;gt;   NA
## 741 2010-05-11 &amp;lt;NA&amp;gt;   NA
## 742 2010-05-12 &amp;lt;NA&amp;gt;   NA
## 743 2010-05-13 &amp;lt;NA&amp;gt;   NA
## 744 2010-05-14 &amp;lt;NA&amp;gt;   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ATM transactions have missing values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(factor(atm$ATM)[!is.na(atm$Cash) &amp;amp; atm$Cash %% 1 != 0])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ATM1 ATM2 ATM3 ATM4 
##    0    0    0  365&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are non-integer transactions at ATM 4 implying that these data are likely debit card purchase transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# time plot represents a line graph that plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  n &amp;lt;- nrow(atm_ts); l &amp;lt;- rep(1, n); m &amp;lt;- rep(20, n); h &amp;lt;- rep(100, n)
  print(plot(cbind(atm_ts, l, m,h), main=paste0(&amp;quot;ATM&amp;quot;, i)))
  
# histogram displays the frequency at which values in a vector occur.
  hist(atm_ts, col=&amp;quot;green&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time Plots and Histograms for ATM1 and ATM2 are unremarkable.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM3 shows the data consists mostly of zero values with a handful of transactions occurring at the end of the series.&lt;/li&gt;
&lt;li&gt;ATM3 will not be modeled due to these degenerative properties.&lt;/li&gt;
&lt;li&gt;Time Plot and Histogram of ATM4 shows an extreme outlier around the three-quarter mark of the series. The horizontal lines in the Time Plots delineate $1, $20, and $100 in red, green, and blue; respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation The xts::xts()function converts data to a time series object which displays better in visualizations than time series objects created using other packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(4, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
for(i in 1:length(levels(atm$ATM))) {
  atm_sub &amp;lt;- subset(atm, ATM == paste0(&amp;quot;ATM&amp;quot;, i))
  atm_ts &amp;lt;- xts::xts(atm_sub$Cash, order.by=atm_sub$DATE)
  acf(na.omit(atm_ts), ylab=paste0(&amp;quot;ACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;) 
  pacf(na.omit(atm_ts), ylab=paste0(&amp;quot;PACF ATM&amp;quot;, i), main=&amp;quot;&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots for ATM1, ATM2, and ATM3 show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations. The ACF and PACF plots for ATM3 however, are not reliable due to the death of observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;p&gt;Data are cleaned using forecast::tsclean() and then converted to a time series object using the ts() function. The tsclean() function imputes nulls and removes outliers. The ts()function converts data to a time series object which is compatible with the forecast package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:length(levels(atm$ATM))) {
  atm_num &amp;lt;- paste0(&amp;quot;ATM&amp;quot;, i)
  atm_sub &amp;lt;- subset(atm, ATM == atm_num, select=-2)
  atm_sub$Cash &amp;lt;- forecast::tsclean(atm_sub$Cash, replace.missing=T)
  assign(atm_num, ts(atm_sub$Cash, frequency = 7, start=start(atm_sub$DATE)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-examine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Examine&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 1), mar = c(0, 4, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(ATM1, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(ATM1, order=7), col=2)  # weekly
lines(forecast::ma(ATM1, order=30), col=4) # monthly
plot(ATM2, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM3&amp;quot;)
lines(forecast::ma(ATM2, order=7), col=2)  # weekly
lines(forecast::ma(ATM2, order=30), col=4) # monthly
plot(ATM4, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM4&amp;quot;)
lines(forecast::ma(ATM4, order=7), col=2)  # weekly
lines(forecast::ma(ATM4, order=30), col=4) # monthly&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 7-day (weekly) and 30-day (monthly) moving average smoother line shows that the data for the ATMs have no apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposition Plot decomposes and plots the observed values, the underlying trend, seasonality, and randomness of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM1), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM2), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(ATM4), col=3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have no apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey-fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey-Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM1): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM1
## Dickey-Fuller = -4.5329, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM2): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM2
## Dickey-Fuller = -6.046, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(ATM4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(ATM4): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  ATM4
## Dickey-Fuller = -5.6304, Lag order = 7, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-values are below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows by index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train/test split
index_train &amp;lt;- 1:(length(ATM1) - 30)
ATM1_train &amp;lt;- ts(ATM1[index_train], frequency=7)
ATM1_test &amp;lt;- ts(ATM1[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM2) - 30)
ATM2_train &amp;lt;- ts(ATM2[index_train], frequency=7)
ATM2_test &amp;lt;- ts(ATM2[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM3) - 30)
ATM3_train &amp;lt;- ts(ATM3[index_train], frequency=7)
ATM3_test &amp;lt;- ts(ATM3[-index_train], frequency=7)
index_train &amp;lt;- 1:(length(ATM4) - 30)
ATM4_train &amp;lt;- ts(ATM4[index_train], frequency=7)
ATM4_test &amp;lt;- ts(ATM4[-index_train], frequency=7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The indexed rows for the test set are a window at the end of the times series. The window sized for the testing set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted.Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda1 &amp;lt;- forecast::BoxCox.lambda(ATM1_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4355901&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda2 &amp;lt;- forecast::BoxCox.lambda(ATM2_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7156895&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda4 &amp;lt;- forecast::BoxCox.lambda(ATM4_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3945256&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameters suggested are around λ=0.5. This rounded (more interpretable) value is suggestive of a 1/yt‾‾√ transformation. These Box-Cox transformations stabilize the variance and make each series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit1 &amp;lt;- forecast::auto.arima(ATM1_train, stepwise=F, approximation=F, d=0, lambda=lambda1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM1_train 
## ARIMA(0,0,2)(1,1,1)[7] 
## Box Cox transformation: lambda= 0.4355901 
## 
## Coefficients:
##          ma1      ma2    sar1     sma1
##       0.1449  -0.1116  0.1320  -0.7243
## s.e.  0.0547   0.0537  0.0893   0.0669
## 
## sigma^2 estimated as 6.441:  log likelihood=-770.86
## AIC=1551.73   AICc=1551.92   BIC=1570.69&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit2 &amp;lt;- forecast::auto.arima(ATM2_train, stepwise=F, approximation=F, d=0, lambda=lambda2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM2_train 
## ARIMA(2,0,2)(0,1,1)[7] with drift 
## Box Cox transformation: lambda= 0.7156895 
## 
## Coefficients:
##           ar1      ar2     ma1     ma2     sma1    drift
##       -0.4282  -0.9254  0.4761  0.8044  -0.7672  -0.0246
## s.e.   0.0464   0.0413  0.0764  0.0555   0.0483   0.0155
## 
## sigma^2 estimated as 66.34:  log likelihood=-1152.83
## AIC=2319.66   AICc=2320.01   BIC=2346.21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit4 &amp;lt;- forecast::auto.arima(ATM4_train, stepwise=F, approximation=F, d=0, lambda=lambda4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: ATM4_train 
## ARIMA(1,0,0)(2,0,0)[7] with non-zero mean 
## Box Cox transformation: lambda= 0.3945256 
## 
## Coefficients:
##          ar1    sar1    sar2     mean
##       0.0814  0.2060  0.1911  22.7977
## s.e.  0.0548  0.0537  0.0547   0.9477
## 
## sigma^2 estimated as 97.25:  log likelihood=-1240.53
## AIC=2491.06   AICc=2491.24   BIC=2510.13&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluate&lt;/h2&gt;
&lt;p&gt;ACF and PACF&lt;/p&gt;
&lt;p&gt;ACF plot shows the autocorrelations between each observation and its immediate predecessor (lagged observation). The PACF plot shows the autocorrelations between the current observation and each individual lagged observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(3, 2), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit1), ylab=&amp;quot;ACF ATM1&amp;quot;); pacf(residuals(fit1), ylab=&amp;quot;PACF ATM1&amp;quot;)
acf(residuals(fit2), ylab=&amp;quot;ACF ATM2&amp;quot;); pacf(residuals(fit2), ylab=&amp;quot;PACF ATM2&amp;quot;)
acf(residuals(fit4), ylab=&amp;quot;ACF ATM4&amp;quot;); pacf(residuals(fit4), ylab=&amp;quot;PACF ATM4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the models appear to display the characteristics of White Noise in the ACF and PACF plots with only one of the twenty residuals (or 0.05%) being significant. At a 95% confidence interval this is within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The arma attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,period,d,and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit1), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit1)
## X-squared = 5.7195, df = 5, p-value = 0.3345&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit2), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit2)
## X-squared = 7.9286, df = 5, p-value = 0.1602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit4), lag=7, fitdf=sum(fit1$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit4)
## X-squared = 4.6833, df = 5, p-value = 0.4557&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the models are not significantly different from zero at α=0.05. The residuals of the models display the characteristics of White Noise. The models pass the required checks and are therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;ATM3 was not modeled due to its degenerative properties. To forecast values for ATM3, the model for an ATM with a similar mean will be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(mean(ATM1), mean(ATM2), mean(ATM3[ATM3!=0]), mean(ATM4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  84.15479  62.59178  87.66667 444.75681&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mean of ATM1 is very close to the mean of the few values in ATM3. Therefore, the ARIMA(0,0,1)(2,0,0)7 ARIMA model for ATM1 will be used to make predictions for ATM3.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- forecast::Arima(ATM3_train, model=fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data were not seasonally adjusted, they need not be reseasonalized prior to forecast. Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a red line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast1 &amp;lt;- forecast::forecast(fit1, h=30)
fcast2 &amp;lt;- forecast::forecast(fit2, h=30)
fcast3 &amp;lt;- forecast::forecast(fit3, h=30)
fcast4 &amp;lt;- forecast::forecast(fit4, h=30)
par(mfrow=c(4, 1), mar = c(0, 4, 0, 0), oma = c(4, 4, 2, 0.5))
plot(fcast1, ylab=&amp;quot;Cash ATM1&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM1_test, -length(ATM1_train)), col=&amp;quot;red&amp;quot;)
plot(fcast2, ylab=&amp;quot;Cash ATM2&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;); 
lines(lag(ATM2_test, -length(ATM2_train)), col=&amp;quot;red&amp;quot;)
plot(fcast3, ylab=&amp;quot;Cash ATM3&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM3_test, -length(ATM3_train)), col=&amp;quot;red&amp;quot;)
plot(fcast4, ylab=&amp;quot;Cash ATM4&amp;quot;, main=&amp;quot;&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(ATM4_test, -length(ATM4_train)), col=&amp;quot;red&amp;quot;)
title(&amp;quot;ATM Predictions&amp;quot;, outer=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/2019-02-07-bank_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The predictions appear to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast1, length(ATM1_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ME   RMSE    MAE      MPE    MAPE  MASE  ACF1
## Training set   2.038 25.007 16.039  -96.186 114.754 0.427 0.011
## Test set     -53.187 53.187 53.187 -177.290 177.290 1.416    NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast2, length(ATM2_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ME   RMSE    MAE      MPE    MAPE MASE   ACF1
## Training set   1.456 24.795 17.275     -Inf     Inf 0.40 -0.013
## Test set     -44.485 44.485 44.485 -148.284 148.284 1.03     NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast4, length(ATM4_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    ME    RMSE     MAE       MPE     MAPE MASE  ACF1
## Training set   96.204 360.567 280.446  -342.343  388.847 0.72 0.017
## Test set     -385.878 385.878 385.878 -1286.261 1286.261 0.99    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications vary. ATM1 and ATM2 predictions are more accurate than ATM4 predictions. The closer the original data are to being White Noise, the less accurate the predictions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Forecasting w/ Time Series Analysis</title>
      <link>/post/residential_energy/residential-energy-usage/</link>
      <pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/residential_energy/residential-energy-usage/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the time series analysis. A simple dataset of residential power usage from January 1998 to December 2013.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through an analysis, model this data and monthly forecast for 2014&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Visualizations&lt;/li&gt;
&lt;li&gt;ACF and PACF&lt;/li&gt;
&lt;li&gt;Clean The Data&lt;/li&gt;
&lt;li&gt;Trend Preview&lt;/li&gt;
&lt;li&gt;Data Decomposition Plot&lt;/li&gt;
&lt;li&gt;Stationarity Test&lt;/li&gt;
&lt;li&gt;Model Data&lt;/li&gt;
&lt;li&gt;Transformation&lt;/li&gt;
&lt;li&gt;ARIMA Model&lt;/li&gt;
&lt;li&gt;Evaluation&lt;/li&gt;
&lt;li&gt;Box-Ljung Test&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;li&gt;Model Accuracy&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/jzuniga123&amp;quot;
file &amp;lt;- &amp;quot;/SPS/master/DATA%20624/ResidentialCustomerForecastLoad-624.xlsx&amp;quot;
download.file(paste0(sourceURL, file), &amp;quot;temp.xlsx&amp;quot;, mode=&amp;quot;wb&amp;quot;)
energy &amp;lt;- xlsx::read.xlsx(&amp;quot;temp.xlsx&amp;quot;, sheetIndex=1, header=T)

# the “YYYY-MMM” format dates are interpreted as factors. They must be converted to dates
energy$YYYY.MMM &amp;lt;- as.Date(paste0(energy$YYYY.MMM,&amp;quot;-01&amp;quot;), format = &amp;quot;%Y-%b-%d&amp;quot;)
invisible(file.remove(&amp;quot;temp.xlsx&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CaseSequence   YYYY.MMM     KWH
## 1          733 1998-01-01 6862583
## 2          734 1998-02-01 5838198
## 3          735 1998-03-01 5420658
## 4          736 1998-04-01 5010364
## 5          737 1998-05-01 4665377
## 6          738 1998-06-01 6467147&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the class of the dataset
class(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    192 obs. of  3 variables:
##  $ CaseSequence: num  733 734 735 736 737 738 739 740 741 742 ...
##  $ YYYY.MMM    : Date, format: &amp;quot;1998-01-01&amp;quot; &amp;quot;1998-02-01&amp;quot; ...
##  $ KWH         : num  6862583 5838198 5420658 5010364 4665377 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview descriptive statistics on quantitative and qualitative variables
summary(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   CaseSequence      YYYY.MMM               KWH          
##  Min.   :733.0   Min.   :1998-01-01   Min.   :  770523  
##  1st Qu.:780.8   1st Qu.:2001-12-24   1st Qu.: 5429912  
##  Median :828.5   Median :2005-12-16   Median : 6283324  
##  Mean   :828.5   Mean   :2005-12-15   Mean   : 6502475  
##  3rd Qu.:876.2   3rd Qu.:2009-12-08   3rd Qu.: 7620524  
##  Max.   :924.0   Max.   :2013-12-01   Max.   :10655730  
##                                       NA&amp;#39;s   :1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the periods between dates in dataset
xts::periodicity(unique(energy$YYYY.MMM))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Monthly periodicity from 1998-01-01 to 2013-12-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe spans monthly from January 1, 1998 to December 1, 2013.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview observations in the dataframe that have no missing values
energy[!complete.cases(energy), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     CaseSequence   YYYY.MMM KWH
## 129          861 2008-09-01  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Dataframe contains one missing value in kWh usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizations&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plots each observed value against the time of the observation, with a single line connecting each observation across the entire period
kWh &amp;lt;- xts::xts(energy$KWH, order.by=energy$YYYY.MMM)
par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
plot(kWh, main=&amp;quot;kWh&amp;quot;)

# display frequency at which values in a vector occur
hist(kWh, col=&amp;quot;yellow&amp;quot;, xlab=&amp;quot;&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obervations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Line plot and Histogram shows an outlier around the three-quarter mark of the series.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;acf-and-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACF and PACF&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
# ACF autocorrelations between each observation and its immediate predecessor (lagged observation)
acf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;) 

# PACF autocorrelations between the current observation and each individual lagged observation
pacf(na.omit(kWh), ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF and PACF plots show autocorrelation between each observation and its immediate predecessor and autocorrelation between the current observation and other individual lagged observations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean The Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data cleaning w/ forecast::tsclean() and converted to a time series object using the ts(). 
# tsclean() function imputes nulls and removes outliers.
# ts() function converts data to a time series object which is compatible with the forecast package.
kWh &amp;lt;- ts(forecast::tsclean(energy$KWH, replace.missing=T), 
          frequency = 12, start=start(energy$YYYY.MMM)) # data sampled monthly = 12
kWh[kWh==min(kWh)] &amp;lt;- mean(kWh[kWh!=min(kWh)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trend-preview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trend Preview&lt;/h2&gt;
&lt;p&gt;A moving average smoother is helpful in examining what kind of trend is involved in a series. Moving average models should not be confused with moving average smoothing. A moving average model is used for forecasting future values while moving average smoothing is used for estimating the trend-cycle component of past values. The ma() function computes a simple moving average smoother of a given time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(kWh, col=8, xaxt = &amp;quot;n&amp;quot;, ylab=&amp;quot;ATM1&amp;quot;)
lines(forecast::ma(kWh, order=6), col=6)  # pink line biannual period
lines(forecast::ma(kWh, order=12), col=4) # blue line annual period&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 6-month and 12-month moving average smoother line shows that the data has a slight apparent trend.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Decomposition Plot&lt;/h2&gt;
&lt;p&gt;Decomposes and plots the &lt;strong&gt;observed&lt;/strong&gt; values, the underlying &lt;strong&gt;trend,&lt;/strong&gt; &lt;strong&gt;seasonality,&lt;/strong&gt; and &lt;strong&gt;randomness&lt;/strong&gt; of the time series data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(decompose(kWh), col=5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obseravations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the trend-cycle and seasonal indices computed by additive decomposition shows that the data have a slight apparent trend, seasonal fluctuations, and fairly random residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationarity Test&lt;/h2&gt;
&lt;div id=&#34;dickey_fuller-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dickey_Fuller Test&lt;/h3&gt;
&lt;p&gt;An augmented Dickey-Fuller unit root test evaluates if the data exhibit a Stationarity process with deterministic trend or a Stationarity process with stochastic trend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tseries::adf.test(kWh)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in tseries::adf.test(kWh): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  kWh
## Dickey-Fuller = -4.5454, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The augmented Dickey-Fuller unit root test p-value is below α=0.05. Therefore, the null hypothesis that the data has unit roots is rejected. The data exhibit stochastic trend which suggests using regression (AR) in lieu of differencing. Autoregressive (AR) modeling acts like partial differencing when ϕ&amp;lt;1. When ϕ=1 the AR(1) model is like a first-order difference.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;model-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Data&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;train&lt;/strong&gt; and &lt;strong&gt;test&lt;/strong&gt; sets are created by referencing rows w/ index. The indexed rows for the testing set are a window at the end of the times series. The window sized for the test set is that of the desired prediction. The training set window is comprised of the indexes which are the complement of the test set indexes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index_train &amp;lt;- 1:(length(kWh) - 12)
kWh_train &amp;lt;- ts(kWh[index_train], frequency=12)
kWh_test &amp;lt;- ts(kWh[index_train], frequency=12)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transformation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transformation&lt;/h2&gt;
&lt;p&gt;The Augmented Dickey-Fuller Test results support not differencing. Data can be seasonally adjusted for modeling and then reseasonalized for predictions. The modeling algorithm being used evaluates seasonal components and produces predictions that reflect the seasonality in the underlying data. Therefore, the data need not be seasonally adjusted. Heteroskedasticity refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable. Box-Cox transformations can help to stabilize the variance of a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(lambda &amp;lt;- forecast::BoxCox.lambda(kWh_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.1733063&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Box-Cox transformation parameter suggested is about λ=−0.25. This rounded (slightly more interpretable) value is suggestive of an inverse quartic root. This Box-Cox transformation stabilizes the variance and makes the series relatively homoskedastic with equal variance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ARIMA Model&lt;/h2&gt;
&lt;p&gt;The auto.arima() function chooses an ARIMA model automatically. It uses a variation of the Hyndman and Khandakar algorithm which combines unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The function takes some short-cuts in order to speed up the computation and will not always yield the best model. Setting stepwise and approximation to FALSE prevents the function from taking short-cuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(fit &amp;lt;- forecast::auto.arima(kWh_train, stepwise=F, approximation=F, d=0, lambda=lambda))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: kWh_train 
## ARIMA(0,0,3)(2,1,0)[12] with drift 
## Box Cox transformation: lambda= -0.1733063 
## 
## Coefficients:
##          ma1     ma2     ma3     sar1     sar2  drift
##       0.2807  0.0855  0.2232  -0.7724  -0.4408  1e-04
## s.e.  0.0757  0.0823  0.0687   0.0742   0.0812  1e-04
## 
## sigma^2 estimated as 3.707e-05:  log likelihood=621.98
## AIC=-1229.95   AICc=-1229.25   BIC=-1208.08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The auto.arima() function suggests an ARIMA(0,0,3)(2,1,0)12 model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(2, 1), mar = c(3, 5, 0, 0), oma = c(0, 0, 0.5, 0.5))
acf(residuals(fit), ylab=&amp;quot;ACF kWh&amp;quot;); pacf(residuals(fit), ylab=&amp;quot;PACF kWh&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residuals of the model appear to display the characteristics of White Noise in both the ACF and PACF plots. None of the residuals are significant. At a 95% confidence interval this is well within probabilistic expectations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;box-ljung-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Box-Ljung Test&lt;/h2&gt;
&lt;p&gt;The Box-Ljung test is helpful in assessing if data follow a White Noise pattern. The ARIMA attribute of the fitted model returns a vector containing the ARIMA model parameters p,q,P,Q,periods,d and D; in that order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Box.test(residuals(fit), lag=7, fitdf=sum(fit$arma[1:2]), type=&amp;quot;Ljung-Box&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Box-Ljung test
## 
## data:  residuals(fit)
## X-squared = 7.2523, df = 4, p-value = 0.1231&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The null hypothesis of independence is not rejected. The Box-Ljung shows that the autocorrelations of the residuals from the model are not significantly different from zero at α=0.05. The residuals of the model displays the characteristics of White Noise. The model passes the required checks and is therefore suitable for forecasting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecasting&lt;/h2&gt;
&lt;p&gt;Forecasts are done using the forecast::forecast() function. Since the data was not seasonally adjusted, they need not be reseasonalized prior to forecast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fcast &amp;lt;- forecast::forecast(fit, h=15)
plot(fcast, ylab=&amp;quot;kWh&amp;quot;, main=&amp;quot;kWh Predictions&amp;quot;, xaxt=&amp;quot;n&amp;quot;)
lines(lag(kWh_test, -length(kWh_train)), col=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/residential_energy/2019-02-07-residential-energy-usage_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The prediction appears to produce a useful forecasts that reflect patterns in the original data.&lt;/li&gt;
&lt;li&gt;Prediction point estimates are represented by a blue line, prediction intervals are represented by blue bands, and actual values are represented by a pink line.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Accuracy&lt;/h2&gt;
&lt;p&gt;The accuracy() function is helpful for obtaining summary measures of the forecast accuracy: Mean Error (ME), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), Mean Percentage Error (MPE), Mean Absolute Percentage Error (MAPE), Mean Absolute Scaled Error (MASE), and Autocorrelation of errors at lag 1 (ACF1).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round(forecast::accuracy(fcast, length(kWh_test)), 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       ME      RMSE       MAE          MPE        MAPE
## Training set    39449.18  581186.1  456353.6        0.056       7.067
## Test set     -9046871.23 9046871.2 9046871.2 -5026039.573 5026039.573
##               MASE  ACF1
## Training set 0.413 0.115
## Test set     8.185    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These accuracy for the predications is fair. The large metrics are representative of the large values found in the data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Efficiency on Buildings</title>
      <link>/post/energye/energye/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energye/energye/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;The dataset is available at [&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Energy+efficiency&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;to explore three data points, and visualize how they influence the energy load.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following variables &lt;strong&gt;Wall.Area&lt;/strong&gt;, &lt;strong&gt;Roof.Area&lt;/strong&gt;, &lt;strong&gt;Glazing.Area&lt;/strong&gt; are identified as key indicators that can influence the energy load efficiency for both (Heating and Cooling spaces).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Plot Load Distribution Using Scatter Plot&lt;/li&gt;
&lt;li&gt;Plot Heating Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Cooling Load Efficiency&lt;/li&gt;
&lt;li&gt;Plot Energy Efficiency&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())

sourceURL &amp;lt;- &amp;quot;https://raw.githubusercontent.com/StephenElston/DataScience350/master/Lecture1/EnergyEfficiencyData.csv&amp;quot;

df &amp;lt;- read.csv( sourceURL, header = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Relative.Compactness Surface.Area Wall.Area Roof.Area Overall.Height
## 1                 0.98        514.5     294.0    110.25              7
## 2                 0.98        514.5     294.0    110.25              7
## 3                 0.98        514.5     294.0    110.25              7
## 4                 0.98        514.5     294.0    110.25              7
## 5                 0.90        563.5     318.5    122.50              7
## 6                 0.90        563.5     318.5    122.50              7
##   Orientation Glazing.Area Glazing.Area.Distribution Heating.Load
## 1           2            0                         0        15.55
## 2           3            0                         0        15.55
## 3           4            0                         0        15.55
## 4           5            0                         0        15.55
## 5           2            0                         0        20.84
## 6           3            0                         0        21.46
##   Cooling.Load
## 1        21.33
## 2        21.33
## 3        21.33
## 4        21.33
## 5        28.28
## 6        25.38&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    768 obs. of  10 variables:
##  $ Relative.Compactness     : num  0.98 0.98 0.98 0.98 0.9 0.9 0.9 0.9 0.86 0.86 ...
##  $ Surface.Area             : num  514 514 514 514 564 ...
##  $ Wall.Area                : num  294 294 294 294 318 ...
##  $ Roof.Area                : num  110 110 110 110 122 ...
##  $ Overall.Height           : num  7 7 7 7 7 7 7 7 7 7 ...
##  $ Orientation              : int  2 3 4 5 2 3 4 5 2 3 ...
##  $ Glazing.Area             : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ Glazing.Area.Distribution: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Heating.Load             : num  15.6 15.6 15.6 15.6 20.8 ...
##  $ Cooling.Load             : num  21.3 21.3 21.3 21.3 28.3 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Categorize useful variables and convert them to a categorical variables, namely &lt;strong&gt;Orientation&lt;/strong&gt;, &lt;strong&gt;Glazing.Area.Distribution&lt;/strong&gt;, and &lt;strong&gt;Glazing.Area&lt;/strong&gt; (variance) variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## change vector values to factor values
df$Orientation &amp;lt;- as.factor(df$Orientation) 

## attributes of variable 
levels(df$Orientation) &amp;lt;- c(&amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

## change vector values to factor values
df$Glazing.Area.Distribution &amp;lt;- as.factor(df$Glazing.Area.Distribution)

## attributes of variable
levels(df$Glazing.Area.Distribution) &amp;lt;- c(&amp;quot;UnKnown&amp;quot;, &amp;quot;Uniform&amp;quot;, &amp;quot;North&amp;quot;, &amp;quot;East&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

## change vector values to factor values
df$Glazing.Area &amp;lt;- as.factor(df$Glazing.Area) 

## attributes of variable
levels(df$Glazing.Area) &amp;lt;- c(&amp;quot;0%&amp;quot;, &amp;quot;10%&amp;quot;, &amp;quot;25%&amp;quot;, &amp;quot;40%&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Relative.Compactness  Surface.Area     Wall.Area       Roof.Area    
##  Min.   :0.6200       Min.   :514.5   Min.   :245.0   Min.   :110.2  
##  1st Qu.:0.6825       1st Qu.:606.4   1st Qu.:294.0   1st Qu.:140.9  
##  Median :0.7500       Median :673.8   Median :318.5   Median :183.8  
##  Mean   :0.7642       Mean   :671.7   Mean   :318.5   Mean   :176.6  
##  3rd Qu.:0.8300       3rd Qu.:741.1   3rd Qu.:343.0   3rd Qu.:220.5  
##  Max.   :0.9800       Max.   :808.5   Max.   :416.5   Max.   :220.5  
##  Overall.Height Orientation Glazing.Area Glazing.Area.Distribution
##  Min.   :3.50   North:192   0% : 48      UnKnown: 48              
##  1st Qu.:3.50   East :192   10%:240      Uniform:144              
##  Median :5.25   South:192   25%:240      North  :144              
##  Mean   :5.25   West :192   40%:240      East   :144              
##  3rd Qu.:7.00                            South  :144              
##  Max.   :7.00                            West   :144              
##   Heating.Load    Cooling.Load  
##  Min.   : 6.01   Min.   :10.90  
##  1st Qu.:12.99   1st Qu.:15.62  
##  Median :18.95   Median :22.08  
##  Mean   :22.31   Mean   :24.59  
##  3rd Qu.:31.67   3rd Qu.:33.13  
##  Max.   :43.10   Max.   :48.03&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-load-distribution-using-scatter-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Load Distribution Using Scatter Plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## visualize if there is any relation between &amp;#39;Roof.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; and how load is distributed using scatter plot.
ggplot(df, aes(x = Cooling.Load, y = Heating.Load), alpha = 0.5)+
  geom_point(aes(colour = Roof.Area ))+
  facet_grid(Overall.Height + Glazing.Area ~ Surface.Area,  space = &amp;quot;free&amp;quot;) +
  ggtitle(&amp;quot;Load distribuiton of energy by Roof Area and Surface Area \n by Glazing Area and Overall Height&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Roof area and Surface area range is high for minimum/ lowest (3.5) over-all height and&lt;/li&gt;
&lt;li&gt;Roof area and Surface area range is low for maximum/ highest (7.0) over-all height.&lt;/li&gt;
&lt;li&gt;There are no data points when the overall height is 7 and highest surface area range and also for low overall height 3.5, we have no data points with the low surface area range.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-heating-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Heating Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## plot how &amp;#39;Wall.Area&amp;#39; influence heating load using raster plot.
ggplot(df, aes( Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Heating.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;steelblue&amp;quot;, high = &amp;quot;red&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Heating Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By looking at the figures, we can conclude that &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in heating, irrespective of Surface Area and Roof Area. (Higher the wall area, higher the heating load).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-cooling-load-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Cooling Load Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## plot how &amp;#39;Wall.Area&amp;#39; influence cooling load using raster plot.
ggplot(df, aes(Surface.Area, Roof.Area)) +
  geom_raster(aes(fill = Cooling.Load), interpolate = TRUE) +
  scale_fill_gradient(low = &amp;quot;grey&amp;quot;, high = &amp;quot;steelblue&amp;quot;)+
  facet_wrap(~Wall.Area, scales = &amp;quot;free&amp;quot; )+
  ggtitle(&amp;#39;Measuring Cooling Load distribution \n by Wall Area, Surface Area and Roof Area&amp;#39;) +
  xlab(&amp;#39;Surface Area&amp;#39;) + ylab(&amp;#39;Roof Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So, &lt;strong&gt;Wall Area&lt;/strong&gt; plays a significant role in both Heating and Cooling Load efficiency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-energy-efficiency&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot Energy Efficiency&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## we have seen more variation in load data when the overall height is (7.0). So lets create a subset named(energy.eff.sub7.0) which contains the filtered data with overall height = 7.0. Lets visualize, if the &amp;#39;Roof.Area&amp;#39;, &amp;#39;Wall.Area&amp;#39;, &amp;#39;Surface.Area&amp;#39; and &amp;#39;Glazing.Area&amp;#39; are influencing the load efficiency.
energy.eff.sub7.0 &amp;lt;- df[ df$Overall.Height ==7.0,]
ggplot(energy.eff.sub7.0,
       aes(x = Cooling.Load, y = Heating.Load, group = factor(round(Wall.Area)), 
           size = Glazing.Area,
           shape = factor(round(Wall.Area))))+
  geom_point(aes(colour= factor(round(Surface.Area))), alpha = 0.3)+
  geom_smooth(method = &amp;quot;lm&amp;quot;,se = TRUE )+
  facet_wrap(~ Roof.Area) +
  ggtitle(&amp;#39;Load efficiency by Roof Area, by Wall Area by Surface Area and by Glazing Area&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyE/2019-02-03-energye_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is clearly evident that the Load efficiency is influenced by the Roof Area, Wall Area, Surface Area, and Glazing Area.&lt;/li&gt;
&lt;li&gt;When the Glazing Area is high, Roof Area is high and Wall Area is high, Load will be high and viceversa.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Energy Demand Analysis w/ Time Series Forecasting</title>
      <link>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/energyd/energy-demand-analysis-w-time-series-forecasting/</guid>
      <description>


&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;R. H. Shumway, D. S. Stoffer. &lt;em&gt;Time Series Analysis and Its Applications&lt;/em&gt;. 2010.&lt;/li&gt;
&lt;li&gt;R. J. Hyndman. &lt;em&gt;Forecasting: principles and practice&lt;/em&gt;. 2013.&lt;/li&gt;
&lt;li&gt;P. S. P. Cowpertwait, A. V. Metcalfe. &lt;em&gt;Introductory Time Series with R&lt;/em&gt;. 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on an analysis of the energy demands of a European country.&lt;/p&gt;
&lt;p&gt;The dataset of the daily energy needs (in GWh) between 2004 and 2010.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reseach-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reseach questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;build a model for energy demand forecasting using time series analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;A time series forecast using the arima model as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;seasonal ARIMA model&lt;/li&gt;
&lt;li&gt;Forecast model&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sourceURL &amp;lt;- &amp;#39;https://gist.githubusercontent.com/Peque/715e91350f0e68e3342f/raw/d28312ac0e49888a5079fcea188770acaf3aa4a2/mme.csv&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download and load data into memory
tmp &amp;lt;- tempfile()
download.file(sourceURL, tmp, method = &amp;#39;curl&amp;#39;)
df &amp;lt;- read.csv(tmp)
unlink(tmp)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory data analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       date demand
## 1 01-01-04 488.07
## 2 02-01-04 582.02
## 3 03-01-04 575.58
## 4 04-01-04 542.39
## 5 05-01-04 600.26
## 6 06-01-04 544.76&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# convert date strings to POSIX dates
df$date &amp;lt;- strptime(df$date, format = &amp;#39;%d-%m-%y&amp;#39;)
# day of week
df$day &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%A&amp;#39;))
# day of year
df$yearday &amp;lt;- as.factor(strftime(df$date, format = &amp;#39;%m%d&amp;#39;))
# structure for analysis
str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3288 obs. of  4 variables:
##  $ date   : POSIXlt, format: &amp;quot;2004-01-01&amp;quot; &amp;quot;2004-01-02&amp;quot; ...
##  $ demand : num  488 582 576 542 600 ...
##  $ day    : Factor w/ 7 levels &amp;quot;Friday&amp;quot;,&amp;quot;Monday&amp;quot;,..: 5 1 3 4 2 6 7 5 1 3 ...
##  $ yearday: Factor w/ 366 levels &amp;quot;0101&amp;quot;,&amp;quot;0102&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df split to create test set
df_test &amp;lt;- subset(df, date &amp;gt;= strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
df &amp;lt;- subset(df, date &amp;lt; strptime(&amp;#39;01-01-2011&amp;#39;, format = &amp;#39;%d-%m-%Y&amp;#39;))
ts &amp;lt;- ts(df$demand, frequency = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# df and time series objects
demandts &amp;lt;- xts(df$demand, df$date)
plot(demandts, main = &amp;#39;Energy Demand Preview&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A seasonal dependency of demand can be easily spotted in the graphics, although there are other factors that may affect the results, such as the temperature, holidays, weekends, etc..&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand by day of the week
ggplot(df, aes(day, demand)) +
  geom_boxplot(fill=&amp;#39;slateblue&amp;#39;, alpha=0.2) + xlab(&amp;#39;Time&amp;#39;) + ylab(&amp;#39;Demand (GWh)&amp;#39;) + ggtitle(&amp;#39;Demand per day of the week&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During weekends, the demand decreases considerably compared to the rest of the week days.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_per_yearday &amp;lt;- aggregate(demand ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday, avg_demand_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand, f = 1 / 45)
l &amp;lt;- length(avg_demand_per_yearday$demand)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting results
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_per_yearday$demand, type = &amp;#39;l&amp;#39;, main = &amp;#39;Average Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;During the winter &amp;amp; summer seasons the demand is clearly higher exept for, vacation periods. Holydays are also easily spotted in the graphics, being the lowest peaks of demand.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_per_yearday$demand - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The graphics show the errors. Notice how the biggest errors are all negative.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(strftime(dates[order(-abs_diff)], format = &amp;#39;%B %d&amp;#39;), 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;January 01&amp;quot;  &amp;quot;December 25&amp;quot; &amp;quot;May 01&amp;quot;      &amp;quot;January 06&amp;quot;  &amp;quot;August 15&amp;quot;  
##  [6] &amp;quot;December 08&amp;quot; &amp;quot;December 31&amp;quot; &amp;quot;October 12&amp;quot;  &amp;quot;November 01&amp;quot; &amp;quot;December 26&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exact dates which are generating these errors are indeed, holidays or the day just before holidays (as is the case for the 25th November and 31th Devember).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(2, 2))
acf(df$demand, 100, main = &amp;#39;Autocorrelation&amp;#39;)
acf(df$demand, 1500, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)
pacf(df$demand, 1500, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The autocorrelation function shows a highly autocorrelated seasonal non-stationary process with, as expected, yearly and weekly cicles. The ACF alone, however, tells us little about the orders of dependence for ARMIA or AR processes. The PACF is better for AR models, and also shows the weekly and yearly seasons, although the correlation is lost faster with the lag.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data decomposition&lt;/h2&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of weeks at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of weekly seasonal time series
wts &amp;lt;- ts(ts, frequency = 7)
dec_wts &amp;lt;- decompose(wts)
plot(dec_wts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# demand minus week seasonal
df$demand_mws &amp;lt;- df$demand - as.numeric(dec_wts$season)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# decomposition of yearly time series
yts &amp;lt;- ts(subset(df, yearday != &amp;#39;0229&amp;#39;)$demand_mws, frequency = 365)
dec_yts &amp;lt;- decompose(yts)
plot(dec_yts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;
Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decomposition of the yearly seasonal time series. 29th February days are excluded for frequency matching. The time series is formed out of the original observation minus the weekly seasonal data.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;days365 &amp;lt;- which(df$yearday != &amp;#39;0229&amp;#39;)
february29ths &amp;lt;- which(df$yearday == &amp;#39;0229&amp;#39;)
df$demand_mwys[days365] &amp;lt;- df$demand_mws[days365] - as.numeric(dec_yts$season)
# Fill values on February 29th
df$demand_mwys[february29ths] &amp;lt;- df$demand_mws[february29ths]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# form new ts from original observations less the weekly and yearly seasonal data
par(mfrow = c(1, 1))
ts_mwys &amp;lt;- ts(df$demand_mwys, frequency = 1)
demandts_mwys &amp;lt;- xts(df$demand_mwys, df$date)
plot(demandts_mwys, main = &amp;#39;Energy Demand Less Seasonal Data&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# aggregating demand by day of the year (average)
avg_demand_mwys_per_yearday &amp;lt;- aggregate(demand_mwys ~ yearday, df, &amp;#39;mean&amp;#39;)

# computing the smooth curve for the time series. Data is replicated before computing the curve in order to achieve continuity
smooth_yearday &amp;lt;- rbind(avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday, avg_demand_mwys_per_yearday)
smooth_yearday &amp;lt;- lowess(smooth_yearday$demand_mwys, f = 1 / 45)
l &amp;lt;- length(avg_demand_mwys_per_yearday$demand_mwys)
l0 &amp;lt;- 2 * l + 1
l1 &amp;lt;- 3 * l
smooth_yearday &amp;lt;- smooth_yearday$y[l0:l1]

# plotting the result
par(mfrow = c(1, 1))

# setting year to 2000 to allow existence of 29th February
dates &amp;lt;- as.Date(paste(levels(df$yearday), &amp;#39;2000&amp;#39;), format = &amp;#39;%m%d%Y&amp;#39;)
plot(dates, avg_demand_mwys_per_yearday$demand_mwys, type = &amp;#39;l&amp;#39;, main = &amp;#39;Mean Daily Demand&amp;#39;, xlab = &amp;#39;Time&amp;#39;, ylab = &amp;#39;Demand (GWh)&amp;#39;)
lines(dates, smooth_yearday, col = &amp;#39;yellow&amp;#39;, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow = c(1, 2))
diff &amp;lt;- avg_demand_mwys_per_yearday$demand_mwys - smooth_yearday
abs_diff &amp;lt;- abs(diff)
barplot(diff[order(-abs_diff)], main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)
boxplot(diff, main = &amp;#39;Smoothing error&amp;#39;, ylab = &amp;#39;Error&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Plotting the average daily demand of the demand less the seasonal data shows a new error rate much lower than the one seen before.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# new acf and pacf created
par(mfrow = c(1, 2))
acf(df$demand_mwys, 100, main = &amp;#39;Autocorrelation&amp;#39;)
pacf(df$demand_mwys, 100, main = &amp;#39;Partial autocorrelation&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seasonal-arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;seasonal ARIMA model&lt;/h2&gt;
&lt;p&gt;The initial ARIMA parameters have been found using the R &lt;span class=&#34;math inline&#34;&gt;\(auto.arima()\)&lt;/span&gt; function. The differencing parameter &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is selected using the KPSS test. If the null hypothesis of stationarity is accepted when the KPSS is applied to the original time series, then &lt;span class=&#34;math inline&#34;&gt;\(d = 0\)&lt;/span&gt;. Otherwise, the series is differenced until the KPSS accepts the null hypothesis. After that, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; are selected using either AIC or BIC. The SARIMA model has been created using those ARIMA parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# forecast the error w/ test dataframe
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  pred &amp;lt;- c(pred, p)
  errs &amp;lt;- c(errs, p - df_test$demand[i])
  perc &amp;lt;- c(perc, (p - df_test$demand[i]) / df_test$demand[i])
  auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.299037&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test datadrame &lt;strong&gt;(2.3%)&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# special days present less demand than others. Those days may be taken into account in order to reduce the error
specialday &amp;lt;- function(day) {
  correction = 0
  if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0101&amp;#39;, &amp;#39;0501&amp;#39;, &amp;#39;0106&amp;#39;, &amp;#39;0815&amp;#39;, &amp;#39;1012&amp;#39;, &amp;#39;1101&amp;#39;, &amp;#39;1206&amp;#39;, &amp;#39;1208&amp;#39;, &amp;#39;1224&amp;#39;, &amp;#39;1225&amp;#39;, &amp;#39;1226&amp;#39;, &amp;#39;1231&amp;#39;))
      correction = -100
  else if (format(day, &amp;#39;%m%d&amp;#39;) %in% c(&amp;#39;0319&amp;#39;))
    correction = -50

# on Sunday, do not apply correction
  if (as.factor(strftime(day, format = &amp;#39;%A&amp;#39;)) == &amp;#39;Sunday&amp;#39;)
    return(0)
  return(correction)
}

model &amp;lt;- Arima(ts, order = c(2, 1, 2), list(order = c(1, 1, 1), period = 7))
auxts &amp;lt;- ts
auxmodel &amp;lt;- model
errs &amp;lt;- c()
pred &amp;lt;- c()
perc &amp;lt;- c()
for (i in 1:nrow(df_test)) {
  p &amp;lt;- as.numeric(predict(auxmodel, newdata = auxts, n.ahead = 1)$pred)
  correction = specialday(df_test$date[i])
  pred &amp;lt;- c(pred, p + correction)
  errs &amp;lt;- c(errs, p + correction - df_test$demand[i])
  perc &amp;lt;- c(perc, (p + correction - df_test$demand[i]) / df_test$demand[i])
  if (!correction)
    auxts &amp;lt;- ts(c(auxts, df_test$demand[i]), frequency = 7)
  else
    auxts &amp;lt;- ts(c(auxts, p), frequency = 7)
  auxmodel &amp;lt;- Arima(auxts, model = auxmodel)
}
par(mfrow = c(1, 1))
plot(errs, type = &amp;#39;l&amp;#39;, main = &amp;#39;Error in the forecast&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(pred, type = &amp;#39;l&amp;#39;, main = &amp;#39;Real vs. Forecast&amp;#39;, col = &amp;#39;green&amp;#39;)
lines(df_test$demand)
legend(&amp;#39;topright&amp;#39;, c(&amp;#39;Real&amp;#39;, &amp;#39;Forecast&amp;#39;), lty = 1, col = c(&amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;abserr &amp;lt;- mean(abs(errs))
percerr &amp;lt;- mean(abs(perc)) * 100
percerr&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.956568&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mean error across test dataframe &lt;strong&gt;(1,96%)&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forecast-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forecast Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(forecast(Arima(tail(ts, 200), model = model))) +
  labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Energy Demand (GWh)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/energyD/2019-02-02-energy-demand-analysis-w-time-series-forecasting_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Time Series Analysis</title>
      <link>/post/time_series_ap/time-series-analysis/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time_series_ap/time-series-analysis/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the airpassengers dataframe.&lt;/p&gt;
&lt;p&gt;The AirPassenger dataset in R provides monthly totals of US airline passengers, from 1949 to 1960.&lt;/p&gt;
&lt;p&gt;Description of dataframe airpassengers can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through analysis and modelling, preview a time series forecast&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess whether a linear regression or arima model is a best fit for the time series forecast as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;Stationarity test&lt;/li&gt;
&lt;li&gt;Fit a model using an algorithm&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(AirPassengers)
AP &amp;lt;- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is already of a time series class.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview of data
AP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Passenger numbers in (’000) per month for the relevant years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for missing values
sum(is.na(AP))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zero missing values GREAT!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test frequency
frequency(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;12 calendar months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test cycle
cycle(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949   1   2   3   4   5   6   7   8   9  10  11  12
## 1950   1   2   3   4   5   6   7   8   9  10  11  12
## 1951   1   2   3   4   5   6   7   8   9  10  11  12
## 1952   1   2   3   4   5   6   7   8   9  10  11  12
## 1953   1   2   3   4   5   6   7   8   9  10  11  12
## 1954   1   2   3   4   5   6   7   8   9  10  11  12
## 1955   1   2   3   4   5   6   7   8   9  10  11  12
## 1956   1   2   3   4   5   6   7   8   9  10  11  12
## 1957   1   2   3   4   5   6   7   8   9  10  11  12
## 1958   1   2   3   4   5   6   7   8   9  10  11  12
## 1959   1   2   3   4   5   6   7   8   9  10  11  12
## 1960   1   2   3   4   5   6   7   8   9  10  11  12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dataset summary
summary(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   104.0   180.0   265.5   280.3   360.5   622.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statistical values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the raw data using the base plot function
autoplot(AP) + labs(x=&amp;quot;Time&amp;quot;, y =&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(AP~cycle(AP), xlab=&amp;quot;Passenger Numbers (&amp;#39;000)&amp;quot;, ylab=&amp;quot;Months&amp;quot;, col=rgb(0.1,0.9,0.3,0.4), main=&amp;quot;Monthly Air Passengers Boxplot from 1949 to 1961&amp;quot;, horizontal=TRUE, notch=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The passenger numbers increase over time with each year which may be indicative of an increasing linear trend. Possible due to an increase in demand for flights and commercialisation of airlines in that time period.&lt;/li&gt;
&lt;li&gt;The boxplot shows more passengers travelling in months 6 to 9 with higher averages and higher variances than the other months, indicating seasonality within an apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US.&lt;/li&gt;
&lt;li&gt;The dataset appears to be a multiplicative time series, since passenger numbers increase, with a pattern of seasonality.&lt;/li&gt;
&lt;li&gt;There do not appear to be any outliers and there are no missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data decomposition&lt;/h3&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The multiplicative model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of passengers at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;decomposeAP &amp;lt;- decompose(AP,&amp;quot;multiplicative&amp;quot;)
autoplot(decomposeAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the “remainder”.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stationarity test&lt;/h3&gt;
&lt;p&gt;A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. I’ll use two methods to test the stationarity.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (ADF)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to test the stationarity of the time series, let’s run the Augmented Dickey-Fuller (ADF) Test. using the adf.test function from the tseries R package.&lt;/p&gt;
&lt;p&gt;First set the hypothesis test:&lt;/p&gt;
&lt;p&gt;The null hypothesis: that the time series is non stationary
The alternative hypothesis: that the time series is stationary&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in adf.test(AP): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  AP
## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a rule of thumb, where the p-value is less than 5%, we reject the null hypothesis. As the p-value is 0.01 which is less than 0.05 we reject the null in favour of the alternative hypothesis that the time series is stationary.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (Autocorrelation)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way to test for stationarity is to use autocorrelation. I’ll use autocorrelation function (acf). This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(acf(AP, plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# review random time series for any missing values
decomposeAP$random &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Jan       Feb       Mar       Apr       May       Jun       Jul
## 1949        NA        NA        NA        NA        NA        NA 0.9516643
## 1950 0.9626030 1.0714668 1.0374474 1.0140476 0.9269030 0.9650406 0.9835566
## 1951 1.0138446 1.0640180 1.0918541 1.0176651 1.0515825 0.9460444 0.9474041
## 1952 1.0258814 1.0939696 1.0134734 0.9695596 0.9632673 1.0003735 0.9468562
## 1953 0.9976684 1.0151646 1.0604644 1.0802327 1.0413329 0.9718056 0.9551933
## 1954 0.9829785 0.9232032 1.0044417 0.9943899 1.0119479 0.9978740 1.0237753
## 1955 1.0154046 0.9888241 0.9775844 1.0015732 0.9878755 1.0039635 1.0385512
## 1956 1.0066157 0.9970250 0.9876248 0.9968224 0.9985644 1.0275560 1.0217685
## 1957 0.9937293 0.9649918 0.9881769 0.9867637 0.9924177 1.0328601 1.0261250
## 1958 0.9954212 0.9522762 0.9469115 0.9383993 0.9715785 1.0261340 1.0483841
## 1959 0.9825176 0.9505736 0.9785278 0.9746440 1.0177637 0.9968613 1.0373136
## 1960 1.0039279 0.9590794 0.8940857 1.0064948 1.0173588 1.0120790        NA
##            Aug       Sep       Oct       Nov       Dec
## 1949 0.9534014 1.0022198 1.0040278 1.0062701 1.0118119
## 1950 0.9733720 1.0225047 0.9721928 0.9389527 1.0067914
## 1951 0.9397599 0.9888637 0.9938809 1.0235337 1.0250824
## 1952 0.9931171 0.9746302 1.0046687 1.0202797 1.0115407
## 1953 0.9894989 0.9934337 1.0192680 1.0009392 0.9915039
## 1954 0.9845184 0.9881036 0.9927613 0.9995143 0.9908692
## 1955 0.9831117 1.0032501 1.0003084 0.9827720 1.0125535
## 1956 1.0004765 1.0008730 0.9835071 0.9932761 0.9894251
## 1957 1.0312668 1.0236147 1.0108432 1.0212995 1.0005263
## 1958 1.0789695 0.9856540 0.9977971 0.9802940 0.9405687
## 1959 1.0531001 0.9974447 1.0013371 1.0134608 0.9999192
## 1960        NA        NA        NA        NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138], plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers Random Component from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acf of the residuals are centered around zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-model-using-an-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit a model using an algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Linear regression Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given there is an upwards trend we’ll look at a linear model first for comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(AP) + geom_smooth(method=&amp;quot;lm&amp;quot;) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This may not be the best model to fit as it doesn’t capture the seasonality and multiplicative effects over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. ARIMA Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arimaAP &amp;lt;- auto.arima(AP)
arimaAP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: AP 
## ARIMA(2,1,1)(0,1,0)[12] 
## 
## Coefficients:
##          ar1     ar2      ma1
##       0.5960  0.2143  -0.9819
## s.e.  0.0888  0.0880   0.0292
## 
## sigma^2 estimated as 132.3:  log likelihood=-504.92
## AIC=1017.85   AICc=1018.17   BIC=1029.35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d), an autoregressive term of second lag (p) and a moving average model of order 1 (q). Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units, in this case months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggtsdiag(arimaAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residual plots appear to be centered around 0 as noise, with no pattern. The arima model is a fairly good fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;forcasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forcasting&lt;/h3&gt;
&lt;p&gt;Plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecastAP &amp;lt;- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict the diamond price based on the 4 c&#39;s</title>
      <link>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the diamonds data frame.&lt;/p&gt;
&lt;p&gt;Descriotion of data frame diamonds can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;am i getting a fair deal when I purchase a diamond?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to build a predictive model for diamonds, that is going to help figure out whether a given diamond is a &lt;strong&gt;good deal&lt;/strong&gt; or a &lt;strong&gt;rip-off!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will use Linear Regression to predict the diamond price using other varaibles in the diamonds dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get to know the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    53940 obs. of  10 variables:
##  $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...
##  $ cut    : Ord.factor w/ 5 levels &amp;quot;Fair&amp;quot;&amp;lt;&amp;quot;Good&amp;quot;&amp;lt;..: 5 4 2 4 2 3 3 3 1 3 ...
##  $ color  : Ord.factor w/ 7 levels &amp;quot;D&amp;quot;&amp;lt;&amp;quot;E&amp;quot;&amp;lt;&amp;quot;F&amp;quot;&amp;lt;&amp;quot;G&amp;quot;&amp;lt;..: 2 2 2 6 7 7 6 5 2 5 ...
##  $ clarity: Ord.factor w/ 8 levels &amp;quot;I1&amp;quot;&amp;lt;&amp;quot;SI2&amp;quot;&amp;lt;&amp;quot;SI1&amp;quot;&amp;lt;..: 2 3 5 4 2 6 7 3 4 5 ...
##  $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...
##  $ table  : num  55 61 65 58 58 57 57 55 61 61 ...
##  $ price  : int  326 326 327 334 335 336 336 337 337 338 ...
##  $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...
##  $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...
##  $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      carat               cut        color        clarity     
##  Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065  
##  1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258  
##  Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194  
##  Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171  
##  3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066  
##  Max.   :5.0100                     I: 5422   VVS1   : 3655  
##                                     J: 2808   (Other): 2531  
##      depth           table           price             x         
##  Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 0.000  
##  1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710  
##  Median :61.80   Median :57.00   Median : 2401   Median : 5.700  
##  Mean   :61.75   Mean   :57.46   Mean   : 3933   Mean   : 5.731  
##  3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540  
##  Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  
##                                                                  
##        y                z         
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 4.720   1st Qu.: 2.910  
##  Median : 5.710   Median : 3.530  
##  Mean   : 5.735   Mean   : 3.539  
##  3rd Qu.: 6.540   3rd Qu.: 4.040  
##  Max.   :58.900   Max.   :31.800  
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scatterplot&lt;/h1&gt;
&lt;p&gt;We’ll start by examining two variables in the set. A scatterplot is a powerful tool to help you understand the relationship between two continuous variables.&lt;/p&gt;
&lt;p&gt;We can quickly see if the relationship is linear or not. In this case, we can use a variety of diamond characteristics to help us figure out whether the price advertised for any given diamond is reasonable or a rip-off.&lt;/p&gt;
&lt;p&gt;Consider the price of a diamond and it’s carat weight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat 
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  xlim(0, quantile(diamonds$carat,0.99)) +
  ylim(0, quantile(diamonds$price,0.99)) +
  ggtitle(&amp;#39;Price vs. Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The larger the diamond is (or the more carats it has), the more expensive the diamond is (price), which is probably what we would have expected.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat with linear trend
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99)) ) +
  scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99)) ) +
  ggtitle(&amp;quot;Price vs. Carat&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The linear trend line doesn’t go through the center of the data at some key places. It should curve in certain parts of the graph, i.e slope up more towards the end. If we tried to use this for predictions, we might be off some key places inside and outside of the existing data that we have displayed.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## sample 10,000 diamonds from the set to get a snapshop of the large dataframe
set.seed(20022012)
diamond_samp &amp;lt;- diamonds[sample(1:length(diamonds$price), 10000), ]
ggpairs(diamond_samp, 
        lower = list(continuous = wrap(&amp;quot;points&amp;quot;, shape = I(&amp;#39;.&amp;#39;))), 
        upper = list(combo = wrap(&amp;quot;box&amp;quot;, outlier.shape = I(&amp;#39;.&amp;#39;))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price is almost linearly correlated with carat: These are the critical factors driving price.&lt;/li&gt;
&lt;li&gt;Price appears related to &lt;strong&gt;cut/color/clarity&lt;/strong&gt; but, is not very clear from this plot.&lt;/li&gt;
&lt;li&gt;Price appears not to be directly related to depth and table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create hist of price and price(log10)
plot1 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;blue&amp;#39;, fill = &amp;#39;blue&amp;#39;, binwidth = 200) +
  scale_x_continuous(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price&amp;#39;) +
  theme_classic()

plot2 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;red&amp;#39;, fill = &amp;#39;red&amp;#39;, binwidth = 0.01) +
  scale_x_log10(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price(log10)&amp;#39;) +
  theme_classic()

grid.arrange(plot1, plot2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price histogram is skewed to the right, while the log10(price) tends to be a bell curve distributed. Also, the two peaks in the log10(price) plot coincides with the 1st and 3rd quantile of price.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create scatterplot of price and price(log10)
p1 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  ggtitle(&amp;quot;Price by Carat&amp;quot;) +
  theme_classic()

p2 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  scale_y_continuous(trans = log10_trans()) +
  ggtitle(&amp;quot;Price(log10) by Carat&amp;quot;) +
  theme_classic()
grid.arrange(p1, p2, ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the log scale, the prices look less dispersed at the high end of carat size and price, however, we can do better. Let’s try using the cube root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. Remember, volume is on a cubic scale!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### create a new function to transform the carat variable
cuberoot_trans = function() trans_new(&amp;#39;cuberoot&amp;#39;,
                                      transform = function(x) x^(1/3),
                                      inverse = function(x) x^3)

### use the cuberoot_trans function
ggplot(diamonds, aes(carat, price, color=clarity)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;quot;jitter&amp;quot;) + 
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The price(log10) is almost linear with cuberoot of carat. We can now move ahead and see how to model our data using just a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;price-vs.carat-and-clarity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Clarity&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=clarity)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Clarity&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Clarity&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clarity factors into the price of a diamond. Hence, a better clarity results in a higher price than lower end clarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-cut&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Cut&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=cut)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Cut&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Cut&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whilst cut does not show as obvious pattern as clarity, it’s still clear that with the same carat the diamonds with the best cut are priced higher. Hence, I think cut should be also included in the price prediction algorithm.
Note, clarity explains a lot of the variance found in price!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-color&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Color&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=color)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Color&amp;#39;, reverse = F,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Color&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds) +
  geom_bar(mapping = aes(clarity, fill=cut), position = &amp;quot;fill&amp;quot; ) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;dodgerblue&amp;quot;, &amp;quot;purple4&amp;quot;)) +
  labs(title = &amp;quot;Clearer diamonds tend to be of higher quality cut&amp;quot;,
       subtitle = &amp;quot;The majority of IF diamonds are an \&amp;quot;Ideal\&amp;quot; cut&amp;quot;) +
  ylab(&amp;quot;proportion&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/diamonds/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This looks similar with previous clarity plot. Color should be also considered as an factor for price.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the Linear Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lm(I(log10(price)) ~ I(carat^(1/3)), diamonds)
m2 &amp;lt;- update(m1,~ . +carat)
m3 &amp;lt;- update(m2,~ . +cut)
m4 &amp;lt;- update(m3,~ . +color)
m5 &amp;lt;- update(m4,~ . +clarity)
mtable(m1, m2, m3, m4, m5, sdigits = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Calls:
## m1: lm(formula = I(log10(price)) ~ I(carat^(1/3)), data = diamonds)
## m2: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat, data = diamonds)
## m3: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut, 
##     data = diamonds)
## m4: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color, data = diamonds)
## m5: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color + clarity, data = diamonds)
## 
## ==============================================================================================
##                        m1             m2             m3             m4              m5        
## ----------------------------------------------------------------------------------------------
##   (Intercept)          1.225***       0.451***       0.380***       0.405***        0.180***  
##                       (0.003)        (0.008)        (0.008)        (0.007)         (0.004)    
##   I(carat^(1/3))       2.414***       3.721***       3.780***       3.665***        3.971***  
##                       (0.003)        (0.014)        (0.013)        (0.012)         (0.007)    
##   carat                              -0.494***      -0.505***      -0.431***       -0.474***  
##                                      (0.005)        (0.005)        (0.004)         (0.003)    
##   cut: .L                                            0.097***       0.097***        0.052***  
##                                                     (0.002)        (0.002)         (0.001)    
##   cut: .Q                                           -0.027***      -0.027***       -0.013***  
##                                                     (0.002)        (0.001)         (0.001)    
##   cut: .C                                            0.022***       0.022***        0.006***  
##                                                     (0.001)        (0.001)         (0.001)    
##   cut: ^4                                            0.008***       0.008***       -0.001     
##                                                     (0.001)        (0.001)         (0.001)    
##   color: .L                                                        -0.162***       -0.191***  
##                                                                    (0.001)         (0.001)    
##   color: .Q                                                        -0.056***       -0.040***  
##                                                                    (0.001)         (0.001)    
##   color: .C                                                         0.001          -0.006***  
##                                                                    (0.001)         (0.001)    
##   color: ^4                                                         0.012***        0.005***  
##                                                                    (0.001)         (0.001)    
##   color: ^5                                                        -0.007***       -0.001*    
##                                                                    (0.001)         (0.001)    
##   color: ^6                                                        -0.010***        0.001     
##                                                                    (0.001)         (0.001)    
##   clarity: .L                                                                       0.394***  
##                                                                                    (0.001)    
##   clarity: .Q                                                                      -0.104***  
##                                                                                    (0.001)    
##   clarity: .C                                                                       0.057***  
##                                                                                    (0.001)    
##   clarity: ^4                                                                      -0.027***  
##                                                                                    (0.001)    
##   clarity: ^5                                                                       0.011***  
##                                                                                    (0.001)    
##   clarity: ^6                                                                      -0.001     
##                                                                                    (0.001)    
##   clarity: ^7                                                                       0.014***  
##                                                                                    (0.001)    
## ----------------------------------------------------------------------------------------------
##   R-squared            0.9236         0.9349         0.9391         0.9514          0.9839    
##   adj. R-squared       0.9236         0.9349         0.9391         0.9514          0.9839    
##   sigma                0.1218         0.1124         0.1087         0.0972          0.0559    
##   F               652012.0628    387489.3661    138654.5235     87959.4667     173791.0840    
##   p                    0.0000         0.0000         0.0000         0.0000          0.0000    
##   Log-likelihood   37025.2108     41356.3916     43150.2943     49222.9505      79078.9821    
##   Deviance           800.2475       681.5220       637.6655       509.1030        168.2821    
##   AIC             -74044.4217    -82704.7832    -86284.5886    -98417.9011    -158115.9642    
##   BIC             -74017.7348    -82669.2007    -86213.4236    -98293.3623    -157929.1560    
##   N                53940          53940          53940          53940           53940         
## ==============================================================================================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We get some very nice R square values. We are accounting for almost all of the variance in price using carat, cut, color and clarity. If we want to know whether the price of a diamond is reasonable, we could use this model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thisDiamond &amp;lt;- data.frame(carat = 1, cut = &amp;#39;Very Good&amp;#39;,
                          color = &amp;#39;G&amp;#39;, clarity = &amp;#39;VS2&amp;#39;)
modelEstimate &amp;lt;- predict(m5, newdata = thisDiamond,
                         interval = &amp;quot;prediction&amp;quot;, level = .95)
10^modelEstimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 5232.111 4065.993 6732.668&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(modelEstimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 41.20984 36.93526 45.97911&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
