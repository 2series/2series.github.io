<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rihad Variawa on Rihad Variawa</title>
    <link>/</link>
    <description>Recent content in Rihad Variawa on Rihad Variawa</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Time Series Analysis</title>
      <link>/post/time_series_ap/time-series-analysis/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/time_series_ap/time-series-analysis/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the airpassengers dataframe.&lt;/p&gt;
&lt;p&gt;The AirPassenger dataset in R provides monthly totals of US airline passengers, from 1949 to 1960.&lt;/p&gt;
&lt;p&gt;Description of dataframe airpassengers can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airpassengers.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-question&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research question:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;through analysis and modelling, preview a time series forecast&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess whether a linear regression or arima model is a best fit for the time series forecast as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Exploratory data analysis&lt;/li&gt;
&lt;li&gt;Data decomposition&lt;/li&gt;
&lt;li&gt;Stationarity test&lt;/li&gt;
&lt;li&gt;Fit a model using an algorithm&lt;/li&gt;
&lt;li&gt;Forecasting&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(AirPassengers)
AP &amp;lt;- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;ts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset is already of a time series class.&lt;/p&gt;
&lt;div id=&#34;exploratory-data-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploratory data analysis&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview of data
AP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949 112 118 132 129 121 135 148 148 136 119 104 118
## 1950 115 126 141 135 125 149 170 170 158 133 114 140
## 1951 145 150 178 163 172 178 199 199 184 162 146 166
## 1952 171 180 193 181 183 218 230 242 209 191 172 194
## 1953 196 196 236 235 229 243 264 272 237 211 180 201
## 1954 204 188 235 227 234 264 302 293 259 229 203 229
## 1955 242 233 267 269 270 315 364 347 312 274 237 278
## 1956 284 277 317 313 318 374 413 405 355 306 271 306
## 1957 315 301 356 348 355 422 465 467 404 347 305 336
## 1958 340 318 362 348 363 435 491 505 404 359 310 337
## 1959 360 342 406 396 420 472 548 559 463 407 362 405
## 1960 417 391 419 461 472 535 622 606 508 461 390 432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Passenger numbers in (’000) per month for the relevant years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test for missing values
sum(is.na(AP))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Zero missing values GREAT!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test frequency
frequency(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;12 calendar months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test cycle
cycle(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
## 1949   1   2   3   4   5   6   7   8   9  10  11  12
## 1950   1   2   3   4   5   6   7   8   9  10  11  12
## 1951   1   2   3   4   5   6   7   8   9  10  11  12
## 1952   1   2   3   4   5   6   7   8   9  10  11  12
## 1953   1   2   3   4   5   6   7   8   9  10  11  12
## 1954   1   2   3   4   5   6   7   8   9  10  11  12
## 1955   1   2   3   4   5   6   7   8   9  10  11  12
## 1956   1   2   3   4   5   6   7   8   9  10  11  12
## 1957   1   2   3   4   5   6   7   8   9  10  11  12
## 1958   1   2   3   4   5   6   7   8   9  10  11  12
## 1959   1   2   3   4   5   6   7   8   9  10  11  12
## 1960   1   2   3   4   5   6   7   8   9  10  11  12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dataset summary
summary(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   104.0   180.0   265.5   280.3   360.5   622.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Statistical values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the raw data using the base plot function
autoplot(AP) + labs(x=&amp;quot;Time&amp;quot;, y =&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(AP~cycle(AP), xlab=&amp;quot;Passenger Numbers (&amp;#39;000)&amp;quot;, ylab=&amp;quot;Months&amp;quot;, col=rgb(0.1,0.9,0.3,0.4), main=&amp;quot;Monthly Air Passengers Boxplot from 1949 to 1961&amp;quot;, horizontal=TRUE, notch=FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The passenger numbers increase over time with each year which may be indicative of an increasing linear trend. Possible due to an increase in demand for flights and commercialisation of airlines in that time period.&lt;/li&gt;
&lt;li&gt;The boxplot shows more passengers travelling in months 6 to 9 with higher averages and higher variances than the other months, indicating seasonality within an apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US.&lt;/li&gt;
&lt;li&gt;The dataset appears to be a multiplicative time series, since passenger numbers increase, with a pattern of seasonality.&lt;/li&gt;
&lt;li&gt;There do not appear to be any outliers and there are no missing values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data decomposition&lt;/h3&gt;
&lt;p&gt;I’ll decompose the time series for estimates of trend, seasonal, and random components using moving average method.&lt;/p&gt;
&lt;p&gt;The multiplicative model is:&lt;/p&gt;
&lt;p&gt;Y[t]=T[t]∗S[t]∗e[t]&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;Y(t) is the number of passengers at time t,
T(t) is the trend component at time t,
S(t) is the seasonal component at time t,
e(t) is the random error component at time t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;decomposeAP &amp;lt;- decompose(AP,&amp;quot;multiplicative&amp;quot;)
autoplot(decomposeAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the “remainder”.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;stationarity-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stationarity test&lt;/h3&gt;
&lt;p&gt;A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. I’ll use two methods to test the stationarity.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (ADF)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to test the stationarity of the time series, let’s run the Augmented Dickey-Fuller (ADF) Test. using the adf.test function from the tseries R package.&lt;/p&gt;
&lt;p&gt;First set the hypothesis test:&lt;/p&gt;
&lt;p&gt;The null hypothesis: that the time series is non stationary
The alternative hypothesis: that the time series is stationary&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(AP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in adf.test(AP): p-value smaller than printed p-value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  AP
## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a rule of thumb, where the p-value is less than 5%, we reject the null hypothesis. As the p-value is 0.01 which is less than 0.05 we reject the null in favour of the alternative hypothesis that the time series is stationary.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Test stationarity of the time series (Autocorrelation)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another way to test for stationarity is to use autocorrelation. I’ll use autocorrelation function (acf). This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(acf(AP, plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# review random time series for any missing values
decomposeAP$random &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Jan       Feb       Mar       Apr       May       Jun       Jul
## 1949        NA        NA        NA        NA        NA        NA 0.9516643
## 1950 0.9626030 1.0714668 1.0374474 1.0140476 0.9269030 0.9650406 0.9835566
## 1951 1.0138446 1.0640180 1.0918541 1.0176651 1.0515825 0.9460444 0.9474041
## 1952 1.0258814 1.0939696 1.0134734 0.9695596 0.9632673 1.0003735 0.9468562
## 1953 0.9976684 1.0151646 1.0604644 1.0802327 1.0413329 0.9718056 0.9551933
## 1954 0.9829785 0.9232032 1.0044417 0.9943899 1.0119479 0.9978740 1.0237753
## 1955 1.0154046 0.9888241 0.9775844 1.0015732 0.9878755 1.0039635 1.0385512
## 1956 1.0066157 0.9970250 0.9876248 0.9968224 0.9985644 1.0275560 1.0217685
## 1957 0.9937293 0.9649918 0.9881769 0.9867637 0.9924177 1.0328601 1.0261250
## 1958 0.9954212 0.9522762 0.9469115 0.9383993 0.9715785 1.0261340 1.0483841
## 1959 0.9825176 0.9505736 0.9785278 0.9746440 1.0177637 0.9968613 1.0373136
## 1960 1.0039279 0.9590794 0.8940857 1.0064948 1.0173588 1.0120790        NA
##            Aug       Sep       Oct       Nov       Dec
## 1949 0.9534014 1.0022198 1.0040278 1.0062701 1.0118119
## 1950 0.9733720 1.0225047 0.9721928 0.9389527 1.0067914
## 1951 0.9397599 0.9888637 0.9938809 1.0235337 1.0250824
## 1952 0.9931171 0.9746302 1.0046687 1.0202797 1.0115407
## 1953 0.9894989 0.9934337 1.0192680 1.0009392 0.9915039
## 1954 0.9845184 0.9881036 0.9927613 0.9995143 0.9908692
## 1955 0.9831117 1.0032501 1.0003084 0.9827720 1.0125535
## 1956 1.0004765 1.0008730 0.9835071 0.9932761 0.9894251
## 1957 1.0312668 1.0236147 1.0108432 1.0212995 1.0005263
## 1958 1.0789695 0.9856540 0.9977971 0.9802940 0.9405687
## 1959 1.0531001 0.9974447 1.0013371 1.0134608 0.9999192
## 1960        NA        NA        NA        NA        NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138], plot=FALSE)) + labs(title=&amp;quot;Correlogram of Air Passengers Random Component from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;acf of the residuals are centered around zero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-a-model-using-an-algorithm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit a model using an algorithm&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. Linear regression Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given there is an upwards trend we’ll look at a linear model first for comparison.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(AP) + geom_smooth(method=&amp;quot;lm&amp;quot;) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;, title=&amp;quot;Air Passengers from 1949 to 1961&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This may not be the best model to fit as it doesn’t capture the seasonality and multiplicative effects over time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. ARIMA Model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;arimaAP &amp;lt;- auto.arima(AP)
arimaAP&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: AP 
## ARIMA(2,1,1)(0,1,0)[12] 
## 
## Coefficients:
##          ar1     ar2      ma1
##       0.5960  0.2143  -0.9819
## s.e.  0.0888  0.0880   0.0292
## 
## sigma^2 estimated as 132.3:  log likelihood=-504.92
## AIC=1017.85   AICc=1018.17   BIC=1029.35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d), an autoregressive term of second lag (p) and a moving average model of order 1 (q). Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units, in this case months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggtsdiag(arimaAP) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The residual plots appear to be centered around 0 as noise, with no pattern. The arima model is a fairly good fit.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;forcasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Forcasting&lt;/h3&gt;
&lt;p&gt;Plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecastAP &amp;lt;- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP) + labs(x=&amp;quot;Time&amp;quot;, y=&amp;quot;Passenger numbers (&amp;#39;000)&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/time_series_ap/2019-01-31-time-series-analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Predict the diamond price based on the 4 c&#39;s</title>
      <link>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/diamonds/predict-the-diamond-price-based-on-the-4-c-s/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the diamonds data frame.&lt;/p&gt;
&lt;p&gt;Descriotion of data frame diamonds can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/diamonds.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;am i getting a fair deal when I purchase a diamond?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The goal is to build a predictive model for diamonds, that is going to help figure out whether a given diamond is a &lt;strong&gt;good deal&lt;/strong&gt; or a &lt;strong&gt;rip-off!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will use Linear Regression to predict the diamond price using other varaibles in the diamonds dataframe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get to know the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    53940 obs. of  10 variables:
##  $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...
##  $ cut    : Ord.factor w/ 5 levels &amp;quot;Fair&amp;quot;&amp;lt;&amp;quot;Good&amp;quot;&amp;lt;..: 5 4 2 4 2 3 3 3 1 3 ...
##  $ color  : Ord.factor w/ 7 levels &amp;quot;D&amp;quot;&amp;lt;&amp;quot;E&amp;quot;&amp;lt;&amp;quot;F&amp;quot;&amp;lt;&amp;quot;G&amp;quot;&amp;lt;..: 2 2 2 6 7 7 6 5 2 5 ...
##  $ clarity: Ord.factor w/ 8 levels &amp;quot;I1&amp;quot;&amp;lt;&amp;quot;SI2&amp;quot;&amp;lt;&amp;quot;SI1&amp;quot;&amp;lt;..: 2 3 5 4 2 6 7 3 4 5 ...
##  $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...
##  $ table  : num  55 61 65 58 58 57 57 55 61 61 ...
##  $ price  : int  326 326 327 334 335 336 336 337 337 338 ...
##  $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...
##  $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...
##  $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(diamonds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      carat               cut        color        clarity     
##  Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065  
##  1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258  
##  Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194  
##  Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171  
##  3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066  
##  Max.   :5.0100                     I: 5422   VVS1   : 3655  
##                                     J: 2808   (Other): 2531  
##      depth           table           price             x         
##  Min.   :43.00   Min.   :43.00   Min.   :  326   Min.   : 0.000  
##  1st Qu.:61.00   1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710  
##  Median :61.80   Median :57.00   Median : 2401   Median : 5.700  
##  Mean   :61.75   Mean   :57.46   Mean   : 3933   Mean   : 5.731  
##  3rd Qu.:62.50   3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540  
##  Max.   :79.00   Max.   :95.00   Max.   :18823   Max.   :10.740  
##                                                                  
##        y                z         
##  Min.   : 0.000   Min.   : 0.000  
##  1st Qu.: 4.720   1st Qu.: 2.910  
##  Median : 5.710   Median : 3.530  
##  Mean   : 5.735   Mean   : 3.539  
##  3rd Qu.: 6.540   3rd Qu.: 4.040  
##  Max.   :58.900   Max.   :31.800  
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Scatterplot&lt;/h1&gt;
&lt;p&gt;We’ll start by examining two variables in the set. A scatterplot is a powerful tool to help you understand the relationship between two continuous variables.&lt;/p&gt;
&lt;p&gt;We can quickly see if the relationship is linear or not. In this case, we can use a variety of diamond characteristics to help us figure out whether the price advertised for any given diamond is reasonable or a rip-off.&lt;/p&gt;
&lt;p&gt;Consider the price of a diamond and it’s carat weight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat 
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  xlim(0, quantile(diamonds$carat,0.99)) +
  ylim(0, quantile(diamonds$price,0.99)) +
  ggtitle(&amp;#39;Price vs. Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 926 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The larger the diamond is (or the more carats it has), the more expensive the diamond is (price), which is probably what we would have expected.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create a scatterplot of price and carat with linear trend
ggplot(diamonds, aes(carat, price)) +
  geom_point(fill = I(&amp;quot;#F79420&amp;quot;), color = I(&amp;quot;black&amp;quot;), shape = 23) +
  stat_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_continuous(lim = c(0, quantile(diamonds$carat, 0.99)) ) +
  scale_y_continuous(lim = c(0, quantile(diamonds$price, 0.99)) ) +
  ggtitle(&amp;quot;Price vs. Carat&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 926 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 926 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 4 rows containing missing values (geom_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The linear trend line doesn’t go through the center of the data at some key places. It should curve in certain parts of the graph, i.e slope up more towards the end. If we tried to use this for predictions, we might be off some key places inside and outside of the existing data that we have displayed.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## sample 10,000 diamonds from the set to get a snapshop of the large dataframe
set.seed(20022012)
diamond_samp &amp;lt;- diamonds[sample(1:length(diamonds$price), 10000), ]
ggpairs(diamond_samp, 
        lower = list(continuous = wrap(&amp;quot;points&amp;quot;, shape = I(&amp;#39;.&amp;#39;))), 
        upper = list(combo = wrap(&amp;quot;box&amp;quot;, outlier.shape = I(&amp;#39;.&amp;#39;))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price is almost linearly correlated with carat: These are the critical factors driving price.&lt;/li&gt;
&lt;li&gt;Price appears related to &lt;strong&gt;cut/color/clarity&lt;/strong&gt; but, is not very clear from this plot.&lt;/li&gt;
&lt;li&gt;Price appears not to be directly related to depth and table.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create hist of price and price(log10)
plot1 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;blue&amp;#39;, fill = &amp;#39;blue&amp;#39;, binwidth = 200) +
  scale_x_continuous(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price&amp;#39;) +
  theme_classic()

plot2 &amp;lt;- ggplot(diamonds, aes(price)) +
  geom_histogram(color = &amp;#39;red&amp;#39;, fill = &amp;#39;red&amp;#39;, binwidth = 0.01) +
  scale_x_log10(breaks = seq(300, 19000, 1000), limit = c(300, 19000)) +
  ggtitle(&amp;#39;Price(log10)&amp;#39;) +
  theme_classic()

grid.arrange(plot1, plot2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_bar).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Price histogram is skewed to the right, while the log10(price) tends to be a bell curve distributed. Also, the two peaks in the log10(price) plot coincides with the 1st and 3rd quantile of price.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## create scatterplot of price and price(log10)
p1 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  ggtitle(&amp;quot;Price by Carat&amp;quot;) +
  theme_classic()

p2 &amp;lt;- ggplot(diamonds, aes(carat, price, color=clarity)) +
  geom_point() +
  scale_y_continuous(trans = log10_trans()) +
  ggtitle(&amp;quot;Price(log10) by Carat&amp;quot;) +
  theme_classic()
grid.arrange(p1, p2, ncol=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the log scale, the prices look less dispersed at the high end of carat size and price, however, we can do better. Let’s try using the cube root of carat in light of our speculation about flaws being exponentially more likely in diamonds with more volume. Remember, volume is on a cubic scale!&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### create a new function to transform the carat variable
cuberoot_trans = function() trans_new(&amp;#39;cuberoot&amp;#39;,
                                      transform = function(x) x^(1/3),
                                      inverse = function(x) x^3)

### use the cuberoot_trans function
ggplot(diamonds, aes(carat, price, color=clarity)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;quot;jitter&amp;quot;) + 
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
                     breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
                     breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1691 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The price(log10) is almost linear with cuberoot of carat. We can now move ahead and see how to model our data using just a linear model.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;price-vs.carat-and-clarity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Clarity&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=clarity)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Clarity&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Clarity&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1693 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clarity factors into the price of a diamond. Hence, a better clarity results in a higher price than lower end clarity.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-cut&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Cut&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=cut)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Cut&amp;#39;, reverse = T,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Cut&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1696 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Whilst cut does not show as obvious pattern as clarity, it’s still clear that with the same carat the diamonds with the best cut are priced higher. Hence, I think cut should be also included in the price prediction algorithm.
Note, clarity explains a lot of the variance found in price!&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;price-vs.carat-and-color&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Price vs. Carat and Color&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## to work around overplotting, the alpha, size, and and jitter options are used in our plot
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 1/2, size = 1, position = &amp;#39;jitter&amp;#39;, aes(color=color)) +
  scale_color_brewer(type = &amp;#39;div&amp;#39;,
    guide = guide_legend(title = &amp;#39;Color&amp;#39;, reverse = F,
    override.aes = list(alpha = 1, size = 2))) +                         
  scale_x_continuous(trans = cuberoot_trans(), limits = c(0.2, 3),
    breaks = c(0.2, 0.5, 1, 2, 3)) + 
  scale_y_continuous(trans = log10_trans(), limits = c(350, 15000),
    breaks = c(350, 1000, 5000, 10000, 15000)) +
  ggtitle(&amp;#39;Price(log10) by Cube-Root of Carat and Color&amp;#39;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1688 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diamonds) +
  geom_bar(mapping = aes(clarity, fill=cut), position = &amp;quot;fill&amp;quot; ) +
  scale_fill_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;dodgerblue&amp;quot;, &amp;quot;purple4&amp;quot;)) +
  labs(title = &amp;quot;Clearer diamonds tend to be of higher quality cut&amp;quot;,
       subtitle = &amp;quot;The majority of IF diamonds are an \&amp;quot;Ideal\&amp;quot; cut&amp;quot;) +
  ylab(&amp;quot;proportion&amp;quot;) +
  theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/2019-01-25-predict-the-diamond-price-based-on-the-4-c-s_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This looks similar with previous clarity plot. Color should be also considered as an factor for price.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the Linear Model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m1 &amp;lt;- lm(I(log10(price)) ~ I(carat^(1/3)), diamonds)
m2 &amp;lt;- update(m1,~ . +carat)
m3 &amp;lt;- update(m2,~ . +cut)
m4 &amp;lt;- update(m3,~ . +color)
m5 &amp;lt;- update(m4,~ . +clarity)
mtable(m1, m2, m3, m4, m5, sdigits = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Calls:
## m1: lm(formula = I(log10(price)) ~ I(carat^(1/3)), data = diamonds)
## m2: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat, data = diamonds)
## m3: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut, 
##     data = diamonds)
## m4: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color, data = diamonds)
## m5: lm(formula = I(log10(price)) ~ I(carat^(1/3)) + carat + cut + 
##     color + clarity, data = diamonds)
## 
## ==============================================================================================
##                        m1             m2             m3             m4              m5        
## ----------------------------------------------------------------------------------------------
##   (Intercept)          1.225***       0.451***       0.380***       0.405***        0.180***  
##                       (0.003)        (0.008)        (0.008)        (0.007)         (0.004)    
##   I(carat^(1/3))       2.414***       3.721***       3.780***       3.665***        3.971***  
##                       (0.003)        (0.014)        (0.013)        (0.012)         (0.007)    
##   carat                              -0.494***      -0.505***      -0.431***       -0.474***  
##                                      (0.005)        (0.005)        (0.004)         (0.003)    
##   cut: .L                                            0.097***       0.097***        0.052***  
##                                                     (0.002)        (0.002)         (0.001)    
##   cut: .Q                                           -0.027***      -0.027***       -0.013***  
##                                                     (0.002)        (0.001)         (0.001)    
##   cut: .C                                            0.022***       0.022***        0.006***  
##                                                     (0.001)        (0.001)         (0.001)    
##   cut: ^4                                            0.008***       0.008***       -0.001     
##                                                     (0.001)        (0.001)         (0.001)    
##   color: .L                                                        -0.162***       -0.191***  
##                                                                    (0.001)         (0.001)    
##   color: .Q                                                        -0.056***       -0.040***  
##                                                                    (0.001)         (0.001)    
##   color: .C                                                         0.001          -0.006***  
##                                                                    (0.001)         (0.001)    
##   color: ^4                                                         0.012***        0.005***  
##                                                                    (0.001)         (0.001)    
##   color: ^5                                                        -0.007***       -0.001*    
##                                                                    (0.001)         (0.001)    
##   color: ^6                                                        -0.010***        0.001     
##                                                                    (0.001)         (0.001)    
##   clarity: .L                                                                       0.394***  
##                                                                                    (0.001)    
##   clarity: .Q                                                                      -0.104***  
##                                                                                    (0.001)    
##   clarity: .C                                                                       0.057***  
##                                                                                    (0.001)    
##   clarity: ^4                                                                      -0.027***  
##                                                                                    (0.001)    
##   clarity: ^5                                                                       0.011***  
##                                                                                    (0.001)    
##   clarity: ^6                                                                      -0.001     
##                                                                                    (0.001)    
##   clarity: ^7                                                                       0.014***  
##                                                                                    (0.001)    
## ----------------------------------------------------------------------------------------------
##   R-squared            0.9236         0.9349         0.9391         0.9514          0.9839    
##   adj. R-squared       0.9236         0.9349         0.9391         0.9514          0.9839    
##   sigma                0.1218         0.1124         0.1087         0.0972          0.0559    
##   F               652012.0628    387489.3661    138654.5235     87959.4667     173791.0840    
##   p                    0.0000         0.0000         0.0000         0.0000          0.0000    
##   Log-likelihood   37025.2108     41356.3916     43150.2943     49222.9505      79078.9821    
##   Deviance           800.2475       681.5220       637.6655       509.1030        168.2821    
##   AIC             -74044.4217    -82704.7832    -86284.5886    -98417.9011    -158115.9642    
##   BIC             -74017.7348    -82669.2007    -86213.4236    -98293.3623    -157929.1560    
##   N                53940          53940          53940          53940           53940         
## ==============================================================================================&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We get some very nice R square values. We are accounting for almost all of the variance in price using carat, cut, color and clarity. If we want to know whether the price of a diamond is reasonable, we could use this model.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;thisDiamond &amp;lt;- data.frame(carat = 1, cut = &amp;#39;Very Good&amp;#39;,
                          color = &amp;#39;G&amp;#39;, clarity = &amp;#39;VS2&amp;#39;)
modelEstimate &amp;lt;- predict(m5, newdata = thisDiamond,
                         interval = &amp;quot;prediction&amp;quot;, level = .95)
10^modelEstimate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 5232.111 4065.993 6732.668&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(modelEstimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        fit      lwr      upr
## 1 41.20984 36.93526 45.97911&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Tidying Project</title>
      <link>/project/data_tidying/data-tidying-project/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/data_tidying/data-tidying-project/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The data hosted at [data.world] and contains information about Sales in the US.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Get Data&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://data.world/retail/department-store-sales&#34;&gt;Data Set 1&lt;/a&gt;: Sales from the Retail Trade and Food Services Report from the US Census. This dataset only covers Department Stores, though the report covers a wide range of retail types. [1992-2016]&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://data.world/garyhoov/retail-sales-growth&#34;&gt;Data Set 2&lt;/a&gt; US Retail Sales by Store Type with Growth Rate [2009-2014]&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1992-2016
#https://data.world/retail/department-store-sales

GET(&amp;quot;https://query.data.world/s/gdk7iwtlisq6vkktmybqqr7hjjty5s&amp;quot;, write_disk(tf &amp;lt;- tempfile(fileext = &amp;quot;.xls&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://download.data.world/file_download/retail/department-store-sales/retail-trade-report-department-stores.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNDIzLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI0YWU5NmEzZjc4Y2EyOGE5MWM1ZDZlMTgxYzg5YjI0NjIzZDY0ZThlIn0.jBFlyy1aloE-EpeYyosD3iRaDPoY75DBqdh7suLoZxKcrsG8N5GtOiFb6sNMjTqqclsHX7P8RUw7T5sAArPbcw]
##   Date: 2019-01-26 21:57
##   Status: 200
##   Content-Type: application/vnd.ms-excel
##   Size: 62.5 kB
## &amp;lt;ON DISK&amp;gt;  /tmp/RtmpbDizpx/file425bddb3fff.xls&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- read_excel(tf)

#2009-2014
# https://data.world/garyhoov/retail-sales-growth
GET(&amp;quot;https://query.data.world/s/py7kinxvyuxjpzwdjs2ti4wdmui6bi&amp;quot;, write_disk(tf &amp;lt;- tempfile(fileext = &amp;quot;.xls&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://download.data.world/file_download/garyhoov/retail-sales-growth/US%20Retail%20Sales%20by%20Store%20Type%202009-2014.xls?auth=eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwcm9kLXVzZXItY2xpZW50OnNoYW5lbGxpcyIsImlzcyI6ImFnZW50OnNoYW5lbGxpczo6OTA5ZDZlNTQtMmQwZC00MDczLWE4Y2UtYWExNzI3OGJkN2ViIiwiaWF0IjoxNTIzOTkwNTAwLCJyb2xlIjpbInVzZXIiLCJ1c2VyX2FwaV9hZG1pbiIsInVzZXJfYXBpX3JlYWQiLCJ1c2VyX2FwaV93cml0ZSJdLCJnZW5lcmFsLXB1cnBvc2UiOmZhbHNlLCJ1cmwiOiI5OWRhMDIwMzRlY2Q1YmZmZTRmODFjYzJlMTg4ZmUxOGQyZmEyNDdlIn0.NLTr571lKSZMKhmvIFFQGuoVeFVFr9DrQ7nxBO3LOcLTJUrKivBxWpUrcJcY8dxnkL4FlGba3wsL65c3wLzzxA]
##   Date: 2019-01-26 21:57
##   Status: 200
##   Content-Type: application/vnd.ms-excel
##   Size: 169 kB
## &amp;lt;ON DISK&amp;gt;  /tmp/RtmpbDizpx/file425b7afcc0a6.xls&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- read_excel(tf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * `` -&amp;gt; `..2`
## * `` -&amp;gt; `..3`
## * `` -&amp;gt; `..4`
## * `` -&amp;gt; `..5`
## * `` -&amp;gt; `..6`
## * … and 24 more&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## the the first row and make that the column names of the data frame
colnames(df2) &amp;lt;- df2[1,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;save-raw-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Save Raw Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## use saveRDS() to save each object as a .rds file 
saveRDS(df1, file = &amp;#39;df_department.rds&amp;#39;)
saveRDS(df2, file = &amp;#39;df_retail.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;wrangle-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wrangle Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## work with df2
df_retail &amp;lt;- df2 %&amp;gt;%
  ## remove the r from the column names of df2
  magrittr::set_colnames(gsub(&amp;quot;r&amp;quot;,&amp;quot;&amp;quot;,df2[1,])) %&amp;gt;% 
  ## add a new column called &amp;quot;business&amp;quot;
  mutate(business = gsub(&amp;quot;[…]|[.]&amp;quot;,&amp;quot;&amp;quot;,`Kind of business`)) %&amp;gt;%
  ## filter to include Retail sales or Department stores sales
  filter(grepl(&amp;#39;Retail sales, total |Department stores&amp;#39;, business)) %&amp;gt;%
  ## only look at columns with year information in them
  select(.,c(matches(&amp;#39;19|20&amp;#39;),business)) %&amp;gt;%
  ## take year column and collapse them into a single column
  gather(., &amp;quot;year&amp;quot;, &amp;quot;n&amp;quot;, 1:(ncol(.)-1)) %&amp;gt;%
  ## make sure the count column `n` is numeric
  mutate(n=as.numeric(n)) %&amp;gt;%
  ## filter to only include the businesses we&amp;#39;re interested in
  filter(business == &amp;quot;Retail sales, total &amp;quot;| business==&amp;quot;Department stores &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## work with df1
df_department &amp;lt;- df1 %&amp;gt;% 
  ## split Period column into one column called &amp;quot;month&amp;quot; and one called &amp;quot;year&amp;quot;
  separate(Period, into = c(&amp;#39;month&amp;#39;, &amp;#39;year&amp;#39;), extra = &amp;#39;drop&amp;#39;, remove = FALSE) %&amp;gt;%
  ## add a column `value` which contains the 
  ## information from the `Value (in millions)` 
  mutate(value = `Value (in millions)`) %&amp;gt;%
  ## group the data frame by the `year` column
  group_by(year) %&amp;gt;%
  ## Summarize the data by creating a new column
  ## call this column `n` 
  ## have it contain the sum of the `value` column
  summarize(n = sum(value)) %&amp;gt;% 
  ### create a new column called `business`
  ## set the value of this column to  be &amp;quot;department stores&amp;quot; 
  ## for the entire data set 
  mutate(business = &amp;#39;department stores&amp;#39;) %&amp;gt;%
  ## reorder column names to be : business, year, n
  select(business, year, n)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;merging-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Merging Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Now, combine the two data frames
df_total &amp;lt;- left_join(df_retail, df_department, by = c(&amp;#39;business&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;n&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plotting Data&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Retail Sales data
ggplot(df_retail, aes(x=year,y=n,colour=business)) +
  geom_point() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Department Sales data
ggplot(df_department, aes(x=year,y=n)) +
  geom_point() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-2.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Plot Combined Data
ggplot(df_total, aes(x=year,y=as.numeric(n), colour=business)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./project/data_tidying/2019-01-14-data-tidying-project_files/figure-html/plot-3.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning using linear regression to predict house prices</title>
      <link>/project/machine_learning_using_lr/machine-learning-using-linear-regression-to-predict-house-prices/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/machine_learning_using_lr/machine-learning-using-linear-regression-to-predict-house-prices/</guid>
      <description>


&lt;div id=&#34;intro-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro &amp;amp; Background&lt;/h1&gt;
&lt;p&gt;Linear regression being one of the most basic and popular algorithms in machine learning, so when aspiring data scientist starts off in this field, linear regression is inevitably the first algorithm they come across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis Approach&lt;/h1&gt;
&lt;p&gt;You can click on the link below to see the working code in Python and reproduce it to test the strength of your own password.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/2series/100_Days_of_ML_Code/blob/master/Linear_Regression_from_Scratch.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/2series/100_Days_of_ML_Code/blob/master/Linear_Regression_from_Scratch.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Build A Simple Blockchain</title>
      <link>/project/simple_blockchain-project/build-a-simple-blockchain/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/simple_blockchain-project/build-a-simple-blockchain/</guid>
      <description>


&lt;div id=&#34;intro-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro &amp;amp; Background&lt;/h1&gt;
&lt;p&gt;Blockchain is a data structure that was first introduced by Satoshi Nakamoto in the Bitcoin protocol white paper a decade ago. Bitcoin’s blockchain stores transaction data, but we can store any type of data in a blockchain.&lt;/p&gt;
&lt;p&gt;Ethereum, for example, enables users to store code snippets called ‘smart contracts’ in their blockchain. In this project, I build a simple blockchain in Python that uses proof-of-work consensus, just like the Bitcoin protocol does. There’s a lot of misunderstanding around what the blockchain is and what it can do, so I hope this project demonstrates how simple it really is. Enjoy!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis Approach&lt;/h1&gt;
&lt;p&gt;You can click on the link below to see the working code in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/2series/100_Days_of_ML_Code/blob/master/Simple_Blockchain.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/2series/100_Days_of_ML_Code/blob/master/Simple_Blockchain.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Password Detection Strength</title>
      <link>/project/password_detection_strength/password-detection-strength/</link>
      <pubDate>Sun, 06 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/project/password_detection_strength/password-detection-strength/</guid>
      <description>


&lt;div id=&#34;intro-background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intro &amp;amp; Background&lt;/h1&gt;
&lt;p&gt;A function written that uses regular expressions to make sure the password string it is passed is strong. A strong password is defined as one that is at least eight characters long, contains both uppercase and lowercase characters, and has at least one digit. You may need to test the string against multiple regex patterns to validate its strength.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-approach&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Analysis Approach&lt;/h1&gt;
&lt;p&gt;You can click on the link below to see the working code in Python and reproduce it to test the strength of your own password.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/2series/100_Days_of_ML_Code/blob/master/Password_Detection_Strength.ipynb&#34; class=&#34;uri&#34;&gt;https://github.com/2series/100_Days_of_ML_Code/blob/master/Password_Detection_Strength.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>mtcars Data Analysis</title>
      <link>/post/mtcars/2019-01-03-r-rmarkdown/</link>
      <pubDate>Thu, 03 Jan 2019 21:13:14 -0500</pubDate>
      
      <guid>/post/mtcars/2019-01-03-r-rmarkdown/</guid>
      <description>


&lt;div id=&#34;preamble&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preamble:&lt;/h2&gt;
&lt;p&gt;This document focuses on the analysis of the mtcars dataframe.&lt;/p&gt;
&lt;p&gt;Description of dataframe mtcars can be found at &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#34; class=&#34;uri&#34;&gt;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;research-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research questions:&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;is a vehicle with auto or manual transmission better in terms of miles p/gallons(mpg)?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;quantify the (mpg) difference between auto &amp;amp; manual transmission.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of analysis:&lt;/h2&gt;
&lt;p&gt;I will asssess both queries from different perspectives employing a set of methodologies that can be broadly grouped as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Univariate Analysis on target varibale (mpg).&lt;/li&gt;
&lt;li&gt;Bivariate Analysis on target varibale &amp;amp; relevant covariates.&lt;/li&gt;
&lt;li&gt;Multivariate Analysis by estimating a set of regresssion models for the conditional mean of mpg. For model selection, I compare the best fit and forward stepwise selection process.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;univariate-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Univariate Analysis&lt;/h1&gt;
&lt;p&gt;Analysing the target variable alone by splitting the observations into two groups, i.e. vehicles with auto or manual transmission. I shall deploy 3 analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute sample means by group ie auto VS manual.&lt;/li&gt;
&lt;li&gt;Validate if the difference of the group means are statistically significant by computing a 95% confidence interval for means’ difference.&lt;/li&gt;
&lt;li&gt;Verify the robustness of this result by executing a permutation test with Monte Carlo trials that shuffle the allocation group &amp;gt; mpg.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;get-to-know-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get to know the data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)
## &amp;#39;data.frame&amp;#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We notice that the set is relatively small! We’ll look at the desriptive statistics for each field - (min, 1st Q, Median, Mean, 3rd Q, max)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mtcars)
##       mpg             cyl             disp             hp       
##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  
##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  
##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  
##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  
##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  
##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  
##       drat             wt             qsec             vs        
##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  
##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  
##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  
##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  
##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  
##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  
##        am              gear            carb      
##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  
##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  
##  Median :0.0000   Median :4.000   Median :2.000  
##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  
##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  
##  Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-means-by-group&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample means by group&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

# dimensions
dim(mtcars)
## [1] 32 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(cars_auto); dim(cars_manu)
## [1] 19 11
## [1] 13 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sample means mpg by group
mean(cars_auto$mpg); mean(cars_manu$mpg)
## [1] 17.14737
## [1] 24.39231&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(cars_auto$mpg); sd(cars_manu$mpg)
## [1] 3.833966
## [1] 6.166504&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# % increase in mpg based on the sample mean
(mean(cars_manu$mpg) - mean(cars_auto$mpg))/mean(cars_auto$mpg)
## [1] 0.4225103&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including plots&lt;/h1&gt;
&lt;p&gt;To get a feel for the distribution of some of the data to be analyzed, we plot some histograms, the first against mpg - auto transmission, the second against mpg - manual transission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boxplot(mpg ~ am, data = mtcars, col=rgb(0.3,0.2,0.5,0.6), ylab = &amp;quot;mpg&amp;quot;, xlab = &amp;quot;am&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg empirical mean of vehicles with manual transmission is greater than cars with auto transmission, however this also has a higher variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;confidence-interval-for-the-difference-of-the-group-means&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;95% confidence interval for the difference of the group means&lt;/h2&gt;
&lt;p&gt;The analysis on sample means concludes that sample mean of mpg for vehicles with manual trasmission is greater than automatic:&lt;/p&gt;
&lt;p&gt;Now I test if this difference (i.e. in the sample means) is statistically significant (from zero).&lt;/p&gt;
&lt;p&gt;I execute a t.test for unpaired samples: I assume inequality in variances for the two groups for the computation of the pooled variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 95% confidence interval for mean difference ####

# Question: is the sample mean difference significant?
t.test(cars_manu$mpg, cars_auto$mpg, paired = F, var.equal = F)
## 
##  Welch Two Sample t-test
## 
## data:  cars_manu$mpg and cars_auto$mpg
## t = 3.7671, df = 18.332, p-value = 0.001374
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   3.209684 11.280194
## sample estimates:
## mean of x mean of y 
##  24.39231  17.14737&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;95% interval does not contain 0&lt;/li&gt;
&lt;li&gt;sample mean difference is significant at 95% (p-value 0.1%)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;permutation-test-on-groups-association&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Permutation test on groups association&lt;/h2&gt;
&lt;p&gt;I test the robustness of results obtained in the previous step.&lt;/p&gt;
&lt;p&gt;I execute a permutation test by shuffling the allocation mean &amp;gt; groups with 100,000 trials of Montecarlo simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### Permutation test ####
# what if I shuffle the am groups and calculate the mean?

# get target variable and group vectors
y = mtcars$mpg
group = mtcars$am
y; group
##  [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2
## [15] 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4
## [29] 15.8 19.7 15.0 21.4
##  [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1

# baseline group means and difference
baselineMeans = tapply(mtcars$mpg, mtcars$am, mean)
baselineMeansDiff = baselineMeans[2] - baselineMeans[1]

tStat = function(w, g) mean(w[g == 1]) - mean(w[g == 0])
observedDiff = tStat(y, group)

# check if function works - should be 0:
baselineMeansDiff - observedDiff
## 1 
## 0

# execute shuffle:
permutations = sapply(1:100000, function(i) tStat(y, sample(group)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis:&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# shuffle experiment results plots:
par(mfrow = c(2, 1), mar = c(4, 4, 2, 2))
hist(permutations, main = &amp;quot;Distribution of shuffled group mean differences&amp;quot;) # distribution of difference of averages of permuted groups
plot(permutations, type = &amp;quot;b&amp;quot;, main = &amp;quot;Shuffled group mean trials&amp;quot;, xlab = &amp;quot;trial&amp;quot;, ylab = &amp;quot;shuffled group mean differences&amp;quot;, ylim = c(-14, 14))
abline(h = observedDiff, col = &amp;quot;red&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there is not even 1 case where by chance I get a difference greater than the observed!
mean(permutations &amp;gt; observedDiff)
## [1] 1e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out of 100,000 trails only 0.002% has breached the observed value for the diffs in the group empirical means.&lt;/li&gt;
&lt;li&gt;concluding that empirical means diffs of groups is robust with regards to random reshuffling and is not likely to be generated by pure chance. &lt;em&gt;is this correct?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bivariate Analysis&lt;/h2&gt;
&lt;p&gt;Analyse the behaviour of target variable (mpg) conditional upon a set of explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### generate subset: automatic and manual cars ####
cars_auto = subset(mtcars, am == 0)
cars_manu = subset(mtcars, am == 1)

#### Visual inspection of all covariates ####
pairs(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### 4 bivariate analysis: hp / wt / drat / disp ####
par(mfrow = c(2, 2), mar = c(2, 3, 2, 3))

# plot1
with(mtcars, plot(hp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs hp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(hp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(hp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model1_auto = lm(mpg ~ hp, data = cars_auto)
model1_manu = lm(mpg ~ hp, data = cars_manu)
abline(model1_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model1_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot2
with(mtcars, plot(wt, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs weight - by transmission type&amp;quot;)) # no data
with(cars_auto, points(wt, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(wt, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
abline(v = 3.2, lty = 2)

# plot 3
with(mtcars, plot(drat, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs drat - by transmission type&amp;quot;)) # no data
with(cars_auto, points(drat, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(drat, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
model2_auto = lm(mpg ~ drat, data = cars_auto)
model2_manu = lm(mpg ~ drat, data = cars_manu)
abline(model2_auto, col = &amp;quot;red&amp;quot;, lwd = 2)
abline(model2_manu, col = &amp;quot;blue&amp;quot;, lwd = 2)
abline(v = 175, lty = 2)

# plot 4
with(mtcars, plot(disp, mpg, type = &amp;quot;n&amp;quot;, main = &amp;quot;mpg vs disp - by transmission type&amp;quot;)) # no data
with(cars_auto, points(disp, mpg, col = &amp;quot;red&amp;quot;, pch = 20))
with(cars_manu, points(disp, mpg, col = &amp;quot;blue&amp;quot;, pch = 20))
legend(&amp;quot;topright&amp;quot;, pch = 20, col = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), legend = c(&amp;quot;auto&amp;quot;, &amp;quot;manu&amp;quot;)) # add legend
labels = with(mtcars, paste(as.character(disp), as.character(mpg), sep = &amp;quot;,&amp;quot;)) # generate point labels
with(mtcars, text(disp, mpg, labels = labels, cex = 0.7, pos = 2))
abline(v = 167.6, lty = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mpg vs hp: linear negative relation: as horse power of the engine (hp) increases, the mileage (mpg) reduces. Vehicles with manual transmission seems however to be more efficient: the group restricted regression (blue) has a higher intercept. It has to be highlighted however, that the parameters of blue regression might be influenced by two extreme values with high hp - the regression should be re-estimated by removing the two datapoints.&lt;/li&gt;
&lt;li&gt;mpg vs weight: negative relation, the functional form might be non-linear (hyperbolic ?), as weight of the vehicle increases, the mileage decreases. The weight variable seems to provide perfect separation between manual and auto transmission vehilces, i.e. all vehicles that are heavier than 3.2 ton (circa) are auto and vice-versa.&lt;/li&gt;
&lt;li&gt;mpg vs drat: the functional form is not clear: it appears also to be an increase in the variance as the rear axel ratio (drat) increases. To verify this a regression model using all observations has to be estimated and analyse the residuals for verifying if the model is heteroskedastic.&lt;/li&gt;
&lt;li&gt;mpg vs disp: seems to have a negative (hyperbolic ?) relation: as the displacement (disp) of the engine increases, the mileage decreases. Also, in this case it seems that disp accounts for perfect separation in the transmission type: almost all vehilces with disp &amp;gt; 180 are auto.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multivariate analysis&lt;/h2&gt;
&lt;p&gt;Run a set of regression models for estimating the impact of some predictions on mpg.&lt;/p&gt;
&lt;p&gt;For model selection, I employ the following techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manual selection of regressors: I hand pick regressors for:&lt;/li&gt;
&lt;li&gt;Best fit procedure&lt;/li&gt;
&lt;li&gt;Forward stepwise procedure&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;manual-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual selection&lt;/h2&gt;
&lt;p&gt;Analysis of covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### analyse covariance matrix for regressor selection:
z &amp;lt;- cor(mtcars)
require(lattice)
## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levelplot(z)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A model with only transmission:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# only am
data = mtcars
data$am = as.factor(data$am)
model2 = lm(mpg ~ am, data = data)

# get results
summary(model2)
## 
## Call:
## lm(formula = mpg ~ am, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3923 -3.0923 -0.2974  3.2439  9.5077 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   17.147      1.125  15.247 1.13e-15 ***
## am1            7.245      1.764   4.106 0.000285 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.902 on 30 degrees of freedom
## Multiple R-squared:  0.3598, Adjusted R-squared:  0.3385 
## F-statistic: 16.86 on 1 and 30 DF,  p-value: 0.000285&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the intercept is 17.15: exactly the same mean of mpg for vehicles with auto transmission.&lt;/li&gt;
&lt;li&gt;the coefficient of am is 7.24: exactly the difference of mpg means for vehicles with manual / auto transmission.&lt;/li&gt;
&lt;li&gt;the sum of intercept and am coefficient gives the mpg unconditional mean for vehicles with manual transmission.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;best-fit-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Best Fit Procedure&lt;/h2&gt;
&lt;p&gt;Run the best fit procedure for identifying the optimal number of regressors that minimises the cp, which is (…)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### model selection using leaps ####
data = mtcars
data$log_mpg = log(data$mpg) # add log of y

#### method 1. best fit ####
regfit.full = regsubsets(log_mpg ~. , data = data, nvmax = 10)
reg.summary = summary(regfit.full)
reg.summary
## Subset selection object
## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10)
## 11 Variables  (and intercept)
##      Forced in Forced out
## mpg      FALSE      FALSE
## cyl      FALSE      FALSE
## disp     FALSE      FALSE
## hp       FALSE      FALSE
## drat     FALSE      FALSE
## wt       FALSE      FALSE
## qsec     FALSE      FALSE
## vs       FALSE      FALSE
## am       FALSE      FALSE
## gear     FALSE      FALSE
## carb     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           mpg cyl disp hp  drat wt  qsec vs  am  gear carb
## 1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# how I selected the optimal number of variables?
plot(reg.summary$cp, xlab = &amp;quot;Number of variables&amp;quot;, ylab = &amp;quot;cp&amp;quot;, type = &amp;quot;b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;forward-stepwise-procedure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Forward Stepwise Procedure&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regfit.fwd = regsubsets(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
summary(regfit.fwd)
## Subset selection object
## Call: regsubsets.formula(log_mpg ~ ., data = data, nvmax = 10, method = &amp;quot;forward&amp;quot;)
## 11 Variables  (and intercept)
##      Forced in Forced out
## mpg      FALSE      FALSE
## cyl      FALSE      FALSE
## disp     FALSE      FALSE
## hp       FALSE      FALSE
## drat     FALSE      FALSE
## wt       FALSE      FALSE
## qsec     FALSE      FALSE
## vs       FALSE      FALSE
## am       FALSE      FALSE
## gear     FALSE      FALSE
## carb     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: forward
##           mpg cyl disp hp  drat wt  qsec vs  am  gear carb
## 1  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 2  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 3  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 4  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 5  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot; &amp;quot; 
## 6  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; 
## 7  ( 1 )  &amp;quot;*&amp;quot; &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 8  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 9  ( 1 )  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot; &amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; 
## 10  ( 1 ) &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot; &amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot; &amp;quot;*&amp;quot; &amp;quot;*&amp;quot;  &amp;quot;*&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(regfit.fwd, scale = &amp;quot;Cp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Appendix&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A model including all regressors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#### lm with all variables / no split ####
# prepare data
data = mtcars
data$am = as.factor(data$am)

model1 = lm(mpg ~ ., data = data)

# get results
summary(model1)
## 
## Call:
## lm(formula = mpg ~ ., data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4506 -1.6044 -0.1196  1.2193  4.6271 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept) 12.30337   18.71788   0.657   0.5181  
## cyl         -0.11144    1.04502  -0.107   0.9161  
## disp         0.01334    0.01786   0.747   0.4635  
## hp          -0.02148    0.02177  -0.987   0.3350  
## drat         0.78711    1.63537   0.481   0.6353  
## wt          -3.71530    1.89441  -1.961   0.0633 .
## qsec         0.82104    0.73084   1.123   0.2739  
## vs           0.31776    2.10451   0.151   0.8814  
## am1          2.52023    2.05665   1.225   0.2340  
## gear         0.65541    1.49326   0.439   0.6652  
## carb        -0.19942    0.82875  -0.241   0.8122  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.65 on 21 degrees of freedom
## Multiple R-squared:  0.869,  Adjusted R-squared:  0.8066 
## F-statistic: 13.93 on 10 and 21 DF,  p-value: 3.793e-07&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-analysis-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot residual analysis
par(mfrow = c(2, 2))
plot(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot hist
par(mfrow = c(1, 1))
hist(model1$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./post/mtcars/2019-01-03-r-rmarkdown_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# normality test on residuals
shapiro.test(model1$residuals)
## 
##  Shapiro-Wilk normality test
## 
## data:  model1$residuals
## W = 0.95694, p-value = 0.2261&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Renewable Energy: The New Sharing Economy</title>
      <link>/publication/whitepaper/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/whitepaper/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/example-slides/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
